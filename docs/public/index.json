[{"authors":["Ben"],"categories":null,"content":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron. Prior to coming to Michigan State University, I worked with the U.S. Army to help soldiers become more psychologically resilient.\nI hold a Ph.D. in Social Science (Economics) from the California Institute of Technology (Caltech), and a B.S. in Economics from the University of Oregon.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"bf008f22d9b0754cde4f6972811c28b7","permalink":"/authors/ben/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ben/","section":"authors","summary":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron.","tags":null,"title":"Ben Bushong","type":"authors"},{"authors":null,"categories":null,"content":"  I have included a bunch of extra resources and guides related to graphic design, visualization, R, data, and other relevant topics. Enjoy!\n","date":1588723200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1588723200,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"I have included a bunch of extra resources and guides related to graphic design, visualization, R, data, and other relevant topics. Enjoy!","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":"   Reflections Exercises Mini projects Final project   You will get the most of out this class if you:\nEngage with the readings and lecture materials Regularly use R  Each type of assignment in this class helps with one of these strategies.\nReflections To encourage engagement with the course content, you’ll need to write a ≈150 word reflection about the readings and lectures for the day. That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).\nYou can do a lot of different things with this memo: discuss something you learned from the course content, write about the best or worst data visualization you saw recently, connect the course content to your own work, etc. These reflections let you explore and answer some of the key questions of this course, including:\n What is truth? How is truth related to visualization? Why do we visualize data? What makes a great visualization? What makes a bad visualization? How do you choose which kind of visualization to use? What is the role of stories in presenting analysis?  The course content for each day will also include a set of questions specific to that topic. You do not have to answer all (or any) of these questions. That would be impossible. They exist to guide your thinking, that’s all.\nI will grade these memos using a check system:\n ✔+: (11.5 points (115%) in gradebook) Reflection shows phenomenal thought and engagement with the course content. I will not assign these often. ✔: (10 points (100%) in gradebook) Reflection is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Reflection is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.  Notice that is essentially a pass/fail or completion-based system. I’m not grading your writing ability, I’m not counting the exact number of words you’re writing, and I’m not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. I’m looking for thoughtful engagement, that’s all. Do good work and you’ll get a ✓.\nYou will turn these reflections in via iCollege. You will write them using R Markdown and they will be the first section of your daily exercises (see below).\n Exercises Each class session has interactive lessons and fully annotated examples of code that teach and demonstrate how to do specific tasks in R. However, without practicing these principles and making graphics on your own, you won’t remember what you learn!\nTo practice working with ggplot2 and making data-based graphics, you will complete a brief set of exercises for each class session. These exercises will have 1–3 short tasks that are directly related to the topic for the day. You need to show that you made a good faith effort to work each question. The problem sets will also be graded using a check system:\n ✔+: (11.5 points (115%) in gradebook) Exercises are 100% completed. Every task was attempted and answered, and most answers are correct. Knitted document is clean and easy to follow. Work is exceptional. I will not assign these often. ✔: (10 points (100%) in gradebook) Exercises are 70–99% complete and most answers are correct. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often.  Note that this is also essentially a pass/fail system. I’m not grading your coding ability, I’m not checking each line of code to make sure it produces some exact final figure, and I’m not looking for perfect. Also note that a ✓ does not require 100% completion—you will sometimes get stuck with weird errors that you can’t solve, or the demands of pandemic living might occasionally become overwhelming. I’m looking for good faith effort, that’s all. Try hard, do good work, and you’ll get a ✓.\nYou may (and should!) work together on the exercises, but you must turn in your own answers.\nYou will turn these exercises in using iCollege. You will include your reflection in the first part of the document—the rest will be your exercise tasks.\n Mini projects To give you practice with the data and design principles you’ll learn in this class, you will complete two mini projects. I will provide you with real-world data and pose one or more questions—you will make a pretty picture to answer those questions.\nThe mini projects will be graded using a check system:\n ✔+: (85 points (≈115%) in gradebook) Project is phenomenally well-designed and uses advanced R techniques. The project uncovers an important story that is not readily apparent from just looking at the raw data. I will not assign these often. ✔: (75 points (100%) in gradebook) Project is fine, follows most design principles, answers a question from the data, and uses R correctly. This is the expected level of performance. ✔−: (37.5 points (50%) in gradebook) Project is missing large components, is poorly designed, does not answer a relevant question, and/or uses R incorrectly. This indicates that you need to improve next time. I will hopefully not assign these often.  Because these mini projects give you practice for the final project, I will provide you with substantial feedback on your design and code.\n Final project At the end of the course, you will demonstrate your data visualization skills by completing a final project.\nComplete details for the final project (including past examples of excellent projects) are here.\nThere is no final exam. This project is your final exam.\nThe project will not be graded using a check system. Instead I will use a rubric to grade four elements of your project:\nTechnical skills Visual design Truth and beauty Story  If you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.\n ","date":1591315200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1591315200,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/","section":"assignment","summary":"Reflections Exercises Mini projects Final project   You will get the most of out this class if you:\nEngage with the readings and lecture materials Regularly use R  Each type of assignment in this class helps with one of these strategies.\nReflections To encourage engagement with the course content, you’ll need to write a ≈150 word reflection about the readings and lectures for the day. That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced).","tags":null,"title":"Assignment overview","type":"docs"},{"authors":null,"categories":null,"content":"  This section contains fully annotated R code that you can use as a reference for creating your own visualizations. In the lessons section, you sequentially build up your understanding of R and ggplot2; here you can see how all the pieces work together.\nVisit this section after you have finished the readings, lecture videos, and lesson. The examples here will be indispensable for you as you work on your assignments and mini projects.\nEach section also contains videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!\n","date":1590969600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1590969600,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"This section contains fully annotated R code that you can use as a reference for creating your own visualizations. In the lessons section, you sequentially build up your understanding of R and ggplot2; here you can see how all the pieces work together.\nVisit this section after you have finished the readings, lecture videos, and lesson. The examples here will be indispensable for you as you work on your assignments and mini projects.","tags":null,"title":"Code examples","type":"docs"},{"authors":null,"categories":null,"content":"  Each class session has an interactive lesson that you will work through after doing the readings and watching the lecture. These lessons are a central part of the class—they will teach you how to use ggplot2 and other packages in the tidyverse to create beautiful and truthful visualizations with R.\nInteractive code sections look like this. Make changes in the text box and click on the green “Run Code” button to see the results. Sometimes there will be a button with a hint or solution.\nYour turn: Modify the code here to show the relationship between health and wealth for 2002 instead of 2007.\n  If you’re curious how this works, each interactive code section is a miniature Shiny app hosted at shinyapps.io. Each app uses learnr to provide interactivity, and these learnr apps are embedded in this website with some HTML and Javascript wizardry.\n","date":1590969600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1590969600,"objectID":"45e63e789e3cb381d68e4ca47e0a453c","permalink":"/lesson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lesson/","section":"lesson","summary":"Each class session has an interactive lesson that you will work through after doing the readings and watching the lecture. These lessons are a central part of the class—they will teach you how to use ggplot2 and other packages in the tidyverse to create beautiful and truthful visualizations with R.\nInteractive code sections look like this. Make changes in the text box and click on the green “Run Code” button to see the results.","tags":null,"title":"Interactive lessons","type":"docs"},{"authors":null,"categories":null,"content":"  Each class session has a set of required readings that you should complete before watching the lecture or working through the lesson.\nI’ve included a set of questions that might guide your reflection response. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections. The lecture slides are special HTML files made with the R package xaringan (R can do so much!). On each class session page you’ll buttons for opening the presentation in a new tab or for downloading a PDF of the slides in case you want to print them or store them on your computer:\n View all slides in new window  Download PDF of all slides The slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).\n","date":1590969600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1590969600,"objectID":"1413f960ec974b9863bc45d887efa8bd","permalink":"/content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Each class session has a set of required readings that you should complete before watching the lecture or working through the lesson.\nI’ve included a set of questions that might guide your reflection response. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read.\nEvery class session also has a YouTube playlist of short recorded videos for each of the lecture sections.","tags":null,"title":"Readings, lectures, and videos","type":"docs"},{"authors":null,"categories":null,"content":"   Instructions Starter code   New York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.\n  NYC keeps incredibly detailed data regarding animal sightings, including rats, and it makes this data publicly available.\nFor this first mini project, you will use R and ggplot2 to tell an interesting story hidden in the data. You can recreate one of these ugly, less-than-helpful graphs, or create a new story by looking at other variables in the data:\nInstructions Here’s what you need to do:\nCreate a new RStudio project and place it on your computer somewhere. Open that new folder in Windows File Explorer or macOS Finder (however you navigate around the files on your computer), and create two subfolders there named data and output.\n Download New York City’s database of rat sightings since 2010:\n  Rat_Sightings.csv\n Place this in the data subfolder you created in step 1. You might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text. The data was originally uploaded by the City of New York to Kaggle, and is provided with a public domain license.\n  Create a new R Markdown file and save it in your project. In RStudio go to File \u0026gt; New File \u0026gt; R Markdown…, choose the default options, and delete all the placeholder text in the new file except for the metadata at the top, which is between --- and ---.\n Verify that your project folder is structured like this:\nyour-project-name/ your-analysis.Rmd your-project-name.Rproj data/ Rat_Sightings.csv output/ NOTHING Summarize the data somehow. The raw data has more than 100,000 rows, which means you’ll need to aggregate the data (filter(), group_by(), and summarize() will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2016.\n Create an appropriate visualization based on the data you summarized.\n Write a memo (no word limit) explaining your process. I’m specifically looking for a discussion of the following:\n What was wrong with the original graphic (if you’re fixing one of the original figures)? What story are you telling with your new graphic? How did you apply the principles of CRAP? How did you apply Kieran Healy’s principles of great visualizations or Alberto Cairo’s five qualities of great visualizations?  Upload the following outputs to iCollege:\n A PDF or Word file of your memo with your final code and graphic embedded in it.1 This means you’ll need to do all your coding in an R Markdown file and embed your code in chunks. A standalone PNG version of your graphic. Use ggsave(plot_name, filename = \"output/blah.png\", width = XX, height = XX) A standalone PDF version of your graphic. Use ggsave(plot_name, filename = \"output/blah.pdf\", width = XX, height = XX)   You will be graded based on completion using the standard ✓ system, but I’ll provide comments on how you use R and ggplot2, how well you apply the principles of CRAP, The Truthful Art, and Effective Data Visualization, and how appropriate the graph is for the data and the story you’re telling. I will use this rubric to make comments and provide you with a simulated grade.\n  mini-project-1-rubric.pdf  For this assignment, I am less concerned with detailed graphic design principles—select appropriate colors, change fonts if you’re brave, and choose a nice ggplot theme and make some adjustments like moving the legend around (theme(legend.position = \"bottom\")).\nThe assignment is due by 11:59 PM on Friday, May 226.\nPlease seek out help when you need it! You know enough R (and have enough examples of code from class and your readings) to be able to do this. Your project has to be turned in individually, and your visualization should be your own (i.e. if you work with others, don’t all turn in the same graph), but you should work with others! Reach out to me for help too—I’m here to help!\nYou can do this, and you’ll feel like a budding dataviz witch/wizard when you’re done.\n Starter code I’ve provided some starter code below. A couple comments about it:\n By default, read_csv() treats cells that are empty or “NA” as missing values. This rat dataset uses “N/A” to mark missing values, so we need to add that as a possible marker of missingness (hence na = c(\"\", \"NA\", \"N/A\")) To make life easier, I’ve renamed some of the key variables you might work with. You can rename others if you want. I’ve also created a few date-related variables (sighting_year, sighting_month, sighting_day, and sighting_weekday). You don’t have to use them, but they’re there if you need them. The functions that create these, like year() and wday() are part of the lubridate library. The date/time variables are formatted like 04/03/2017 12:00:00 AM, which R is not able to automatically parse as a date when reading the CSV file. You can use the mdy_hms() function in the lubridate library to parse dates that are structured as “month-day-year-hour-minute”. There are also a bunch of other iterations of this function, like ymd(), dmy(), etc., for other date formats. There’s one row with an unspecified borough, so I filter that out.  library(tidyverse) library(lubridate) rats_raw \u0026lt;- read_csv(\u0026quot;data/Rat_Sightings.csv\u0026quot;, na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;N/A\u0026quot;)) # If you get an error that says \u0026quot;All formats failed to parse. No formats # found\u0026quot;, it\u0026#39;s because the mdy_hms function couldn\u0026#39;t parse the date. The date # variable *should* be in this format: \u0026quot;04/03/2017 12:00:00 AM\u0026quot;, but in some # rare instances, it might load without the seconds as \u0026quot;04/03/2017 12:00 AM\u0026quot;. # If there are no seconds, use mdy_hm() instead of mdy_hms(). rats_clean \u0026lt;- rats_raw %\u0026gt;% rename(created_date = `Created Date`, location_type = `Location Type`, borough = Borough) %\u0026gt;% mutate(created_date = mdy_hms(created_date)) %\u0026gt;% mutate(sighting_year = year(created_date), sighting_month = month(created_date), sighting_day = day(created_date), sighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %\u0026gt;% filter(borough != \u0026quot;Unspecified\u0026quot;) You’ll summarize the data with functions from dplyr, including stuff like count(), arrange(), filter(), group_by(), summarize(), and mutate(). Here are some examples of ways to summarize the data:\n# See the count of rat sightings by weekday rats_clean %\u0026gt;% count(sighting_weekday) # Assign a summarized data frame to an object to use it in a plot rats_by_weekday \u0026lt;- rats_clean %\u0026gt;% count(sighting_weekday, sighting_year) ggplot(rats_by_weekday, aes(x = fct_rev(sighting_weekday), y = n)) + geom_col() + coord_flip() + facet_wrap(~ sighting_year) # See the count of rat sightings by weekday and borough rats_clean %\u0026gt;% count(sighting_weekday, borough, sighting_year) # An alternative to count() is to specify the groups with group_by() and then # be explicit about how you\u0026#39;re summarizing the groups, such as calculating the # mean, standard deviation, or number of observations (we do that here with # `n()`). rats_clean %\u0026gt;% group_by(sighting_weekday, borough) %\u0026gt;% summarize(n = n())   You can approach this in a couple different ways—you can write the memo and then include the full figure and code at the end, similar to this blog post, or you can write the memo in an incremental way, describing the different steps of creating the figure, ultimately arriving at a clean final figure, like this blog post.↩︎\n   ","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590105600,"objectID":"9b28187cb20ce4d6b54b6b87a6c394fc","permalink":"/assignment/01-mini-project/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/assignment/01-mini-project/","section":"assignment","summary":"Instructions Starter code   New York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.\n  NYC keeps incredibly detailed data regarding animal sightings, including rats, and it makes this data publicly available.\nFor this first mini project, you will use R and ggplot2 to tell an interesting story hidden in the data.","tags":null,"title":"Mini project 1","type":"docs"},{"authors":null,"categories":null,"content":"   Task 1: Make an RStudio Project Task 2: Make an R Markdown file with a plot in it   Task 1: Make an RStudio Project Use either RStudio.cloud or RStudio on your computer (preferably RStudio on your computer! Follow these instructions to get started!) to create a new RStudio Project.\n Create a folder named “data” in the project folder you just made.\n Download this CSV file and place it in that folder:\n  cars.csv  In RStudio, go to “File” \u0026gt; “New File…” \u0026gt; “R Markdown…” and click “OK” in the dialog without changing anything.\n Delete all the placeholder text in that new file and replace it with this:\n--- title: \u0026quot;Exercise 1\u0026quot; author: \u0026quot;Put your name here\u0026quot; output: html_document --- # Reflection Replace this text with your reflection # My first plot ```{r load-libraries-data, warning=FALSE, message=FALSE} library(tidyverse) cars \u0026lt;- read_csv(\u0026quot;data/cars.csv\u0026quot;) ``` Replace this line with a code chunk and use it to create a plot.  Save the R Markdown file with some sort of name (without any spaces!)\n Your project folder should look something like this:\n   Task 2: Make an R Markdown file with a plot in it Add your reading reflection to the appropriate place in the R Markdown file. You can type directly in RStudio if you want (though there’s no spell checker), or you can type it in Word or Google Docs and then paste it into RStudio.\n Remove the text that says “Replace this line with a code chunk” and insert a new R code chunk. Either type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS, or use the “Insert Chunk” menu:\n Use ggplot() to create a scatterplot using the mpg dataset. Use whatever variables you want. Type the code to create the plot in the new empty chunk.\n Knit your document as a Word file (or PDF if you’re brave and installed LaTeX). Use the “Knit” menu:\n Upload the knitted document to iCollege.\n 🎉 Party! 🎉\n  You’ll be doing this same process for all your future exercises. Each exercise will involve an R Markdown file. You can either create a new RStudio Project directory for all your work:\nOr you can create individual projects for each assignment and mini-project:\n  ","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"fcce0f7ada10fbf30daa67e2ac98e630","permalink":"/assignment/01-exercise/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/assignment/01-exercise/","section":"assignment","summary":"Task 1: Make an RStudio Project Task 2: Make an R Markdown file with a plot in it   Task 1: Make an RStudio Project Use either RStudio.cloud or RStudio on your computer (preferably RStudio on your computer! Follow these instructions to get started!) to create a new RStudio Project.\n Create a folder named “data” in the project folder you just made.\n Download this CSV file and place it in that folder:","tags":null,"title":"Introduction to R and the tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"   Basic process for working with RStudio   Basic process for working with RStudio For this example, I’m going to create a new RStudio project, download some data, put the data in the project, and make a graph of it using R Markdown. You’ll follow this same process any time you start a new project or exercise.\nTo follow along, download this CSV file here (you may need to right click on it and select “Save As…”):\n  gapminder.csv  Here’s a video walkthrough of how to get started:\n   ","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"059bb398e999a9d10b388c3df2b5644f","permalink":"/example/01-example/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/example/01-example/","section":"example","summary":"Basic process for working with RStudio   Basic process for working with RStudio For this example, I’m going to create a new RStudio project, download some data, put the data in the project, and make a graph of it using R Markdown. You’ll follow this same process any time you start a new project or exercise.\nTo follow along, download this CSV file here (you may need to right click on it and select “Save As…”):","tags":null,"title":"Introduction to R and the tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"   Part 1: The basics of R and dplyr Part 2: Getting familiar with RStudio Part 3: RStudio Projects Part 4: Getting familiar with R Markdown   Part 1: The basics of R and dplyr For the first part of today’s lesson, you need to work through a few of RStudio’s introductory primers. You’ll do these in your browser and type code and see results there.\nYou’ll learn some of the basics of R, as well as some powerful methods for manipulating data with the dplyr package.\nComplete these:\n The Basics  Visualization Basics Programming Basics  Work with Data  Working with Tibbles Isolating Data with dplyr Deriving Information with dplyr   The content from these primers comes from the (free and online!) book R for Data Science by Garrett Grolemund and Hadley Wickham. I highly recommend the book as a reference and for continuing to learn and use R in the future (like running regression models and other types of statistical analysis)\n Part 2: Getting familiar with RStudio The RStudio primers you just worked through are a great introduction to writing and running R code, but you typically won’t type code in a browser when you work with R. Instead, you’ll use a nicer programming environment like RStudio, which lets you type and save code in scripts, run code from those scripts, and see the output of that code, all in the same program.\nTo get familiar with RStudio, watch this video:\n   Part 3: RStudio Projects One of the most powerful and useful aspects of RStudio is its ability to manage projects.\nWhen you first open R, it is “pointed” at some folder on your computer, and anything you do will be relative to that folder. The technical term for this is a “working directory.”\nWhen you first open RStudio, look in the area right at the top of the Console pane to see your current working directory. Most likely you’ll see something cryptic: ~/\nThat tilde sign (~) is a shortcut that stands for your user directory. On Windows this is C:\\Users\\your_user_name\\; on macOS this is /Users/your_user_name/. With the working directory set to ~/, R is “pointed” at that folder, and anything you save will end up in that folder, and R will expect any data that you load to be there too.\nIt’s always best to point R at some other directory. If you don’t use RStudio, you need to manually set the working directory to where you want it with setwd(), and many R scripts in the wild include something like setwd(\"C:\\\\Users\\\\bill\\\\Desktop\\\\Important research project\") at the beginning to change the directory. THIS IS BAD THOUGH (see here for an explanation). If you ever move that directory somewhere else, or run the script on a different computer, or share the project with someone, the path will be wrong and nothing will run and you will be sad.\nThe best way to deal with working directories with RStudio is to use RStudio Projects. These are special files that RStudio creates for you that end in a .Rproj extension. When you open one of these special files, a new RStudio instance will open up and be pointed at the correct directory automatically. If you move the folder later or open it on a different computer, it will work just fine and you will not be sad.\nRead this super short chapter on RStudio projects.\n Part 4: Getting familiar with R Markdown To ensure that the analysis and graphics you make are reproducible, you’ll do the majority of your work in this class using R Markdown files.\nDo the following things:\nWatch this video:     Skim through the content at these pages:\n Using Markdown Using R Markdown How it Works Code Chunks Inline Code Markdown Basics (The R Markdown Reference Guide is super useful here.) Output Formats  Watch this video:\n     ","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"2248ef62e0fddbf88b2832cdafb38b9a","permalink":"/lesson/01-lesson/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/lesson/01-lesson/","section":"lesson","summary":"Part 1: The basics of R and dplyr Part 2: Getting familiar with RStudio Part 3: RStudio Projects Part 4: Getting familiar with R Markdown   Part 1: The basics of R and dplyr For the first part of today’s lesson, you need to work through a few of RStudio’s introductory primers. You’ll do these in your browser and type code and see results there.\nYou’ll learn some of the basics of R, as well as some powerful methods for manipulating data with the dplyr package.","tags":null,"title":"Introduction to R and the tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"   RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse Install tinytex    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nRStudio.cloud R is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nGo to https://rstudio.cloud/ and create an account. You’ll receive a link to join the shared class workspace separately. If you don’t get this link, let me know and I will invite you.\n RStudio on your computer RStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\nInstall R First you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n Click on “Download R for XXX”, where XXX is either Mac or Windows:\n If you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.\n If you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n  Double click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n If you use macOS, download and install XQuartz. You do not need to do this on Windows.\n   Install RStudio Next, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n Double click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n  Double click on RStudio to run it (check your applications folder or start menu).\n Install tidyverse R packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n Install tinytex When you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced “lay-tek” or “lah-tex”; for goofy nerdy reasons, the x is technically the “ch” sound in “Bach”, but most people just say it as “k”—saying “layteks” is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB! To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console. Run tinytex::install_tinytex() in the console. Wait for a bit while R downloads and installs everything you need. The end! You should now be able to knit to PDF.     It’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n   ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"/resource/install/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse Install tinytex    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"   Accessibility Colors Fonts Graphic assets  Images Vectors Vectors, photos, videos, and other assets    Accessibility  Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based) Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes. ColourLovers: Like Facebook for color palettes. viridis: Percetually uniform color scales. Scientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with scico. ColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account. Colorgorical: Create color palettes based on fancy mathematical rules for perceptual distance. Colorpicker for data: More fancy mathematical rules for color palettes (explanation). iWantHue: Yet another perceptual distance-based color palette builder. Photochrome: Word-based color pallettes. PolicyViz Design Color Tools: Large collection of useful color resources   Fonts  Google Fonts: Huge collection of free, well-made fonts. The Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).   Graphic assets Images  Use the Creative Commons filters on Google Images or Flickr Unsplash Pexels Pixabay StockSnap.io Burst freephotos.cc   Vectors  Noun Project: Thousands of free simple vector images aiconica: 1,000+ vector icons Vecteezy: Thousands of free vector images   Vectors, photos, videos, and other assets  Stockio    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"16fd04c4714e3d096bffcf19e6c524ca","permalink":"/resource/design/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/design/","section":"resource","summary":"Accessibility Colors Fonts Graphic assets  Images Vectors Vectors, photos, videos, and other assets    Accessibility  Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based) Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes. ColourLovers: Like Facebook for color palettes.","tags":null,"title":"Design","type":"docs"},{"authors":null,"categories":null,"content":"   Instructions Data cleaning code Data to possibly use in your plot  Country totals over time Cumulative country totals over time Continent totals over time Cumulative continent totals over time  Visualization ideas   The United States has resettled more than 600,000 refugees from 60 different countries since 2006.\nIn this mini project, you will use R, ggplot, and Illustrator, Inkscape, or Gravit Designer to explore where these refugees have come from.\nInstructions Here’s what you need to do:\n Create a new RStudio project and place it on your computer somewhere. Open that new folder in Windows File Explorer or macOS Finder (however you navigate around the files on your computer), and create two subfolders there named data and output.\n Download the Department of Homeland Security’s annual count of people granted refugee status between 2006-2015:\n DHS refugees, 2006-2015\nPlace this in the data subfolder you created in step 1. You might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text. This data was originally uploaded by the Department of Homeland Security to Kaggle, and is provided with a public domain license.\n Create a new R Markdown file and save it in your project. In RStudio go to File \u0026gt; New File \u0026gt; R Markdown…, choose the default options, and delete all the placeholder text in the new file except for the metadata at the top, which is between --- and ---.\n Verify that your project folder is structured like this:\nyour-project-name/ your-analysis.Rmd your-project-name.Rproj data/ refugee_status.csv output/ NOTHING Clean the data using the code I’ve given you below.\n Summarize the data somehow. There is data for 60 countries over 10 years, so you’ll probably need to aggregate or reshape the data somehow (unless you do a 60-country sparkline). I’ve included some examples down below.\n Create an appropriate time-based visualization based on the data. I’ve shown a few different ways to summarize the data so that it’s plottable down below. Don’t just calculate overall averages or totals per country—the visualization needs to deal with change over time. Do as much polishing and refining in R—make adjustments to the colors, scales, labels, grid lines, and even fonts, etc.\n Save the figure as a PDF. Use ggsave(plot_name, filename = \"output/blah.pdf\", width = XX, height = XX)\n Refine and polish the saved PDF in Illustrator or Inkscape or Gravit Designer, adding annotations, changing colors, and otherwise enhancing it.\n Export the polished image as a PDF and a PNG file.\n Write a memo (no word limit) explaining your process. I’m specifically looking for the following:\n What story are you telling with your graphic? How did you apply the principles of CRAP? How did you apply Kieran Healy’s principles of great visualizations or Alberto Cairo’s five qualities of great visualizations?  Upload the following outputs to iCollege:\n A PDF or Word file of your memo with your final code, intermediate graphic (the one you create in R), and final graphic (the one you enhance) in it. Remember to use ![Caption](path/to/figure/here) to place external images in Markdown. A standalone PNG version of your graphic. You’ll export this from Illustrator or Inkscape. A standalone PDF version of your graphic. You’ll export this from Illustrator or Inkscape.   You will be graded based on completion using the standard ✓ system, but I’ll provide comments on how you use R and ggplot2, how well you apply the principles of CRAP, The Truthful Art, and Effective Data Visualization, and how appropriate the graph is for the data and the story you’re telling. I will use this rubric to make comments and provide you with a simulated grade.\n  mini-project-2-rubric.pdf  For this assignment, I am less concerned with the code (that’s why I gave most of it to you), and more concerned with the design. Choose good colors based on palettes. Choose good, clean fonts. Use the heck out of theme(). Add informative design elements in Illustrator/Inkscape/Gravit Designer. Make it look beautiful and CRAPpy. Refer to the design resources here.\nThe assignment is due by 11:59 PM on Friday, May 29.\nPlease seek out help when you need it! You know enough R (and have enough examples of code from class and your readings) to be able to do this. Your project has to be turned in individually, and your visualization should be your own (i.e. if you work with others, don’t all turn in the same graph), but you should work with others! Reach out to me for help too—I’m here to help!\nYou can do this, and you’ll feel like a budding dataviz witch/wizard when you’re done.\n Data cleaning code The data isn’t perfectly clean and tidy, but it’s real world data, so this is normal. Because the emphasis for this assignment is on design, not code, I’ve provided code to help you clean up the data.\nThese are the main issues with the data:\n There are non-numeric values in the data, like -, X, and D. The data isn’t very well documented; I’m assuming - indicates a missing value, but I’m not sure what X and D mean, so for this assignment, we’ll just assume they’re also missing.\n The data generally includes rows for dozens of countries, but there are also rows for some continents, “unknown,” “other,” and a total row. Because Africa is not a country, and neither are the other continents, we want to exclude all non-countries.\n Maintaining consistent country names across different datasets is literally the woooooooorst. Countries have different formal official names and datasets are never consistent in how they use those names.1 It’s such a tricky problem that social scientists have spent their careers just figuring out how to properly name and code countries. Really.2 There are international standards for country codes, though, like ISO 3166-1 alpha 3 (my favorite), also known as ISO3. It’s not perfect—it omits microstates (some Polynesian countries) and gray area states (Palestine, Kosovo)—but it’s an international standard, so it has that going for it.\n To ensure that country names are consistent in this data, we use the countrycode package (install it if you don’t have it), which is amazing. The countrycode() function will take a country name in a given coding scheme and convert it to a different coding scheme using this syntax:\n countrycode(variable, \u0026quot;current-coding-scheme\u0026quot;, \u0026quot;new-coding-scheme\u0026quot;) It also does a farily good job at guessing and parsing inconsistent country names (e.g. it will recognize “Congo, Democratic Republic”, even though it should technically be “Democratic Republic of the Congo”). Here, we use countrycode() to convert the inconsistent country names into ISO3 codes. We then create a cleaner version of the origin_country column by converting the ISO3 codes back into country names. Note that the function chokes on North Korea initially, since it’s included as “Korea, North”—we use the custom_match argument to help the function out.\n The data isn’t tidy—there are individual columns for each year. gather() takes every column and changes it to a row. We exclude the country, region, continent, and ISO3 code from the change-into-rows transformation with -origin_country, -iso3, -origin_region, -origin_continent.\n Currently, the year is being treated as a number, but it’s helpful to also treat it as an actual date. We create a new variable named year_date that converts the raw number (e.g. 2009) into a date. The date needs to have at least a month, day, and year, so we actually convert it to January 1, 2009 with ymd(paste0(year, \"-01-01\")).\n  library(tidyverse) # For ggplot, dplyr, and friends library(countrycode) # For dealing with country names, abbreviations, and codes library(lubridate) # For dealing with dates refugees_raw \u0026lt;- read_csv(\u0026quot;data/refugee_status.csv\u0026quot;, na = c(\u0026quot;-\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;D\u0026quot;))  non_countries \u0026lt;- c(\u0026quot;Africa\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Europe\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;Oceania\u0026quot;, \u0026quot;South America\u0026quot;, \u0026quot;Unknown\u0026quot;, \u0026quot;Other\u0026quot;, \u0026quot;Total\u0026quot;) refugees_clean \u0026lt;- refugees_raw %\u0026gt;% # Make this column name easier to work with rename(origin_country = `Continent/Country of Nationality`) %\u0026gt;% # Get rid of non-countries filter(!(origin_country %in% non_countries)) %\u0026gt;% # Convert country names to ISO3 codes mutate(iso3 = countrycode(origin_country, \u0026quot;country.name\u0026quot;, \u0026quot;iso3c\u0026quot;, custom_match = c(\u0026quot;Korea, North\u0026quot; = \u0026quot;PRK\u0026quot;))) %\u0026gt;% # Convert ISO3 codes to country names, regions, and continents mutate(origin_country = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;country.name\u0026quot;), origin_region = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;region\u0026quot;), origin_continent = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;continent\u0026quot;)) %\u0026gt;% # Make this data tidy gather(year, number, -origin_country, -iso3, -origin_region, -origin_continent) %\u0026gt;% # Make sure the year column is numeric + make an actual date column for years mutate(year = as.numeric(year), year_date = ymd(paste0(year, \u0026quot;-01-01\u0026quot;)))  Data to possibly use in your plot Here are some possible summaries of the data you might use…\nCountry totals over time This is just the refugees_clean data frame I gave you. You’ll want to filter it and select specific countries, though—you won’t really be able to plot 60 countries all at once unless you use sparklines.\n## # A tibble: 6 x 7 ## origin_country iso3 origin_region origin_continent year number year_date ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; ## 1 Afghanistan AFG South Asia Asia 2006 651 2006-01-01 ## 2 Angola AGO Sub-Saharan Afr… Africa 2006 13 2006-01-01 ## 3 Armenia ARM Europe \u0026amp; Centra… Asia 2006 87 2006-01-01 ## 4 Azerbaijan AZE Europe \u0026amp; Centra… Asia 2006 77 2006-01-01 ## 5 Belarus BLR Europe \u0026amp; Centra… Europe 2006 350 2006-01-01 ## 6 Bhutan BTN South Asia Asia 2006 3 2006-01-01  Cumulative country totals over time Note the cumsum() function—it calculates the cumulative sum of a column.\nrefugees_countries_cumulative \u0026lt;- refugees_clean %\u0026gt;% arrange(year_date) %\u0026gt;% group_by(origin_country) %\u0026gt;% mutate(cumulative_total = cumsum(number)) ## # A tibble: 6 x 7 ## # Groups: origin_country [1] ## origin_country iso3 origin_continent year number year_date cumulative_total ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan AFG Asia 2006 651 2006-01-01 651 ## 2 Afghanistan AFG Asia 2007 441 2007-01-01 1092 ## 3 Afghanistan AFG Asia 2008 576 2008-01-01 1668 ## 4 Afghanistan AFG Asia 2009 349 2009-01-01 2017 ## 5 Afghanistan AFG Asia 2010 515 2010-01-01 2532 ## 6 Afghanistan AFG Asia 2011 428 2011-01-01 2960  Continent totals over time Note the na.rm = TRUE argument in sum(). This makes R ignore any missing data when calculating the total. Without it, if R finds a missing value in the column, it will mark the final sum as NA too, which we don’t want.\nrefugees_continents \u0026lt;- refugees_clean %\u0026gt;% group_by(origin_continent, year_date) %\u0026gt;% summarize(total = sum(number, na.rm = TRUE)) ## # A tibble: 6 x 3 ## # Groups: origin_continent [1] ## origin_continent year_date total ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 2006-01-01 18116 ## 2 Africa 2007-01-01 17473 ## 3 Africa 2008-01-01 8931 ## 4 Africa 2009-01-01 9664 ## 5 Africa 2010-01-01 13303 ## 6 Africa 2011-01-01 7677  Cumulative continent totals over time Note that there are two group_by() functions here. First we get the total number of refugees per continent per year, then we group by continent only to get the cumulative sum of refugees across continents.\nrefugees_continents_cumulative \u0026lt;- refugees_clean %\u0026gt;% group_by(origin_continent, year_date) %\u0026gt;% summarize(total = sum(number, na.rm = TRUE)) %\u0026gt;% arrange(year_date) %\u0026gt;% group_by(origin_continent) %\u0026gt;% mutate(cumulative_total = cumsum(total)) ## # A tibble: 6 x 4 ## # Groups: origin_continent [1] ## origin_continent year_date total cumulative_total ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 2006-01-01 18116 18116 ## 2 Africa 2007-01-01 17473 35589 ## 3 Africa 2008-01-01 8931 44520 ## 4 Africa 2009-01-01 9664 54184 ## 5 Africa 2010-01-01 13303 67487 ## 6 Africa 2011-01-01 7677 75164   Visualization ideas You can redesign one of these ugly, less-than-helpful graphs, or create a brand new visualization (like a map!).\nOr be super brave and make a map!\n  For instance, “North Korea”, “Korea, North”, “DPRK”, “Korea, Democratic People’s Republic of”, and “Democratic People’s Republic of Korea”, and “Korea (DPRK)” are all perfectly normal versions of the country’s name and you’ll find them all in the wild.↩︎\n See Gleditsch, Kristian S. \u0026amp; Michael D. Ward. 1999. “Interstate System Membership: A Revised List of the Independent States since 1816.” International Interactions 25: 393-413; or the “ICOW Historical State Names Data Set”.↩︎\n   ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"68edf08752e231077b06d41229b4ac8a","permalink":"/assignment/02-mini-project/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/assignment/02-mini-project/","section":"assignment","summary":"Instructions Data cleaning code Data to possibly use in your plot  Country totals over time Cumulative country totals over time Continent totals over time Cumulative continent totals over time  Visualization ideas   The United States has resettled more than 600,000 refugees from 60 different countries since 2006.\nIn this mini project, you will use R, ggplot, and Illustrator, Inkscape, or Gravit Designer to explore where these refugees have come from.","tags":null,"title":"Mini project 2","type":"docs"},{"authors":null,"categories":null,"content":"   Task 1: Reflection Task 2: CRAP critique Task 3: CRAP redesign Turning everything in   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: CRAP critique Critique the design of the poster for the BYU Student Wellness Center workshop below. Go through the CRAP checklist and analyze how well or poorly the poster follows each of the principles. Discuss how the poster’s adherence to (or non-adherence to) these principles influences its effectiveness.\n(This would have been some random poster from GSU, but I haven’t been on campus since mid-March 😭)\n Task 3: CRAP redesign Redesign the poster for the BYU Student Wellness Center workshop. Use whatever program you want—even PowerPoint if you’re most comfortable with that, though it’ll probably be easier to use something like Canva or Adobe Illustrator. If you use Canva, don’t use any of the built-in templates—start from scratch with a blank page.\nTo save you from retyping everything, I’ve included all the text and Student Wellness hex logo in the zip file below:\n  02-exercise.zip  I didn’t include the Instagram logo. If you want to use that, go find one online. You don’t have to use it. You don’t have to use the big paragraph of text either—you can rewrite it to shrink it down if you want.\nCritique your new design using the CRAP checklist. How did you use contrast, repetition, alignment, and proximity in your improved design?\n Turning everything in You don’t need to worry about using R Markdown for this assignment (unless you really want to). On iCollege, submit a PDF of your new poster, along with a PDF of your reflection and your critiques of the original poster and your new poster.\n ","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589241600,"objectID":"0e0716b3c85ed8432440cb6d4065ba47","permalink":"/assignment/02-exercise/","publishdate":"2020-05-12T00:00:00Z","relpermalink":"/assignment/02-exercise/","section":"assignment","summary":"Task 1: Reflection Task 2: CRAP critique Task 3: CRAP redesign Turning everything in   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: CRAP critique Critique the design of the poster for the BYU Student Wellness Center workshop below. Go through the CRAP checklist and analyze how well or poorly the poster follows each of the principles. Discuss how the poster’s adherence to (or non-adherence to) these principles influences its effectiveness.","tags":null,"title":"Graphic design","type":"docs"},{"authors":null,"categories":null,"content":"   Critique Redesign in Canva Redesign in Illustrator Final versions   For this example, I’m going to critique and improve this random flyer I found posted in the BYU library in September 2018:\nIt’s not the best designed poster, but it’s incredibly typical of what you see in the real world. By applying the principles of CRAP, we can improve the poster significantly.\nIf you download and unzip this file, you can follow along too (but you don’t have to—you can just sit back and enjoy the ride).\n  02-example.zip  Critique    Redesign in Canva    Redesign in Illustrator    Final versions  ","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589241600,"objectID":"ce879375c9ab42490f4d0d112e48c07c","permalink":"/example/02-example/","publishdate":"2020-05-12T00:00:00Z","relpermalink":"/example/02-example/","section":"example","summary":"Critique Redesign in Canva Redesign in Illustrator Final versions   For this example, I’m going to critique and improve this random flyer I found posted in the BYU library in September 2018:\nIt’s not the best designed poster, but it’s incredibly typical of what you see in the real world. By applying the principles of CRAP, we can improve the poster significantly.\nIf you download and unzip this file, you can follow along too (but you don’t have to—you can just sit back and enjoy the ride).","tags":null,"title":"Graphic design","type":"docs"},{"authors":null,"categories":null,"content":"   File types Select the best file type   File types Recall from the last section of the lecture that you’ll typically work with one of two image file types: bitmap images and vector images.\nBitmaps store image information as tiny squares, or pixels. Specific files types compress these images in different ways: JPEG files smudge together groups of similarly colored pixels to save repetition, while PNG and GIF files look for fields of the exact same color.\nYou use bitmap images for things that go on the internet and when you place images in Word (technically modern versions of Word can handle some types of vector images, but that support isn’t universal yet).\nVector images, on the other hand, do not store image information as pixels. Instead, these use mathematical formulas to draw lines and curves and fill areas with specific colors. This makes them a little more complicated to draw and create, but it also means that you can scale them up or down infinitely—a vector image will look just as crisp on a postage stamp as it would on a billboard.\nHere are some general guidelines:\n If an image has lots of colors (like a photograph), you should use a bitmap file type designed for lots of colors, like JPEG. This is the case regardless of where the image will ultimately end up. If you’re putting it on the internet, it needs to be a JPEG. If you’re blowing it up to fit on a billboard, it will still need to be a JPEG (and you have to use a fancy super high quality camera to get a high enough resolution for that kind of expansion)\n If an image has a few colors and some text and is not a photograph and you’re using the image in Word or on the internet, you should use a bitmap file type designed for carefully compressing a few colors, like PNG.\n If an image has a few colors and some text and is not a photograph and you’re planning on using it in multiple sizes (like a logo), or using it in fancier production software like Adobe InDesign (for print) or Adobe After Effects (for video), you should use a vector file type like PDF or SVG.\n   Select the best file type Practice deciding what kind of file type you should use by looking at these images and choosing what you think works the best.\nPNG\nJPG\nPDF\n  function validate_form_1() {var x, text; var x = document.forms[\"form_1\"][\"answer_1\"].value;if (x == \"JPG\"){text = 'Correct! This is a photograph, so it should be a JPG. It might seem a little tricky since there are so few colors, but it still needs to be a JPG because the black paint on the brick is actually a range of thousands of different shades of black pixels.';} else {text = 'Not quite—this image has a lot of colors in it…';} document.getElementById('result_1').innerHTML = text; return false;}   PNG\nJPG\nPDF\n  function validate_form_2() {var x, text; var x = document.forms[\"form_2\"][\"answer_2\"].value;if (x == \"PNG\"|x == \"PDF\"){text = 'Correct! This is a logo with a few colors in it, so it’s vector-based. If you use a PDF of the logo, you can rescale it infinitely big or small. If you use a PNG, it will work nicely online.';} else {text = 'Not quite—this image doesn’t have a lot of colors in it…';} document.getElementById('result_2').innerHTML = text; return false;}   PNG\nJPG\nPDF\n  function validate_form_3() {var x, text; var x = document.forms[\"form_3\"][\"answer_3\"].value;if (x == \"PNG\"|x == \"PDF\"){text = 'Correct! This is a grpah with a few colors in it, so should be vector-based. If you’re using this in a fancy publication or report, use a PDF. If you’e using Word or HTML, use a PNG.';} else {text = 'Not quite—this image doesn’t have a lot of colors in it…';} document.getElementById('result_3').innerHTML = text; return false;}   PNG\nJPG\nPDF\n  function validate_form_4() {var x, text; var x = document.forms[\"form_4\"][\"answer_4\"].value;if (x == \"JPG\"){text = 'Correct! This has a ton of colors in it and is mostly a photograph. You may have been thrown off by the text in the bottom section, or the stylized shapes of the Millennium Falcon’s windows at the top. Those shapes and the text are both vector-based, but because the majority of the image is a photogrpah, it still needs to be saved as a JPG. To keep the text nice and crisp, it needs to be exported at a high resolution.';} else {text = 'Not quite—this image has a lot of colors in it…';} document.getElementById('result_4').innerHTML = text; return false;}   PNG\nJPG\nPDF\n  function validate_form_5() {var x, text; var x = document.forms[\"form_5\"][\"answer_5\"].value;if (x == \"PNG\"|x == \"PDF\"){text = 'Correct! Even though this is very colorful, it should be a PNG or PDF, since it’s vector-based and not a photograph. ';} else {text = 'Not quite—this image doesn’t have a lot of colors in it…';} document.getElementById('result_5').innerHTML = text; return false;}   PNG\nJPG\nPDF\n  function validate_form_6() {var x, text; var x = document.forms[\"form_6\"][\"answer_6\"].value;if (x == \"JPG\"){text = 'Correct! This is a photograph and should be a JPG.';} else {text = 'Not quite—this image has a lot of colors in it…';} document.getElementById('result_6').innerHTML = text; return false;}    ","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589241600,"objectID":"911ebe9376618b75c6ca9ac02110b8b9","permalink":"/lesson/02-lesson/","publishdate":"2020-05-12T00:00:00Z","relpermalink":"/lesson/02-lesson/","section":"lesson","summary":"File types Select the best file type   File types Recall from the last section of the lecture that you’ll typically work with one of two image file types: bitmap images and vector images.\nBitmaps store image information as tiny squares, or pixels. Specific files types compress these images in different ways: JPEG files smudge together groups of similarly colored pixels to save repetition, while PNG and GIF files look for fields of the exact same color.","tags":null,"title":"Graphic design","type":"docs"},{"authors":null,"categories":null,"content":"   Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph. More text in the next paragraph. Always use empty lines between paragraphs.  Some text in a paragraph.\nMore text in the next paragraph. Always use empty lines between paragraphs.\n  *Italic* _Italic_ Italic  **Bold** __Bold__ Bold  # Heading 1  Heading 1   ## Heading 2  Heading 2   ### Heading 3  Heading 3   (Go up to heading level 6 with ######)    [Link text](http://www.example.com)  Link text  ![Image caption](/path/to/image.png)    `Inline code` with backticks  Inline code with backticks  \u0026gt; Blockquote   Blockquote\n  - Things in - an unordered - list * Things in * an unordered * list  Things in an unordered list   1. Things in 2. an ordered 3. list 1) Things in 2) an ordered 3) list Things in an ordered list   Horizontal line --- Horizontal line *** Horizontal line\n     Math Markdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n    Type… …to get    Based on the DAG, the regression model for estimating the effect of education on wages is $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or $\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$. Based on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).    To put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math: $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ But now we just use computers to solve for $x$. …to get…\n The quadratic equation was an important part of high school math:\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\nBut now we just use computers to solve for \\(x\\).\n Because dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n Tables There are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: Caption goes here …to get…\n Caption goes here  Right Left Center Default    12 12 12 12  123 123 123 123  1 1 1 1    For pipe tables, type…\n| Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Table: Caption goes here …to get…\n Caption goes here  Right Left Default Center    12 12 12 12  123 123 123 123  1 1 1 1     Footnotes There are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags]. [^1]: This is a note. [^note-on-dags]: DAGs are neat. And here\u0026#39;s more of the document. …to get…\n Here is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n  This is a note.↩︎   DAGs are neat.↩︎     You can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!] …to get…\n Causal inference is neat.1\n  But it can be hard too!↩︎      Front matter You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; --- You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n--- title: \u0026quot;My cool title: a subtitle\u0026quot; --- If you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n--- title: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39; ---  Citations One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib --- Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib csl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot; --- Some of the most common CSLs are:\n Chicago author-date Chicago note-bibliography Chicago full note-bibliography (no shortened notes or ibids) APA 7th edition MLA 8th edition  Cite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n    Type… …to get…    Causal inference is neat [@Rohrer:2018; @AngristPischke:2015]. Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).  Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1]. Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).  Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018]. Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).  @AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees. Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.    After compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n Angrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n   Other references These websites have additional details and examples and practice tools:\n CommonMark’s Markdown tutorial: A quick interactive Markdown tutorial. Markdown tutorial: Another interactive tutorial to practice using Markdown. Markdown cheatsheet: Useful one-page reminder of Markdown syntax. The Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.   ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578873600,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  The Stories Behind a Line Australia as 100 people: You can make something like this with d3 and the potato project. Marrying Later, Staying Single Longer   How to select the appropriate chart type Many people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:\n The Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations. The Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.). From Data to Viz: A decision tree for dozens of chart types with links to R and Python code. The Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R. R Graph Catalog: R code for 124 ggplot graphs. Emery’s Essentials: Descriptions and examples of 26 different chart types.   General resources  Storytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic. Ann K. Emery’s blog: Blog and tutorials by Ann Emery. Evergreen Data: Helful resources by Stephanie Evergreen. PolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch. Visualising Data: Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk. Info We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field. FlowingData: Blog by Nathan Yau. Information is Beautiful: Blog by David McCandless. Junk Charts: Blog by Kaiser Fung. WTF Visualizations: Visualizations that make you ask “wtf?” The Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic. Data Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway. Seeing Data: A series of research projects about perceptions and visualizations.   Visualization in Excel  How to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel. Ann Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel.   Visualization in Tableau Because it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ca403ba352e0871f06b445d2470037b3","permalink":"/resource/visualization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/visualization/","section":"resource","summary":"Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  The Stories Behind a Line Australia as 100 people: You can make something like this with d3 and the potato project. Marrying Later, Staying Single Longer   How to select the appropriate chart type Many people have created many useful tools for selecting the correct chart type for a given dataset or question.","tags":null,"title":"Visualization","type":"docs"},{"authors":null,"categories":null,"content":"   Data from the internet  Nonprofit management Federal, state, and local government management Business management  Instructions Final deliverables Past examples  Travel runs in Yellowstone Firefighter fatalities Scripture use by The Killers Utah nonprofits Buckethead    You made it to the end of our whirlwind tour of data visualization principles! Congratulations!\nNow you get to show off all the tools you learned with a beautiful, truthful, narrative visualization.\nFor your final project, you will take a dataset, explore it, tinker with it, and tell a nuanced story about it using at least three graphs.\nI want this project to be as useful for you and your future career as possible—you’ll hopefully want to show off your final project in a portfolio or during job interviews.\nAccordingly, you have some choice in what data you can use for this project. I’ve found several different high-quality datasets online related to the core MPA/MPP tracks. You do not have to choose a dataset in your given field (especially if you’re not an MPA or MPP student!) Choose whatever one you are most interested in or will have the most fun with.\nData from the internet Go to this list of data sources and find something interesting! The things in the “Data is Plural” newsletter are often especially interesting and fun. Here are some different high-quality datasets that students have worked with before:\nNonprofit management  U.S. Charities and Non-profits: All of the charities and nonprofits registered with the IRS. This is actually split into six separate files. You can combine them all into one massive national database with bind_rows(), or filter the data to include specific states (or a single state). It all depends on the story you’re telling. Source: IRS. Nonprofit Grants 2010 to 2016: Nonprofit grants made in the US as listed in Schedule I of the IRS 990 tax form between 2010 to 2016. Source: IRS.   Federal, state, and local government management  Deadly traffic accidents in the UK (2015): List of all traffic-related deaths in the UK in 2015. Source: data.gov.uk. Firefighter Fatalities in the United States: Name, rank, and cause of death for all firefighters killed since 2000. Source: FEMA. Federal Emergencies and Disasters, 1953–Present: Every federal emergency or disaster declared by the President of the United States since 1953. Source: FEMA. Global Terrorism Database (1970–2016): 170,000 terrorist attacks worldwide, 1970-2016. Source: National Consortium for the Study of Terrorism and Responses to Terrorism (START), University of Maryland. City of Austin 311 Unified Data: All 311 calls to the City of Austin since 2014. Source: City of Austin.   Business management  515K Hotel Reviews Data in Europe: 515,000 customer reviews and scoring of 1,493 luxury hotels across Europe. Source: Booking.com. Chase Bank Branch Deposits, 2010–2016: Records for every branch of Chase Bank in the United States. This dataset is not quite tidy and will require a little bit of reshaping with gather() or pivot_longer(), since there are individual columns of deposits per year. Source: Chase Bank.    Instructions Here’s what you’ll need to do:\nDownload a dataset and explore it. Many of these datasets are large and will not open (well) in Excel, so you’ll need to load the CSV file into R with read_csv(). Most of these datasets have nice categorical variables that you can use for grouping and summarizing, and many have time components too, so you can look at trends. Your past problem sets and in-class examples will come in handy here.\n Find a story in the data. Explore that story and make sure it’s true and insightful.\n Use R to create multiple graphs to tell the story. You can make as many graphs as you want, but you must use at least three different chart types (i.e. don’t just make three scatterplots or three maps).\n Export these figures as PDF files, place them in Adobe Illustrator (or InDesign or Gravit Designer or Inkscape), and make one combined graphic or handout where you tell the complete story. You have a lot of latitude in how you do this. You can make a graphic-heavy one-page handout. You can make something along the lines of the this, with one big graphic + smaller subgraphics + explanatory text. Just don’t make a goofy infographic. Whatever you do, the final figure must include all the graphics, must have some explanatory text to help summarize the narrative, and must be well designed.\n Export the final graphic from Illustrator as a PDF and a PNG.\n Write a memo using R Markdown to introduce, frame, and describe your story and figure. Use this template to get started. You should include the following in the memo:\n Executive summary Background information and summary of the data Explanation, description, and code for each individual figure Explanation and description for the final figure Final figure should be included as an image (remember ![Caption goes here](path/to/file.png))   Remember to follow R Markdown etiquette rules and style—don’t have it output extraneous messages or warnings, include summary tables in nice tables, adjust the dimensions for your figures, and remove the placeholder text that’s in the template already (i.e. I don’t want to see stuff like “Describe and show how you cleaned and reshaped the data” in the final report.)\nYou should download a full example of what a final project might look like (but don’t make your final combined visualization look exactly like this—show some creativity!)\n Final deliverables Upload the following files to iCollege:\nA memo introducing and describing your final graphic (see full instructions above) A standalone PDF of your graphic exported from Illustrator A standalone PNG of your graphic exported from Illustrator  No late work will be accepted for this project since it’s the last project and it counts as your final.\nI will use this rubric to grade the final product:\n  final-project-rubric.xlsx  I am happy to give feedback and help along the way—please don’t hesitate to get help! My goal is for you to have a beautiful graphic in the end that you’ll want to show off to all your friends, family, neighbors, employers, and strangers on the street—I’m not trying to trip you up or give you trick questions!\nAnd that’s it. You’re done! Go out into the world now and make beautiful, insightful, and truthful graphics.\nGo forth and make awesomeness.\n Past examples Download a full example of what a final project might look like.\nHere are some great examples of student projects from past versions of this class.\nTravel runs in Yellowstone  Project description  Final PDF  Final PNG\n\n Firefighter fatalities  Project description  Final PDF  Final PNG\n\n Scripture use by The Killers  Project description  Final PDF  Final PNG\n\n Utah nonprofits  Project description  Final PDF  Final PNG\n\n Buckethead  Project description  Final PDF  Final PNG\n\n  ","date":1591315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591315200,"objectID":"8d16837a0c729f9c31150a71deaf1f1e","permalink":"/assignment/final-project/","publishdate":"2020-06-05T00:00:00Z","relpermalink":"/assignment/final-project/","section":"assignment","summary":"Data from the internet  Nonprofit management Federal, state, and local government management Business management  Instructions Final deliverables Past examples  Travel runs in Yellowstone Firefighter fatalities Scripture use by The Killers Utah nonprofits Buckethead    You made it to the end of our whirlwind tour of data visualization principles! Congratulations!\nNow you get to show off all the tools you learned with a beautiful, truthful, narrative visualization.","tags":null,"title":"Final project","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Lord of the Rings Turning everything in   Getting started You’ll be doing all your R work in R Markdown this time (and from now on). You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nYou’ll need to download three CSV files and put them somewhere on your computer or upload them to RStudio.cloud—preferably in a folder named data in your project folder:\n  The_Fellowship_Of_The_Ring.csv  The_Two_Towers.csv  The_Return_Of_The_King.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise. Download that here and include it in your project:\n  03-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 03-exercise.Rmd your-project-name.Rproj data\\ The_Fellowship_Of_The_Ring.csv The_Two_Towers.csv The_Return_Of_The_King.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  03-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Lord of the Rings Answer the following questions:\n Use group_by() and summarize() on the lotr data to find the total number of words spoken by race. Don’t worry about plotting it. How many words did male hobbits say in the movies?\n Use group_by() and summarize() to answer these questions with bar plots (geom_col())\n Does a certain race dominate the entire trilogy? (hint: group by Race)\n Does a certain gender dominate a movie? (lolz of course it does, but still, graph it) (Hint: group by both Gender and Film.) Experiment with filling by Gender or Film and faceting by Gender or Film.\n Does the dominant race differ across the three movies? (Hint: group by both Race and Film.) Experiment with filling by Race or Film and faceting by Race or Film.\n  Create a plot that visualizes the number of words spoken by race, gender, and film simultaneously. Use the complete tidy lotr data frame. You don’t need to create a new summarized dataset (with group_by(Race, Gender, Film)) because the original data already has a row for each of those (you could make a summarized dataset, but it would be identical to the full version).\nYou need to show Race, Gender, and Film at the same time, but you only have two possible aesthetics (x and fill), so you’ll also need to facet by the third. Play around with different combinations (e.g. try x = Race, then x = Film) until you find one that tells the clearest story. For fun, add a labs() layer to add a title and subtitle and caption.\n  You’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1589328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589328000,"objectID":"a49092d63e9e17433d06e649eff932b8","permalink":"/assignment/03-exercise/","publishdate":"2020-05-13T00:00:00Z","relpermalink":"/assignment/03-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Lord of the Rings Turning everything in   Getting started You’ll be doing all your R work in R Markdown this time (and from now on). You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.","tags":null,"title":"Mapping data to graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load and clean data Histograms Points Boxplots Summaries    For this example, I’m going to use real world data to demonstrate the typical process for loading data, cleaning it up a bit, and mapping specific columns of the data onto the parts of a graph using the grammar of graphics and ggplot().\nThe data I’ll use comes from the BBC’s corporate charity, BBC Children in Need, which makes grants to smaller UK nonprofit organizations that work on issues related to childhood poverty. An organization in the UK named 360Giving helps nonprofits and foundations publish data about their grant giving activities in an open and standardized way, and (as of May 2020) they list data from 126 different charities, including BBC Children in Need.\nIf you want to follow along with this example (highly recommended!), you can download the data directly from 360Giving or by using this link:\n  360-giving-data.xlsx  Live coding example Warning: I got carried away with this because I wanted to make it as comprehensive and detailed as possible, so it starts off with nothing and walks through the process of downloading data, creating a new project, and getting everything started. As such, it is ridiculously long (1 hour 😱 😱). Remember that there’s no requirement that you watch these things—they’re simply for your reference so you can see what doing this R stuff looks like in real time. The content all below the video is roughly the same (more polished even).\nThat said, it is a useful demonstration of how to get everything started and what it looks like to do an entire analysis, so there is value in it. Watch just the first part, or watch it on 2x or something.\nAnd I promise future examples will not be this long!\n   Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and clean data First, we need to load a few libraries: tidyverse (as always), along with readxl for reading Excel files and lubridate for working with dates:\n# Load libraries library(tidyverse) # For ggplot, dplyr, and friends library(readxl) # For reading Excel files library(lubridate) # For working with dates We’ll then load the original Excel file. I placed this file in a folder named data in my RStudio Project folder for this example. I like to read original data into an object named whatever_raw just in case it takes a long time to load (that way I don’t have to keep reloading it every time I add a new column or do anything else with it). It’s also good practice to keep a pristine, untouched copy of your data.\n# Load the original Excel file bbc_raw \u0026lt;- read_excel(\u0026quot;data/360-giving-data.xlsx\u0026quot;) There may be some errors reading the file—you can ignore those in this case.\nNext we’ll add a couple columns and clean up the data a little. In the video I did this non-linearly—I came back to the top of the document to add columns when I needed them and then reran the chunk to create the data.\nWe’ll extract the year from the Award Date column, rename some of the longer-named columns, and make a new column that shows the duration of grants. We’ll also get rid of 2015 since there are so few observations then.\nNote the strange use of `s around column names like `Award Date`. This is because R technically doesn’t allow special characters like spaces in column names. If there are spaces, you have to wrap the column names in backticks. Because typing backticks all the time gets tedious, we’ll use rename() to rename some of the columns:\nbbc \u0026lt;- bbc_raw %\u0026gt;% # Extract the year from the award date mutate(grant_year = year(`Award Date`)) %\u0026gt;% # Rename some columns rename(grant_amount = `Amount Awarded`, grant_program = `Grant Programme:Title`, grant_duration = `Planned Dates:Duration (months)`) %\u0026gt;% # Make a new text-based version of the duration column, recoding months # between 12-23, 23-35, and 36+. The case_when() function here lets us use # multiple if/else conditions at the same time. mutate(grant_duration_text = case_when( grant_duration \u0026gt;= 12 \u0026amp; grant_duration \u0026lt; 24 ~ \u0026quot;1 year\u0026quot;, grant_duration \u0026gt;= 24 \u0026amp; grant_duration \u0026lt; 36 ~ \u0026quot;2 years\u0026quot;, grant_duration \u0026gt;= 36 ~ \u0026quot;3 years\u0026quot; )) %\u0026gt;% # Get rid of anything before 2016 filter(grant_year \u0026gt; 2015) %\u0026gt;% # Make a categorical version of the year column mutate(grant_year_category = factor(grant_year))  Histograms First let’s look at the distribution of grant amounts with a histogram. Map grant_amount to the x-axis and don’t map anything to the y-axis, since geom_histogram() will calculate the y-axis values for us:\nggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Notice that ggplot warns you about bin widths. By default it will divide the data into 30 equally spaced bins, which will most likely not be the best for your data. You should always set your own bin width to something more appropriate. There are no rules for correct bin widths. Just don’t have them be too wide:\nggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram(binwidth = 100000) Or too small:\nggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram(binwidth = 500) £10,000 seems to fit well. It’s often helpful to add a white border to the histogram bars, too:\nggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram(binwidth = 10000, color = \u0026quot;white\u0026quot;) We can map other variables onto the plot, like mapping grant_year_category to the fill aesthetic:\nggplot(bbc, aes(x = grant_amount, fill = grant_year_category)) + geom_histogram(binwidth = 10000, color = \u0026quot;white\u0026quot;) That gets really hard to interpret though, so we can facet by year with facet_wrap():\nggplot(bbc, aes(x = grant_amount, fill = grant_year_category)) + geom_histogram(binwidth = 10000, color = \u0026quot;white\u0026quot;) + facet_wrap(vars(grant_year)) Neat!\n Points Next let’s look at the data using points, mapping year to the x-axis and grant amount to the y-axis:\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point() We have some serious overplotting here, with dots so thick that it looks like lines. We can fix this a couple different ways. First, we can make the points semi-transparent using alpha, which ranges from 0 (completely invisible) to 1 (completely solid).\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point(alpha = 0.1) We can also randomly space the points to spread them out using position_jitter():\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point(position = position_jitter()) One issue with this, though, is that the points are jittered along the x-axis (which is fine, since they’re all within the same year) and the y-axis (which is bad, since the amounts are actual numbers). We can tell ggplot to only jitter in one direction by specifying the height argument—we don’t want any up-and-down jittering:\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point(position = position_jitter(height = 0)) There are some weird clusters around £30,000 and below. Let’s map grant_program to the color aesthetic, which has two categories—regular grants and small grants—and see if that helps explain why:\nggplot(bbc, aes(x = grant_year_category, y = grant_amount, color = grant_program)) + geom_point(position = position_jitter(height = 0)) It does! We appear to have two different distributions of grants: small grants have a limit of £30,000, while regular grants have a much higher average amount.\n Boxplots We can add summary information to the plot by only changing the geom we’re using. Switch from geom_point() to geom_boxplot():\nggplot(bbc, aes(x = grant_year_category, y = grant_amount, color = grant_program)) + geom_boxplot()  Summaries We can also make smaller summarized datasets with dplyr functions like group_by() and summarize() and plot those. First let’s look at grant totals, averages, and counts over time:\nbbc_by_year \u0026lt;- bbc %\u0026gt;% group_by(grant_year) %\u0026gt;% # Make invisible subgroups for each year summarize(total = sum(grant_amount), # Find the total awarded in each group avg = mean(grant_amount), # Find the average awarded in each group number = n()) # n() is a special function that shows the number of rows in each group # Look at our summarized data bbc_by_year ## # A tibble: 4 x 4 ## grant_year total avg number ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2016 17290488 78238. 221 ## 2 2017 62394278 59765. 1044 ## 3 2018 61349392 60205. 1019 ## 4 2019 41388816 61136. 677 Because we used summarize(), R shrank our data down significantly. We now only have a row for each of the subgroups we made: one for each year. We can plot this smaller data. We’ll use geom_col() for now (but in tomorrow’s session you’ll learn why this is actually bad for averages!)\n# Plot our summarized data ggplot(bbc_by_year, aes(x = grant_year, y = avg)) + geom_col() ggplot(bbc_by_year, aes(x = grant_year, y = total)) + geom_col() ggplot(bbc_by_year, aes(x = grant_year, y = number)) + geom_col() Based on these charts, it looks like 2016 saw the largest average grant amount. In all other years, grants averaged around £60,000, but in 2016 it jumped up to £80,000. If we look at total grants, though, we can see that there were far fewer grants awarded in 2016—only 221! 2017 and 2018 were much bigger years with far more money awarded.\nWe can also use multiple aesthetics to reveal more information from the data. First we’ll make a new small summary dataset and group by both year and grant program. With those groups, we’ll again calculate the total, average, and number.\nbbc_year_size \u0026lt;- bbc %\u0026gt;% group_by(grant_year, grant_program) %\u0026gt;% summarize(total = sum(grant_amount), avg = mean(grant_amount), number = n()) bbc_year_size ## # A tibble: 8 x 5 ## # Groups: grant_year [4] ## grant_year grant_program total avg number ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2016 Main Grants 16405586 86345. 190 ## 2 2016 Small Grants 884902 28545. 31 ## 3 2017 Main Grants 48502923 90154. 538 ## 4 2017 Small Grants 13891355 27453. 506 ## 5 2018 Main Grants 47347789 95652. 495 ## 6 2018 Small Grants 14001603 26721. 524 ## 7 2019 Main Grants 33019492 96267. 343 ## 8 2019 Small Grants 8369324 25058. 334 Next we’ll plot the data, mapping the grant_program column to the fill aesthetic:\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col() By default, ggplot will stack the different fill colors within the same bar, but this makes it a little hard to make comparisons. While we can see that the average small grant amount was a little bigger in 2017 than in 2019, it’s harder to compare average main grant amount, since the bottoms of those sections don’t align.\nTo fix this, we can use position_dodge() to tell the columns to fit side-by-side:\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col(position = position_dodge()) Instead of dodging, we can also facet by grant_program to separate the bars:\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col() + facet_wrap(vars(grant_program)) We can put these in one column if we want:\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col() + facet_wrap(vars(grant_program), ncol = 1) Finally, we can include even more variables! We have a lot of aesthetics we can work with (size, alpha, color, fill, linetype, etc.), as well as facets, so let’s add one more to show the duration of the awarded grant.\nFirst we’ll make another smaller summarized dataset, grouping by year, program, and duration and summarizing the total, average, and number of awards.\nbbc_year_size_duration \u0026lt;- bbc %\u0026gt;% group_by(grant_year, grant_program, grant_duration_text) %\u0026gt;% summarize(total = sum(grant_amount), avg = mean(grant_amount), number = n()) bbc_year_size_duration ## # A tibble: 21 x 6 ## # Groups: grant_year, grant_program [8] ## grant_year grant_program grant_duration_text total avg number ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2016 Main Grants 2 years 97355 48678. 2 ## 2 2016 Main Grants 3 years 16308231 86746. 188 ## 3 2016 Small Grants 3 years 884902 28545. 31 ## 4 2017 Main Grants 1 year 59586 29793 2 ## 5 2017 Main Grants 2 years 825732 82573. 10 ## 6 2017 Main Grants 3 years 47617605 90528. 526 ## 7 2017 Small Grants 1 year 10000 10000 1 ## 8 2017 Small Grants 2 years 245227 18864. 13 ## 9 2017 Small Grants 3 years 13636128 27716. 492 ## 10 2018 Main Grants 1 year 118134 59067 2 ## # … with 11 more rows Next, we’ll fill by grant program and facet by duration and show the total number of grants awarded\nggplot(bbc_year_size_duration, aes(x = grant_year, y = number, fill = grant_program)) + geom_col(position = position_dodge(preserve = \u0026quot;single\u0026quot;)) + facet_wrap(vars(grant_duration_text), ncol = 1) The vast majority of BBC Children in Need’s grants last for 3 years. Super neat.\n  ","date":1589328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589328000,"objectID":"73664027ae41c739f0c70a62c901a4c5","permalink":"/example/03-example/","publishdate":"2020-05-13T00:00:00Z","relpermalink":"/example/03-example/","section":"example","summary":"Live coding example Complete code  Load and clean data Histograms Points Boxplots Summaries    For this example, I’m going to use real world data to demonstrate the typical process for loading data, cleaning it up a bit, and mapping specific columns of the data onto the parts of a graph using the grammar of graphics and ggplot().\nThe data I’ll use comes from the BBC’s corporate charity, BBC Children in Need, which makes grants to smaller UK nonprofit organizations that work on issues related to childhood poverty.","tags":null,"title":"Mapping data to graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Part 1: Data visualization with ggplot2 Part 2: Reshaping data with tidyr   Part 1: Data visualization with ggplot2 For the first part of today’s lesson, you need to work through RStudio’s introductory primers for ggplot2. You’ll do these in your browser and type code and see results there.\nIt seems like there are a lot, but they’re short and go fairly quickly (especially as you get the hang of the ggplot() syntax). Complete these:\n Visualize Data  Exploratory Data Analysis Bar Charts Histograms Boxplots and Counts Scatterplots Line plots Overplotting and Big Data Customize Your Plots    Part 2: Reshaping data with tidyr For the last part of today’s lesson, you’ll work through just one RStudio primer to learn how to use the tidyr package to reshape data from wide to long and back to wide.\nComplete this:\n Tidy Your Data  Reshape Data   Recent versions of tidyr have renamed these core functions: gather() is now pivot_longer() and spread() is now pivot_wider(). The syntax for these pivot_*() functions is slightly different from what it was in gather() and spread(), so you can’t just replace the names. Fortunately, both gather() and spread() still work and won’t go away for a while, so you can still use them as you learn about reshaping and tidying data. It would be worth learning how the newer pivot_*() functions work, eventually, though (see here for examples).\n  ","date":1589328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589328000,"objectID":"e88a62444161c21a7f4779be93acbf33","permalink":"/lesson/03-lesson/","publishdate":"2020-05-13T00:00:00Z","relpermalink":"/lesson/03-lesson/","section":"lesson","summary":"Part 1: Data visualization with ggplot2 Part 2: Reshaping data with tidyr   Part 1: Data visualization with ggplot2 For the first part of today’s lesson, you need to work through RStudio’s introductory primers for ggplot2. You’ll do these in your browser and type code and see results there.\nIt seems like there are a lot, but they’re short and go fairly quickly (especially as you get the hang of the ggplot() syntax).","tags":null,"title":"Mapping data to graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms  Document: A Markdown file where you type stuff\n Chunk: A piece of R code that is included in your document. It looks like this:\n```{r} # Code goes here ``` There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n Knit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n   Add chunks There are three ways to insert chunks:\n Press ⌘⌥I on macOS or control + alt + I on Windows\n Click on the “Insert” button at the top of the editor window\n Manually type all the backticks and curly braces (don’t do this)\n   Chunk names You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk} # Code goes here ```  Chunk options There are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE} # Code goes here ``` The most common chunk options are these:\n fig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures echo=FALSE: The code is not shown in the final document, but the results are message=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted warning=FALSE: Any warnings that R generates are omitted include=FALSE: The chunk still runs, but the code and results are not included in the final document  You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n Inline chunks You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE} avg_mpg \u0026lt;- mean(mtcars$mpg) ``` The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon. … would knit into this:\n The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n  Output formats You can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot; output: html_document: default pdf_document: default word_document: default You can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n--- title: \u0026quot;My document\u0026quot; author: \u0026quot;My name\u0026quot; date: \u0026quot;January 13, 2020\u0026quot; output: html_document: toc: yes fig_caption: yes fig_height: 8 fig_width: 10 pdf_document: latex_engine: xelatex # More modern PDF typesetting engine toc: yes word_document: toc: yes fig_caption: yes fig_height: 4 fig_width: 5 ---  ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578873600,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   Learning R R in the wild   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n R for Data Science: A free online book for learning the basics of R and the tidyverse. R and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things. Stat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online. STA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. CSE 631: Principles \u0026amp; Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.   R in the wild A popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.1 Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\nHere are some of the best examples I’ve come across:\n Text analysis of Trump’s tweets confirms he writes only the (angrier) Android half (with a follow-up) Bob Ross - Joy of Painting Bechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight. Sexism on the Silver Screen: Exploring film’s gender divide Comparison of Quentin Tarantino Movies by Box Office and the Bechdel Test Who came to vote in Utah’s caucuses? Health care indicators in Utah counties Song lyrics across the United States A decade (ish) of listening to Sigur Rós When is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here. Mapping Fall Foliage General (Attys) Distributions Disproving Approval    If you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That’s actually how this site is built (see the source code). You can build your own site with this tutorial.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f6270d48011ac62645a8455a86a24bf","permalink":"/resource/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/r/","section":"resource","summary":"Learning R R in the wild   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.","tags":null,"title":"R","type":"docs"},{"authors":null,"categories":null,"content":"   R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n Main style things to pay attention to for this class  Important note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n Spacing  See the “Spacing” section in the tidyverse style guide.\n Put spaces after commas (like in regular English):\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg , cty \u0026gt; 10) filter(mpg ,cty \u0026gt; 10) filter(mpg,cty \u0026gt; 10) Put spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg, cty\u0026gt;10) filter(mpg, cty\u0026gt; 10) filter(mpg, cty \u0026gt;10) Don’t put spaces around parentheses that are parts of functions:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter (mpg, cty \u0026gt; 10) filter ( mpg, cty \u0026gt; 10) filter( mpg, cty \u0026gt; 10 )  Long lines  See the “Long lines” section in the tidyverse style guide.\n It’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Bad filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;)) # Good filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))  Pipes (%\u0026gt;%) and ggplot layers (+) Put each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad and won\u0026#39;t even work ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() Put each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad and won\u0026#39;t even work mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))  Comments  See the “Comments” section in the tidyverse style guide.\n Comments should start with a comment symbol and a single space: #\n# Good #Bad #Bad If the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group You can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good # Happy families are all alike; every unhappy family is unhappy in its own way. # Everything was in confusion in the Oblonskys’ house. The wife had discovered # that the husband was carrying on an intrigue with a French girl, who had been # a governess in their family, and she had announced to her husband that she # could not go on living in the same house with him. This position of affairs # had now lasted three days, and not only the husband and wife themselves, but # all the members of their family and household, were painfully conscious of it. # Bad # Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Though, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Essential pandemic construction Turning everything in   Getting started The New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided an interactive map here where you can see the different projects. There’s also a link there to download the complete dataset.\nFor this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).\nYou’ll be doing all your R work in R Markdown this time (and from now on). You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nYou’ll need to download one CSV file and put it somewhere on your computer or upload it to RStudio.cloud—preferably in a folder named data in your project folder. You can download the data from the DOB’s map, or use this link to get it directly:\n  EssentialConstruction.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  04-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 04-exercise.Rmd your-project-name.Rproj data\\ EssentialConstruction.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  04-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Essential pandemic construction Make the following plots and briefly explain what they show:\n Show the count or proportion of approved projects by borough using a bar chart\n Show the count or proportion of approved projects by category using a lollipop chart\n Show the proportion of approved projects by borough and category simultaneously using a heatmap\n  The example for today’s session will be incredibly helpful for this exercise. Reference it.\nYou don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing fill colors with scale_fill_manual() or with viridis palettes.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"c45fa1fb46890d1b86efbc088ae863c7","permalink":"/assignment/04-exercise/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/assignment/04-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Essential pandemic construction Turning everything in   Getting started The New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided an interactive map here where you can see the different projects. There’s also a link there to download the complete dataset.\nFor this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).","tags":null,"title":"Amounts and proportions","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load data Wrangle data Bar plot Lollipop chart Strip plot Beeswarm plot Heatmap    For this example, we’re going to use real world data to demonstrate some different ways to visualize amounts and proportions. We’ll use data from the CDC and the Social Security Administration about the number of daily births in the United States from 1994–2014. FiveThirtyEight reported a story using this data in 2016 and they posted relatively CSV files on GitHub, so we can download and use those.\nIf you want to follow along with this example, you can download the data directly from GitHub or by using these links (you’ll likely need to right click on these and choose “Save Link As…”):\n  US_births_1994-2003_CDC_NCHS.csv  US_births_2000-2014_SSA.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad data There are two CSV files:\n US_births_1994-2003_CDC_NCHS.csv contains U.S. births data for the years 1994 to 2003, as provided by the Centers for Disease Control and Prevention’s National Center for Health Statistics. US_births_2000-2014_SSA.csv contains U.S. births data for the years 2000 to 2014, as provided by the Social Security Administration.  Since the two datasets overlap in 2000–2003, we use Social Security Administration data for those years.\nWe downloaded the data from GitHub and placed the CSV files in a folder named data. We’ll then load them with read_csv() and combine them into one data frame.\nlibrary(tidyverse) library(scales) # For nice labels in charts births_1994_1999 \u0026lt;- read_csv(\u0026quot;data/US_births_1994-2003_CDC_NCHS.csv\u0026quot;) %\u0026gt;% # Ignore anything after 2000 filter(year \u0026lt; 2000) births_2000_2014 \u0026lt;- read_csv(\u0026quot;data/US_births_2000-2014_SSA.csv\u0026quot;) births_combined \u0026lt;- bind_rows(births_1994_1999, births_2000_2014)  Wrangle data Let’s look at the first few rows of the data to see what we’re working with:\nhead(births_combined) ## # A tibble: 6 x 5 ## year month date_of_month day_of_week births ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1994 1 1 6 8096 ## 2 1994 1 2 7 7772 ## 3 1994 1 3 1 10142 ## 4 1994 1 4 2 11248 ## 5 1994 1 5 3 11053 ## 6 1994 1 6 4 11406 The columns for year and births seem straightforward and ready to use. The columns for month and day of the week could be improved if we changed them to text (i.e. January instead of 1; Tuesday instead of 3). To fix this, we can convert these columns to categorical variables, or factors in R. We can also specify that these categories (or factors) are ordered, meaning that Feburary comes after January, etc. Without ordering, R will plot them alphabetically, which isn’t very helpful.\nWe’ll make a new dataset named births that’s based on the combined births data, but with some new columns added:\n# The c() function lets us make a list of values month_names \u0026lt;- c(\u0026quot;January\u0026quot;, \u0026quot;February\u0026quot;, \u0026quot;March\u0026quot;, \u0026quot;April\u0026quot;, \u0026quot;May\u0026quot;, \u0026quot;June\u0026quot;, \u0026quot;July\u0026quot;, \u0026quot;August\u0026quot;, \u0026quot;September\u0026quot;, \u0026quot;October\u0026quot;, \u0026quot;November\u0026quot;, \u0026quot;December\u0026quot;) day_names \u0026lt;- c(\u0026quot;Monday\u0026quot;, \u0026quot;Tuesday\u0026quot;, \u0026quot;Wednesday\u0026quot;, \u0026quot;Thursday\u0026quot;, \u0026quot;Friday\u0026quot;, \u0026quot;Saturday\u0026quot;, \u0026quot;Sunday\u0026quot;) births \u0026lt;- births_combined %\u0026gt;% # Make month an ordered factor, using the month_name list as labels mutate(month = factor(month, labels = month_names, ordered = TRUE)) %\u0026gt;% mutate(day_of_week = factor(day_of_week, labels = day_names, ordered = TRUE), date_of_month_categorical = factor(date_of_month)) %\u0026gt;% # Add a column indicating if the day is on a weekend mutate(weekend = ifelse(day_of_week %in% c(\u0026quot;Saturday\u0026quot;, \u0026quot;Sunday\u0026quot;), TRUE, FALSE)) head(births) ## # A tibble: 6 x 7 ## year month date_of_month day_of_week births date_of_month_categori… weekend ## \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;lgl\u0026gt; ## 1 1994 January 1 Saturday 8096 1 TRUE ## 2 1994 January 2 Sunday 7772 2 TRUE ## 3 1994 January 3 Monday 10142 3 FALSE ## 4 1994 January 4 Tuesday 11248 4 FALSE ## 5 1994 January 5 Wednesday 11053 5 FALSE ## 6 1994 January 6 Thursday 11406 6 FALSE If you look at the data now, you can see the columns are changed and have different types. year and date_of_month are still numbers, but month, and day_of_week are ordered factors (ord) and date_of_month_categorical is a regular factor (fct). Technically it’s also ordered, but because it’s already alphabetical (i.e. 2 naturally comes after 1), we don’t need to force it to be in the right order.\nOur births data is now clean and ready to go!\n Bar plot First we can look at a bar chart showing the total number of births each day. We need to make a smaller summarized dataset and then we’ll plot it:\ntotal_births_weekday \u0026lt;- births %\u0026gt;% group_by(day_of_week) %\u0026gt;% summarize(total = sum(births)) ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, fill = day_of_week)) + geom_col() + # Turn off the fill legend because it\u0026#39;s redundant guides(fill = FALSE) If we fill by day of the week, we get 7 different colors, which is fine (I guess), but doesn’t really help tell a story. The main story here is that there are far fewer births during weekends. If we create a new column that flags if a row is Saturday or Sunday, we can fill by that column instead:\ntotal_births_weekday \u0026lt;- births %\u0026gt;% group_by(day_of_week) %\u0026gt;% summarize(total = sum(births)) %\u0026gt;% mutate(weekend = ifelse(day_of_week %in% c(\u0026quot;Saturday\u0026quot;, \u0026quot;Sunday\u0026quot;), TRUE, FALSE)) ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, fill = weekend)) + geom_col() Neat! Those default colors are kinda ugly, though, so let’s use the principles of preattentive processing and contrast to highlight the weekend bars:\nggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, fill = weekend)) + geom_col() + # Use grey and orange scale_fill_manual(values = c(\u0026quot;grey70\u0026quot;, \u0026quot;#f2ad22\u0026quot;)) + # Use commas instead of scientific notation scale_y_continuous(labels = comma) + # Turn off the legend since the title shows what the orange is guides(fill = FALSE) + labs(title = \u0026quot;Weekends are unpopular times for giving birth\u0026quot;, x = NULL, y = \u0026quot;Total births\u0026quot;)  Lollipop chart Since the ends of the bars are often the most important part of the graph, we can use a lollipop chart to emphasize them. We’ll keep all the same code from our bar chart and make a few changes:\n Color by weekend instead of fill by weekend, since points and lines are colored in ggplot, not filled Switch scale_fill_manual() to scale_color_manual() and turn off the color legend in the guides() layer Switch geom_col() to geom_pointrange(). The geom_pointrange() layer requires two additional aesthetics: ymin and ymax for the ends of the lines that come out of the point. Here we’ll set ymin to 0 so it starts at the x-axis, and we’ll set ymax to total so it ends at the point.  ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, color = weekend)) + geom_pointrange(aes(ymin = 0, ymax = total), # Make the lines a little thicker and the dots a little bigger fatten = 5, size = 1.5) + # Use grey and orange scale_color_manual(values = c(\u0026quot;grey70\u0026quot;, \u0026quot;#f2ad22\u0026quot;)) + # Use commas instead of scientific notation scale_y_continuous(labels = comma) + # Turn off the legend since the title shows what the orange is guides(color = FALSE) + labs(title = \u0026quot;Weekends are unpopular times for giving birth\u0026quot;, x = NULL, y = \u0026quot;Total births\u0026quot;)  Strip plot However, we want to #barbarplots! (Though they’re arguably okay here, since they show totals and not averages). Let’s show all the data with points. We’ll use the full dataset now, map x to weekday, y to births, and change geom_col() to geom_point(). We’ll tell geom_point() to jitter the points randomly.\nggplot(data = births, mapping = aes(x = day_of_week, y = births, color = weekend)) + scale_color_manual(values = c(\u0026quot;grey70\u0026quot;, \u0026quot;#f2ad22\u0026quot;)) + geom_point(size = 0.5, position = position_jitter(height = 0)) + guides(color = FALSE) There are some interesting points in the low ends, likely because of holidays like Labor Day and Memorial Day (for the Mondays) and Thanksgiving (for the Thursday). If we had a column that indicated whether a day was a holiday, we could color by that and it would probably explain most of those low numbers. Unfortunately we don’t have that column, and it’d be hard to make. Some holidays are constant (Halloween is always October 31), but some aren’t (Thanksgiving is the fourth Thursday in November, so we’d need to find out which November 20-somethingth each year is the fourth Thursday, and good luck doing that at scale).\n Beeswarm plot We can add some structure to these points if we use the ggbeeswarm package, with either geom_beeswarm() or geom_quasirandom(). geom_quasirandom() actually works better here since there are so many points—geom_beeswarm() makes the clusters of points way too wide.\nlibrary(ggbeeswarm) ggplot(data = births, mapping = aes(x = day_of_week, y = births, color = weekend)) + scale_color_manual(values = c(\u0026quot;grey70\u0026quot;, \u0026quot;#f2ad22\u0026quot;)) + # Make these points suuuper tiny geom_quasirandom(size = 0.0001) + guides(color = FALSE)  Heatmap Finally, let’s use something non-traditional to show the average births by day in a somewhat proportional way. We can calculate the average number of births every day and then make a heatmap that fills each square by that average, thus showing the relative differences in births per day.\nTo do this, we need to make a summarized data frame with group_by() %\u0026gt;% summarize() to calculate the average number of births by month and day of the month (i.e. average for January 1, January 2, etc.).\nWe’ll then make a sort of calendar with date of the month on the x axis, month on the y axis, with heat map squares filled by the daily average. We’ll use geom_tile() to add squares for each day, and then add some extra scale, coordinates, and theme layers to clean up the plot:\navg_births_month_day \u0026lt;- births %\u0026gt;% group_by(month, date_of_month_categorical) %\u0026gt;% summarize(avg_births = mean(births)) ggplot(data = avg_births_month_day, # By default, the y-axis will have December at the top, so use fct_rev() to reverse it mapping = aes(x = date_of_month_categorical, y = fct_rev(month), fill = avg_births)) + geom_tile() + # Add viridis colors scale_fill_viridis_c(option = \u0026quot;inferno\u0026quot;, labels = comma) + # Add nice labels labs(x = \u0026quot;Day of the month\u0026quot;, y = NULL, title = \u0026quot;Average births per day\u0026quot;, subtitle = \u0026quot;1994-2014\u0026quot;, fill = \u0026quot;Average births\u0026quot;) + # Force all the tiles to have equal widths and heights coord_equal() + # Use a cleaner theme theme_minimal() Neat! There are some really interesting trends here. Most obvious, probably, is that very few people are born on New Year’s Day, July 4th, Halloween, Thanksgiving, and Christmas.\navg_births_month_day %\u0026gt;% arrange(avg_births) ## # A tibble: 366 x 3 ## # Groups: month [12] ## month date_of_month_categorical avg_births ## \u0026lt;ord\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 December 25 6601. ## 2 January 1 7827. ## 3 December 24 8103. ## 4 July 4 8825. ## 5 January 2 9356. ## 6 December 26 9599. ## 7 November 27 9770. ## 8 November 23 9919. ## 9 November 25 10001 ## 10 October 31 10030. ## # … with 356 more rows The days with the highest average are in mid-September (lol my birthday is #2), likely because that’s about 9 months after the first week of January. July 7th at #7 is odd and I have no idea why it might be so popular 🤷.\navg_births_month_day %\u0026gt;% arrange(desc(avg_births)) ## # A tibble: 366 x 3 ## # Groups: month [12] ## month date_of_month_categorical avg_births ## \u0026lt;ord\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 September 9 12344. ## 2 September 19 12285. ## 3 September 12 12282. ## 4 September 17 12201. ## 5 September 10 12190. ## 6 September 20 12162. ## 7 July 7 12147. ## 8 September 15 12126. ## 9 September 16 12114. ## 10 September 18 12112. ## # … with 356 more rows The funniest trend is the very visible dark column for the 13th of every month. People really don’t want to give birth on the 13th.\n  ","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"c977a82d5f9c9c6bf14f43f56de0b41e","permalink":"/example/04-example/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/example/04-example/","section":"example","summary":"Live coding example Complete code  Load data Wrangle data Bar plot Lollipop chart Strip plot Beeswarm plot Heatmap    For this example, we’re going to use real world data to demonstrate some different ways to visualize amounts and proportions. We’ll use data from the CDC and the Social Security Administration about the number of daily births in the United States from 1994–2014. FiveThirtyEight reported a story using this data in 2016 and they posted relatively CSV files on GitHub, so we can download and use those.","tags":null,"title":"Amounts and proportions","type":"docs"},{"authors":null,"categories":null,"content":"   Manipulating data with dplyr  Filtering with filter() Adding new columns with mutate() Combining multiple verbs with pipes (%\u0026gt;%) Summarizing data by groups with group_by() %\u0026gt;% summarize() Selecting with select() Arranging data with arrange() That’s it!  Changing colors, shapes, and sizes, with scale_*()   When you visualize proportions with ggplot, you’ll typically go through a two-step process:\nSummarize the data with dplyr (typically with a combination of group_by() and summarize()) Plot the summarized data  Manipulating data with dplyr You had some experience with dplyr functions in the RStudio primers, but we’ll briefly review them here.\nThere are 6 important verbs that you’ll typically use when working with data:\n Extract rows/cases with filter() Extract columns/variables with select() Arrange/sort rows with arrange() Make new columns/variables with mutate() Make group summaries with group_by %\u0026gt;% summarize()  Every dplyr verb follows the same pattern. The first argument is always a data frame, and the function always returns a data frame:\nVERB(DATA_TO_TRANSFORM, STUFF_IT_DOES) Filtering with filter() The filter() function takes two arguments: a data frame to transform, and a set of tests. It will return each row for which the test is TRUE.\nThis code, for instance, will look at the gapminder dataset and return all rows where country is equal to “Denmark”:\nfilter(gapminder, country == \"Denmark\")  ## # A tibble: 12 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Denmark Europe 1952 70.8 4334000 9692. ## 2 Denmark Europe 1957 71.8 4487831 11100. ## 3 Denmark Europe 1962 72.4 4646899 13583. ## 4 Denmark Europe 1967 73.0 4838800 15937. ## 5 Denmark Europe 1972 73.5 4991596 18866. ## 6 Denmark Europe 1977 74.7 5088419 20423. ## 7 Denmark Europe 1982 74.6 5117810 21688. ## 8 Denmark Europe 1987 74.8 5127024 25116. ## 9 Denmark Europe 1992 75.3 5171393 26407. ## 10 Denmark Europe 1997 76.1 5283663 29804. ## 11 Denmark Europe 2002 77.2 5374693 32167. ## 12 Denmark Europe 2007 78.3 5468120 35278.  Notice that there are two equal signs (==). This is because it’s a logical test, similar to greater than (\u0026gt;) or less than (\u0026lt;). When you use a single equal sign, you set an argument (like data = gapminder); when you use two, you are doing a test. There are lots of different ways to do logical tests:\n  Test Meaning    x \u0026lt; y Less than  x \u0026gt; y Greater than  x == y Equal to  x \u0026lt;= y Less than or equal to  x \u0026gt;= y Greater than or equal to  x != y Not equal to  x %in% y In (group membership)  is.na(x) Is missing  !is.na(x) Is not missing    Your turn: Use filter() and logical tests to show:\nThe data for Canada All data for countries in Oceania Rows where life expectancy is greater than 82    You can also use multiple conditions, and these will extract rows that meet every test. By default, if you separate the tests with a comma, R will consider this an “and” test and find rows that are both Denmark and greater than 2000.\nfilter(gapminder, country == \"Denmark\", year  2000)  ## # A tibble: 2 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Denmark Europe 2002 77.2 5374693 32167. ## 2 Denmark Europe 2007 78.3 5468120 35278.  You can also use “or” with “|” and “not” with “!”:\n  Operator Meaning    a \u0026amp; b and  a | b or  !a not    Your turn: Use filter() and logical tests to show:\nCanada before 1970 Countries where life expectancy in 2007 is below 50 Countries where life expectancy in 2007 is below 50 and are not in Africa    Beware of some common mistakes! You can’t collapse multiple tests into one. Instead, use two separate tests:\n# This won\u0026#39;t work! filter(gapminder, 1960 \u0026lt; year \u0026lt; 1980) # This will work filter(gapminder, 1960 \u0026lt; year, year \u0026lt; 1980) Also, you can avoid stringing together lots of tests by using the %in% operator, which checks to see if a value is in a list of values.\n# This works, but is tedious filter(gapminder, country == \u0026quot;Mexico\u0026quot; | country == \u0026quot;Canada\u0026quot; | country == \u0026quot;United States\u0026quot;) # This is more concise and easier to add other countries later filter(gapminder, country %in% c(\u0026quot;Mexico\u0026quot;, \u0026quot;Canada\u0026quot;, \u0026quot;United States\u0026quot;))  Adding new columns with mutate() You create new columns with the mutate() function. You can create a single column like this:\nmutate(gapminder, gdp = gdpPercap * pop)  ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap gdp ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. ## # … with 1,694 more rows  And you can create multiple columns by including a comma-separated list of new columns to create:\nmutate(gapminder, gdp = gdpPercap * pop,\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;pop_mill = round(pop / 1000000))  ## # A tibble: 1,704 x 8 ## country continent year lifeExp pop gdpPercap gdp pop_mill ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. 8 ## 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. 9 ## 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. 10 ## 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. 12 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. 13 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. 15 ## 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. 13 ## 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. 14 ## 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. 16 ## 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. 22 ## # … with 1,694 more rows  You can also do conditional tests within mutate() using the ifelse() function. This works like the =IFELSE function in Excel. Feed the function three arguments: (1) a test, (2) the value if the test is true, and (3) the value if the test is false:\nifelse(TEST, VALUE_IF_TRUE, VALUE_IF_FALSE) We can create a new column that is a binary indicator for whether the country’s row is after 1960:\nmutate(gapminder, after_1960 = ifelse(year  1960, TRUE, FALSE))  ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap after_1960 ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. FALSE ## 2 Afghanistan Asia 1957 30.3 9240934 821. FALSE ## 3 Afghanistan Asia 1962 32.0 10267083 853. TRUE ## 4 Afghanistan Asia 1967 34.0 11537966 836. TRUE ## 5 Afghanistan Asia 1972 36.1 13079460 740. TRUE ## 6 Afghanistan Asia 1977 38.4 14880372 786. TRUE ## 7 Afghanistan Asia 1982 39.9 12881816 978. TRUE ## 8 Afghanistan Asia 1987 40.8 13867957 852. TRUE ## 9 Afghanistan Asia 1992 41.7 16317921 649. TRUE ## 10 Afghanistan Asia 1997 41.8 22227415 635. TRUE ## # … with 1,694 more rows  We can also use text labels instead of TRUE and FALSE:\nmutate(gapminder, \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;after_1960 = ifelse(year  1960, \"After 1960\", \"Before 1960\"))  ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap after_1960 ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. Before 1960 ## 2 Afghanistan Asia 1957 30.3 9240934 821. Before 1960 ## 3 Afghanistan Asia 1962 32.0 10267083 853. After 1960 ## 4 Afghanistan Asia 1967 34.0 11537966 836. After 1960 ## 5 Afghanistan Asia 1972 36.1 13079460 740. After 1960 ## 6 Afghanistan Asia 1977 38.4 14880372 786. After 1960 ## 7 Afghanistan Asia 1982 39.9 12881816 978. After 1960 ## 8 Afghanistan Asia 1987 40.8 13867957 852. After 1960 ## 9 Afghanistan Asia 1992 41.7 16317921 649. After 1960 ## 10 Afghanistan Asia 1997 41.8 22227415 635. After 1960 ## # … with 1,694 more rows  Your turn: Use mutate() to:\nAdd an africa column that is TRUE if the country is on the African continent Add a column for logged GDP per capita Add an africa_asia column that says “Africa or Asia” if the country is in Africa or Asia, and “Not Africa or Asia” if it’s not     Combining multiple verbs with pipes (%\u0026gt;%) What if you want to filter to include only rows from 2002 and make a new column with the logged GDP per capita? Doing this requires both filter() and mutate(), so we need to find a way to use both at once.\nOne solution is to use intermediate variables for each step:\ngapminder_2002_filtered gapminder_2002_logged gapminder_2002_filtered, log_gdpPercap = log(gdpPercap)) That works fine, but your environment panel will start getting full of lots of intermediate data frames.\nAnother solution is to nest the functions inside each other. Remember that all dplyr functions return data frames, so you can feed the results of one into another:\nfilter(mutate(gapminder, log_gdpPercap = log(gdpPercap)), \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;year == 2002) That works too, but it gets really complicated once you have even more functions, and it’s hard to keep track of which function’s arguments go where. I’d avoid doing this entirely.\nOne really nice solution is to use a pipe, or %\u0026gt;%. The pipe takes an object on the left and passes it as the first argument of the function on the right.\n# gapminder will automatically get placed in the _____ spot gapminder %\u0026gt;% filter(_____, country == \u0026quot;Canada\u0026quot;) These two lines of code do the same thing:\nfilter(gapminder, country == \"Canada\")\ngapminder %% filter(country == \"Canada\") Using pipes, you can start with a data frame, pass it to one verb, then pass the output of that verb to the next verb, and so on. When reading any code with a %\u0026gt;%, it’s easiest to read the %\u0026gt;% as “and then”. This would read:\n Take the gapminder dataset and then filter it so that it only has rows from 2002 and then add a new column with the logged GDP per capita\n gapminder %\u0026gt;% filter(year == 2002) %\u0026gt;% mutate(log_gdpPercap = log(gdpPercap)) Here’s another way to think about pipes more conceptually. This isn’t valid R code, obviously, but imagine you’re going to take yourself, and then wake up, get out of bed, get dressed, and leave the house. Writing that whole process as nested functions would look like this:\nleave_house(get_dressed(get_out_of_bed(wake_up(me, time = \"8:00\"), side = \"correct\"), pants = TRUE, shirt = TRUE), car = TRUE, bike = FALSE) Instead of nesting everything, we can use pipes to chain these together. This would read\n Take myself, and then wake up at 8:00, and then get out of bed on the correct side, and then get dressed with pants and a shirt, and then leave the house in a car\n me %% \u0026nbsp;\u0026nbsp;wake_up(time = \"8:00\") %% \u0026nbsp;\u0026nbsp;get_out_of_bed(side = \"correct\") %% \u0026nbsp;\u0026nbsp;get_dressed(pants = TRUE, shirt = TRUE) %% \u0026nbsp;\u0026nbsp;leave_house(car = TRUE, bike = FALSE)  Summarizing data by groups with group_by() %\u0026gt;% summarize() The summarize() verb takes an entire frame and calculates summary information about it. For instance, this will find the average life expectancy for the whole gapminder data:\ngapminder %% summarize(mean_life = mean(lifeExp))  ## # A tibble: 1 x 1 ## mean_life ## \u0026lt;dbl\u0026gt; ## 1 59.5  You can also make multiple summary variables, just like mutate(), and it will return a column for each:\ngapminder %% summarize(mean_life = mean(lifeExp),\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;min_life = min(lifeExp))  ## # A tibble: 1 x 2 ## mean_life min_life ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 59.5 23.6  Your turn: Use summarize() to calculate:\nThe first (minimum) year in the gapminder dataset The last (maximum) year in the dataset The number of rows in the dataset (use the dplyr cheatsheet) The number of distinct countries in the dataset (use the dplyr cheatsheet)    Your turn: Use filter() and summarize() to calculate the median life expectancy on the African continent in 2007:\n  Notice that summarize() on its own summarizes the whole dataset, so you only get a single row back. These values are the averages and minimums for the entire data frame. If you group your data into separate subgroups, you can use summarize() to calculate summary statistics for each group. Do this with group_by().\nThe group_by() function puts rows into groups based on values in a column. If you run this:\ngapminder %% group_by(continent)  ## # A tibble: 1,704 x 6 ## # Groups: continent [5] ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows  …you won’t see anything different! R has put the dataset into separate invisible groups behind the scenes, but you haven’t done anything with those groups, so nothing has really happened. If you do things with those groups with summarize(), though, group_by() becomes much more useful.\nFor instance, this will take the gapminder data frame, group it by continent, and then summarize it by calculating the number of distinct countries in each group. It will return one row for each group, so there should be a row for each continent:\ngapminder %\u0026gt;% group_by(continent) %\u0026gt;% summarize(n_countries = n_distinct(country))  ## # A tibble: 5 x 2 ## continent n_countries ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Africa 52 ## 2 Americas 25 ## 3 Asia 33 ## 4 Europe 30 ## 5 Oceania 2 You can calculate multiple summary statistics, as before:\ngapminder %\u0026gt;% group_by(continent) %\u0026gt;% summarize(n_countries = n_distinct(country), avg_life_exp = mean(lifeExp))  ## # A tibble: 5 x 3 ## continent n_countries avg_life_exp ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 52 48.9 ## 2 Americas 25 64.7 ## 3 Asia 33 60.1 ## 4 Europe 30 71.9 ## 5 Oceania 2 74.3 Your turn: Find the minimum, maximum, and median life expectancy for each continent:\n  Your turn: Find the minimum, maximum, and median life expectancy for each continent in 2007 only:\n  Finally, you can group by multiple columns and R will create subgroups for every combination of the groups and return the number of rows of combinations. For instance, we can calculate the average life expectancy by both year and continent and we’ll get 60 rows, since there are 5 continents and 12 years (5 × 12 = 60):\ngapminder %\u0026gt;% group_by(continent, year) %\u0026gt;% summarize(avg_life_exp = mean(lifeExp))  ## # A tibble: 60 x 3 ## # Groups: continent [5] ## continent year avg_life_exp ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 1952 39.1 ## 2 Africa 1957 41.3 ## 3 Africa 1962 43.3 ## 4 Africa 1967 45.3 ## 5 Africa 1972 47.5 ## 6 Africa 1977 49.6 ## 7 Africa 1982 51.6 ## 8 Africa 1987 53.3 ## 9 Africa 1992 53.6 ## 10 Africa 1997 53.6 ## # … with 50 more rows  Selecting with select() The last two verbs are far simpler than filter(), mutate(), and group_by() %\u0026gt;% summarize().\nYou can choose specific columns with the select() verb. This will only keep two columns: lifeExp and year:\ngapminder %% select(lifeExp, year)  ## # A tibble: 1,704 x 2 ## lifeExp year ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 28.8 1952 ## 2 30.3 1957 ## 3 32.0 1962 ## 4 34.0 1967 ## 5 36.1 1972 ## 6 38.4 1977 ## 7 39.9 1982 ## 8 40.8 1987 ## 9 41.7 1992 ## 10 41.8 1997 ## # … with 1,694 more rows  You can remove specific columns by prefacing the column names with -, like -lifeExp:\ngapminder %% select(-lifeExp)  ## # A tibble: 1,704 x 5 ## country continent year pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 8425333 779. ## 2 Afghanistan Asia 1957 9240934 821. ## 3 Afghanistan Asia 1962 10267083 853. ## 4 Afghanistan Asia 1967 11537966 836. ## 5 Afghanistan Asia 1972 13079460 740. ## 6 Afghanistan Asia 1977 14880372 786. ## 7 Afghanistan Asia 1982 12881816 978. ## 8 Afghanistan Asia 1987 13867957 852. ## 9 Afghanistan Asia 1992 16317921 649. ## 10 Afghanistan Asia 1997 22227415 635. ## # … with 1,694 more rows  You can also rename columns using select(). Follow this pattern: select(old_name = new_name).\ngapminder %% select(year, country, life_expectancy = lifeExp)  ## # A tibble: 1,704 x 3 ## year country life_expectancy ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1952 Afghanistan 28.8 ## 2 1957 Afghanistan 30.3 ## 3 1962 Afghanistan 32.0 ## 4 1967 Afghanistan 34.0 ## 5 1972 Afghanistan 36.1 ## 6 1977 Afghanistan 38.4 ## 7 1982 Afghanistan 39.9 ## 8 1987 Afghanistan 40.8 ## 9 1992 Afghanistan 41.7 ## 10 1997 Afghanistan 41.8 ## # … with 1,694 more rows  Alternatively, there’s a special rename() verb that will, um, rename, while keeping all the other columns:\ngapminder %% rename(life_expectancy = lifeExp)  ## # A tibble: 1,704 x 6 ## country continent year life_expectancy pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows   Arranging data with arrange() The arrange() verb sorts data. By default it sorts ascendingly, putting the lowest values first:\ngapminder %% arrange(lifeExp)  ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Rwanda Africa 1992 23.6 7290203 737. ## 2 Afghanistan Asia 1952 28.8 8425333 779. ## 3 Gambia Africa 1952 30 284320 485. ## 4 Angola Africa 1952 30.0 4232095 3521. ## 5 Sierra Leone Africa 1952 30.3 2143249 880. ## 6 Afghanistan Asia 1957 30.3 9240934 821. ## 7 Cambodia Asia 1977 31.2 6978607 525. ## 8 Mozambique Africa 1952 31.3 6446316 469. ## 9 Sierra Leone Africa 1957 31.6 2295678 1004. ## 10 Burkina Faso Africa 1952 32.0 4469979 543. ## # … with 1,694 more rows  You can reverse that by wrapping the column name with desc():\ngapminder %% arrange(desc(lifeExp))  ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Japan Asia 2007 82.6 127467972 31656. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. ## 3 Japan Asia 2002 82 127065841 28605. ## 4 Iceland Europe 2007 81.8 301931 36181. ## 5 Switzerland Europe 2007 81.7 7554661 37506. ## 6 Hong Kong, China Asia 2002 81.5 6762476 30209. ## 7 Australia Oceania 2007 81.2 20434176 34435. ## 8 Spain Europe 2007 80.9 40448191 28821. ## 9 Sweden Europe 2007 80.9 9031088 33860. ## 10 Israel Asia 2007 80.7 6426679 25523. ## # … with 1,694 more rows  You can sort by multiple columns by specifying them in a comma separated list. For example, we can sort by continent and then sort by life expectancy within the continents:\ngapminder %% \u0026nbsp;\u0026nbsp;arrange(continent, desc(lifeExp))  ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Reunion Africa 2007 76.4 798094 7670. ## 2 Reunion Africa 2002 75.7 743981 6316. ## 3 Reunion Africa 1997 74.8 684810 6072. ## 4 Libya Africa 2007 74.0 6036914 12057. ## 5 Tunisia Africa 2007 73.9 10276158 7093. ## 6 Reunion Africa 1992 73.6 622191 6101. ## 7 Tunisia Africa 2002 73.0 9770575 5723. ## 8 Mauritius Africa 2007 72.8 1250882 10957. ## 9 Libya Africa 2002 72.7 5368585 9535. ## 10 Algeria Africa 2007 72.3 33333216 6223. ## # … with 1,694 more rows   That’s it! Those are the main verbs you’ll deal with in this class. There are dozens of other really useful ones—check out the dplyr and tidyr cheat sheet for examples.\n  Changing colors, shapes, and sizes, with scale_*() Recall from session 3 that the grammar of graphics uses a set of layers to define elements of plots:\nIn tomorrow’s session, you’ll learn all about the Theme layer. Here we’ll briefly cover the Scales layer, which we use for changing aspects of the different aesthetics, like using logged axes or changing colors or shapes.\nAll the functions that deal with scales conveniently follow the same naming pattern:\nscale_AESTHETIC_DETAILS() Here are some common scale functions:\nscale_x_continuous()\nscale_y_reverse()\nscale_color_viridis_c()\nscale_shape_manual(values = c(19, 13, 15))\nscale_fill_manual(values = c(\"red\", \"orange\", \"blue\")) You can see a list of all of the possible scale functions here, and you should reference that documentation (and the excellent examples) often when working with these functions.\nAs long as you have mapped a variable to an aesthetic with aes(), you can use the scale_*() functions to deal with it. For instance, in this ggplot, we have mapped variables to x, y, and fill, which means we can use those corresponding scale functions to manipulate how those aesthetics are shown. Here we reverse the y-axis (ew, don’t really do this), and we use a discrete viridis color palette:\ncontinent_counts \u0026lt;- gapminder %\u0026gt;% group_by(continent) %\u0026gt;% summarize(countries = n_distinct(country)) ggplot(continent_counts, aes(x = continent, y = countries, fill = continent)) + geom_col() + scale_y_reverse() + # lol this is bad; don\u0026#39;t do it in real life scale_fill_viridis_d() You can also use different arguments in the scale functions—again, check the documentation for examples. For instance, if we want to use the plasma palette from the viridis package, we can set that as an option:\nggplot(continent_counts, aes(x = continent, y = countries, fill = continent)) + geom_col() + scale_fill_viridis_d(option = \u0026quot;plasma\u0026quot;) That yellow might be too bright and hard to see, so we can tell ggplot to not use the full range of the palette, ending at 90% of the range instead:\nggplot(continent_counts, aes(x = continent, y = countries, fill = continent)) + geom_col() + scale_fill_viridis_d(option = \u0026quot;plasma\u0026quot;, end = 0.9) Instead of letting R calculate the colors from a general palette, you can also specify your own colors with scale_fill_manual() and feeding it a list of values—generally as hex codes or a name from a list of built-in R colors:\nggplot(continent_counts, aes(x = continent, y = countries, fill = continent)) + geom_col() + scale_fill_manual(values = c(\u0026quot;chartreuse4\u0026quot;, \u0026quot;cornsilk4\u0026quot;, \u0026quot;black\u0026quot;, \u0026quot;#fc03b6\u0026quot;, \u0026quot;#5c47d6\u0026quot;)) Scale functions also work for other aesthetics like shape or color or size. For instance, consider this plot, which has all three:\ngapminder_2007 \u0026lt;- gapminder %\u0026gt;% filter(year == 2007) ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, shape = continent, size = pop)) + geom_point() + scale_x_log10() We can change the colors of the points with scale_color_*():\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, shape = continent, size = pop)) + geom_point() + scale_x_log10() + scale_color_manual(values = c(\u0026quot;chartreuse4\u0026quot;, \u0026quot;cornsilk4\u0026quot;, \u0026quot;black\u0026quot;, \u0026quot;#fc03b6\u0026quot;, \u0026quot;#5c47d6\u0026quot;)) We can change the shapes with scale_shape_*(). If you run ?pch in your console or search for pch in the help, you can see all the possible shapes.\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, shape = continent, size = pop)) + geom_point() + scale_x_log10() + scale_shape_manual(values = c(12, 9, 17, 19, 15)) You can change the size with scale_size_*(). Here we make it so the smallest possible size is 1 and the largest is 15:\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, shape = continent, size = pop)) + geom_point() + scale_x_log10() + scale_size_continuous(range = c(1, 15)) We can even do all three at once:\nggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, shape = continent, size = pop)) + geom_point() + scale_x_log10() + scale_color_manual(values = c(\u0026quot;chartreuse4\u0026quot;, \u0026quot;cornsilk4\u0026quot;, \u0026quot;black\u0026quot;, \u0026quot;#fc03b6\u0026quot;, \u0026quot;#5c47d6\u0026quot;)) + scale_shape_manual(values = c(12, 9, 17, 19, 15)) + scale_size_continuous(range = c(1, 15)) Phew. That’s ugly.\nOne last thing we can do with scales is format how they show up on the plot. Notice how the population legend uses scientific notation like 2.50e+08. This means you need to move the decimal point 8 places to the right, making it 250000000. Leaving it in scientific notation isn’t great because it makes it really hard to read and interpret.\nIf you load the scales library (which is installed as part of tidyverse but isn’t automatically loaded), you can use some neat helper functions to reformat the text that shows up in plots. For instance, we can make it so population is formatted as a number with commas every 3 numbers, and the x-axis is formatted as dollars:\nlibrary(scales) ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, shape = continent, size = pop)) + geom_point() + scale_x_log10(labels = dollar) + scale_size_continuous(labels = comma) Check the documentation for scales for details about all the labelling functions it has, including dates, percentages, p-values, LaTeX math, etc.\n ","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"5a003ab3247913ac8f1034d05b4692ee","permalink":"/lesson/04-lesson/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/lesson/04-lesson/","section":"lesson","summary":"Manipulating data with dplyr  Filtering with filter() Adding new columns with mutate() Combining multiple verbs with pipes (%\u0026gt;%) Summarizing data by groups with group_by() %\u0026gt;% summarize() Selecting with select() Arranging data with arrange() That’s it!  Changing colors, shapes, and sizes, with scale_*()   When you visualize proportions with ggplot, you’ll typically go through a two-step process:\nSummarize the data with dplyr (typically with a combination of group_by() and summarize()) Plot the summarized data  Manipulating data with dplyr You had some experience with dplyr functions in the RStudio primers, but we’ll briefly review them here.","tags":null,"title":"Amounts and proportions","type":"docs"},{"authors":null,"categories":null,"content":"  Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n Unzipping files on Windows tl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n ","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"  There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.\n Kaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n 360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\n US City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n Political science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n François Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities. Thomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.). Erik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: The ugliest plot in the world Turning everything in   Getting started For this assignment, you’re going to work with data compiled by data journalist Duncan Greere related to 48 Soviet dogs who flew as test subjects in USSR’s space program in the 1950s and 60s. The original data can be found here.\nYou’ll need to download one CSV file and put them somewhere on your computer or upload them to RStudio.cloud—preferably in a folder named data in your project folder:\n  Dogs-Database.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to clean up the data a little. Download that here and include it in your project:\n  05-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 05-exercise.Rmd your-project-name.Rproj data\\ Dogs-Database.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  05-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: The ugliest plot in the world For this assignment, you’re going to forget all the wonderful CRAP design principles you just learned and try your hardest to make the ugliest plot in the world. Modify the color scale and change theme elements to make this plot truly hideous.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex and you followed the instructions here for how to make Cairo fonts work with knitted PDFs) of your document.\nInclude a chunk that uses ggsave() to save the plot to your computer as a PNG file.\nUpload the knitted document and the saved PNG file of your plot to iCollege.\n ","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589500800,"objectID":"ae6bdb9cc7518a780be99848063a951b","permalink":"/assignment/05-exercise/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/assignment/05-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: The ugliest plot in the world Turning everything in   Getting started For this assignment, you’re going to work with data compiled by data journalist Duncan Greere related to 48 Soviet dogs who flew as test subjects in USSR’s space program in the 1950s and 60s. The original data can be found here.\nYou’ll need to download one CSV file and put them somewhere on your computer or upload them to RStudio.","tags":null,"title":"Themes","type":"docs"},{"authors":null,"categories":null,"content":"   Basic plot Nice pre-built themes Bonus: ggthemeassist Saving plots   The lesson for today’s session is a fairly comprehensive introduction to using the theme() function in ggplot, and this page by Henry Wang is a good cheat sheet for remembering which theme elements are which on a plot.\nFor your exercise, you’re going to create the world’s ugliest plot. For this example, we’ll use the principles of CRAP to make a great theme.\nI’m going to build the theme semi-incrementally here. Instead of showing how the plot updates with each change in setting, I do most of the updates all at once, with tons of comments explaining what each line does. Importantly, I did not write this all at once. When you’re tinkering with themes, you generally start with something like theme_minimal() or theme_bw() and then gradually add new things to theme(), like modifying plot.title, then plot.subtitle, etc. It’s a very iterative process with lots of tinkering. Because of this, there is no live-coding video for this example—it would be incredibly long and boring. Instead, look through each of the lines and see what they’re doing.\nFor this example, I’m going to use the gapminder dataset that we’ve been using throughout this week. You can get it if you install the gapminder package in R, or you can download this CSV file (you may need to right click on it and select “Save As…”):\n  gapminder.csv  I’m also going to use the Roboto Condensed font in the theme. Download and install it on your computer if you don’t have it.\nBasic plot When I’m creating a theme, I like to use a basic plot with everything that might show up, complete with a title, subtitle, caption, legend, facets, and other elements.\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(gapminder) # For gapminder data library(scales) # For nice axis labels gapminder_filtered \u0026lt;- gapminder %\u0026gt;% filter(year \u0026gt; 2000) base_plot \u0026lt;- ggplot(data = gapminder_filtered, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + geom_point() + # Use dollars, and get rid of the cents part (i.e. $300 instead of $300.00) scale_x_log10(labels = dollar_format(accuracy = 1)) + # Format with commas scale_size_continuous(labels = comma) + # Use viridis scale_color_viridis_d(option = \u0026quot;plasma\u0026quot;, end = 0.9) + labs(x = \u0026quot;GDP per capita\u0026quot;, y = \u0026quot;Life expectancy\u0026quot;, color = \u0026quot;Continent\u0026quot;, size = \u0026quot;Population\u0026quot;, title = \u0026quot;Here\u0026#39;s a cool title\u0026quot;, subtitle = \u0026quot;And here\u0026#39;s a neat subtitle\u0026quot;, caption = \u0026quot;Source: The Gapminder Project\u0026quot;) + facet_wrap(vars(year)) base_plot Now we have base_plot to work with. Here’s what it looks like with theme_minimal() applied to it:\nbase_plot + theme_minimal() That gets rid of the grey background and is a good start, but we can make lots of improvements. First let’s deal with the gridlines. There are too many. We can get rid of the minor gridlines with by setting them to element_blank():\nbase_plot + theme_minimal() + theme(panel.grid.minor = element_blank()) Next let’s add some typographic contrast. We’ll use Roboto Condensed Regular as the base font. Before trying this, make sure you do the following:\nOn macOS:\n Run capabilities() in your console and verify that TRUE shows up under cairo If not, download and install XQuartz  On Windows:\n Run windowsFonts() in your console and you’ll see a list of all the fonts you can use with R. It’s not a very big list.\n#\u0026gt; $serif #\u0026gt; [1] \u0026quot;TT Times New Roman\u0026quot; #\u0026gt; #\u0026gt; $sans #\u0026gt; [1] \u0026quot;TT Arial\u0026quot; #\u0026gt; #\u0026gt; $mono #\u0026gt; [1] \u0026quot;TT Courier New\u0026quot; You can add Roboto Condensed to your current R session by running this in your console:\nwindowsFonts(`Roboto Condensed` = windowsFont(\u0026quot;Roboto Condensed\u0026quot;)) Now if you run windowsFonts(), you’ll see it in the list:\n#\u0026gt; $serif #\u0026gt; [1] \u0026quot;TT Times New Roman\u0026quot; #\u0026gt; #\u0026gt; $sans #\u0026gt; [1] \u0026quot;TT Arial\u0026quot; #\u0026gt; #\u0026gt; $mono #\u0026gt; [1] \u0026quot;TT Courier New\u0026quot; #\u0026gt; #\u0026gt; $`Roboto Condensed` #\u0026gt; [1] \u0026quot;Roboto Condensed\u0026quot; This only takes effect for your current R session, so if you are knitting a document or if you ever plan on closing RStudio, you’ll need to incorporate this font creation code into your script.\n  We’ll use the font as the base_family argument. Note how I make it bold with face and change the size with rel(). Instead of manually setting some arbitrary size, I use rel() to resize the text in relation to the base_size argument. Using rel(1.7) means 1.7 × base_size, or 20.4 That will rescale according to whatever base_size is—if I shrink it to base_size = 8, the title will scale down accordingly.\nplot_with_good_typography \u0026lt;- base_plot + theme_minimal(base_family = \u0026quot;Roboto Condensed\u0026quot;, base_size = 12) + theme(panel.grid.minor = element_blank(), # Bold, bigger title plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.7)), # Plain, slightly bigger subtitle that is grey plot.subtitle = element_text(face = \u0026quot;plain\u0026quot;, size = rel(1.3), color = \u0026quot;grey70\u0026quot;), # Italic, smaller, grey caption that is left-aligned plot.caption = element_text(face = \u0026quot;italic\u0026quot;, size = rel(0.7), color = \u0026quot;grey70\u0026quot;, hjust = 0), # Bold legend titles legend.title = element_text(face = \u0026quot;bold\u0026quot;), # Bold, slightly larger facet titles that are left-aligned for the sake of repetition strip.text = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.1), hjust = 0), # Bold axis titles axis.title = element_text(face = \u0026quot;bold\u0026quot;), # Add some space above the x-axis title and make it left-aligned axis.title.x = element_text(margin = margin(t = 10), hjust = 0), # Add some space to the right of the y-axis title and make it top-aligned axis.title.y = element_text(margin = margin(r = 10), hjust = 1)) plot_with_good_typography Whoa. That gets us most of the way there! We have good contrast with the typography, with the strong bold and the lighter regular font (✓ contrast). Everything is aligned left (✓ alignment and ✓ repetition). By moving the axis titles a little bit away from the labels, we’ve enhanced proximity, since they were too close together (✓ proximity). We repeat grey in both the caption and the subtitle (✓ repetition).\nThe only thing I don’t like is that the 2002 isn’t quite aligned with the title and subtitle. This is because the facet labels are in boxes along the top of each plot, and in some themes (like theme_grey() and theme_bw()) those facet labels have grey backgrounds. We can turn off the margin in those boxes, or we can add a background, which will then be perfectly aligned with the title and subtitle.\nplot_with_good_typography + # Add a light grey background to the facet titles, with no borders theme(strip.background = element_rect(fill = \u0026quot;grey90\u0026quot;, color = NA), # Add a thin grey border around all the plots to tie in the facet titles panel.border = element_rect(color = \u0026quot;grey90\u0026quot;, fill = NA)) 👩‍🍳 💋! That looks great!\nTo save ourselves time in the future, we can store this whole thing as an object that we can then reuse on other plots:\nmy_pretty_theme \u0026lt;- theme_minimal(base_family = \u0026quot;Roboto Condensed\u0026quot;, base_size = 12) + theme(panel.grid.minor = element_blank(), # Bold, bigger title plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.7)), # Plain, slightly bigger subtitle that is grey plot.subtitle = element_text(face = \u0026quot;plain\u0026quot;, size = rel(1.3), color = \u0026quot;grey70\u0026quot;), # Italic, smaller, grey caption that is left-aligned plot.caption = element_text(face = \u0026quot;italic\u0026quot;, size = rel(0.7), color = \u0026quot;grey70\u0026quot;, hjust = 0), # Bold legend titles legend.title = element_text(face = \u0026quot;bold\u0026quot;), # Bold, slightly larger facet titles that are left-aligned for the sake of repetition strip.text = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.1), hjust = 0), # Bold axis titles axis.title = element_text(face = \u0026quot;bold\u0026quot;), # Add some space above the x-axis title and make it left-aligned axis.title.x = element_text(margin = margin(t = 10), hjust = 0), # Add some space to the right of the y-axis title and make it top-aligned axis.title.y = element_text(margin = margin(r = 10), hjust = 1), # Add a light grey background to the facet titles, with no borders strip.background = element_rect(fill = \u0026quot;grey90\u0026quot;, color = NA), # Add a thin grey border around all the plots to tie in the facet titles panel.border = element_rect(color = \u0026quot;grey90\u0026quot;, fill = NA)) Now we can use it on any plot. Remember that first plot you made in your exercise from session 1 with the cars dataset? Let’s throw this theme on it! (only here the dataset is named mpg instead of cars; the mpg dataset is loaded invisibly whenever you load ggplot)\nmpg_example \u0026lt;- ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = class)) + geom_point(size = 3) + scale_color_viridis_d() + facet_wrap(vars(drv)) + labs(x = \u0026quot;Displacement\u0026quot;, y = \u0026quot;Highway MPG\u0026quot;, color = \u0026quot;Car class\u0026quot;, title = \u0026quot;Heavier cars get worse mileage\u0026quot;, subtitle = \u0026quot;Except two-seaters?\u0026quot;, caption = \u0026quot;Here\u0026#39;s a caption\u0026quot;) + my_pretty_theme mpg_example Super neat!\n Nice pre-built themes This custom theme we just made is just one iteration of a theme. There are countless ways to tinker with a theme and have it meet the different CRAP principles. People have even published their own themes in different R packages. Check these out to see lots of different examples:\n hrbrthemes ggthemes ggthemr ggtech tvthemes ggpomological (this one is incredible!)  Check this blog post for examples of a bunch of others\n Bonus: ggthemeassist If you’re intimidated by constantly referring to the documentation and figuring out what little line of code affects which part of the graph, install and check out the ggthemeassist package. It provides an interactive menu for manipulating different theme elements, and then generates all the corresponding code, which is really magical.\nHere’s a brief example of how to use it.\n   Saving plots If we want to save these plots, we can use ggsave(). For that to work, we need to store the plot as an object, which I already did in the examples above:\nname_of_plot_object \u0026lt;- ggplot(...) We then feed our saved plot object to ggsave() and specify the filename and dimensions we want to use. If we’re using PNG, we don’t need to worry about any extra options. If we’re using PDF, we need to tell R to use the Cairo PDF writing engine instead of R’s normal one, since R’s normal one can’t deal with custom fonts.\n# Add my_pretty_theme to the gapminder base_plot and save as an object final_gampinder_plot \u0026lt;- base_plot + my_pretty_theme # Save as PNG and PDF ggsave(\u0026quot;fancy_gapminder.png\u0026quot;, final_gampinder_plot, width = 8, height = 5, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;fancy_gapminder.pdf\u0026quot;, final_gampinder_plot, width = 8, height = 5, units = \u0026quot;in\u0026quot;, device = cairo_pdf) # Save the mpg plot as PNG and PDF ggsave(\u0026quot;fancy_mpg.png\u0026quot;, mpg_example, width = 8, height = 5, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;fancy_mpg.pdf\u0026quot;, mpg_example, width = 8, height = 5, units = \u0026quot;in\u0026quot;, device = cairo_pdf)  ","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589500800,"objectID":"ac11eb4b300887ac6e4fc5c1906670ba","permalink":"/example/05-example/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/example/05-example/","section":"example","summary":"Basic plot Nice pre-built themes Bonus: ggthemeassist Saving plots   The lesson for today’s session is a fairly comprehensive introduction to using the theme() function in ggplot, and this page by Henry Wang is a good cheat sheet for remembering which theme elements are which on a plot.\nFor your exercise, you’re going to create the world’s ugliest plot. For this example, we’ll use the principles of CRAP to make a great theme.","tags":null,"title":"Themes","type":"docs"},{"authors":null,"categories":null,"content":"   Complete ggplot themes Modifying plot elements with theme()  Deal with general plot elements Disable elements completely with element_blank() Deal with borders and backgrounds with element_rect() Deal with lines with element_line() Deal with text with element_text()  Important note about ordering Fonts Reusing themes Saving plots   Complete ggplot themes There are many built-in complete themes that have a good combination of all the different theme() options already set for you. By default, ggplot uses theme_gray() (also spelled theme_grey() for UK English; because the first developer of ggplot (Hadley Wickham) is from New Zealand, British spelling works throughout (e.g. you can use colour instead of color))\nYour turn: Add theme_minimal() to this plot:\n  Hopefully that was easy!\nIf you look at the documentation for the different theme functions, you’ll notice that there are a few optional arguments, like base_size and base_family. The base_size argument changes the base font size for the text in the plot, and it is 11 by default. Changing it to something like 20 will not make all the text in the plot be sized at 20—functions like theme_minimal() set the size of plot elements based on the base_size. For instance, in theme_minimal(), the plot title is set to be 120% of base_size, while the caption is 80%. Changing base_size will resize all the different elements accordingly.\nYour turn: Modify this plot to use theme_minimal() with a base size of 16:\n  Hopefully that was also fairly straightforward!\n Modifying plot elements with theme() Using a complete theme like theme_minimal() or theme_bw() is a great starting point for getting a nice, clean, well designed plot. You’ll often need to make adjustments to smaller, more specific parts of the plot though. To do this, you can use the theme() function.\ntheme() is a massive function and has perhaps the most possible arguments of any function in R. It is impossible to remember everything it can possibly do. Fortunately its documentation is incredible. Run ?theme in your R console to see the help page, or go to this page online.\nDeal with general plot elements A few arguments to theme() don’t use any special function—you can just specify settings with text like \"bottom\" or \"right\"\nYour turn: Look at the documentation for theme() online. Make this plot’s legend appear on the bottom instead of the left.\n   Disable elements completely with element_blank() Any plot element can be disabled by using element_blank(). For instance, if you want to remove the axis ticks, you can use theme(axis.ticks = element_blank()).\nYour turn: Look at the documentation for theme() online. Disable the panel grid in this plot.\n  You can also target more specific plot elements. You can specify something like axis.text, which applies to all axis text, or you can use axis.text.y to only target the text on the y-axis.\nYour turn: Look at the documentation for theme() online. Make the following changes to this plot:\n Disable the major panel grid for the x-axis Disable the minor panel grid for the x-axis Disable the minor panel grid for the y-axis.  You should only have three horizontal lines for the grid.\n  Almost every other plot element fits into one of three categories: a rectangle, a line, or text. Changing the settings on these elements requires specific functions that correspond to these categories.\n Deal with borders and backgrounds with element_rect() Things like the plot background or the panel background or the legend background are rectangles and can be manipulated with element_rect(). If you want the legend box to be yellow with a thin black border, you would use theme(legend.box.background = element_rect(fill = \"yellow\", color = \"black\", size = 1).\nYour turn: Look at the documentation for theme() and the documentation for element() online. Make the following changes to this plot:\n Fill the plot background with #F2D8CE Fill the panel background with #608BA6, and make the border #184759 with size = 5  This will be a fairly ugly plot.\n   Deal with lines with element_line() Things like the panel grid, tick marks, and axis lines are all lines and can be manipulated with element_line(). If you want the x-axis line to be a dotted orange like, you would use theme(axis.line.x = element_line(color = \"orange\", linetype = \"dotted\").\nYour turn: Look at the documentation for theme() and the documentation for element() online. Make the following changes to this plot:\n Make the major panel gridlines blue and dashed with size = 1  This will also be a fairly ugly plot.\n   Deal with text with element_text() Finally, anything with text can be manipulated with element_text(), and you can specify all sorts of things, including font family (family), font weight (face), color (color), horizontal justification (hjust), angle (angle), and a bunch of other options. If you want the x-axis text to be italicized and rotated at a 45º angle, you would use theme(axis.text.x = element_text(face = \"italic\", angle = 45)).\nYour turn: Look at the documentation for theme() and the documentation for element() online. Make the following changes to this plot:\n Make the y-axis text italic Make the plot title right aligned, bold, and colored with #8C7811 Make the plot subtitle right aligned      Important note about ordering Things like theme_grey() or theme_minimal() are really just collections of changes to theme(), so the order is important when using a complete theme. If you do something like this to turn off the gridlines in the plot panel:\nggplot(...) + geom_point(...) + theme(panel.grid = element_blank()) + theme_bw() …you’ll still have panel gridlines! That’s because theme_bw() turns them on, and you typed it after you turned it off. If you want to use both theme_bw() and remove the gridlines, you need to make sure any theme adjustments come after theme_bw():\nggplot(...) + geom_point(...) + theme_bw() + theme(panel.grid = element_blank())  Fonts You can use theme() to change the fonts as well, though sometimes it’s a little tricky to get R to see the fonts on your computer—especially if you use Windows. This detailed blog post explains how to work with custom fonts in ggplot and shows how to get it set up on Windows. It should Just Work™ on macOS.\nIn short, as long as you load the fonts correctly, you can specify different fonts either in a complete theme like theme_minimal(base_family = \"Comic Sans MS\") or in theme() like theme(plot.title = element_text(family = \"Papyrus\")).\n Reusing themes If you want to repeat specific theme settings throughout a document, you can save yourself a ton of typing by storing the results of theme() to an object and reusing it. For instance, suppose you want your plots to be based on theme_minimal, have right aligned title and subtitle text, have the legend at the bottom, and have no minor gridlines. You can save all of that into an object named my_neato_theme or something, and then reuse it:\nmy_neato_theme \u0026lt;- theme_minimal() + theme(plot.title = element_text(hjust = 1), plot.subtitle = element_text(hjust = 1), legend.position = \u0026quot;bottom\u0026quot;, panel.grid.minor = element_blank()) # Make one plot ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point(size = 3) + labs(title = \u0026quot;Engine displacement and highway MPG\u0026quot;, subtitle = \u0026quot;Heavier cars get worse mileage\u0026quot;) + my_neato_theme # Make another plot ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = cty)) + geom_point(size = 3) + labs(title = \u0026quot;Engine displacement and highway MPG\u0026quot;, subtitle = \u0026quot;Points colored by city MPG\u0026quot;) + my_neato_theme  Saving plots So far, all your plots have ended up either in RStudio or in a knitted HTML, Word, or PDF document. But what if you want to save just the plot to your computer so you can send it out to the world?! You could take a screenshot, but that won’t provide the highest resolution, and that will only save the plot as a bitmap-based PNG, not an infinitely resizable vector-based PDF!\nFortunately it’s pretty easy to save a plot using the special ggsave() function. You can specify whatever dimensions you want and whatever file type you want and save the standalone plot to your computer. You should look at the documentation for ggsave() for complete details of all the different options and arguments it can take. Typically, you do something like this.\nFirst create a plot and store it as an object. We haven’t done that yet in this lesson—so far we’ve just run ggplot() and seen the output immediately. If you save the output of ggplot() to an object, you actually won’t see anything until you run the name of the object.\na_cool_plot \u0026lt;- ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point(size = 3) + labs(title = \u0026quot;Engine displacement and highway MPG\u0026quot;, subtitle = \u0026quot;Heavier cars get worse mileage\u0026quot;) # Make sure you run this so you can see the plot a_cool_plot Next you can feed your saved plot to ggsave() to save it. It will automatically determine how to save it based on the filename you provide. If you tell it to be something.png, R will make a PNG; if you tell it to be something.pdf, R will make a PDF, and so on. Common types are PDF, PNG, JPEG (ew though), SVG, TIFF, and others.\nYou can also save the plot as multiple files. I typically make PNG and PDF versions of any plots I export like so:\nggsave(filename = \u0026quot;a_cool_plot.pdf\u0026quot;, plot = a_cool_plot, width = 6, height = 4.5, units = \u0026quot;in\u0026quot;) ggsave(filename = \u0026quot;a_cool_plot.png\u0026quot;, plot = a_cool_plot, width = 6, height = 4.5, units = \u0026quot;in\u0026quot;) From a file management perspective, it often makes sense to store all your output in a separate folder in your project, like output or figures or something. If you want to put saved images in a subfolder, include the name in the file name:\nggsave(filename = \u0026quot;figures/a_cool_plot.png\u0026quot;, plot = a_cool_plot, width = 6, height = 4.5, units = \u0026quot;in\u0026quot;) And finally, if you’re using custom fonts, you need to add one bit of wizardry to get the fonts to embed correctly in PDFs. This is something you just have to memorize or copy and paste a lot—if you want to know the full details, see this blog post. In short, R’s default PDF writer doesn’t know how to embed fonts and will panic if you make it try. R can use a different PDF-writing engine named Cairo that embeds fonts just fine, though, so you need to tell ggsave() to use it:\nggsave(filename = \u0026quot;figures/a_cool_plot.pdf\u0026quot;, plot = a_cool_plot, width = 6, height = 4.5, units = \u0026quot;in\u0026quot;, device = cairo_pdf)  ","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589500800,"objectID":"aa54929a56a6d237dc8692bd2c9cd024","permalink":"/lesson/05-lesson/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/lesson/05-lesson/","section":"lesson","summary":"Complete ggplot themes Modifying plot elements with theme()  Deal with general plot elements Disable elements completely with element_blank() Deal with borders and backgrounds with element_rect() Deal with lines with element_line() Deal with text with element_text()  Important note about ordering Fonts Reusing themes Saving plots   Complete ggplot themes There are many built-in complete themes that have a good combination of all the different theme() options already set for you.","tags":null,"title":"Themes","type":"docs"},{"authors":null,"categories":null,"content":"  You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"68be32a8da6a38dd54a9e724ab3904a0","permalink":"/resource/citations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/citations/","section":"resource","summary":"You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.","tags":null,"title":"Citations and bibliography","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Visualizing uncertainty with gapminder Turning everything in   Getting started For this exercise you’ll revisit Hans Rosling’s gapminder data on health and wealth.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nYou don’t need to download any CSV files for this assignment. If you run library(gapminder) you’ll have access to a data frame named gapminder that contains all the data.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  06-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 06-exercise.Rmd your-project-name.Rproj To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  06-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Visualizing uncertainty with gapminder Make the following plots and briefly explain what they show:\n Make a histogram of logged GDP per capita for 1997 only, across all five continents\n Make a ridge plot of global life expectancy over time, from 1952 to 2007. You’ll need to use the full gapminder data, not the 1997-only data. Each ridge should show the distribution of the world’s life expectancy for each given year (similar to the temperature ridge plot in the example).\nImportant note: year will be on the y-axis, but it must be a categorical variable to work with ggridges, so you’ll either need to wrap it in as.factor() like aes(..., y = as.factor(year)), or add a new categorical/factor year column to the gapminder dataset with mutate().\n Make a filtered dataset that selects data from only 2007 and removes Oceania. Show the distribution of logged GDP per capita across the four continents using some combination of boxplots and/or violin plots and/or strip plots, either overlaid on top of each other, or using their geom_half_*() counterparts from gghalves.\n  The example for today’s session will be incredibly helpful for this exercise. Reference it.\nYou don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"4cf822fb9736dd7176ca2ccc76d5e892","permalink":"/assignment/06-exercise/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/assignment/06-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Visualizing uncertainty with gapminder Turning everything in   Getting started For this exercise you’ll revisit Hans Rosling’s gapminder data on health and wealth.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.","tags":null,"title":"Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load and clean data Histograms Density plots Box, violin, and rain cloud plots    For this example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  atl-weather-2019.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) library(lubridate) library(ggridges) library(gghalves) Then we load the data with read_csv(). Here I assume that the CSV file lives in a subfolder in my project named data:\nweather_atl_raw \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;) We’ll add a couple columns that we can use for faceting and filling using the month() and wday() functions from lubridate for extracting parts of the date:\nweather_atl \u0026lt;- weather_atl_raw %\u0026gt;% mutate(Month = month(time, label = TRUE, abbr = FALSE), Day = wday(time, label = TRUE, abbr = FALSE)) Now we’re ready to go!\n Histograms We can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:\nggplot(weather_atl, aes(x = windSpeed)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) This is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the boundary argument for that. We also add scale_x_continuous() to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:\nggplot(weather_atl, aes(x = windSpeed)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) We can show the distribution of wind speed by month if we map the Month column we made onto the fill aesthetic:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) This is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) + guides(fill = FALSE) + facet_wrap(vars(Month)) Neat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.\n Density plots The code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the geom layer:\nggplot(weather_atl, aes(x = windSpeed)) + geom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;) If we want, we can mess with some of the calculus options like the kernel and bandwidth:\nggplot(weather_atl, aes(x = windSpeed)) + geom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;, bw = 0.1, kernel = \u0026quot;epanechnikov\u0026quot;) We can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_density(alpha = 0.5) Even with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_density(alpha = 0.5) + guides(fill = FALSE) + facet_wrap(vars(Month)) Or we can stack the density plots behind each other with ggridges. For that to work, we also need to map Month to the y-axis. We can reverse the y-axis so that January is at the top if we use the fct_rev() function:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) + geom_density_ridges() + guides(fill = FALSE) We can add some extra information to geom_density_ridges() with some other arguments like quantile_lines. We can use the quantiles argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) + geom_density_ridges(quantile_lines = TRUE, quantiles = 2) + guides(fill = FALSE) Now that we have good working code, we can easily substitute in other variables by changing the x mapping:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) + geom_density_ridges(quantile_lines = TRUE, quantiles = 2) + guides(fill = FALSE) We can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use geom_density_ridges_gradient(), and we need to change the fill mapping to the strange looking ..x.., which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, fill = temperatureHigh doesn’t work 🤷:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) + geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) + scale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) + labs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;) And finally, we can get extra fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: temperatureLow and temperatureHigh. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like linetype), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using pivot_longer() from tidyr, which was already loaded with library(tidyverse). In the RStudio primers, you did this same thing with gather()—pivot_longer() is the newer version of gather():\nweather_atl_long \u0026lt;- weather_atl %\u0026gt;% pivot_longer(cols = c(temperatureLow, temperatureHigh), names_to = \u0026quot;temp_type\u0026quot;, values_to = \u0026quot;temp\u0026quot;) %\u0026gt;% # Clean up the new temp_type column so that \u0026quot;temperatureHigh\u0026quot; becomes \u0026quot;High\u0026quot;, etc. mutate(temp_type = recode(temp_type, temperatureHigh = \u0026quot;High\u0026quot;, temperatureLow = \u0026quot;Low\u0026quot;)) %\u0026gt;% # This is optional—just select a handful of columns select(time, temp_type, temp, Month) # Show the first few rows head(weather_atl_long) ## # A tibble: 6 x 4 ## time temp_type temp Month ## \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; ## 1 2019-01-01 05:00:00 Low 50.6 January ## 2 2019-01-01 05:00:00 High 63.9 January ## 3 2019-01-02 05:00:00 Low 49.0 January ## 4 2019-01-02 05:00:00 High 57.4 January ## 5 2019-01-03 05:00:00 Low 53.1 January ## 6 2019-01-03 05:00:00 High 55.3 January Now we have a column for the temperature (temp) and a column indicating if it is high or low (temp_type). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the linetype aesthetic to show high/low in the border of the plots:\nggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month), fill = ..x.., linetype = temp_type)) + geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) + scale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) + labs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;) Super neat! We can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.\n Box, violin, and rain cloud plots Finally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the Day variable we made indicating weekday:\nggplot(weather_atl, aes(y = windSpeed, fill = Day)) + geom_boxplot() We can switch this to a violin plot by just changing the geom layer and mapping Day to the x-axis:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() With violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) We can also add larger points for the daily averages. We’ll use a special layer for this: stat_summary(). It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here \"mean\") and then plotting that result:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + stat_summary(geom = \u0026quot;point\u0026quot;, fun = \u0026quot;mean\u0026quot;, size = 5, color = \u0026quot;white\u0026quot;) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) We can also show the mean and confidence interval at the same time by changing the summary function:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, size = 1, color = \u0026quot;white\u0026quot;) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) Overlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use the gghalves package, we can use special halved versions of some of these geoms like so:\nggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) + geom_half_boxplot(aes(fill = Day), side = \u0026quot;r\u0026quot;) + guides(color = FALSE, fill = FALSE) Note the side argument for specifying which half of the column the geom goes. We can also use geom_half_violin():\nggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) + geom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) + guides(color = FALSE, fill = FALSE) If we flip the plot, we can make a rain cloud plot:\nggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_boxplot(aes(fill = Day), side = \u0026quot;l\u0026quot;, width = 0.5, nudge = 0.1) + geom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) + geom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) + guides(color = FALSE, fill = FALSE) + coord_flip() Neat!\n  ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"a66b8951081722f5755747f3314e83f0","permalink":"/example/06-example/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/example/06-example/","section":"example","summary":"Live coding example Complete code  Load and clean data Histograms Density plots Box, violin, and rain cloud plots    For this example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):","tags":null,"title":"Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"   Histograms Density plots Boxes, violins, and dots   Throughout this lesson, you’ll use the built-in mpg dataset to make histograms, density plots, box plots, violin plots, and other graphics that show uncertainty.\nSorry if mpg is getting repetitive! For short interactive things like this, it’s easier to use built-in and easy-to-load datasets like mpg and gapminder instead of loading CSV files, hence our constant reuse of the dataset. This is fairly normal too—the majority of examples in R help pages (and in peoples’ blog posts) use things like mpg orgapminder, or even iris, which measures the lengths and widths of a bunch of iris flowers in the 1930s (fun fact! I don’t like using iris because the data was originally used in an article in the Annals of Eugenics (😬) in 1936, and the data was collected to advance eugenics, and there’s no good reason to use data like that in 2020.)\nSo we work with cars instead of racist flower data.\nThe mpg dataset is available in R as soon as you load ggplot2 (or tidyverse). Yu don’t have to run read_csv() or anything—it’s just there in the background already.\nAs a reminder, here are the first few rows of the mpg dataset:\nhead(mpg) ## # A tibble: 6 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compa… ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compa… ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compa… ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compa… ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compa… ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compa… Histograms When working with histograms, you always need to think about the bin width. Histograms calculate the counts of rows within specific ranges of data, and the shape of the histogram will change depending on how wide or narrow these ranges (or bins, or buckets) are.\nYour turn: Change this code to add a specific bin width for city miles per gallon cty (hint: binwidth). Play around with different widths until you find one that represents the data well.\n  By default, histograms are filled with a dark grey color and the bars have no borders. Additionally, R places the center of the bars at specific numbers: if you have a bin width of 5, for instance, a bar will show the range from 7.5 to 12.5 instead of 5-10 or 10-15.\nYour turn: Do the following:\nAdd a specific bin width Add a white border (hint: color) Fill with #E16462 Make it so the bars start at whole numbers like 10 or 20 (hint: boundary)    You can add extra aesthetics to encode additional information about the distribution of variables across categories.\nYour turn: Make a histogram of cty and fill by drv (drive: front, rear, and 4-wheel). Make sure you specify a good bin width.\n  That’s too much information! Instead of only filling, you can separate the data into multiple plots.\nYour turn: Make a histogram of cty fill and facet by drv. Make sure you specify a good bin width. Make sure you specify a good bin width.\n   Density plots When working with density plots in this class you don’t need to worry too much about the calculus behind the scenes that creates the curves. But you can change those settings if you really want.\nYour turn: Do the following:\nFill this density plot with #E16462 Add a border (hint: color) using #9C3836, with size = 1 Change the bandwidth (hint: bw) to 0.5, then 1, then 10    Like histograms, you can map other variables onto the plot. It’s often a good idea to make the curves semi-transparent so you can see the different distributions.\nYour turn: Do the following:\nFill this plot using the drv variable Make the density plots 50% transparent    Even with transparency, it’s often difficult to interpret density plots like this. As an alternative, you can use the ggridges package to make ridge plots. Look at the documentation and examples for ggridges for lots of details about different plots you can make.\nYour turn: Convert this plot into a ridge plot.\n   Boxes, violins, and dots Finally, you can use things like boxplots and violin plots to show the distribution of variables, either by themselves or across categories.\nBox plots show the distribution of a variable by highlighting specific details, like the 25th, 50th (median) and 75th percentile, as well as the assumed minimum, assumed maximum, and outliers:\nAnatomy of a boxplot\n When making boxplots with ggplot, you need to map the variable of interest to the x aesthetic (or y if you want a vertical boxplot), and you can optionally map a second categorical variable to the y aesthetic (or x if you want a vertical boxplot).\nYou can adjust the fill and color of the plot, and you can change what counts as outliers with the coef argument. By default outliers are any point that is beyond the 75th percentile + 1.5 × the interquartile range (or below the 25th percentile + 1.5 × IQR), but that’s adjustable.\nYour turn: Do the following:\nFill the boxplot with #E6AD3C Color the boxplot with #5ABD51 Change the definition of outliers to be 5 times the IQR    You can also use violin plots instead of boxplot, which show the mirrored density distribution. When doing this, it’s often helpful to add other geoms like jittered points to show more of the data\nYour turn: Do the following\nChange this boxplot to use violins instead Add jittered points with a jittering width of 0.1 and sized at 0.5     ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"f4cd52ded401299a83d62039ff42e8b4","permalink":"/lesson/06-lesson/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/lesson/06-lesson/","section":"lesson","summary":"Histograms Density plots Boxes, violins, and dots   Throughout this lesson, you’ll use the built-in mpg dataset to make histograms, density plots, box plots, violin plots, and other graphics that show uncertainty.\nSorry if mpg is getting repetitive! For short interactive things like this, it’s easier to use built-in and easy-to-load datasets like mpg and gapminder instead of loading CSV files, hence our constant reuse of the dataset.","tags":null,"title":"Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Combining plots Task 3: Visualizing regression  Coefficient plot Marginal effects  Bonus task! Correlograms Turning everything in   Getting started For this exercise you’ll use precinct-level data from the 2016 presidential election to visualize relationships between variables. This data comes from the MIT Election Data and Science Lab.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\n  results_2016.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  07-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 07-exercise.Rmd your-project-name.Rproj data\\ results_2016.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  07-exercise.zip  The example for today’s session will be incredibly helpful for this exercise. Reference it.\nAgain, you don’t need to make your plots super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Combining plots Make 2–3 plots of anything you want from the results_2016 data (histogram, density, boxplot, scatterplot, whatever) and combine them with patchwork. Look at the documentation to see fancy ways of combining them, like having two rows inside a column.\n Task 3: Visualizing regression Coefficient plot Use the results_2016 data to create a model that predicts the percent of Democratic votes in a precinct based on age, race, income, rent, and state (hint: the formula will look like this: percent_dem ~ median_age + percent_white + per_capita_income + median_rent + state)\nUse tidy() in the broom package and geom_pointrange() to create a coefficient plot for the model estimates. You’ll have 50 rows for all the states, and that’s excessive for a plot like this, so you’ll want to filter out the state rows. You can do that by adding this:\ntidy(...) %\u0026gt;% filter(!str_detect(term, \u0026quot;state\u0026quot;)) The str_detect() function looks for the characters “state” in the term column. The ! negates it. This is thus saying “only keep rows where the word ‘state’ is not in the term name”.\nYou should also get rid of the intercept (filter(term != \"(Intercept)\")).\n Marginal effects Create a new data frame with tibble() that contains a column for the average value for each variable in your model except for one, which you vary. For state, you’ll need to choose a single state. The new dataset should look something like this (though this is incomplete! You’ll need to include all the variables in your model, and you’ll need to vary one using seq()) (like seq(9000, 60000, by = 100) for per_capita_income). The na.rm argument in mean() here makes it so missing values are removed—without it, R can’t calculate the mean and will return NA instead.\ndata_to_predict \u0026lt;- tibble(median_age = mean(results_2016$median_age, na.rm = TRUE), percent_white = mean(results_2016$percent_white, na.rm = TRUE), state = \u0026quot;Georgia\u0026quot;) # Or whatever Use augment() to generate predictions from this dataset using the model you created before. Plot your varied variable on the x-axis, the fitted values (.fitted) on the y-axis, show the relationship with a line, and add a ribbon to show the 95% confidence interval.\n  Bonus task! Correlograms This is entirely optional but might be fun.\nFor extra fun times, if you feel like it, create a correlogram heatmap, either with geom_tile() or with points sized by the correlation. Use any variables you want from results_2016.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1589846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589846400,"objectID":"803416200f21fb99bbbc80253665b27a","permalink":"/assignment/07-exercise/","publishdate":"2020-05-19T00:00:00Z","relpermalink":"/assignment/07-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Combining plots Task 3: Visualizing regression  Coefficient plot Marginal effects  Bonus task! Correlograms Turning everything in   Getting started For this exercise you’ll use precinct-level data from the 2016 presidential election to visualize relationships between variables. This data comes from the MIT Election Data and Science Lab.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.","tags":null,"title":"Relationships","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load and clean data Legal dual y-axes Combining plots Scatterplot matrices Correlograms Simple regression Coefficient plots Marginal effects plots    For this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  atl-weather-2019.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(patchwork) # For combining ggplot plots library(GGally) # For scatterplot matrices library(broom) # For converting model objects to data frames Then we load the data with read_csv(). Here I assume that the CSV file lives in a subfolder in my project named data:\nweather_atl \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;)  Legal dual y-axes It is fine (and often helpful!) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.\nTo do this, you need to add an argument (sec.axis) to scale_y_continuous() to tell it to use a second axis. This sec.axis argument takes a sec_axis() function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a ~, which indicates that it’s a function, and it needs to use . to stand in for the original value in the original axis.\nSince the equation for converting Fahrenheit to Celsius is this…\n\\[ \\text{C} = (32 - \\text{F}) \\times -\\frac{5}{9} \\]\n…we can specify this with code like so (where . stands for the Fahrenheit value):\n~ (32 - .) * -5 / 9 Here’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) + geom_line() + scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9, name = \u0026quot;Celsius\u0026quot;)) + labs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) + theme_minimal() For fun, we could also convert it to Kelvin, which uses this formula:\n\\[ \\text{K} = (\\text{F} - 32) \\times \\frac{5}{9} + 273.15 \\]\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) + geom_line() + scale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15, name = \u0026quot;Kelvin\u0026quot;)) + labs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) + theme_minimal()  Combining plots A good alternative to using two y-axes is to use two plots instead. The patchwork package makes this really easy to do with R. There are other similar packages that do this, like cowplot and gridExtra, but I’ve found that patchwork is the easiest to use and it actually aligns the different plot elements like axis lines and legends (yay alignment in CRAP!). The documentation for patchwork is really great and full of examples—you should check it out to see all the things you can do with it!\nTo use patchwork, we need to (1) save our plots as objects and (2) add them together with +.\nFor instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:\n# Temperature in Atlanta temp_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) + geom_line() + geom_smooth() + scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9, name = \u0026quot;Celsius\u0026quot;)) + labs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) + theme_minimal() temp_plot # Humidity in Atlanta humidity_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = humidity)) + geom_line() + geom_smooth() + labs(x = NULL, y = \u0026quot;Humidity\u0026quot;) + theme_minimal() humidity_plot Right now, these are two separate plots, but we can combine them with + if we load patchwork:\nlibrary(patchwork) temp_plot + humidity_plot By default, patchwork will put these side-by-side, but we can change that with the plot_layout() function:\ntemp_plot + humidity_plot + plot_layout(ncol = 1) We can also play with other arguments in plot_layout(). If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:\ntemp_plot + humidity_plot + plot_layout(ncol = 1, heights = c(0.7, 0.3))  Scatterplot matrices We can visualize the correlations between pairs of variables with the ggpairs() function in the GGally package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into ggpairs() to see all the correlation information:\nlibrary(GGally) weather_correlations \u0026lt;- weather_atl %\u0026gt;% select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) ggpairs(weather_correlations) It looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind spped and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).\nEven though ggpairs() doesn’t use the standard ggplot(...) + geom_whatever() syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:\nggpairs(weather_correlations) + labs(title = \u0026quot;Correlations!\u0026quot;) + theme_dark()  Correlograms Scatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In the readings for today, Claus Wilke showed a type of plot called a correlogram which is more appropriate for publication.\nThese are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.\nFirst we need to build a correlation matrix of the main variables we care about. Ordinarily the cor() function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into cor() though, it’ll calculate the correlation between each pair of columns\n# Create a correlation matrix things_to_correlate \u0026lt;- weather_atl %\u0026gt;% select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %\u0026gt;% cor() things_to_correlate ## temperatureHigh temperatureLow humidity windSpeed precipProbability ## temperatureHigh 1.00 0.920 -0.030 -0.377 -0.124 ## temperatureLow 0.92 1.000 0.112 -0.450 -0.026 ## humidity -0.03 0.112 1.000 0.011 0.722 ## windSpeed -0.38 -0.450 0.011 1.000 0.196 ## precipProbability -0.12 -0.026 0.722 0.196 1.000 The two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to NA):\n# Get rid of the lower triangle things_to_correlate[lower.tri(things_to_correlate)] \u0026lt;- NA things_to_correlate ## temperatureHigh temperatureLow humidity windSpeed precipProbability ## temperatureHigh 1 0.92 -0.03 -0.377 -0.124 ## temperatureLow NA 1.00 0.11 -0.450 -0.026 ## humidity NA NA 1.00 0.011 0.722 ## windSpeed NA NA NA 1.000 0.196 ## precipProbability NA NA NA NA 1.000 Finally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the things_to_correlate matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named measure1, and take all the correlation numbers and put them in a column named cor In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)\nthings_to_correlate_long \u0026lt;- things_to_correlate %\u0026gt;% # Convert from a matrix to a data frame as.data.frame() %\u0026gt;% # Matrixes have column names that don\u0026#39;t get converted to columns when using # as.data.frame(), so this adds those names as a column rownames_to_column(\u0026quot;measure2\u0026quot;) %\u0026gt;% # Make this long. Take all the columns except measure2 and put their names in # a column named measure1 and their values in a column named cor pivot_longer(cols = -measure2, names_to = \u0026quot;measure1\u0026quot;, values_to = \u0026quot;cor\u0026quot;) %\u0026gt;% # Make a new column with the rounded version of the correlation value mutate(nice_cor = round(cor, 2)) %\u0026gt;% # Remove rows where the two measures are the same (like the correlation # between humidity and humidity) filter(measure2 != measure1) %\u0026gt;% # Get rid of the empty triangle filter(!is.na(cor)) %\u0026gt;% # Put these categories in order mutate(measure1 = fct_inorder(measure1), measure2 = fct_inorder(measure2)) things_to_correlate_long ## # A tibble: 10 x 4 ## measure2 measure1 cor nice_cor ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 temperatureHigh temperatureLow 0.920 0.92 ## 2 temperatureHigh humidity -0.0301 -0.03 ## 3 temperatureHigh windSpeed -0.377 -0.38 ## 4 temperatureHigh precipProbability -0.124 -0.12 ## 5 temperatureLow humidity 0.112 0.11 ## 6 temperatureLow windSpeed -0.450 -0.45 ## 7 temperatureLow precipProbability -0.0255 -0.03 ## 8 humidity windSpeed 0.0108 0.01 ## 9 humidity precipProbability 0.722 0.72 ## 10 windSpeed precipProbability 0.196 0.2 Phew. With the data all tidied like that, we can make a correlogram with a heatmap. This is just like the heatmap you made in session 4, but here we manipulate the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.\nggplot(things_to_correlate_long, aes(x = measure2, y = measure1, fill = cor)) + geom_tile() + geom_text(aes(label = nice_cor)) + scale_fill_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;, limits = c(-1, 1)) + labs(x = NULL, y = NULL) + coord_equal() + theme_minimal() + theme(panel.grid = element_blank()) Instead of using a heatmap, we can also use points, which encode the correlation information both as color and as size. To do that, we just need to switch geom_tile() to geom_point() and set the size = cor mapping:\nggplot(things_to_correlate_long, aes(x = measure2, y = measure1, color = cor)) + # Size by the absolute value so that -0.7 and 0.7 are the same size geom_point(aes(size = abs(cor))) + scale_color_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;, limits = c(-1, 1)) + scale_size_area(max_size = 15, limits = c(-1, 1), guide = FALSE) + labs(x = NULL, y = NULL) + coord_equal() + theme_minimal() + theme(panel.grid = element_blank())  Simple regression We can also visualize the relationships between variables using regression. Simple regression is easy to visualize, since you’re only working with an X and a Y. For instance, what’s the relationship between humidity and high temperatures during the summer?\nFirst, let’s filter the data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.\nweather_atl_summer \u0026lt;- weather_atl %\u0026gt;% filter(time \u0026gt;= \u0026quot;2019-05-01\u0026quot;, time \u0026lt;= \u0026quot;2019-09-30\u0026quot;) %\u0026gt;% mutate(humidity_scaled = humidity * 100, moonPhase_scaled = moonPhase * 100, precipProbability_scaled = precipProbability * 100, cloudCover_scaled = cloudCover * 100) Then we can build a simple regression model:\nmodel_simple \u0026lt;- lm(temperatureHigh ~ humidity_scaled, data = weather_atl_summer) tidy(model_simple, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 104. 2.35 44.3 1.88e-88 99.5 109. ## 2 humidity_scaled -0.241 0.0358 -6.74 3.21e-10 -0.312 -0.170 We can interpret these coefficients like so:\n The intercept shows that the average temperature when there’s 0% humidity is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line. The coefficient for humidity_scaled shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.  Visualizing this model is simple, since there are only two variables:\nggplot(weather_atl_summer, aes(x = humidity_scaled, y = temperatureHigh)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; And indeed, as humidity increases, temperatures decrease.\n Coefficient plots But if we use multiple variables in the model, it gets really hard to visualize the results since we’re working with multiple dimensions. Instead, we can use coefficient plots to see the individual coefficients in the model.\nFirst, let’s build a more complex model:\nmodel_complex \u0026lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled + precipProbability_scaled + windSpeed + pressure + cloudCover_scaled, data = weather_atl_summer) tidy(model_complex, conf.int = TRUE) ## # A tibble: 7 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 262. 125. 2.09 0.0380 14.8 510. ## 2 humidity_scaled -0.111 0.0757 -1.47 0.143 -0.261 0.0381 ## 3 moonPhase_scaled 0.0116 0.0126 0.917 0.360 -0.0134 0.0366 ## 4 precipProbability_scaled 0.0356 0.0203 1.75 0.0820 -0.00458 0.0758 ## 5 windSpeed -1.78 0.414 -4.29 0.0000326 -2.59 -0.958 ## 6 pressure -0.157 0.122 -1.28 0.203 -0.398 0.0854 ## 7 cloudCover_scaled -0.0952 0.0304 -3.14 0.00207 -0.155 -0.0352 We can interpret these coefficients like so:\n Holding everything else constant, a 1% increase in humidity is associated with a 0.11° decrease in the high temperature, on average, but the effect is not statistically significant Holding everything else constant, a 1% increase in moon visibility is associated with a 0.01° increase in the high temperature, on average, and the effect is not statistically significant Holding everything else constant, a 1% increase in the probability of precipitation is associated with a 0.04° increase in the high temperature, on average, and the effect is not statistically significant Holding everything else constant, a 1 mph increase in the wind speed is associated with a 1.8° decrease in the high temperature, on average, and the effect is statistically significant Holding everything else constant, a 1 unit increase in barometric pressure is associated with a 0.15° decrease in the high temperature, on average, and the effect is not statistically significant Holding everything else constant, a 1% increase in cloud cover is associated with a 0.01° decrease in the high temperature, on average, and the effect is statistically significant The intercept is pretty useless. It shows that the predicted temperature will be 262° when humidity is 0%, the moon is invisible, there’s no chance of precipitation, no wind, no barometric pressure, and no cloud cover. Yikes.  To plot all these things at once, we’ll store the results of tidy(model_complex) as a data frame, remove the useless intercept, and plot it using geom_pointrange():\nmodel_tidied \u0026lt;- tidy(model_complex, conf.int = TRUE) %\u0026gt;% filter(term != \u0026quot;(Intercept)\u0026quot;) ggplot(model_tidied, aes(x = estimate, y = term)) + geom_vline(xintercept = 0, color = \u0026quot;red\u0026quot;, linetype = \u0026quot;dotted\u0026quot;) + geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) + labs(x = \u0026quot;Coefficient estimate\u0026quot;, y = NULL) + theme_minimal() Neat! Now we can see how big these different coefficients are and how close they are to zero. Wind speed has a big significant effect on temperature. The others are all very close to zero.\n Marginal effects plots Instead of just looking at the coefficients, we can also see the effect of moving different variables up and down like sliders and switches. Remember that regression coefficients allow us to build actual mathematical formulas that predict the value of Y. The coefficients from model_compex yield the following big hairy ugly equation:\n\\[ \\begin{aligned} \\hat{\\text{High temperature}} =\u0026amp; 262 - 0.11 \\times \\text{humidity_scaled } \\\\ \u0026amp; + 0.01 \\times \\text{moonPhase_scaled } + 0.04 \\times \\text{precipProbability_scaled } \\\\ \u0026amp; - 1.78 \\times \\text{windSpeed} - 0.16 \\times \\text{pressure} - 0.095 \\times \\text{cloudCover_scaled} \\end{aligned} \\]\nIf we plug in values for each of the explanatory variables, we can get the predicted value of high temperature, or \\(\\hat{y}\\).\nThe augment() function in the broom library allows us to take a data frame of explanatory variable values, plug them into the model equation, and get predictions out. For example, let’s set each of the variables to some arbitrary values (50% for humidity, moon phase, chance of rain, and cloud cover; 1000 for pressure, and 1 MPH for wind speed).\nnewdata_example \u0026lt;- tibble(humidity_scaled = 50, moonPhase_scaled = 50, precipProbability_scaled = 50, windSpeed = 1, pressure = 1000, cloudCover_scaled = 50) newdata_example ## # A tibble: 1 x 6 ## humidity_scaled moonPhase_scaled precipProbability_scaled windSpeed pressure cloudCover_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 50 50 50 1 1000 50 We can plug these values into the model with augment():\n# I use select() here because augment() returns columns for all the explanatory # variables, and the .fitted column with the predicted value is on the far right # and gets cut off augment(model_complex, newdata = newdata_example) %\u0026gt;% select(.fitted, .se.fit) ## # A tibble: 1 x 2 ## .fitted .se.fit ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 96.2 3.19 Given these weather conditions, the predicted high temperature is 96.2°. Now you’re an armchair meteorologist!\nWe can follow the same pattern to show how the predicted temperature changes as specific variables change across a whole range. Here, we create a data frame of possible wind speeds and keep all the other explanatory variables at their means:\nnewdata \u0026lt;- tibble(windSpeed = seq(0, 8, 0.5), pressure = mean(weather_atl_summer$pressure), precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled), moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled), humidity_scaled = mean(weather_atl_summer$humidity_scaled), cloudCover_scaled = mean(weather_atl_summer$cloudCover_scaled)) newdata ## # A tibble: 17 x 6 ## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 1016. 40.2 50.7 64.8 29.5 ## 2 0.5 1016. 40.2 50.7 64.8 29.5 ## 3 1 1016. 40.2 50.7 64.8 29.5 ## 4 1.5 1016. 40.2 50.7 64.8 29.5 ## 5 2 1016. 40.2 50.7 64.8 29.5 ## 6 2.5 1016. 40.2 50.7 64.8 29.5 ## 7 3 1016. 40.2 50.7 64.8 29.5 ## 8 3.5 1016. 40.2 50.7 64.8 29.5 ## 9 4 1016. 40.2 50.7 64.8 29.5 ## 10 4.5 1016. 40.2 50.7 64.8 29.5 ## 11 5 1016. 40.2 50.7 64.8 29.5 ## 12 5.5 1016. 40.2 50.7 64.8 29.5 ## 13 6 1016. 40.2 50.7 64.8 29.5 ## 14 6.5 1016. 40.2 50.7 64.8 29.5 ## 15 7 1016. 40.2 50.7 64.8 29.5 ## 16 7.5 1016. 40.2 50.7 64.8 29.5 ## 17 8 1016. 40.2 50.7 64.8 29.5 If we feed this big data frame into augment(), we can get the predicted high temperature for each row. We can also use the .se.fit column to calculate the 95% confidence interval for each predicted value. We take the standard error, multiply it by -1.96 and 1.96 (or the quantile of the normal distribution at 2.5% and 97.5%), and add that value to the estimate.\npredicted_values \u0026lt;- augment(model_complex, newdata = newdata) %\u0026gt;% mutate(conf.low = .fitted + (-1.96 * .se.fit), conf.high = .fitted + (1.96 * .se.fit)) predicted_values %\u0026gt;% select(windSpeed, .fitted, .se.fit, conf.low, conf.high) %\u0026gt;% head() ## # A tibble: 6 x 5 ## windSpeed .fitted .se.fit conf.low conf.high ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 95.3 1.63 92.2 98.5 ## 2 0.5 94.5 1.42 91.7 97.2 ## 3 1 93.6 1.22 91.2 96.0 ## 4 1.5 92.7 1.03 90.7 94.7 ## 5 2 91.8 0.836 90.1 93.4 ## 6 2.5 90.9 0.653 89.6 92.2 Cool! Just looking at the data in the table, we can see that predicted temperature drops as windspeed increases. We can plot this to visualize the effect:\nggplot(predicted_values, aes(x = windSpeed, y = .fitted)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \u0026quot;#BF3984\u0026quot;, alpha = 0.5) + geom_line(size = 1, color = \u0026quot;#BF3984\u0026quot;) + labs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) + theme_minimal() We just manipulated one of the model coefficients and held everything else at its mean. We can manipulate multiple variables too and encode them all on the graph. For instance, what is the effect of windspeed and cloud cover on the temperature?\nWe’ll follow the same process, but vary both windSpeed and cloudCover_scaled. Instead of using tibble(), we use exapnd_grid(), which creates every combination of the variables we specify. Instead of varying cloud cover by every possible value (like from 0 to 100), we’ll choose four possible cloud cover types: 0%, 33%, 66%, and 100%. Everything else will be at its mean.\nnewdata_fancy \u0026lt;- expand_grid(windSpeed = seq(0, 8, 0.5), pressure = mean(weather_atl_summer$pressure), precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled), moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled), humidity_scaled = mean(weather_atl_summer$humidity_scaled), cloudCover_scaled = c(0, 33, 66, 100)) newdata_fancy ## # A tibble: 68 x 6 ## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 1016. 40.2 50.7 64.8 0 ## 2 0 1016. 40.2 50.7 64.8 33 ## 3 0 1016. 40.2 50.7 64.8 66 ## 4 0 1016. 40.2 50.7 64.8 100 ## 5 0.5 1016. 40.2 50.7 64.8 0 ## 6 0.5 1016. 40.2 50.7 64.8 33 ## 7 0.5 1016. 40.2 50.7 64.8 66 ## 8 0.5 1016. 40.2 50.7 64.8 100 ## 9 1 1016. 40.2 50.7 64.8 0 ## 10 1 1016. 40.2 50.7 64.8 33 ## # … with 58 more rows Notice now that windSpeed repeats four times (0, 0, 0, 0, 0.5, 0.5, etc.), since there are four possible cloudCover_scaled values (0, 33, 66, 100).\nWe can plot this now, just like before, with wind speed on the x-axis, the predicted temperature on the y-axis, and colored and faceted by cloud cover:\npredicted_values_fancy \u0026lt;- augment(model_complex, newdata = newdata_fancy) %\u0026gt;% mutate(conf.low = .fitted + (-1.96 * .se.fit), conf.high = .fitted + (1.96 * .se.fit)) %\u0026gt;% # Make cloud cover a categorical variable mutate(cloudCover_scaled = factor(cloudCover_scaled)) ggplot(predicted_values_fancy, aes(x = windSpeed, y = .fitted)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled), alpha = 0.5) + geom_line(aes(color = cloudCover_scaled), size = 1) + labs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) + theme_minimal() + guides(fill = FALSE, color = FALSE) + facet_wrap(vars(cloudCover_scaled), nrow = 1) That’s so neat! Temperatures go down slightly as cloud cover increases. If we wanted to improve the model, we’d add an interaction term between cloud cover and windspeed so that each line would have a different slope in addition to a different intercept, but that’s beyond the scope of this class.\n  ","date":1589846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589846400,"objectID":"40eeabbfe9d21a7292000fc4fe538d43","permalink":"/example/07-example/","publishdate":"2020-05-19T00:00:00Z","relpermalink":"/example/07-example/","section":"example","summary":"Live coding example Complete code  Load and clean data Legal dual y-axes Combining plots Scatterplot matrices Correlograms Simple regression Coefficient plots Marginal effects plots    For this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.","tags":null,"title":"Relationships","type":"docs"},{"authors":null,"categories":null,"content":"  There isn’t really a lesson for today, and as we get further into the semester, the need for lessons will continue to decrease. Now that each section is focused on a few specific geoms and how to apply them, you don’t need to go through interactive tutorials so much, since you should (hopefully!) be getting the hang of how ggplot works. (IF NOT, please reach out for help on Slack or via e-mail! I’m more than happy and ready to help!)\nFor the lesson, read through the code examples in the example to see how to make dual y-axes, scatterplot matrices, coefficient plots, and marginal effects plots.\n","date":1589846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589846400,"objectID":"36f453f698c7a3972914c894cffd917b","permalink":"/lesson/07-lesson/","publishdate":"2020-05-19T00:00:00Z","relpermalink":"/lesson/07-lesson/","section":"lesson","summary":"There isn’t really a lesson for today, and as we get further into the semester, the need for lessons will continue to decrease. Now that each section is focused on a few specific geoms and how to apply them, you don’t need to go through interactive tutorials so much, since you should (hopefully!) be getting the hang of how ggplot works. (IF NOT, please reach out for help on Slack or via e-mail!","tags":null,"title":"Relationships","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Small multiples Task 3: Slopegraphs Bonus task! Bump charts Turning everything in Postscript: how I got this unemployment data   Getting started For this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, I describe how I built this dataset down below).\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\n  unemployment.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  08-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 08-exercise.Rmd your-project-name.Rproj data\\ unemployment.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  08-exercise.zip  The example for today’s session will be incredibly helpful for this exercise. Reference it.\nAgain, you don’t need to make your plots super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Small multiples Use data from the US Bureau of Labor Statistics (BLS) to show the trends in employment rate for all 50 states between 2006 and 2016. What stories does this plot tell? Which states struggled to recover from the 2008–09 recession?\nSome hints/tips:\n You won’t need to filter out any missing rows because the data here is complete—there are no state-year combinations with missing unemployment data.\n You’ll be plotting 51 facets. You can filter out DC if you want to have a better grid (like 5 × 10), or you can try using facet_geo() from the geofacet package to lay out the plots like a map of the US (try this!).\n Plot the date column along the x-axis, not the year column. If you plot by year, you’ll get weird looking lines (try it for fun?), since these observations are monthly. If you really want to plot by year only, you’ll need to create a different data frame where you group by year and state and calculate the average unemployment rate for each year/state combination (i.e. group_by(year, state) %\u0026gt;% summarize(avg_unemployment = mean(unemployment)))\n Try mapping other aesthetics onto the graph too. You’ll notice there are columns for region and division—play with those as colors, for instance.\n This plot might be big, so make sure you adjust fig.width and fig.height in the chunk options so that it’s visible when you knit it. You might also want to used ggsave() to save it with extra large dimensions.\n   Task 3: Slopegraphs Use data from the BLS to create a slopegraph that compares the unemployment rate in January 2006 with the unemployment rate in January 2009, either for all 50 states at once (good luck with that!) or for a specific region or division. Make sure the plot doesn’t look too busy or crowded in the end.\nWhat story does this plot tell? Which states in the US (or in the specific region you selected) were the most/least affected the Great Recession?\nSome hints/tips:\n You should use filter() to only select rows where the year is 2006 or 2009 (i.e. filter(year %in% c(2006, 2009)) and to select rows where the month is January (filter(month == 1) or filter(month_name == \"January\"))\n In order for the year to be plotted as separate categories on the x-axis, it needs to be a factor, so use mutate(year = factor(year)) to convert it.\n To make ggplot draw lines between the 2006 and 2009 categories, you need to include group = state in the aesthetics.\n   Bonus task! Bump charts This is entirely optional but might be fun.\nFor extra fun times, if you feel like it, create a bump chart showing something from the unemployment data (perhaps the top 10 states or bottom 10 states in unemployment?) Adapt the code in the example for today’s session.\nIf you do this, plotting 51 lines is going to be a huge mess. But filtering the data is also a bad idea, because states could drop in and out of the top/bottom 10 over time, and we don’t want to get rid of them. Instead, you can zoom in on a specific range of data in your plot with coord_cartesian(ylim = c(1, 10)), for instance.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n Postscript: how I got this unemployment data For the curious, here’s the code I used to download the unemployment data from the BLS.\nAnd to pull the curtain back and show how much googling is involved in data visualization (and data analysis and programming in general), here was my process for getting this data:\nI thought “I want to have students show variation in something domestic over time” and then I googled “us data by state”. Nothing really came up (since it was an exceedingly vague search in the first place), but some results mentioned unemployment rates, so I figured that could be cool. I googled “unemployment statistics by state over time” and found that the BLS keeps statistics on this. I clicked on the “Data Tools” link in their main navigation bar, clicked on “Unemployment”, and then clicked on the “Multi-screen data search” button for the Local Area Unemployment Statistics (LAUS). I walked through the multiple screens and got excited that I’d be able to download all unemployment stats for all states for a ton of years, BUT THEN the final page had links to 51 individual Excel files, which was dumb. So I went back to Google and searched for “download bls data r” and found a few different packages people have written to do this. The first one I clicked on was blscrapeR at GitHub, and it looked like it had been updated recently, so I went with it. I followed the examples in the blscrapeR package and downloaded data for every state.  Another day in the life of doing modern data science. I had no idea people had written R packages to access BLS data, but there are like 3 packages out there! After a few minutes of tinkering, I got it working and it’s super magic.\n ","date":1589932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589932800,"objectID":"f6fd06f7395b10b70999e7c191a2bd73","permalink":"/assignment/08-exercise/","publishdate":"2020-05-20T00:00:00Z","relpermalink":"/assignment/08-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Small multiples Task 3: Slopegraphs Bonus task! Bump charts Turning everything in Postscript: how I got this unemployment data   Getting started For this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, I describe how I built this dataset down below).\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.","tags":null,"title":"Comparisons","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load and clean data Small multiples Sparklines Slopegraphs Bump charts    For this example, we’re going to use cross-national data, but instead of using the typical gapminder dataset, we’re going to collect data directly from the World Bank’s Open Data portal\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  wdi_raw.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(WDI) # For getting data from the World Bank library(geofacet) # For map-shaped facets library(scales) # For helpful scale functions like dollar() library(ggrepel) # For non-overlapping labels The World Bank has a ton of country-level data at data.worldbank.org. We can use a package named WDI (world development indicators) to access their servers and download the data directly into R.\nTo do this, we need to find the special World Bank codes for specific variables we want to get. These codes come from the URLs of the World Bank’s website. For instance, if you search for “access to electricity” at the World Bank’s website, you’ll find this page. If you look at the end of the URL, you’ll see a cryptic code: EG.ELC.ACCS.ZS. That’s the World Bank’s ID code for the “Access to electricity (% of population)” indicator.\nWe can feed a list of ID codes to the WDI() function to download data for those specific indicators. We want data from 1995-2015, so we set the start and end years accordingly. The extra=TRUE argument means that it’ll also include other helpful details like region, aid status, etc. Without it, it would only download the indicators we listed.\nindicators \u0026lt;- c(\u0026quot;SP.DYN.LE00.IN\u0026quot;, # Life expectancy \u0026quot;EG.ELC.ACCS.ZS\u0026quot;, # Access to electricity \u0026quot;EN.ATM.CO2E.PC\u0026quot;, # CO2 emissions \u0026quot;NY.GDP.PCAP.KD\u0026quot;) # GDP per capita wdi_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE, start = 1995, end = 2015) head(wdi_raw) Downloading data from the World Bank every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). It’s good practice to save this raw data as a CSV file and then work with that.\nwrite_csv(wdi_raw, \u0026quot;data/wdi_raw.csv\u0026quot;) Since we care about reproducibility, we still want to include the code we used to get data from the World Bank, we just don’t want it to actually run. You can include chunks but not run them by setting eval=FALSE in the chunk options. In this little example, we show the code for downloading the data, but we don’t evaluate the chunk. We then include a chunk that loads the data from a CSV file with read_csv(), but we don’t include it (include=FALSE). That way, in the knitted file we see the WDI() code, but in reality it’s loading the data from CSV. Super tricky.\nI first download data from the World Bank: ```{r get-wdi-data, eval=FALSE} wdi_raw \u0026lt;- WDI(...) write_csv(wdi_raw, \u0026quot;data/wdi_raw.csv\u0026quot;) ``` ```{r load-wdi-data-real, include=FALSE} wdi_raw \u0026lt;- read_csv(\u0026quot;data/wdi_raw.csv\u0026quot;) ``` Then we clean up the data a little, filtering out rows that aren’t actually countries and renaming the ugly World Bank code columns to actual words:\nwdi_clean \u0026lt;- wdi_raw %\u0026gt;% filter(region != \u0026quot;Aggregates\u0026quot;) %\u0026gt;% select(iso2c, country, year, life_expectancy = SP.DYN.LE00.IN, access_to_electricity = EG.ELC.ACCS.ZS, co2_emissions = EN.ATM.CO2E.PC, gdp_per_cap = NY.GDP.PCAP.KD, region, income) head(wdi_clean) ## # A tibble: 6 x 9 ## iso2c country year life_expectancy access_to_electricity co2_emissions gdp_per_cap region income ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AD Andorra 2015 NA 100 NA 41768. Europe \u0026amp; Central Asia High income ## 2 AD Andorra 2004 NA 100 7.36 47033. Europe \u0026amp; Central Asia High income ## 3 AD Andorra 2001 NA 100 7.79 41421. Europe \u0026amp; Central Asia High income ## 4 AD Andorra 2002 NA 100 7.59 42396. Europe \u0026amp; Central Asia High income ## 5 AD Andorra 2014 NA 100 5.83 40790. Europe \u0026amp; Central Asia High income ## 6 AD Andorra 1995 NA 100 6.66 32918. Europe \u0026amp; Central Asia High income  Small multiples First we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country.\nlife_expectancy_small \u0026lt;- wdi_clean %\u0026gt;% filter(country %in% c(\u0026quot;Argentina\u0026quot;, \u0026quot;Bolivia\u0026quot;, \u0026quot;Brazil\u0026quot;, \u0026quot;Belize\u0026quot;, \u0026quot;Canada\u0026quot;, \u0026quot;Chile\u0026quot;)) ggplot(data = life_expectancy_small, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country)) Small multiples! That’s all we need to do.\nWe can do some fancier things, though. We can make this plot hyper minimalist:\nggplot(data = life_expectancy_small, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country), scales = \u0026quot;free_y\u0026quot;) + theme_void() + theme(strip.text = element_text(face = \u0026quot;bold\u0026quot;)) We can do a whole part of a continent (poor Iraq and Syria 😭)\nlife_expectancy_mena \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;Middle East \u0026amp; North Africa\u0026quot;) ggplot(data = life_expectancy_mena, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country), scales = \u0026quot;free_y\u0026quot;, nrow = 3) + theme_void() + theme(strip.text = element_text(face = \u0026quot;bold\u0026quot;)) We can use the geofacet package to arrange these facets by geography:\nlife_expectancy_eu \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;Europe \u0026amp; Central Asia\u0026quot;) ggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_geo(vars(country), grid = \u0026quot;eu_grid1\u0026quot;, scales = \u0026quot;free_y\u0026quot;) + labs(x = NULL, y = NULL, title = \u0026quot;Life expectancy from 1995–2015\u0026quot;, caption = \u0026quot;Source: The World Bank (SP.DYN.LE00.IN)\u0026quot;) + theme_minimal() + theme(strip.text = element_text(face = \u0026quot;bold\u0026quot;), plot.title = element_text(face = \u0026quot;bold\u0026quot;), axis.text.x = element_text(angle = 45, hjust = 1)) Neat!\n Sparklines Sparklines are just line charts (or bar charts) that are really really small.\nindia_co2 \u0026lt;- wdi_clean %\u0026gt;% filter(country == \u0026quot;India\u0026quot;) plot_india \u0026lt;- ggplot(india_co2, aes(x = year, y = co2_emissions)) + geom_line() + theme_void() plot_india ggsave(\u0026quot;india_co2.pdf\u0026quot;, plot_india, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;india_co2.png\u0026quot;, plot_india, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;) china_co2 \u0026lt;- wdi_clean %\u0026gt;% filter(country == \u0026quot;China\u0026quot;) plot_china \u0026lt;- ggplot(china_co2, aes(x = year, y = co2_emissions)) + geom_line() + theme_void() plot_china ggsave(\u0026quot;china_co2.pdf\u0026quot;, plot_china, width = 1, heighlt = 0.15, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;china_co2.png\u0026quot;, plot_china, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;) You can then use those saved tiny plots in your text.\n Both India and China have seen increased CO2 emissions over the past 20 years.\n  Slopegraphs We can make a slopegraph to show changes in GDP per capita between two time periods. We need to first filter our WDI to include only the start and end years (here 1995 and 2015). Then, to make sure that we’re using complete data, we’ll get rid of any country that has missing data for either 1995 or 2015. The group_by(...) %\u0026gt;% filter(...) %\u0026gt;% ungroup() pipeline does this, with the !any(is.na(gdp_per_cap)) test keeping any rows where any of the gdp_per_cap values are not missing for the whole country.\nWe then add a couple special columns for labels. The paste0() function concatenates strings and variables together, so that paste0(\"2 + 2 = \", 2 + 2) would show “2 + 2 = 4”. Here we make labels that say either “Country name: $GDP” or “$GDP” depending on the year.\ngdp_south_asia \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;South Asia\u0026quot;) %\u0026gt;% filter(year %in% c(1995, 2015)) %\u0026gt;% # Look at each country individually group_by(country) %\u0026gt;% # Remove the country if any of its gdp_per_cap values are missing filter(!any(is.na(gdp_per_cap))) %\u0026gt;% ungroup() %\u0026gt;% # Make year a factor mutate(year = factor(year)) %\u0026gt;% # Make some nice label columns # If the year is 1995, format it like \u0026quot;Country name: $GDP\u0026quot;. If the year is # 2015, format it like \u0026quot;$GDP\u0026quot; mutate(label_first = ifelse(year == 1995, paste0(country, \u0026quot;: \u0026quot;, dollar(round(gdp_per_cap))), NA), label_last = ifelse(year == 2015, dollar(round(gdp_per_cap, 0)), NA)) With the data filtered like this, we can plot it by mapping year to the x-axis, GDP per capita to the y-axis, and coloring by country. To make the lines go across the two categorical labels in the x-axis (since we made year a factor/category), we need to also specify the group aesthetic.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) Cool! We’re getting closer. We can definitely see different slopes, but with 7 different colors, it’s hard to see exactly which country is which. Instead, we can directly label each of these lines with geom_text():\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text(aes(label = country)) + guides(color = FALSE) That gets us a little closer, but the country labels are hard to see, and we could include more information, like the actual values. Remember those label_first and label_last columns we made? Let’s use those instead:\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text(aes(label = label_first)) + geom_text(aes(label = label_last)) + guides(color = FALSE) Now we have dollar amounts and country names, but the labels are still overlapping and really hard to read. To fix this, we can make the labels repel away from each other and randomly position in a way that makes them not overlap. The ggrepel package lets us do this with geom_text_repel()\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first)) + geom_text_repel(aes(label = label_last)) + guides(color = FALSE) Now none of the labels are on top of each other, but the labels are still on top of the lines. Also, some of the labels moved inward and outward along the x-axis, but they don’t need to do that—they just need to shift up and down. We can force the labels to only move up and down by setting the direction = \"y\" argument, and we can move all the labels to the left or right with the nudge_x argument. The seed argument makes sure that the random label placement is the same every time we run this. It can be whatever number you want—it just has to be a number.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first), direction = \u0026quot;y\u0026quot;, nudge_x = -1, seed = 1234) + geom_text_repel(aes(label = label_last), direction = \u0026quot;y\u0026quot;, nudge_x = 1, seed = 1234) + guides(color = FALSE) That’s it! Let’s take the theme off completely, change the colors a little, and it should be perfect.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first), direction = \u0026quot;y\u0026quot;, nudge_x = -1, seed = 1234) + geom_text_repel(aes(label = label_last), direction = \u0026quot;y\u0026quot;, nudge_x = 1, seed = 1234) + guides(color = FALSE) + scale_color_viridis_d(option = \u0026quot;magma\u0026quot;, end = 0.9) + theme_void()  Bump charts Finally, we can make a bump chart that shows changes in rankings over time. We’ll look at CO2 emissions in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the rank() function to rank countries by the co2_emissions column.\nsa_co2 \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;South Asia\u0026quot;) %\u0026gt;% filter(year \u0026gt;= 2004, year \u0026lt; 2015) %\u0026gt;% group_by(year) %\u0026gt;% mutate(rank = rank(co2_emissions)) We then plot this with points and lines, reversing the y-axis so 1 is at the top:\nggplot(sa_co2, aes(x = year, y = rank, color = country)) + geom_line() + geom_point() + scale_y_reverse(breaks = 1:8) Afghanistan and Nepal switched around for the number 1 spot, while India dropped from 4 to 6, switching places with Pakistan.\nAs with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use geom_text() again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the data argument in geom_text() though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner.\nggplot(sa_co2, aes(x = year, y = rank, color = country)) + geom_line(size = 2) + geom_point(size = 4) + geom_text(data = filter(sa_co2, year == 2004), aes(label = iso2c, x = 2003.25), fontface = \u0026quot;bold\u0026quot;) + geom_text(data = filter(sa_co2, year == 2014), aes(label = iso2c, x = 2014.75), fontface = \u0026quot;bold\u0026quot;) + guides(color = FALSE) + scale_y_reverse(breaks = 1:8) + scale_x_continuous(breaks = 2004:2014) + scale_color_viridis_d(option = \u0026quot;magma\u0026quot;, begin = 0.2, end = 0.9) + labs(x = NULL, y = \u0026quot;Rank\u0026quot;) + theme_minimal() + theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank()) If you want to be super fancy, you can use flags instead of country codes, but that’s a little more complicated (you need to install the ggflags package. See here for an example.\n  ","date":1589932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589932800,"objectID":"66dae6a89dc933d1691fce47e0612205","permalink":"/example/08-example/","publishdate":"2020-05-20T00:00:00Z","relpermalink":"/example/08-example/","section":"example","summary":"Live coding example Complete code  Load and clean data Small multiples Sparklines Slopegraphs Bump charts    For this example, we’re going to use cross-national data, but instead of using the typical gapminder dataset, we’re going to collect data directly from the World Bank’s Open Data portal\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):","tags":null,"title":"Comparisons","type":"docs"},{"authors":null,"categories":null,"content":"  Like yesterday, there isn’t really a lesson today. You’re not learning how to use any new functions—you’re learning how to apply the geoms you already know in cool and exciting ways. But don’t worry! You’ll have a lesson for session 9!\nFor the lesson, read through the code examples in the example to see how to make small multiples, sparklines, geofacets, and slopegraphs.\n","date":1589932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589932800,"objectID":"943054b07389b7a8559401831f3a8cd6","permalink":"/lesson/08-lesson/","publishdate":"2020-05-20T00:00:00Z","relpermalink":"/lesson/08-lesson/","section":"lesson","summary":"Like yesterday, there isn’t really a lesson today. You’re not learning how to use any new functions—you’re learning how to apply the geoms you already know in cool and exciting ways. But don’t worry! You’ll have a lesson for session 9!\nFor the lesson, read through the code examples in the example to see how to make small multiples, sparklines, geofacets, and slopegraphs.","tags":null,"title":"Comparisons","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Annotations Turning everything in   Getting started For this exercise, you’ll use whatever data you want to make a plot and add annotations to it. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the gapminder package, download stuff from the World Bank using the WDI package, or use something from this list of datasets.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  09-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 09-exercise.Rmd your-project-name.Rproj data\\ WHATEVER.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  09-exercise.zip  The documentation for annotate(), geom_text() and geom_label(), and geom_text_repel() and geom_label_repel() will be incredibly helpful for this exercise. The example for today’s session is also helpful for seeing annotations in real life.\nAgain, you don’t need to make your plots super fancy (except for these annotations), but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Annotations Do the following:\nMake a plot. Any kind of plot will do (though it might be easiest to work with geom_point()).\n Label (some or all of) the points using one of geom_text(), geom_label(), geom_text_repel(), or geom_label_repel(). You might need to make a new indicator variable so that you only highlight a few of the points instead of all of them. See this slide for an example, as well as the complete example plot on the example page for today’s session.\n Add *at least two each** the following annotations somewhere on the plot using annotate():\n Text An arrow (look at this page, or search for “arrow” on this page for examples). Make a curved arrow for bonus fun. A rectangle  You can add more if you want, but those three are the minimum. Try to incorporate the annotations into the design of the plot rather than just placing them wherever.\n   Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1590019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590019200,"objectID":"65422182c1ee407c07ce684fa4f6e663","permalink":"/assignment/09-exercise/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/assignment/09-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Annotations Turning everything in   Getting started For this exercise, you’ll use whatever data you want to make a plot and add annotations to it. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the gapminder package, download stuff from the World Bank using the WDI package, or use something from this list of datasets.","tags":null,"title":"Annotations","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load data Clean and reshape data Plot the data and annotate    For this example, we’re again going to use cross-national data from the World Bank’s Open Data portal. We’ll download the data with the WDI package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  wdi_co2.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(WDI) # Get data from the World Bank library(ggrepel) # For non-overlapping labels # You need to install ggtext from GitHub. Follow the instructions at # https://github.com/wilkelab/ggtext library(ggtext) # For fancier text handling indicators \u0026lt;- c(\u0026quot;SP.POP.TOTL\u0026quot;, # Population \u0026quot;EN.ATM.CO2E.PC\u0026quot;, # CO2 emissions \u0026quot;NY.GDP.PCAP.KD\u0026quot;) # GDP per capita wdi_co2_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE, start = 1995, end = 2015) Then we clean the data by removing non-country countries and renaming some of the columns.\nwdi_clean \u0026lt;- wdi_co2_raw %\u0026gt;% filter(region != \u0026quot;Aggregates\u0026quot;) %\u0026gt;% select(iso2c, iso3c, country, year, population = SP.POP.TOTL, co2_emissions = EN.ATM.CO2E.PC, gdp_per_cap = NY.GDP.PCAP.KD, region, income)  Clean and reshape data Next we’ll do some substantial filtering and reshaping so that we can end up with the rankings of CO2 emissions in 1995 and 2014. I annotate as much as possible below so you can see what’s happening in each step.\nco2_rankings \u0026lt;- wdi_clean %\u0026gt;% # Get rid of smaller countries filter(population \u0026gt; 200000) %\u0026gt;% # Only look at two years filter(year %in% c(1995, 2014)) %\u0026gt;% # Get rid of all the rows that have missing values in co2_emissions drop_na(co2_emissions) %\u0026gt;% # Look at each year individually and rank countries based on their emissions that year group_by(year) %\u0026gt;% mutate(ranking = rank(co2_emissions)) %\u0026gt;% ungroup() %\u0026gt;% # Only select a handful of columns, mostly just the newly created \u0026quot;ranking\u0026quot; # column and some country identifiers select(iso3c, country, year, region, income, ranking) %\u0026gt;% # Right now the data is tidy and long, but we want to widen it and create # separate columns for emissions in 1995 and in 2014. pivot_wider() will make # new columns based on the existing \u0026quot;year\u0026quot; column (that\u0026#39;s what `names_from` # does), and it will add \u0026quot;rank_\u0026quot; as the prefix, so that the new columns will # be \u0026quot;rank_1995\u0026quot; and \u0026quot;rank_2014\u0026quot;. The values that go in those new columns will # come from the existing \u0026quot;ranking\u0026quot; column pivot_wider(names_from = year, names_prefix = \u0026quot;rank_\u0026quot;, values_from = ranking) %\u0026gt;% # Find the difference in ranking between 2014 and 1995 mutate(rank_diff = rank_2014 - rank_1995) %\u0026gt;% # Remove all rows where there\u0026#39;s a missing value in the rank_diff column drop_na(rank_diff) %\u0026gt;% # Make an indicator variable that is true of the absolute value of the # difference in rankings is greater than 25. 25 is arbitrary here—that just # felt like a big change in rankings mutate(big_change = ifelse(abs(rank_diff) \u0026gt;= 25, TRUE, FALSE)) %\u0026gt;% # Make another indicator variable that indicates if the rank improved by a # lot, worsened by a lot, or didn\u0026#39;t change much. We use the case_when() # function, which is like a fancy version of ifelse() that takes multiple # conditions. This is how it generally works: # # case_when( # some_test ~ value_if_true, # some_other_test ~ value_if_true, # TRUE ~ value_otherwise #) mutate(better_big_change = case_when( rank_diff \u0026lt;= -25 ~ \u0026quot;Rank improved\u0026quot;, rank_diff \u0026gt;= 25 ~ \u0026quot;Rank worsened\u0026quot;, TRUE ~ \u0026quot;Rank changed a little\u0026quot; )) Here’s what that reshaped data looked like before:\nhead(wdi_clean) ## # A tibble: 6 x 9 ## iso2c iso3c country year population co2_emissions gdp_per_cap region income ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AD AND Andorra 2015 78011 NA 41768. Europe \u0026amp; Central Asia High income ## 2 AD AND Andorra 2004 76244 7.36 47033. Europe \u0026amp; Central Asia High income ## 3 AD AND Andorra 2001 67341 7.79 41421. Europe \u0026amp; Central Asia High income ## 4 AD AND Andorra 2002 70049 7.59 42396. Europe \u0026amp; Central Asia High income ## 5 AD AND Andorra 2014 79213 5.83 40790. Europe \u0026amp; Central Asia High income ## 6 AD AND Andorra 1995 63850 6.66 32918. Europe \u0026amp; Central Asia High income And here’s what it looks like now:\nhead(co2_rankings) ## # A tibble: 6 x 9 ## iso3c country region income rank_1995 rank_2014 rank_diff big_change better_big_change ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; ## 1 ARE United Arab Emirates Middle East \u0026amp; North Africa High income 167 171 4 FALSE Rank changed a little ## 2 AFG Afghanistan South Asia Low income 8 24 16 FALSE Rank changed a little ## 3 ALB Albania Europe \u0026amp; Central Asia Upper middle income 54 78 24 FALSE Rank changed a little ## 4 ARM Armenia Europe \u0026amp; Central Asia Upper middle income 71 76 5 FALSE Rank changed a little ## 5 AGO Angola Sub-Saharan Africa Lower middle income 59 61 2 FALSE Rank changed a little ## 6 ARG Argentina Latin America \u0026amp; Caribbean High income 103 119 16 FALSE Rank changed a little  Plot the data and annotate I use IBM Plex Sans in this plot. You can download it from Google Fonts.\n# These three functions make it so all geoms that use text, label, and # label_repel will use IBM Plex Sans as the font. Those layers are *not* # influenced by whatever you include in the base_family argument in something # like theme_bw(), so ordinarily you\u0026#39;d need to specify the font in each # individual annotate(geom = \u0026quot;text\u0026quot;) layer or geom_label() layer, and that\u0026#39;s # tedious! This removes that tediousness. update_geom_defaults(\u0026quot;text\u0026quot;, list(family = \u0026quot;IBM Plex Sans\u0026quot;)) update_geom_defaults(\u0026quot;label\u0026quot;, list(family = \u0026quot;IBM Plex Sans\u0026quot;)) update_geom_defaults(\u0026quot;label_repel\u0026quot;, list(family = \u0026quot;IBM Plex Sans\u0026quot;)) ggplot(co2_rankings, aes(x = rank_1995, y = rank_2014)) + # Add a reference line that goes from the bottom corner to the top corner annotate(geom = \u0026quot;segment\u0026quot;, x = 0, xend = 175, y = 0, yend = 175) + # Add points and color them by the type of change in rankings geom_point(aes(color = better_big_change)) + # Add repelled labels. Only use data where big_change is TRUE. Fill them by # the type of change (so they match the color in geom_point() above) and use # white text geom_label_repel(data = filter(co2_rankings, big_change == TRUE), aes(label = country, fill = better_big_change), color = \u0026quot;white\u0026quot;) + # Add notes about what the outliers mean in the bottom left and top right # corners. These are italicized and light grey. The text in the bottom corner # is justified to the right with hjust = 1, and the text in the top corner is # justified to the left with hjust = 0 annotate(geom = \u0026quot;text\u0026quot;, x = 170, y = 6, label = \u0026quot;Outliers improving\u0026quot;, fontface = \u0026quot;italic\u0026quot;, hjust = 1, color = \u0026quot;grey50\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, x = 2, y = 170, label = \u0026quot;Outliers worsening\u0026quot;, fontface = \u0026quot;italic\u0026quot;, hjust = 0, color = \u0026quot;grey50\u0026quot;) + # Add mostly transparent rectangles in the bottom right and top left corners annotate(geom = \u0026quot;rect\u0026quot;, xmin = 0, xmax = 25, ymin = 0, ymax = 25, fill = \u0026quot;#2ECC40\u0026quot;, alpha = 0.25) + annotate(geom = \u0026quot;rect\u0026quot;, xmin = 150, xmax = 175, ymin = 150, ymax = 175, fill = \u0026quot;#FF851B\u0026quot;, alpha = 0.25) + # Add text to define what the rectangles abovee actually mean. The \\n in # \u0026quot;highest\\nemitters\u0026quot; will put a line break in the label annotate(geom = \u0026quot;text\u0026quot;, x = 40, y = 6, label = \u0026quot;Lowest emitters\u0026quot;, hjust = 0, color = \u0026quot;#2ECC40\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, x = 162.5, y = 135, label = \u0026quot;Highest\\nemitters\u0026quot;, hjust = 0.5, vjust = 1, lineheight = 1, color = \u0026quot;#FF851B\u0026quot;) + # Add arrows between the text and the rectangles. These use the segment geom, # and the arrows are added with the arrow() function, which lets us define the # angle of the arrowhead and the length of the arrowhead pieces. Here we use # 0.5 lines, which is a unit of measurement that ggplot uses internally (think # of how many lines of text fit in the plot). We could also use unit(1, \u0026quot;cm\u0026quot;) # or unit(0.25, \u0026quot;in\u0026quot;) or anything else annotate(geom = \u0026quot;segment\u0026quot;, x = 38, xend = 20, y = 6, yend = 6, color = \u0026quot;#2ECC40\u0026quot;, arrow = arrow(angle = 15, length = unit(0.5, \u0026quot;lines\u0026quot;))) + annotate(geom = \u0026quot;segment\u0026quot;, x = 162.5, xend = 162.5, y = 140, yend = 155, color = \u0026quot;#FF851B\u0026quot;, arrow = arrow(angle = 15, length = unit(0.5, \u0026quot;lines\u0026quot;))) + # Use three different colors for the points scale_color_manual(values = c(\u0026quot;grey50\u0026quot;, \u0026quot;#0074D9\u0026quot;, \u0026quot;#FF4136\u0026quot;)) + # Use two different colors for the filled labels. There are no grey labels, so # we don\u0026#39;t have to specify that color scale_fill_manual(values = c(\u0026quot;#0074D9\u0026quot;, \u0026quot;#FF4136\u0026quot;)) + # Make the x and y axes expand all the way to the edges of the plot area and # add breaks every 25 units from 0 to 175 scale_x_continuous(expand = c(0, 0), breaks = seq(0, 175, 25)) + scale_y_continuous(expand = c(0, 0), breaks = seq(0, 175, 25)) + # Add labels! There are a couple fancy things here. # 1. In the title we wrap the 2 of CO2 in the HTML \u0026lt;sub\u0026gt;\u0026lt;/sub\u0026gt; tag so that the # number gets subscripted. The only way this will actually get parsed as # HTML is if we tell the plot.title to use element_markdown() in the # theme() function, and element_markdown() comes from the ggtext package. # 2. In the subtitle we bold the two words **improved** and **worsened** using # Markdown asterisks. We also wrap these words with HTML span tags with # inline CSS to specify the color of the text. Like the title, this will # only be processed and parsed as HTML and Markdown if we tell the p # lot.subtitle to use element_markdown() in the theme() function. labs(x = \u0026quot;Rank in 1995\u0026quot;, y = \u0026quot;Rank in 2014\u0026quot;, title = \u0026quot;Changes in CO\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt; emission rankings between 1995 and 2014\u0026quot;, subtitle = \u0026quot;Countries that \u0026lt;span style=\u0026#39;color: #0074D9\u0026#39;\u0026gt;**improved**\u0026lt;/span\u0026gt; or \u0026lt;span style=\u0026#39;color: #FF4136\u0026#39;\u0026gt;**worsened**\u0026lt;/span\u0026gt; more than 25 positions in the rankings highlighted\u0026quot;, caption = \u0026quot;Source: The World Bank.\\nCountries with populations of less than 200,000 excluded.\u0026quot;) + # Turn off the legends for color and fill, since the subtitle includes that guides(color = FALSE, fill = FALSE) + # Use theme_bw() with IBM Plex Sans theme_bw(base_family = \u0026quot;IBM Plex Sans\u0026quot;) + # Tell the title and subtitle to be treated as Markdown/HTML, make the title # 1.6x the size of the base font, and make the subtitle 1.3x the size of the # base font. Also add a little larger margin on the right of the plot so that # the 175 doesn\u0026#39;t get cut off. theme(plot.title = element_markdown(face = \u0026quot;bold\u0026quot;, size = rel(1.6)), plot.subtitle = element_markdown(size = rel(1.3)), plot.margin = unit(c(0.5, 1, 0.5, 0.5), units = \u0026quot;lines\u0026quot;))   ","date":1590019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590019200,"objectID":"4ca8d8f57d9585dcab132a17e9a5a6e7","permalink":"/example/09-example/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/example/09-example/","section":"example","summary":"Live coding example Complete code  Load data Clean and reshape data Plot the data and annotate    For this example, we’re again going to use cross-national data from the World Bank’s Open Data portal. We’ll download the data with the WDI package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):","tags":null,"title":"Annotations","type":"docs"},{"authors":null,"categories":null,"content":"  Ha, so in the video I said there would be interactive lessons, but I changed my mind! You’re only working with a few new functions this session (annotate(), geom_text(), geom_label(), geom_text_repel(), and geom_label_repel()), and the best way to figure out how to use them is to use them!\nThere are some helpful blog posts and other resources online with examples and explanations. Read through these in addition to the documentation for annotate(), geom_text()/geom_label() and ggrepelhttps://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html:\n  “Add shapes with annotate()”  “Annotations”  ","date":1590019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590019200,"objectID":"9ba3922ca694430b88b632d8dbab3bdb","permalink":"/lesson/09-lesson/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/lesson/09-lesson/","section":"lesson","summary":"Ha, so in the video I said there would be interactive lessons, but I changed my mind! You’re only working with a few new functions this session (annotate(), geom_text(), geom_label(), geom_text_repel(), and geom_label_repel()), and the best way to figure out how to use them is to use them!\nThere are some helpful blog posts and other resources online with examples and explanations. Read through these in addition to the documentation for annotate(), geom_text()/geom_label() and ggrepelhttps://cran.","tags":null,"title":"Annotations","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Interactive plots Task 3: Dashboard Turning everything in   This exercise is a little different from past ones because you will not knit to PDF or Word. Pay attention to the instructions below.\nGetting started For this exercise, you’ll use whatever data you want to create an interactive HTML plot and a dashboard. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the gapminder package, download stuff from the World Bank using the WDI package, or use something from this list of datasets.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  10-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 10-exercise.Rmd your-project-name.Rproj data\\ WHATEVER.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  10-exercise.zip  The example for today’s session will be helpful as you tinker with ggplotly(), and the resources listed at the bottom of the example will be helpful for making a dashboard.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Interactive plots In the R Markdown file with your reflection, create an interactive plot with ggplotly() from the plotly package. Modify the tooltip to show something more useful than every single mapped aesthetic.\n Task 3: Dashboard Install the flexdashboard package and create a new R Markdown file in your project by going to File \u0026gt; New File… \u0026gt; R Markdown… \u0026gt; From Template \u0026gt; Flexdashboard.\nUsing the documentation for flexdashboard online, create a super basic dashboard that shows a plot (static or interactive) in at least two chart areas. Play with the layout if you’re feeling brave.\n Turning everything in Here’s where this is all different this time. You will not upload a knitted PDF or Word file to iCollege, since those can’t handle interactivity. Instead, do this:\nKnit the document with Tasks 1 and 2 in it to HTML and publish it to RPubs using the “Publish document” menu in the preview of the knitted file. Take note of the URL.\n Knit the dashboard from Task 3 to HTML and publish it to RPubs using the same menu. Take note of the URL.\n In iCollege, paste the two URLs into the submission form for exercise 10 following this template:\nTask 1 and 2: URL HERE Task 3 dashboard: URL HERE   ","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590105600,"objectID":"f56fe53aa8a62856cf1a83d457f269e0","permalink":"/assignment/10-exercise/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/assignment/10-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Interactive plots Task 3: Dashboard Turning everything in   This exercise is a little different from past ones because you will not knit to PDF or Word. Pay attention to the instructions below.\nGetting started For this exercise, you’ll use whatever data you want to create an interactive HTML plot and a dashboard. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the gapminder package, download stuff from the World Bank using the WDI package, or use something from this list of datasets.","tags":null,"title":"Interactivity","type":"docs"},{"authors":null,"categories":null,"content":"      Live coding example Complete code  Get and clean data Creating a basic interactive chart Modifying the tooltip Including more information in the tooltip Making a dashboard with flexdashboard    For this example we’ll use data from the World Bank once again, which we download using the WDI package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  wdi_parl.csv  Live coding example There is no video for this one, since it really only involves feeding a few ggplot plots fed into ggplotly().\n Complete code Get and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(WDI) # Get data from the World Bank library(scales) # For nicer label formatting library(plotly) # For easy interactive plots indicators \u0026lt;- c(\u0026quot;SP.POP.TOTL\u0026quot;, # Population \u0026quot;SG.GEN.PARL.ZS\u0026quot;, # Proportion of seats held by women in national parliaments (%) \u0026quot;NY.GDP.PCAP.KD\u0026quot;) # GDP per capita wdi_parl_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE, start = 2000, end = 2019) Then we clean the data by removing non-country countries and renaming some of the columns.\nwdi_clean \u0026lt;- wdi_parl_raw %\u0026gt;% filter(region != \u0026quot;Aggregates\u0026quot;) %\u0026gt;% select(iso2c, iso3c, country, year, population = SP.POP.TOTL, prop_women_parl = SG.GEN.PARL.ZS, gdp_per_cap = NY.GDP.PCAP.KD, region, income)  Creating a basic interactive chart Let’s make a chart that shows the distribution of the proportion of women in national parliaments in 2019, by continent. We’ll use a strip plot with jittered points.\nFirst we need to make a regular static plot with ggplot:\nwdi_2019 \u0026lt;- wdi_clean %\u0026gt;% filter(year == 2019) %\u0026gt;% drop_na(prop_women_parl) %\u0026gt;% # Scale this down from 0-100 to 0-1 so that scales::percent() can format it as # an actual percent mutate(prop_women_parl = prop_women_parl / 100) static_plot \u0026lt;- ggplot(wdi_2019, aes(y = fct_rev(region), x = prop_women_parl, color = region)) + geom_point(position = position_jitter(width = 0, height = 0.15, seed = 1234)) + guides(color = FALSE) + scale_x_continuous(labels = percent) + # I used https://medialab.github.io/iwanthue/ to generate these colors scale_color_manual(values = c(\u0026quot;#425300\u0026quot;, \u0026quot;#e680ff\u0026quot;, \u0026quot;#01bd71\u0026quot;, \u0026quot;#ff3aad\u0026quot;, \u0026quot;#9f3e00\u0026quot;, \u0026quot;#0146bf\u0026quot;, \u0026quot;#671d56\u0026quot;)) + labs(x = \u0026quot;% women in parliament\u0026quot;, y = NULL, caption = \u0026quot;Source: The World Bank\u0026quot;) + theme_bw() static_plot Great! That looks pretty good.\nTo make it interactive, all we have to do is feed the static_plot object into ggplotly(). That’s it.\nggplotly(static_plot)  {\"x\":{\"url\":\"/example/10-example_files/figure-html//widgets/widget_strip-plot-basic-interactive-real.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} Not everything translates over to JavaScript—the caption is gone now, and the legend is back (which is fine, I guess, since the legend is interactive). But still, this is magic.\n Modifying the tooltip Right now, the default tooltip you see when you hover over the points includes the actual proportion of women in parliament for each point, along with the continent, which is neat, but it’d be great if we could see the country name too. The tooltip picks up the information to include from the variables we use in aes(), and we never map the country column to any aesthetic, so it doesn’t show up.\nTo get around this, we can add a new aesthetic for country to the points. Instead of using one of the real ggplot aesthetics like color or fill, we’ll use a fake one called text (we can call it whatever we want! asdf would also work). ggplot has no idea how to do anything with the text aesthetic, and it’ll give you a warning, but that’s okay. The static plot looks the same:\nstatic_plot_toolip \u0026lt;- ggplot(wdi_2019, aes(y = fct_rev(region), x = prop_women_parl, color = region)) + geom_point(aes(text = country), position = position_jitter(width = 0, height = 0.15, seed = 1234)) + guides(color = FALSE) + scale_x_continuous(labels = percent) + # I used https://medialab.github.io/iwanthue/ to generate these colors scale_color_manual(values = c(\u0026quot;#425300\u0026quot;, \u0026quot;#e680ff\u0026quot;, \u0026quot;#01bd71\u0026quot;, \u0026quot;#ff3aad\u0026quot;, \u0026quot;#9f3e00\u0026quot;, \u0026quot;#0146bf\u0026quot;, \u0026quot;#671d56\u0026quot;)) + labs(x = \u0026quot;% women in parliament\u0026quot;, y = NULL, caption = \u0026quot;Source: The World Bank\u0026quot;) + theme_bw() ## Warning: Ignoring unknown aesthetics: text static_plot_toolip Now we can tell plotly to use this fake text aesthetic for the tooltip:\nggplotly(static_plot_toolip, tooltip = \u0026quot;text\u0026quot;)  {\"x\":{\"url\":\"/example/10-example_files/figure-html//widgets/widget_strip-plot-text-interactive-real.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} Now we should just see the country names in the tooltips!\n Including more information in the tooltip We have country names, but we lost the values in the x-axis. Rwanda has the highest proportion of women in parliament, but what’s the exact number? It’s somewhere above 60%, but that’s all we can see now.\nTo fix this, we can make a new column in the data with all the text we want to include in the tooltip. We’ll use paste0() to combine text and variable values to make the tooltip follow this format:\nName of country X% women in parliament Let’s add a new column with mutate(). A couple things to note here:\n The \u0026lt;br\u0026gt; is HTML code for a line break\n We use the percent() function to format numbers as percents. The accuracy argument tells R how many decimal points to use. If we used 1, it would say 12%; if we used 0.01, it would say 12.08%; etc.\n  wdi_2019 \u0026lt;- wdi_clean %\u0026gt;% filter(year == 2019) %\u0026gt;% drop_na(prop_women_parl) %\u0026gt;% # Scale this down from 0-100 to 0-1 so that scales::percent() can format it as # an actual percent mutate(prop_women_parl = prop_women_parl / 100) %\u0026gt;% mutate(fancy_label = paste0(country, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;, percent(prop_women_parl, accuracy = 0.1), \u0026quot; women in parliament\u0026quot;)) Let’s check to see if it worked:\nwdi_2019 %\u0026gt;% select(country, prop_women_parl, fancy_label) %\u0026gt;% head() ## # A tibble: 6 x 3 ## country prop_women_parl fancy_label ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Andorra 0.5 Andorra\u0026lt;br\u0026gt;50.0% women in parliament ## 2 United Arab Emirates 0.225 United Arab Emirates\u0026lt;br\u0026gt;22.5% women in parliament ## 3 Afghanistan 0.279 Afghanistan\u0026lt;br\u0026gt;27.9% women in parliament ## 4 Antigua and Barbuda 0.111 Antigua and Barbuda\u0026lt;br\u0026gt;11.1% women in parliament ## 5 Albania 0.295 Albania\u0026lt;br\u0026gt;29.5% women in parliament ## 6 Armenia 0.242 Armenia\u0026lt;br\u0026gt;24.2% women in parliament Now instead of using text = country we’ll use text = fancy_label to map that new column onto the plot. Again, this won’t be visible in the static plot (and you’ll get a warning), but it will show up in the interactive plot.\nstatic_plot_toolip_fancy \u0026lt;- ggplot(wdi_2019, aes(y = fct_rev(region), x = prop_women_parl, color = region)) + geom_point(aes(text = fancy_label), position = position_jitter(width = 0, height = 0.15, seed = 1234)) + guides(color = FALSE) + scale_x_continuous(labels = percent) + # I used https://medialab.github.io/iwanthue/ to generate these colors scale_color_manual(values = c(\u0026quot;#425300\u0026quot;, \u0026quot;#e680ff\u0026quot;, \u0026quot;#01bd71\u0026quot;, \u0026quot;#ff3aad\u0026quot;, \u0026quot;#9f3e00\u0026quot;, \u0026quot;#0146bf\u0026quot;, \u0026quot;#671d56\u0026quot;)) + labs(x = \u0026quot;% women in parliament\u0026quot;, y = NULL, caption = \u0026quot;Source: The World Bank\u0026quot;) + theme_bw() ## Warning: Ignoring unknown aesthetics: text ggplotly(static_plot_toolip_fancy, tooltip = \u0026quot;text\u0026quot;)  {\"x\":{\"url\":\"/example/10-example_files/figure-html//widgets/widget_strip-plot-text-interactive-fancy-real.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} Perfect!\nFinally, if we want to save this plot as a standalone self-contained HTML file, we can use the saveWidget() function from the htmlwidgets package.\n# This is like ggsave, but for interactive HTML plots interactive_plot \u0026lt;- static_plot_toolip_fancy htmlwidgets::saveWidget(interactive_plot, \u0026quot;fancy_plot.html\u0026quot;)  Making a dashboard with flexdashboard The documentation for flexdashboard is so great and complete that I’m not going to include a full example here. There is also a brief overview in chapter 5 of the official R Markdown book. You can also watch this really quick video here. She uses a package called dimple instead of plotly, which doesn’t work with ggplot like ggplotly(), so ignore her code about dimple() and use your ggplotly() skills instead. You can search YouTube for a bunch of other short tutorial videos, too.\nThe quickest and easiest way to get started is to install the flexdashboard package and then in RStudio go to File \u0026gt; New File… \u0026gt; R Markdown… \u0026gt; From Template \u0026gt; Flexdashboard:\nThat will give you an empty dashboard with three chart areas spread across two columns. Put static or dynamic graphs in the different chart areas, knit, and you’ll be good to go!\nIf you’re interested in making the dashboard reactive with Shiny-like elements, check out this tutorial.\n  ","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590105600,"objectID":"0d7091da7131dcaeb0a7a2758ca2db8e","permalink":"/example/10-example/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/example/10-example/","section":"example","summary":"Live coding example Complete code  Get and clean data Creating a basic interactive chart Modifying the tooltip Including more information in the tooltip Making a dashboard with flexdashboard    For this example we’ll use data from the World Bank once again, which we download using the WDI package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):","tags":null,"title":"Interactivity","type":"docs"},{"authors":null,"categories":null,"content":"  Again, there’s no lesson for this. The only way to learn how to use ggplotly() and create dashboards with flexdashboard is to try them out in RStudio, not in a mini browser-based R session here.\nSo head over to the exercise to get started!\n","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590105600,"objectID":"86a399491d0977aee8295e2950c3a1b2","permalink":"/lesson/10-lesson/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/lesson/10-lesson/","section":"lesson","summary":"Again, there’s no lesson for this. The only way to learn how to use ggplotly() and create dashboards with flexdashboard is to try them out in RStudio, not in a mini browser-based R session here.\nSo head over to the exercise to get started!","tags":null,"title":"Interactivity","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Visualizing time Turning everything in   Getting started For this exercise, you’ll visualize something over time. You can use whatever data you want. Use a dataset from a past exercise, use one of the built-in datasets like gapminder from the gapminder package, download stuff from the World Bank with the WDI package, or download stuff from FRED using the tidyquant package.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  11-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 11-exercise.Rmd your-project-name.Rproj data\\ WHATEVER.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  11-exercise.zip  The example from today’s session shows how to get data from FRED, and the examples from sessions 8 and 9 show. You can also use gapminder, or any other dataset that includes a time-related column (so not mpg).\nThere’s no specific way you should visualize time. Show it as a line, or as bars, or with a heatmap, or with ridgeplots, or with whatever is most appropriate for the story you’re telling. You do not have to recreate the example from today. You’re free to do whatever you want!\nThis can be as simple or as complex as you want. You don’t need to make your plot super fancy, but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Visualizing time Do the following:\nLoad some time-related data\n Make a plot to show how that data changes over time.\n Explain why you chose to visualize the data the way you did.\n   Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590451200,"objectID":"e1b1e2016bed2bc6b0b60f1d762ee82c","permalink":"/assignment/11-exercise/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/assignment/11-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Visualizing time Turning everything in   Getting started For this exercise, you’ll visualize something over time. You can use whatever data you want. Use a dataset from a past exercise, use one of the built-in datasets like gapminder from the gapminder package, download stuff from the World Bank with the WDI package, or download stuff from FRED using the tidyquant package.","tags":null,"title":"Time","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Get data Look at and clean data Plotting time Improving graphics Decomposition    For this example, we’re going to use economic data from the US Federal Reserve (the Fed). The St. Louis Fed is in charge of publishing Fed economic data, and they host it all at an online portal named FRED. Instead of downloading individual time series data from the FRED website, we’ll do what with did with the World Bank WDI data and download it directly from the internet with the tidyquant package, which includes a function for working with the FRED API/website.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  fred_raw.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nGet data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(tidyquant) # For accessing FRED data library(scales) # For nicer labels The US Federal Reserve provides thousands of economic datasets at FRED. We can use the tidyquant R package to access their servers and download the data directly into R.\nLike we did with the WDI indicators in session 8, we need to find the special internal code for the variables we want to get. We need to pay close attention to the details of each variable, since the same measure can be offered with different combinations of real (adjusted for inflation) or nominal (not adjusted for inflation); monthly, quarterly, or annually; and seasonally adjusted or not seasonally adjusted. For instance, if you want to see US GDP, here are some possibilities (all the possible GDP measures are listed here):\n GDPC1: Real (2012 dollars), quarterly, seasonally adjusted ND000334Q: Real (2012 dollars), quarterly, not seasonally adjusted GDPCA: Real (2012 dollars), annual, not seasonally adjusted GDP: Nominal, quarterly, seasonally adjusted GDPA: Nominal, annual, not seasonally adjusted  The code for getting data from FRED works a little differently than WDI(), and the output is a little different too, but it’s hopefully not too complicated. We need to feed the tq_get() function (1) a list of indicators we want, (2) a source for those indicators, and (3) a starting and/or ending date.\ntq_get() can actually get data from a ton of different sources like stocks from Yahoo Finance and general financial data from Bloomberg, Quandl, and Tiingo. Most of those other sources require a subscription and a fancy API key that logs you into their servers when getting data, but FRED is free (yay public goods!).\nWe’ll first make a new dataset named fred_raw that gets a bunch of interesting variables from FRED from January 1, 1990 until today.\nfred_raw \u0026lt;- tq_get(c(\u0026quot;RSXFSN\u0026quot;, # Advance retail sales \u0026quot;GDPC1\u0026quot;, # GDP \u0026quot;ICSA\u0026quot;, # Initial unemployment claims \u0026quot;FPCPITOTLZGUSA\u0026quot;, # Inflation \u0026quot;UNRATE\u0026quot;, # Unemployment rate \u0026quot;USREC\u0026quot;), # Recessions get = \u0026quot;economic.data\u0026quot;, # Use FRED from = \u0026quot;1990-01-01\u0026quot;) Downloading data from FRED every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). As with the World Bank data we used, it’s good practice to save this raw data as a CSV file and then work with that.\nwrite_csv(fred_raw, \u0026quot;data/fred_raw.csv\u0026quot;) Since we care about reproducibility, we still want to include the code we used to get data from FRED, we just don’t want it to actually run. You can include chunks but not run them by setting eval=FALSE in the chunk options. In this little example, we show the code for downloading the data, but we don’t evaluate the chunk. We then include a chunk that loads the data from a CSV file with read_csv(), but we don’t include it (include=FALSE). That way, in the knitted file we see the WDI() code, but in reality it’s loading the data from CSV. Super tricky.\nI first download data from FRED: ```{r get-fred-data, eval=FALSE} fred_raw \u0026lt;- tq_get(...) write_csv(fred_raw, \u0026quot;data/fred_raw.csv\u0026quot;) ``` ```{r load-fred-data-real, include=FALSE} fred_raw \u0026lt;- read_csv(\u0026quot;data/fred_raw.csv\u0026quot;) ```  Look at and clean data The data we get from FRED is in a slightly different format than we’re used to with WDI(), but with good reason. With World Bank data, you get data for every country and every year, so there are rows for Afghanistan 2000, Afghanistan 2001, etc. You then get a column for each of the variables you want (population, life expectancy, GDP/capita, etc.)\nWith FRED data, that kind of format doesn’t work for every possible time series variable because time is spaced differently. If you want to work with annual GDP, you should have a row for each year. If you want quarterly GDP, you should have a row for every quarter. If you put these in the same dataset, you’ll end up with all sorts of missing data issues:\n  time annual_gdp quarterly_gdp    2019, Q1 X X  2019, Q2  X  2019, Q3  X  2019, Q4  X  2020, Q1 X X  2020, Q2  X    To fix this, the tidyquant package gives you data in tidy (or long) form and only provides three columns:\nhead(fred_raw) ## # A tibble: 6 x 3 ## symbol date price ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 RSXFSN 1992-01-01 130683 ## 2 RSXFSN 1992-02-01 131244 ## 3 RSXFSN 1992-03-01 142488 ## 4 RSXFSN 1992-04-01 147175 ## 5 RSXFSN 1992-05-01 152420 ## 6 RSXFSN 1992-06-01 151849 The symbol column is the ID of the variable from FRED , date is… the date, and price is the value. These columns are called symbol and price because the tidyquant package was designed to get and process stock data, so you’d typically see stock symbols (like AAPL or MSFT) and stock prices. When working with FRED data, the price column shows the value of whatever you’re interested in—it’s not technically a price (so unemployment claims, inflation rates, and GDP values are still called price).\nRight now, our fred_raw dataset has only 3 columns, but nearly 3,000 rows since the six indicators we got from the server are all stacked on top of each other. To actually work with these, we need to filter the raw data so that it only includes the indicators we’re interested in. For instance, if we want to plot retail sales, we need to select only the rows where the symbol is RSXFSN. Make a smaller dataset with filter() to do that:\nretail_sales \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;RSXFSN\u0026quot;) retail_sales ## # A tibble: 340 x 3 ## symbol date price ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 RSXFSN 1992-01-01 130683 ## 2 RSXFSN 1992-02-01 131244 ## 3 RSXFSN 1992-03-01 142488 ## 4 RSXFSN 1992-04-01 147175 ## 5 RSXFSN 1992-05-01 152420 ## 6 RSXFSN 1992-06-01 151849 ## 7 RSXFSN 1992-07-01 152586 ## 8 RSXFSN 1992-08-01 152476 ## 9 RSXFSN 1992-09-01 148158 ## 10 RSXFSN 1992-10-01 155987 ## # … with 330 more rows If multiple variables have the same spacing (annual, quarterly, monthly, weekly), you can use filter to select all of them and then the use pivot_wider() or spread() to make separate columns for each. Inflation, unemployment, and retail sales are all monthly, so we can make a dataset for just those:\nfred_monthly_things \u0026lt;- fred_raw %\u0026gt;% filter(symbol %in% c(\u0026quot;FPCPITOTLZGUSA\u0026quot;, \u0026quot;UNRATE\u0026quot;, \u0026quot;RSXFSN\u0026quot;)) %\u0026gt;% # Convert the symbol column into multiple columns, using the \u0026quot;prices\u0026quot; for values pivot_wider(names_from = symbol, values_from = price) fred_monthly_things ## # A tibble: 364 x 4 ## date RSXFSN FPCPITOTLZGUSA UNRATE ## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1992-01-01 130683 3.03 7.3 ## 2 1992-02-01 131244 NA 7.4 ## 3 1992-03-01 142488 NA 7.4 ## 4 1992-04-01 147175 NA 7.4 ## 5 1992-05-01 152420 NA 7.6 ## 6 1992-06-01 151849 NA 7.8 ## 7 1992-07-01 152586 NA 7.7 ## 8 1992-08-01 152476 NA 7.6 ## 9 1992-09-01 148158 NA 7.6 ## 10 1992-10-01 155987 NA 7.3 ## # … with 354 more rows But wait! There’s a problem! The inflation rate we got isn’t actually monthly—it seems to be annual, which explains all the NAs. Let’s fix it by not including it. We’ll also rename the columns so they’re easier to work with\nfred_monthly_things \u0026lt;- fred_raw %\u0026gt;% filter(symbol %in% c(\u0026quot;UNRATE\u0026quot;, \u0026quot;RSXFSN\u0026quot;)) %\u0026gt;% # Convert the symbol column into multiple columns, using the \u0026quot;prices\u0026quot; for values pivot_wider(names_from = symbol, values_from = price) %\u0026gt;% rename(unemployment = UNRATE, retail_sales = RSXFSN) fred_monthly_things ## # A tibble: 364 x 3 ## date retail_sales unemployment ## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1992-01-01 130683 7.3 ## 2 1992-02-01 131244 7.4 ## 3 1992-03-01 142488 7.4 ## 4 1992-04-01 147175 7.4 ## 5 1992-05-01 152420 7.6 ## 6 1992-06-01 151849 7.8 ## 7 1992-07-01 152586 7.7 ## 8 1992-08-01 152476 7.6 ## 9 1992-09-01 148158 7.6 ## 10 1992-10-01 155987 7.3 ## # … with 354 more rows All better.\nWe can make as many subsets of the long, tidy, raw data as we want.\n Plotting time Let’s plot some of these and see what the trends look like. We’ll just use geom_line().\nHere’s GDP:\n# Get just GDP data from the raw FRED data gdp_only \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;GDPC1\u0026quot;) ggplot(gdp_only, aes(x = date, y = price)) + geom_line() Here’s retail sales:\n# Get just GDP data from the raw FRED data retail_sales_only \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;RSXFSN\u0026quot;) ggplot(retail_sales_only, aes(x = date, y = price)) + geom_line() And here’s unemployment claims:\nunemployment_claims_only \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;ICSA\u0026quot;) ggplot(unemployment_claims_only, aes(x = date, y = price)) + geom_line() Yikes COVID-19.\nThere, we visualized time. ✅\n Improving graphics These were simple graphs and they’re kind of helpful, but they’re not incredibly informative. We can clean these up a little. First we can change the labels and themes and colors:\nggplot(gdp_only, aes(x = date, y = price)) + geom_line(color = \u0026quot;#0074D9\u0026quot;, size = 1) + scale_y_continuous(labels = dollar) + labs(y = \u0026quot;Billions of 2012 dollars\u0026quot;, x = NULL, title = \u0026quot;US Gross Domestic Product\u0026quot;, subtitle = \u0026quot;Quarterly data; real 2012 dollars\u0026quot;, caption = \u0026quot;Source: US Bureau of Economic Analysis and FRED\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) That’s great and almost good enough to publish! We can add one additional layer of information onto the plot and highlight when recessions start and end. We included a recessions variable (USREC) when we got data from FRED, so let’s see what it looks like:\nfred_raw %\u0026gt;% filter(symbol == \u0026quot;USREC\u0026quot;) ## # A tibble: 364 x 3 ## symbol date price ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 USREC 1990-01-01 0 ## 2 USREC 1990-02-01 0 ## 3 USREC 1990-03-01 0 ## 4 USREC 1990-04-01 0 ## 5 USREC 1990-05-01 0 ## 6 USREC 1990-06-01 0 ## 7 USREC 1990-07-01 0 ## 8 USREC 1990-08-01 1 ## 9 USREC 1990-09-01 1 ## 10 USREC 1990-10-01 1 ## # … with 354 more rows This is monthly data that shows a 1 if we were in a recession that month and a 0 if we weren’t. The Fed doesn’t decide when recessions happen—the National Bureau of Economic Research (NBER) does, and they have specific guidelines for defining one. We’re probably in one right now, but there’s not enough data for NBER to formally declare it yet.\nThis data is long and tidy, but that makes it harder to work with given our GDP. We want the start and end dates for each recession so that we can shade those areas on the plot. To find those dates, we need to do a little data reshaping. First, we’ll create a temporary variable that marks if there was a switch from 0 to 1 or 1 to 0 in a given row by looking at the previous row\nrecessions_tidy \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;USREC\u0026quot;) %\u0026gt;% mutate(recession_change = price - lag(price)) recessions_tidy ## # A tibble: 364 x 4 ## symbol date price recession_change ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 USREC 1990-01-01 0 NA ## 2 USREC 1990-02-01 0 0 ## 3 USREC 1990-03-01 0 0 ## 4 USREC 1990-04-01 0 0 ## 5 USREC 1990-05-01 0 0 ## 6 USREC 1990-06-01 0 0 ## 7 USREC 1990-07-01 0 0 ## 8 USREC 1990-08-01 1 1 ## 9 USREC 1990-09-01 1 0 ## 10 USREC 1990-10-01 1 0 ## # … with 354 more rows Notice the new column we have that is mostly 0s, but 1 when there’s a switch, like in August 1990. 1 means we went from 0 to 1 (no recession → recession), while -1 means we went from 1 to 0 (recession → no recession).\nWe can see all the start and end dates if we filter:\nrecessions_start_end \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;USREC\u0026quot;) %\u0026gt;% mutate(recession_change = price - lag(price)) %\u0026gt;% filter(recession_change != 0) recessions_start_end ## # A tibble: 6 x 4 ## symbol date price recession_change ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 USREC 1990-08-01 1 1 ## 2 USREC 1991-04-01 0 -1 ## 3 USREC 2001-04-01 1 1 ## 4 USREC 2001-12-01 0 -1 ## 5 USREC 2008-01-01 1 1 ## 6 USREC 2009-07-01 0 -1 Finally, we can use tibble() to create a brand new little dataset that includes columns for the start and end dates:\nrecessions \u0026lt;- tibble(start = filter(recessions_start_end, recession_change == 1)$date, end = filter(recessions_start_end, recession_change == -1)$date) recessions ## # A tibble: 3 x 2 ## start end ## \u0026lt;date\u0026gt; \u0026lt;date\u0026gt; ## 1 1990-08-01 1991-04-01 ## 2 2001-04-01 2001-12-01 ## 3 2008-01-01 2009-07-01 We can now add this tiny dataset to our plot using geom_rect(). Notice how we put geom_rect() before geom_line()—that’s so the recession rectangles go under the line instead of on top of it. Also notice that we have to specify 4 new aesthetics for geom_rect(): min and max values for both x and y. We use the recession start and end dates for xmin and xmax, and then use −∞ and ∞ for ymin and ymax to make the rectangles stretch from the bottom of the plot to the top.\nThe last odd/new thing here is that we also use inherit.aes = FALSE in geom_rect(). That’s because we specified a global x and y aesthetic in ggplot(), which applies to all the other layers we add. geom_rect() doesn’t use x or y, though, and it’ll complain that those columns are missing. The inherit.aes argument tells ggplot that the geom_rect() layer should not get any of the global aesthetics like x or y.\nggplot(gdp_only, aes(x = date, y = price)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line(color = \u0026quot;#0074D9\u0026quot;, size = 1) + scale_y_continuous(labels = dollar) + labs(y = \u0026quot;Billions of 2012 dollars\u0026quot;, x = NULL, title = \u0026quot;US Gross Domestic Product\u0026quot;, subtitle = \u0026quot;Quarterly data; real 2012 dollars\u0026quot;, caption = \u0026quot;Source: US Bureau of Economic Analysis and FRED\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) And that’s it!\nNow that we have the tiny recessions data frame, we can add it to any plot we want. Here’s initial unemployment claims with some extra annotations for fun:\nggplot(unemployment_claims_only, aes(x = date, y = price)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line(color = \u0026quot;#FF4136\u0026quot;, size = 0.5) + annotate(geom = \u0026quot;label\u0026quot;, x = as.Date(\u0026quot;2010-01-01\u0026quot;), y = 1000000, label = \u0026quot;The Great Recession\u0026quot;, size = 3, family = \u0026quot;Roboto Condensed\u0026quot;) + annotate(geom = \u0026quot;label\u0026quot;, x = as.Date(\u0026quot;2020-01-01\u0026quot;), y = 6000000, label = \u0026quot;COVID-19\u0026quot;, size = 3, family = \u0026quot;Roboto Condensed\u0026quot;, hjust = 1) + scale_y_continuous(labels = comma) + labs(y = \u0026quot;Initial unemployment claims\u0026quot;, x = NULL, title = \u0026quot;Initial unemployment claims\u0026quot;, subtitle = \u0026quot;Weekly data\u0026quot;, caption = \u0026quot;Source: US Employment and Training Administration and FRED\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;))  Decomposition The mechanics of decomposing and forecasting time series goes beyond the scope of this class, but there are lots of resources you can use to learn more, including this phenomenal free textbook.\nThere’s a whole ecosystem of time-related packages that make working with time and decomposing trends easy (named tidyverts):\n lubridate: Helpful functions for manipulating dates (you’ve used this before) tsibble: Add fancy support for time variables to data frames feasts: Decompose time series and do other statistical things with time fable: Make forecasts  Here’s a super short example of how these all work.\nThe retail sales data we got from FRED was not seasonally adjusted, so it looks like it has a heartbeat embedded in it:\nretail_sales \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;RSXFSN\u0026quot;) ggplot(retail_sales, aes(x = date, y = price)) + geom_line() We can divide this trend into its main components: the trend, the seasonality, and stuff that’s not explained by either the trend or the seasonality. To do that, we need to first modify our little dataset and tell it to be a time-enabled data frame (a tsibble) that is indexed by the year+month for each row. We’ll create a new column called year_month and then use as_tsibble() to tell R that this is really truly dealing with time:\nlibrary(tsibble) # For embedding time things into data frames retail_sales \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;RSXFSN\u0026quot;) %\u0026gt;% mutate(year_month = yearmonth(date)) %\u0026gt;% as_tsibble(index = year_month) retail_sales ## # A tsibble: 340 x 4 [1M] ## symbol date price year_month ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;mth\u0026gt; ## 1 RSXFSN 1992-01-01 130683 1992 Jan ## 2 RSXFSN 1992-02-01 131244 1992 Feb ## 3 RSXFSN 1992-03-01 142488 1992 Mar ## 4 RSXFSN 1992-04-01 147175 1992 Apr ## 5 RSXFSN 1992-05-01 152420 1992 May ## 6 RSXFSN 1992-06-01 151849 1992 Jun ## 7 RSXFSN 1992-07-01 152586 1992 Jul ## 8 RSXFSN 1992-08-01 152476 1992 Aug ## 9 RSXFSN 1992-09-01 148158 1992 Sep ## 10 RSXFSN 1992-10-01 155987 1992 Oct ## # … with 330 more rows Notice that the year_month column is now just the year+month. Neato.\nNext we need to create a time series model using that data. There are lots of different ways to model time series, and distinguishing between the different types is way beyond the scope of this class. Rob Hyndman’s free books covers them all. We’ll do this with STL decomposition (“Seasonal and Trend decomposition using Loess”) There are other models we can use, like ETS or ARIMA, but again, that’s all beyond this class.\nlibrary(feasts) # For decomposition things like STL() retail_model \u0026lt;- retail_sales %\u0026gt;% model(stl = STL(price)) retail_model ## # A mable: 1 x 1 ## stl ## \u0026lt;model\u0026gt; ## 1 \u0026lt;STL\u0026gt; The decomposition model we create is kind of boring and useless—it’s all stored in a single cell.\nWe can extract the different components of the decomposition with the components() function:\nretail_components \u0026lt;- components(retail_model) retail_components ## # A dable: 340 x 7 [1M] ## # Key: .model [1] ## # STL Decomposition: price = trend + season_year + remainder ## .model year_month price trend season_year remainder season_adjust ## \u0026lt;chr\u0026gt; \u0026lt;mth\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 stl 1992 Jan 130683 148453. -22505. 4735. 153188. ## 2 stl 1992 Feb 131244 148960. -23009. 5292. 154253. ## 3 stl 1992 Mar 142488 149468. -1326. -5654. 143814. ## 4 stl 1992 Apr 147175 149976. -2978. 177. 150153. ## 5 stl 1992 May 152420 150513. 5927. -4020. 146493. ## 6 stl 1992 Jun 151849 151051. 3205. -2407. 148644. ## 7 stl 1992 Jul 152586 151589. 294. 703. 152292. ## 8 stl 1992 Aug 152476 152155. 4343. -4022. 148133. ## 9 stl 1992 Sep 148158 152722. -6162. 1598. 154320. ## 10 stl 1992 Oct 155987 153289. -33.3 2732. 156020. ## # … with 330 more rows And we can use the autoplot() function from the feasts package to quickly plot all the components. The plot that autoplot() creates is made with ggplot, so any normal ggplot layers work with it:\nautoplot(retail_components) + labs(x = NULL) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) We can also plot individual components on their own using the retail_components dataset we made. Here’s seasonality by itself:\nggplot(retail_components, aes(x = year_month, y = season_year)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line() + scale_y_continuous(labels = dollar) + labs(x = NULL, y = \u0026quot;Difference from trend, millions of dollars\u0026quot;, title = \u0026quot;Seasonal trends in retail sales\u0026quot;, subtitle = \u0026quot;Nominal US dollars\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) And here’s the trend by itself:\nggplot(retail_components, aes(x = year_month, y = trend)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line() + scale_y_continuous(labels = dollar) + labs(x = NULL, y = \u0026quot;Trend, millions of dollars\u0026quot;, title = \u0026quot;Seasonally adjusted trends in retail sales\u0026quot;, subtitle = \u0026quot;Nominal US dollars\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) If you want more control over the combined decomposed plot you can either (1) make individual plots for each of the components and then stitch them together with patchwork, or (2) make the components dataset tidy and facet by component. Here’s what that looks like:\nretail_components_tidy \u0026lt;- retail_components %\u0026gt;% # Get rid of this column select(-season_adjust) %\u0026gt;% # Take all these component columns and put them into a long column pivot_longer(cols = c(price, trend, season_year, remainder), names_to = \u0026quot;component\u0026quot;, values_to = \u0026quot;value\u0026quot;) %\u0026gt;% # Recode this values so they\u0026#39;re nicer mutate(component = recode(component, price = \u0026quot;Actual data\u0026quot;, trend = \u0026quot;Trend\u0026quot;, season_year = \u0026quot;Seasonality\u0026quot;, remainder = \u0026quot;Remainder\u0026quot;)) %\u0026gt;% # Make the component categories follow the order they\u0026#39;re in in the data so # that \u0026quot;Actual data\u0026quot; is first, etc. mutate(component = fct_inorder(component)) retail_components_tidy ## # A tibble: 1,360 x 4 ## .model year_month component value ## \u0026lt;chr\u0026gt; \u0026lt;mth\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 stl 1992 Jan Actual data 130683 ## 2 stl 1992 Jan Trend 148453. ## 3 stl 1992 Jan Seasonality -22505. ## 4 stl 1992 Jan Remainder 4735. ## 5 stl 1992 Feb Actual data 131244 ## 6 stl 1992 Feb Trend 148960. ## 7 stl 1992 Feb Seasonality -23009. ## 8 stl 1992 Feb Remainder 5292. ## 9 stl 1992 Mar Actual data 142488 ## 10 stl 1992 Mar Trend 149468. ## # … with 1,350 more rows Now that we have a long dataset, we can facet by component:\nggplot(retail_components_tidy, aes(x = year_month, y = value)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line() + scale_y_continuous(labels = dollar) + labs(x = NULL, y = \u0026quot;Millions of dollars\u0026quot;, title = \u0026quot;Decomposed US Advance Retail Sales\u0026quot;, subtitle = \u0026quot;Nominal US dollars\u0026quot;, caption = \u0026quot;Source: US Census Bureau and FRED (RSXFSN)\u0026quot;) + facet_wrap(vars(component), ncol = 1, scales = \u0026quot;free_y\u0026quot;) + theme_minimal(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;), plot.title.position = \u0026quot;plot\u0026quot;, strip.text = element_text(face = \u0026quot;bold\u0026quot;, hjust = 0)) Beautiful!\n  ","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590451200,"objectID":"04fd6e11389955617e7779fefe3c7a53","permalink":"/example/11-example/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/example/11-example/","section":"example","summary":"Live coding example Complete code  Get data Look at and clean data Plotting time Improving graphics Decomposition    For this example, we’re going to use economic data from the US Federal Reserve (the Fed). The St. Louis Fed is in charge of publishing Fed economic data, and they host it all at an online portal named FRED. Instead of downloading individual time series data from the FRED website, we’ll do what with did with the World Bank WDI data and download it directly from the internet with the tidyquant package, which includes a function for working with the FRED API/website.","tags":null,"title":"Time","type":"docs"},{"authors":null,"categories":null,"content":"  Once again, there’s no lesson this time. You’re all understanding the basics of R and ggplot and dplyr really well (I’m seriously so impressed and proud of you all!).\nIn your exercise today you’ll visualize trends in time using one of three different real-world datasets. In the example I demonstrate how to remove seasonality from time series data, which is a useful skill, but not always applicable to every time series dataset. If there’s no seasonality in your data, you don’t need to remove it.\nSo head over to the example or the exercise to get started!\n","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590451200,"objectID":"2cd5967b5ce567cec7f3684c70946623","permalink":"/lesson/11-lesson/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/lesson/11-lesson/","section":"lesson","summary":"Once again, there’s no lesson this time. You’re all understanding the basics of R and ggplot and dplyr really well (I’m seriously so impressed and proud of you all!).\nIn your exercise today you’ll visualize trends in time using one of three different real-world datasets. In the example I demonstrate how to remove seasonality from time series data, which is a useful skill, but not always applicable to every time series dataset.","tags":null,"title":"Time","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: World map Bonus (optional) task!: Personal map Turning everything in   Getting started For this exercise, you’ll visualize the proportion of the world that uses the interent. You’ll use data from Max Roser’s Our World in Data project, which collects all sorts of interesting cross-national data. You’ll also use national shapefiles from Natural Earth.\nDownload these two data files:\n  share-of-individuals-using-the-internet-1990-2015.csv  ne_110m_admin_0_countries.zip. This is the “110m Admin 0—Countries” shapefile from Natural Earth. It will download as a .zip file. Unzip the file and move the entire ne_110m_admin_0_countries directory into your data folder.  You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and join the two datasets. Download that here and include it in your project:\n  12-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 12-exercise.Rmd your-project-name.Rproj data\\ share-of-individuals-using-the-internet-1990-2015.csv ne_110m_admin_0_countries/ ... ne_110m_admin_0_countries.shp ... To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  12-exercise.zip  The example from today’s session shows how to load and plot shapefiles and will be incredibly helpful as you do this exercise.\nThis can be as simple or as complex as you want. You don’t need to make your plot super fancy, but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: World map Make a map showing the proportion of individuals in each country that had access to the internet in 2015.\n I’ve provided some starter code in the R Markdown file. You’ll want to fill each country by the users column. Make sure you choose a good projection. See the “Projections and coordinate reference systems” section from the example.  Bonus optional extra fun: Use your comparison/time skills to show the change in internet access between 2000 and 2015, perhaps with facetting some years, or calculating ratios or proportions or percent changes\n Bonus (optional) task!: Personal map Draw your own map with your own points. This could be a map of places you’ve lived, or a map of places you’ve visited, or a map of places you want to visit. Anything!\nThe only requirement is that you find an appropriate shapefile (states, counties, world, etc.), collect latitude and longitude data from Google Maps, and plot the points (with or without labels) on a map. Use multiple shapefiles if you want—add roads, rivers, lakes, whatever.\nHint: Basically follow the code from the example in the section named “Making your own geoencoded data”\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590537600,"objectID":"cd5c294a4f440d092a92586ea0c1b346","permalink":"/assignment/12-exercise/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/assignment/12-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: World map Bonus (optional) task!: Personal map Turning everything in   Getting started For this exercise, you’ll visualize the proportion of the world that uses the interent. You’ll use data from Max Roser’s Our World in Data project, which collects all sorts of interesting cross-national data. You’ll also use national shapefiles from Natural Earth.\nDownload these two data files:","tags":null,"title":"Space","type":"docs"},{"authors":null,"categories":null,"content":"   Shapefiles Projections and coordinate reference systems Shapefiles to download Live coding example Complete code  Load and look at data Basic plotting World map with different projections US map with different projections Individual states Plotting multiple shapefile layers Plotting multiple shapefile layers when some are bigger than the parent shape Plotting schools in Georgia Making your own geoencoded data Plotting other data on maps    Shapefiles Shapefiles are special types of data that include information about geography, such as points (latitude, longitude), paths (a bunch of connected latitudes and longitudes) and areas (a bunch of connected latitudes and longitudes that form a complete shape). Nowadays, most government agencies provide shapefiles for their jurisdictions. For global mapping data, you can use the Natural Earth project:\n Natural Earth US Census Bureau Georgia GIS Clearinghouse (requires a free account; the interface is incredibly clunky) Atlanta Regional Council Fulton County GIS Portal City of Atlanta, Department of City Planning   Projections and coordinate reference systems As you read in this week’s readings, projections matter a lot for maps. You can convert your geographic data between different coordinate systems (or projections)1 fairly easily with sf. You can use coord_sf(crs = XXXX) to convert coordinate reference systems (CRS) as you plot, or use st_transform() to convert data frames to a different CRS.\nThere are standard indexes of more than 4,000 of these projections (!!!) at spatialreference.org or at epsg.io. Here are some common ones:\n 54002: Equidistant cylindrical projection for the world2 54004: Mercator projection for the world 54008: Sinusoidal projection for the world 54009: Mollweide projection for the world 54030: Robinson projection for the world3 4326: WGS 84: DOD GPS coordinates (standard -180 to 180 system) 4269: NAD 83: Relatively common projection for North America 102003: Albers projection specifically for the contiguous United States  Alternatively, instead of using these index numbers, you can use any of the names listed here, such as:\n \"+proj=merc\": Mercator \"+proj=robin\": Robinson \"+proj=moll\": Mollweide \"+proj=aeqd\": Azimuthal Equidistant \"+proj=cass\": Cassini-Soldner   Shapefiles to download I use a lot of different shapefiles in this example. To save you from having to go find and download each individual one, you can download this zip file:\n  shapefiles.zip  Unzip this and put all the contained folders in a folder named data if you want to follow along. You don’t need to follow along!\nYour project should be structured like this:\nyour-project-name\\ some-name.Rmd your-project-name.Rproj data\\ cb_2018_us_county_5m\\ ... cb_2018_us_county_5m.shp ... cb_2018_us_state_20m\\ ne_10m_admin_1_states_provinces\\ ne_10m_lakes\\ ne_10m_rivers_lake_centerlines\\ ne_10m_rivers_north_america\\ ne_110m_admin_0_countries\\ schools_2009\\ These shapefiles all came from these sources:\n  World map: 110m “Admin 0 - Countries” from Natural Earth  US states: 20m 2018 state boundaries from the US Census Bureau  US counties: 5m 2018 county boundaries from the US Census Bureau  US states high resolution: 10m “Admin 1 – States, Provinces” from Natural Earth  Global rivers: 10m “Rivers + lake centerlines” from Natural Earth  North American rivers: 10m “Rivers + lake centerlines, North America supplement” from Natural Earth  Global lakes: 10m “Lakes + Reservoirs” from Natural Earth  Georgia K–12 schools, 2009: “Georgia K-12 Schools” from the Georgia Department of Education (you must be logged in to access this)   Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and look at data First we’ll load the libraries we’re going to use:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(sf) # For GIS magic Next we’ll load all the different shapefiles we downloaded using read_sf():\n# Download \u0026quot;Admin 0 – Countries\u0026quot; from # https://www.naturalearthdata.com/downloads/110m-cultural-vectors/ world_map \u0026lt;- read_sf(\u0026quot;data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\u0026quot;) # Download cb_2018_us_state_20m.zip under \u0026quot;States\u0026quot; from # https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html us_states \u0026lt;- read_sf(\u0026quot;data/cb_2018_us_state_20m/cb_2018_us_state_20m.shp\u0026quot;) # Download cb_2018_us_county_5m.zip under \u0026quot;County\u0026quot; from # https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html us_counties \u0026lt;- read_sf(\u0026quot;data/cb_2018_us_county_5m/cb_2018_us_county_5m.shp\u0026quot;) # Download \u0026quot;Admin 1 – States, Provinces\u0026quot; from # https://www.naturalearthdata.com/downloads/10m-cultural-vectors/ us_states_hires \u0026lt;- read_sf(\u0026quot;data/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp\u0026quot;) # Download \u0026quot;Rivers + lake centerlines\u0026quot; from # https://www.naturalearthdata.com/downloads/10m-physical-vectors/ rivers_global \u0026lt;- read_sf(\u0026quot;data/ne_10m_rivers_lake_centerlines/ne_10m_rivers_lake_centerlines.shp\u0026quot;) # Download \u0026quot;Rivers + lake centerlines, North America supplement\u0026quot; from # https://www.naturalearthdata.com/downloads/10m-physical-vectors/ rivers_na \u0026lt;- read_sf(\u0026quot;data/ne_10m_rivers_north_america/ne_10m_rivers_north_america.shp\u0026quot;) # Download \u0026quot;Lakes + Reservoirs\u0026quot; from # https://www.naturalearthdata.com/downloads/10m-physical-vectors/ lakes \u0026lt;- read_sf(\u0026quot;data/ne_10m_lakes/ne_10m_lakes.shp\u0026quot;) # Download from https://data.georgiaspatial.org/index.asp?body=preview\u0026amp;dataId=41516 # after creating an account and logging in ga_schools \u0026lt;- read_sf(file.path(\u0026quot;data\u0026quot;, \u0026quot;schools_2009\u0026quot;, \u0026quot;DOE Schools 2009.shp\u0026quot;))  Basic plotting If you look at the world_map dataset in RStudio, you’ll see it’s just a standard data frame with 177 rows and 95 columns. The last column is the magical geometry column with the latitude/longitude details for the borders for every country. RStudio only shows you 50 columns at a time in the RStudio viewer, so you’ll need to move to the next page of columns with the » button in the top left corner.\nBecause this is just a data frame, we can do all our normal dplyr things to it. Let’s get rid of Antarctica, since it takes up a big proportion of the southern hemisphere:\nworld_sans_antarctica \u0026lt;- world_map %\u0026gt;% filter(ISO_A3 != \u0026quot;ATA\u0026quot;) Ready to plot a map? Here’s all you need to do:\nggplot() + geom_sf(data = world_sans_antarctica) If you couldn’t tell from the lecture, I’m completely blown away by how amazingly easy this every time I plot a map :)\nBecause this a regular ggplot geom, all our regular aesthetics and themes and everything work:\nggplot() + geom_sf(data = world_sans_antarctica, fill = \u0026quot;#669438\u0026quot;, color = \u0026quot;#32481B\u0026quot;, size = 0.25) + theme_void() The Natural Earth dataset happens to come with some columns with a coloring scheme with 7–13 colors (MAPCOLOR7, MAPCOLOR9, etc.) so that no countries with a shared border share a color. We can fill by that column:\nggplot() + geom_sf(data = world_sans_antarctica, aes(fill = as.factor(MAPCOLOR7)), color = \u0026quot;#401D16\u0026quot;, size = 0.25) + scale_fill_viridis_d(option = \u0026quot;plasma\u0026quot;) + guides(fill = FALSE) + theme_void()  World map with different projections Changing projections is trivial: add a coord_sf() layer where you specify the CRS you want to use.\nHere’s Robinson (yay):\nggplot() + geom_sf(data = world_sans_antarctica, fill = \u0026quot;#669438\u0026quot;, color = \u0026quot;#32481B\u0026quot;, size = 0.25) + coord_sf(crs = 54030) + # Robinson # Or use the name instead of the number # coord_sf(crs = \u0026quot;+proj=robin\u0026quot;) theme_void() Here’s sinusoidal:\nggplot() + geom_sf(data = world_sans_antarctica, fill = \u0026quot;#669438\u0026quot;, color = \u0026quot;#32481B\u0026quot;, size = 0.25) + coord_sf(crs = 54008) + # Sinusoidal theme_void() And here’s Mercator (ewww):\nggplot() + geom_sf(data = world_sans_antarctica, fill = \u0026quot;#669438\u0026quot;, color = \u0026quot;#32481B\u0026quot;, size = 0.25) + coord_sf(crs = 3785) + # Mercator # Or use the name instead of the number # coord_sf(crs = \u0026quot;+proj=merc\u0026quot;) theme_void() Note: Sometimes Windows doesn’t like using the raw number like coord_sf(crs = 54030). If you get an error about a missing or unknown CRS, there are two workarounds: find and look up the name abbreviation like coord_sf(crs = \"+proj=robin\"), or add the prefix “ESRI” like coord_sf(crs = \"ESRI:54030\")\n  US map with different projections This same process works for any shapefile. The map of the US can also be projected differently—two common projections are NAD83 and Albers. We’ll take the us_states dataset, remove Alaska, Hawaii, and Puerto Rico (they’re so far from the rest of the lower 48 states that they make an unusable map—if you want to include them, it’s easiest to plot them as separate plots and use patchwork to stitch them together), and plot it.\nlower_48 \u0026lt;- us_states %\u0026gt;% filter(!(NAME %in% c(\u0026quot;Alaska\u0026quot;, \u0026quot;Hawaii\u0026quot;, \u0026quot;Puerto Rico\u0026quot;))) ggplot() + geom_sf(data = lower_48, fill = \u0026quot;#192DA1\u0026quot;, color = \u0026quot;white\u0026quot;, size = 0.25) + coord_sf(crs = 4269) + # NAD83 theme_void() ggplot() + geom_sf(data = lower_48, fill = \u0026quot;#192DA1\u0026quot;, color = \u0026quot;white\u0026quot;, size = 0.25) + coord_sf(crs = 102003) + # Albers theme_void()  Individual states Again, because these shapefiles are really just fancy data frames, we can filter them with normal dplyr functions. Let’s plot just Georgia:\nonly_georgia \u0026lt;- lower_48 %\u0026gt;% filter(NAME == \u0026quot;Georgia\u0026quot;) ggplot() + geom_sf(data = only_georgia, fill = \u0026quot;#EC8E55\u0026quot;) + theme_void() We can also use a different projection. If we look at spatialreference.org, there’s a version of NAD83 that’s focused specifically on Georgia.\nggplot() + geom_sf(data = only_georgia, fill = \u0026quot;#EC8E55\u0026quot;) + theme_void() + coord_sf(crs = 2239) # NAD83 focused on Georgia There’s one small final issue though: we’re missing all the Atlantic islands in the southeast like Cumberland Island and Amelia Island. That’s because we’re using the Census’s low resolution (20m) data. That’s fine for the map of the whole country, but if we’re looking at a single state, we probably want better detail in the borders. We can use the Census’s high resolution (500k) data, but even then it doesn’t include the islands for whatever reason, but Natural Earth has high resolution US state data that does have the islands, so we can use that:\nonly_georgia_high \u0026lt;- us_states_hires %\u0026gt;% filter(iso_3166_2 == \u0026quot;US-GA\u0026quot;) ggplot() + geom_sf(data = only_georgia_high, fill = \u0026quot;#EC8E55\u0026quot;) + theme_void() + coord_sf(crs = 2239) # NAD83 focused on Georgia Perfect.\n Plotting multiple shapefile layers The state shapefiles from the Census Bureau only include state boundaries. If we want to see counties in Georgia, we need to download and load the Census’s county shapefiles (which we did above). We can then add a second geom_sf() layer for the counties.\nFirst we need to filter the county data to only include Georgia counties. The counties data doesn’t include a column with the state name or state abbreviation, but it does include a column named STATEFP, which is the state FIPS code. Looking at lower_48 we can see that the state FIPS code for Georgia is 13, so we use that to filter.\nga_counties \u0026lt;- us_counties %\u0026gt;% filter(STATEFP == 13) Now we can plot just the counties:\nggplot() + geom_sf(data = ga_counties) + theme_void() Technically we can just draw the county boundaries instead of layer the state boundary + the counties, since the borders of the counties make up the border of the state. But there’s an advantage to including both: we can use different aesthetics on each, like adding a thicker border on the state:\nggplot() + geom_sf(data = only_georgia_high, color = \u0026quot;#EC8E55\u0026quot;, size = 3) + geom_sf(data = ga_counties, fill = \u0026quot;#A5D46A\u0026quot;, color = \u0026quot;white\u0026quot;) + theme_void() It’s also useful if we want to only show some counties, like metropolitan Atlanta:\natl_counties \u0026lt;- ga_counties %\u0026gt;% filter(NAME %in% c(\u0026quot;Cherokee\u0026quot;, \u0026quot;Clayton\u0026quot;, \u0026quot;Cobb\u0026quot;, \u0026quot;DeKalb\u0026quot;, \u0026quot;Douglas\u0026quot;, \u0026quot;Fayette\u0026quot;, \u0026quot;Fulton\u0026quot;, \u0026quot;Gwinnett\u0026quot;, \u0026quot;Henry\u0026quot;, \u0026quot;Rockdale\u0026quot;)) ggplot() + geom_sf(data = only_georgia_high, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = atl_counties, fill = \u0026quot;#A5D46A\u0026quot;, color = \u0026quot;white\u0026quot;) + theme_void()  Plotting multiple shapefile layers when some are bigger than the parent shape So far we’ve been able to filter out states and counties that we don’t want to plot using filter(), which works because the shapefiles have geometry data for each state or county. But what if you’re plotting stuff that doesn’t follow state or county boundaries, like freeways, roads, rivers, or lakes?\nAt the beginning we loaded a shapefile for all large and small rivers in the US. Look at the first few rows of rivers_na:\nhead(rivers_na) ## Simple feature collection with 6 features and 37 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: -100 ymin: 29 xmax: -86 ymax: 36 ## CRS: 4326 ## # A tibble: 6 x 38 ## featurecla scalerank rivernum dissolve name name_alt note name_full min_zoom strokeweig uident min_label label wikidataid name_ar name_bn name_de ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 River 10 22360 22360Ri… Colo… \u0026lt;NA\u0026gt; ID i… Colorado… 6 0.3 1.99e6 7 Colo… Q847785 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Colora… ## 2 River 10 22572 22572Ri… Cima… \u0026lt;NA\u0026gt; ID i… Cimarron… 6 0.25 2.15e6 7 Cima… Q1092055 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Cimarr… ## 3 River 10 22519 22519Ri… Wash… \u0026lt;NA\u0026gt; ID i… Washita … 6 0.25 1.95e6 7 Wash… Q2993598 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Washita ## 4 River 10 22519 22519Ri… Wash… \u0026lt;NA\u0026gt; ID i… Washita … 6 0.15 1.95e6 7 Wash… Q2993598 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Washita ## 5 River 11 22422 22422Ri… Cone… \u0026lt;NA\u0026gt; ID i… Conecuh … 6.7 0.15 2.17e6 7.7 Cone… Q5159475 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 River 10 22421 22421Ri… Pea \u0026lt;NA\u0026gt; ID i… Pea River 6 0.15 1.96e6 7 Pea Q7157190 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## # … with 21 more variables: name_en \u0026lt;chr\u0026gt;, name_es \u0026lt;chr\u0026gt;, name_fr \u0026lt;chr\u0026gt;, name_el \u0026lt;chr\u0026gt;, name_hi \u0026lt;chr\u0026gt;, name_hu \u0026lt;chr\u0026gt;, name_id \u0026lt;chr\u0026gt;, name_it \u0026lt;chr\u0026gt;, ## # name_ja \u0026lt;chr\u0026gt;, name_ko \u0026lt;chr\u0026gt;, name_nl \u0026lt;chr\u0026gt;, name_pl \u0026lt;chr\u0026gt;, name_pt \u0026lt;chr\u0026gt;, name_ru \u0026lt;chr\u0026gt;, name_sv \u0026lt;chr\u0026gt;, name_tr \u0026lt;chr\u0026gt;, name_vi \u0026lt;chr\u0026gt;, ## # name_zh \u0026lt;chr\u0026gt;, wdid_score \u0026lt;int\u0026gt;, ne_id \u0026lt;dbl\u0026gt;, geometry \u0026lt;MULTILINESTRING [°]\u0026gt; The first row is the whole Colorado river, which flows through seven states. We can’t just use filter() to only select some parts of it based on states.\nHere’s what happens if we combine our Georgia map with rivers and lakes:\nggplot() + geom_sf(data = only_georgia, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = rivers_na) + theme_void() It plots Georgia, and it’s filled with orange, but it also plots every single river in North America. Oops.\nWe need to do a little GIS work to basically use only_georgia as a cookie cutter and keep only the rivers that are contained in the only_georgia boundaries. Fortunately, there’s a function in the sf package that does this: st_intersection(). Feed it two shapefile datasets and it will select the parts of the second that fall within the boundaries of the first:\nga_rivers_na \u0026lt;- st_intersection(only_georgia, rivers_na) ## Error in geos_op2_geom(\u0026quot;intersection\u0026quot;, x, y): st_crs(x) == st_crs(y) is not TRUE Oh no! An error! It’s complaining that the reference systems used in these two datasets don’t match. We can check the CRS with st_crs():\nst_crs(only_georgia) ## Coordinate Reference System: ## User input: 4269 ## wkt: ## GEOGCS[\u0026quot;NAD83\u0026quot;, ## DATUM[\u0026quot;North_American_Datum_1983\u0026quot;, ## SPHEROID[\u0026quot;GRS 1980\u0026quot;,6378137,298.257222101, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;7019\u0026quot;]], ## TOWGS84[0,0,0,0,0,0,0], ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;6269\u0026quot;]], ## PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;8901\u0026quot;]], ## UNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;9122\u0026quot;]], ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;4269\u0026quot;]] st_crs(rivers_na) ## Coordinate Reference System: ## User input: 4326 ## wkt: ## GEOGCS[\u0026quot;WGS 84\u0026quot;, ## DATUM[\u0026quot;WGS_1984\u0026quot;, ## SPHEROID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;7030\u0026quot;]], ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;6326\u0026quot;]], ## PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;8901\u0026quot;]], ## UNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;9122\u0026quot;]], ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;4326\u0026quot;]] The Georgia map uses 4269 (or NAD83), while the rivers map uses 4326 (or the GPS system of latitude and longitude). We need to convert one of them to make them match. It doesn’t matter which one.\nonly_georgia_4326 \u0026lt;- only_georgia %\u0026gt;% st_transform(crs = 4326) ga_rivers_na \u0026lt;- st_intersection(only_georgia_4326, rivers_na) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant throughout all geometries You’ll get an ominous warning, but you should be okay—it’s just because flattening globes into flat planes is hard, and the cutting might not be 100% accurate, but it’ll be close enough for our mapping purposes.\nNow we can plot our state shape and the truncated rivers:\nggplot() + geom_sf(data = only_georgia, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = ga_rivers_na) + theme_void() Hey! It worked! Let’s put all the rivers and lakes on at once and make it a little more artsy. We’ll use the high resolution Georgia map too, which conveniently already matches the CRS of the rivers and lakes:\nga_rivers_na \u0026lt;- st_intersection(only_georgia_high, rivers_na) ga_rivers_global \u0026lt;- st_intersection(only_georgia_high, rivers_global) ga_lakes \u0026lt;- st_intersection(only_georgia_high, lakes) ggplot() + geom_sf(data = only_georgia_high, color = \u0026quot;black\u0026quot;, size = 0.1, fill = \u0026quot;black\u0026quot;) + geom_sf(data = ga_rivers_global, size = 0.3, color = \u0026quot;grey80\u0026quot;) + geom_sf(data = ga_rivers_na, size = 0.15, color = \u0026quot;grey80\u0026quot;) + geom_sf(data = ga_lakes, size = 0.3, fill = \u0026quot;grey80\u0026quot;, color = NA) + coord_sf(crs = 4269) + # NAD83 theme_void() Heck yeah. That’s a great map. This is basically what Kieran Healy did here, but he used even more detailed shapefiles from the US Geological Survey.\n Plotting schools in Georgia Shapefiles are not limited to just lines and areas—they can also contain points. I made a free account at the Georgia GIS Clearinghouse, searched for “schools” and found a shapefile of all the K–12 schools in 2009. This is the direct link to the page, but it only works if you’re logged in to their system. This is the official metadata for the shapefile, which you can see if you’re not logged in, but you can’t download anything. It’s a dumb system and other states are a lot better at offering their GIS data (like, here’s a shapefile of all of Utah’s schools and libraries as of 2017, publicly accessible without an account).\nWe loaded the shapefile up at the top, but now let’s look at it:\nhead(ga_schools) ## Simple feature collection with 6 features and 16 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2100000 ymin: 320000 xmax: 2200000 ymax: 5e+05 ## proj4string: +proj=tmerc +lat_0=30 +lon_0=-84.16666666666667 +k=0.9999 +x_0=700000.0000001107 +y_0=0 +ellps=GRS80 +units=us-ft +no_defs ## # A tibble: 6 x 17 ## ID DATA COUNTY DISTRICT SCHOOLNAME GRADES ADDRESS CITY STATE ZIP TOTAL SCHOOLID DOE_CONGRE CONGRESS SENATE HOUSE geometry ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;POINT [US_survey_foot]\u0026gt; ## 1 4313 224 Early Early Co… Early Coun… PK,KK,… 283 Ma… Blak… GA 3982… 1175 43549 2 002 011 149 (2052182 494322) ## 2 4321 227 Early Early Co… ETN Eckerd… 06,07,… 313 E … Blak… GA 3982… 30 47559 2 002 011 149 (2053200 5e+05) ## 3 4329 226 Early Early Co… Early Coun… 06,07,… 12053 … Blak… GA 3982… 539 43550 2 002 011 149 (2055712 5e+05) ## 4 4337 225 Early Early Co… Early Coun… 09,10,… 12020 … Blak… GA 3982… 716 43552 2 002 011 149 (2055712 5e+05) ## 5 4345 189 Decatur Decatur … John Johns… PK,KK,… 1947 S… Bain… GA 3981… 555 43279 2 002 011 172 (2168090 321781) ## 6 4353 192 Decatur Decatur … Potter Str… PK,KK,… 725 Po… Bain… GA 3981… 432 43273 2 002 011 172 (2168751 327375) We have a bunch of columns like GRADES that has a list of what grades are included in the school, and TOTAL, which I’m guessing is the number of students. Let’s map it!\nIf we add a geom_sf() layer just for ga_schools, it’ll plot a bunch of points:\nggplot() + geom_sf(data = ga_schools) One of these rows is wildly miscoded and ended up Indonesia! If you sort by the geometry column in RStudio, you’ll see that it’s most likely Allatoona High School in Cobb County (id = 22097). The coordinates are different from all the others, and it has no congressional district information. Let’s remove it.\nga_schools_fixed \u0026lt;- ga_schools %\u0026gt;% filter(ID != 22097) ggplot() + geom_sf(data = ga_schools_fixed) That’s better. However, all we’re plotting now are the points—we’ve lost the state and/or county boundaries. Let’s include those:\nggplot() + geom_sf(data = only_georgia_high) + geom_sf(data = ga_schools_fixed) + theme_void() We’re getting closer. We have some issues with overplotting, so let’s shrink the points down and make them a little transparent:\nggplot() + geom_sf(data = only_georgia_high) + geom_sf(data = ga_schools_fixed, size = 0.5, alpha = 0.5) + theme_void() Neat. One last thing we can do is map the TOTAL column to the color aesthetic and color the points by how many students attend each school:\nggplot() + geom_sf(data = only_georgia_high) + geom_sf(data = ga_schools_fixed, aes(color = TOTAL), size = 0.75, alpha = 0.5) + scale_color_viridis_c() + theme_void() Most schools appear to be under 1,000 students, except for a cluster in Gwinnett County north of Atlanta. Its high schools have nearly 4,000 students each!\nga_schools_fixed %\u0026gt;% select(COUNTY, SCHOOLNAME, TOTAL) %\u0026gt;% arrange(desc(TOTAL)) %\u0026gt;% head() ## Simple feature collection with 6 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2300000 ymin: 1400000 xmax: 2400000 ymax: 1500000 ## proj4string: +proj=tmerc +lat_0=30 +lon_0=-84.16666666666667 +k=0.9999 +x_0=700000.0000001107 +y_0=0 +ellps=GRS80 +units=us-ft +no_defs ## # A tibble: 6 x 4 ## COUNTY SCHOOLNAME TOTAL geometry ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;POINT [US_survey_foot]\u0026gt; ## 1 Gwinnett Mill Creek High School 3997 (2384674 1482772) ## 2 Gwinnett Collins Hill High School 3720 (2341010 1461730) ## 3 Gwinnett Brookwood High School 3455 (2334543 1413396) ## 4 Gwinnett Grayson High School 3230 (2370186 1408579) ## 5 Gwinnett Peachtree Ridge High School 3118 (2319344 1459458) ## 6 Gwinnett Berkmar High School 3095 (2312983 1421933)  Making your own geoencoded data So, plotting shapefiles with geom_sf() is magical because sf deals with all of the projection issues for us automatically and it figures out how to plot all the latitude and longitude data for us automatically. But lots of data doesn’t some as shapefiles. The rats data from mini project 1, for instance, has two columns indicating the latitude and longitude of each rat sighting, but those are stored as just numbers. If we try to use geom_sf() with the rat data, it won’t work. We need that magical geometry column.\nFortunately, if we have latitude and longitude information, we can make our own geometry column.\nLet’s say we want to mark some cities on our map of Georgia. We can make a mini dataset using tribble(). I found these points from Google Maps: right click anywhere in Google Maps, select “What’s here?”, and you’ll see the exact coordinates for that spot.\nga_cities \u0026lt;- tribble( ~city, ~lat, ~long, \u0026quot;Atlanta\u0026quot;, 33.748955, -84.388099, \u0026quot;Athens\u0026quot;, 33.950794, -83.358884, \u0026quot;Savannah\u0026quot;, 32.113192, -81.089350 ) ga_cities ## # A tibble: 3 x 3 ## city lat long ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Atlanta 33.7 -84.4 ## 2 Athens 34.0 -83.4 ## 3 Savannah 32.1 -81.1 This is just a normal dataset, and the lat and long columns are just numbers. R doesn’t know that those are actually geographic coordinates. This is similar to the rats data, or any other data that has columns for latitude and longitude.\nWe can convert those two columns to the magic geometry column with the st_as_sf() function. We have to define two things in the function: which coordinates are the longitude and latitude, and what CRS the coordinates are using. Google Maps uses 4326, or the GPS system, so we specify that:\nga_cities_geometry \u0026lt;- ga_cities %\u0026gt;% st_as_sf(coords = c(\u0026quot;long\u0026quot;, \u0026quot;lat\u0026quot;), crs = 4326) ga_cities_geometry ## Simple feature collection with 3 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: -84 ymin: 32 xmax: -81 ymax: 34 ## CRS: EPSG:4326 ## # A tibble: 3 x 2 ## city geometry ## \u0026lt;chr\u0026gt; \u0026lt;POINT [°]\u0026gt; ## 1 Atlanta (-84 34) ## 2 Athens (-83 34) ## 3 Savannah (-81 32) The longitude and latitude columns are gone now, and we have a single magical geometry column. That means we can plot it with geom_sf():\nggplot() + geom_sf(data = only_georgia_high, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = ga_cities_geometry, size = 3) + theme_void() We can use geom_sf_label() (or geom_sf_text()) to add labels in the correct locations too. It will give you a warning, but you can ignore it—again, it’s complaining that the positioning might not be 100% accurate because of issues related to taking a globe and flattening it. It’s fine.\nggplot() + geom_sf(data = only_georgia_high, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = ga_cities_geometry, size = 3) + geom_sf_label(data = ga_cities_geometry, aes(label = city), nudge_y = 0.2) + theme_void()  Plotting other data on maps So far we’ve just plotted whatever data the shapefile creators decided to include and publish in their data. But what if you want to visualize some other variable on a map? We can do this by combining our shapefile data with any other kind of data, as long as the two have a shared column. For instance, we can make a choropleth map of life expectancy with data from the World Bank.\nFirst, let’s grab some data from the World Bank for just 2015:\nlibrary(WDI) # For getting data from the World Bank indicators \u0026lt;- c(\u0026quot;SP.DYN.LE00.IN\u0026quot;) # Life expectancy wdi_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE, start = 2015, end = 2015) Let’s see what we got:\nhead(wdi_raw) ## # A tibble: 6 x 11 ## iso2c country SP.DYN.LE00.IN year iso3c region capital longitude latitude income lending ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1A Arab World 71.2 2015 ARB Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 2 1W World 71.9 2015 WLD Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 3 4E East Asia \u0026amp; Pacific (excluding high i… 74.5 2015 EAP Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 4 7E Europe \u0026amp; Central Asia (excluding high… 72.7 2015 ECA Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 5 8S South Asia 68.6 2015 SAS Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 6 AD Andorra NA 2015 AND Europe \u0026amp; Central … Andorra la Vel… 1.52 42.5 High inc… Not classif… We have a bunch of columns here, but we care about two in particular: life expectancy, and the ISO3 code. This three-letter code is a standard system for identifying countries (see the full list here), and that column will let us combine this World Bank data with the global shapefile, which also has a column for the ISO3 code.\n(We also have columns for the latitude and longitude for each capital, so we could theoretically convert those to a geometry column with st_as_sf() and plot world capitals, which would be neat, but we won’t do that now.)\nLet’s clean up the WDI data and shrink it down substantially:\nwdi_clean_small \u0026lt;- wdi_raw %\u0026gt;% select(life_expectancy = SP.DYN.LE00.IN, iso3c) wdi_clean_small ## # A tibble: 264 x 2 ## life_expectancy iso3c ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 71.2 ARB ## 2 71.9 WLD ## 3 74.5 EAP ## 4 72.7 ECA ## 5 68.6 SAS ## 6 NA AND ## 7 77.3 ARE ## 8 63.4 AFG ## 9 76.5 ATG ## 10 78.0 ALB ## # … with 254 more rows Next we need to merge this tiny dataset into the world_map_sans_antarctica shapefile data we were using earlier. To do this we’ll use a function named left_join(). We feed two data frames into left_join(), and R will keep all the rows from the first and include all the columns from both the first and the second wherever the two datasets match with one specific column. That’s wordy and weird—stare at this animation here for a few seconds to see what’s really going to happen. We’re essentially going to append the World Bank data to the end of the world shapefiles and line up rows that have matching ISO3 codes. The ISO3 column is named ISO_A3 in the shapefile data, and it’s named iso3c in the WDI data, so we tell left_join() that those are the same column:\nworld_map_with_life_expectancy \u0026lt;- world_sans_antarctica %\u0026gt;% left_join(wdi_clean_small, by = c(\u0026quot;ISO_A3\u0026quot; = \u0026quot;iso3c\u0026quot;)) If you look at this dataset in RStudio now and look at the last column, you’ll see the WDI life expectancy right next to the magic geometry column.\nWe technically didn’t need to shrink the WDI data down to just two columns—had we left everything else, all the WDI columns would have come over to the world_sans_antarctica, including columns for region and income level, etc. But I generally find it easier and cleaner to only merge in the columns I care about instead of making massive datasets with a billion extra columns.\nNow that we have a column for life expectancy, we can map it to the fill aesthetic and fill each country by 2015 life expectancy:\nggplot() + geom_sf(data = world_map_with_life_expectancy, aes(fill = life_expectancy), size = 0.25) + coord_sf(crs = 54030) + # Robinson scale_fill_viridis_c(option = \u0026quot;viridis\u0026quot;) + labs(fill = \u0026quot;Life expectancy\u0026quot;) + theme_void() + theme(legend.position = \u0026quot;bottom\u0026quot;) Voila! Global life expectancy in 2015!\n(Sharp-eyed readers will notice that France and Norway are grayed out because they’re missing data. That’s because the ISO_A3 code in the Natural Earth data is missing for both France and Norway for whatever reason, so the WDI data didn’t merge with those rows. To fix that, we can do some manual recoding before joining in the WDI data)\nworld_sans_antarctica_fixed \u0026lt;- world_sans_antarctica %\u0026gt;% mutate(ISO_A3 = case_when( # If the country name is Norway or France, redo the ISO3 code ADMIN == \u0026quot;Norway\u0026quot; ~ \u0026quot;NOR\u0026quot;, ADMIN == \u0026quot;France\u0026quot; ~ \u0026quot;FRA\u0026quot;, # Otherwise use the existing ISO3 code TRUE ~ ISO_A3 )) %\u0026gt;% left_join(wdi_clean_small, by = c(\u0026quot;ISO_A3\u0026quot; = \u0026quot;iso3c\u0026quot;)) ggplot() + geom_sf(data = world_sans_antarctica_fixed, aes(fill = life_expectancy), size = 0.25) + coord_sf(crs = 54030) + # Robinson scale_fill_viridis_c(option = \u0026quot;viridis\u0026quot;) + labs(fill = \u0026quot;Life expectancy\u0026quot;) + theme_void() + theme(legend.position = \u0026quot;bottom\u0026quot;)    TECHNICALLY coordinate systems and projection systems are different things, but I’m not a geographer and I don’t care that much about the nuance.↩︎\n This is essentially the Gall-Peters projection from the West Wing clip.↩︎\n This is my favorite world projection.↩︎\n   ","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590537600,"objectID":"8f294a1c92be1a918ba3fa24cc427a78","permalink":"/example/12-example/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/example/12-example/","section":"example","summary":"Shapefiles Projections and coordinate reference systems Shapefiles to download Live coding example Complete code  Load and look at data Basic plotting World map with different projections US map with different projections Individual states Plotting multiple shapefile layers Plotting multiple shapefile layers when some are bigger than the parent shape Plotting schools in Georgia Making your own geoencoded data Plotting other data on maps    Shapefiles Shapefiles are special types of data that include information about geography, such as points (latitude, longitude), paths (a bunch of connected latitudes and longitudes) and areas (a bunch of connected latitudes and longitudes that form a complete shape).","tags":null,"title":"Space","type":"docs"},{"authors":null,"categories":null,"content":"   Combining datasets vertically Combining datasets horizontally inner_join() left_join() Common column names right_join()   There is a short lesson today! You’ll learn the basics of joining two different datasets together, both vertically and horizontally.\nThere are a few imaginary datasets I’ve created for you to play with:\nx ## # A tibble: 3 x 2 ## id some_variable ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 x1 ## 2 2 x2 ## 3 3 x3 y ## # A tibble: 3 x 2 ## id some_other_variable ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 y1 ## 2 2 y2 ## 3 4 y4 national_data ## # A tibble: 9 x 5 ## state year unemployment inflation population ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 GA 2018 5 2 100 ## 2 GA 2019 5.3 1.8 200 ## 3 GA 2020 5.2 2.5 300 ## 4 NC 2018 6.1 1.8 350 ## 5 NC 2019 5.9 1.6 375 ## 6 NC 2020 5.3 1.8 400 ## 7 CO 2018 4.7 2.7 200 ## 8 CO 2019 4.4 2.6 300 ## 9 CO 2020 5.1 2.5 400 national_data_2019 ## # A tibble: 3 x 4 ## state unemployment inflation population ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 GA 5.3 1.8 200 ## 2 NC 5.9 1.6 375 ## 3 CO 4.4 2.6 300 national_libraries ## # A tibble: 6 x 4 ## state year libraries schools ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 CO 2018 230 470 ## 2 CO 2019 240 440 ## 3 CO 2020 270 510 ## 4 NC 2018 200 610 ## 5 NC 2019 210 590 ## 6 NC 2020 220 530 national_libraries_2019 ## # A tibble: 2 x 3 ## state libraries schools ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 CO 240 440 ## 2 NC 210 590 puerto_rico_data ## # A tibble: 3 x 4 ## state unemployment population year ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 PR 3.1 150 2018 ## 2 PR 3.2 250 2019 ## 3 PR 3.3 350 2020 state_regions ## # A tibble: 51 x 2 ## region state ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 West AK ## 2 South AL ## 3 South AR ## 4 West AZ ## 5 West CA ## 6 West CO ## 7 Northeast CT ## 8 South DC ## 9 South DE ## 10 South FL ## # … with 41 more rows Combining datasets vertically Recall from the Lord of the Rings data in exercise 3 that you had to combine three different CSV files into dataset. You used bind_rows() to stack each of these on top of each other.\nlotr \u0026lt;- bind_rows(fellowship, tt, rotk) That worked well because each of the individual data frames had the same columns in them, and R was able to line up the matching columns. If columns were missing, R would have placed NA in the appropriate locations.\nYour turn: Combine national_data and puerto_rico_data into a single dataset named us_data using bind_rows. Pay attention to what happens with the inflation column. Also notice that the columns in the Puerto Rico data are in a different order.\n   Combining datasets horizontally Binding rows vertically is the easiest way to combine two datasets, but most often you won’t be doing that. You’ll only do this if you’re combining datasets that come from the same source, like if a state offers separate CSV files of the same data for each county.\nIn most cases, though, you’ll need to combine completely different datasets, bringing one or more columns from one into another. With vertical combining, R needs column names with the same names in order to figure out where the data lines up. With horizontal combining, R needs values inside one or more columns to be the same in order to figure out where the data lines up.\nThere is technically a function named bind_cols(), but you’ll rarely want to use it. It doesn’t attempt to match any rows—it just glues two datasets together:\nbind_cols(national_data, # Repeat PR 3 times so that it has the same number of rows as national_data bind_rows(puerto_rico_data, puerto_rico_data, puerto_rico_data)) ## # A tibble: 9 x 9 ## state year unemployment inflation population state1 unemployment1 population1 ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 GA 2018 5 2 100 PR 3.1 150 ## 2 GA 2019 5.3 1.8 200 PR 3.2 250 ## 3 GA 2020 5.2 2.5 300 PR 3.3 350 ## 4 NC 2018 6.1 1.8 350 PR 3.1 150 ## 5 NC 2019 5.9 1.6 375 PR 3.2 250 ## 6 NC 2020 5.3 1.8 400 PR 3.3 350 ## 7 CO 2018 4.7 2.7 200 PR 3.1 150 ## 8 CO 2019 4.4 2.6 300 PR 3.2 250 ## 9 CO 2020 5.1 2.5 400 PR 3.3 350 ## # … with 1 more variable: year1 \u0026lt;dbl\u0026gt; That’s… not great.\nInstead, we need to use a function that is more careful about bringing in data. Fortunately there are a few good options:\n inner_join() left_join() right_join()  The most helpful way of understanding these different functions is to go here and stare at the animations for a little while to see which pieces of which dataset go where. (There are lots of others, like full_join(), semi_join(), and anti_join(), and they have helpful animations, but I rarely use those.)\nFor each of these functions, you need at least one common ID column in both datasets in order for R to know where things line up.\nLet’s practice how these all work and see what the differences between them are.\n inner_join() First, go to this page in a new tab and stare at the mesmerizing animation.\nLet’s look at two datasets, x and y:\nx ## # A tibble: 3 x 2 ## id some_variable ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 x1 ## 2 2 x2 ## 3 3 x3 y ## # A tibble: 3 x 2 ## id some_other_variable ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 y1 ## 2 2 y2 ## 3 4 y4 Both datasets have an id column that is the same across each (though the values aren’t necessarily the same). Because there’s a shared column, we can join these two based on that column.\nIf we use inner_join(), the resulting dataset will only keep the rows from the first where there are matching values from the second:\ninner_join(x, y, by = \u0026quot;id\u0026quot;) ## # A tibble: 2 x 3 ## id some_variable some_other_variable ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 x1 y1 ## 2 2 x2 y2 Notice how it got rid of the row with id = 3 from the first and the row with id = 4 from the second.\nYou can also write this with pipes, which is really common when working with dplyr:\nx %\u0026gt;% inner_join(y, by = \u0026quot;id\u0026quot;) ## # A tibble: 2 x 3 ## id some_variable some_other_variable ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 x1 y1 ## 2 2 x2 y2 Let’s say we have two datasets: national_data_2019 and national_libraries_2019:\nnational_data_2019 ## # A tibble: 3 x 4 ## state unemployment inflation population ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 GA 5.3 1.8 200 ## 2 NC 5.9 1.6 375 ## 3 CO 4.4 2.6 300 national_libraries_2019 ## # A tibble: 2 x 3 ## state libraries schools ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 CO 240 440 ## 2 NC 210 590 We want to bring the libraries and schools columns into the general national data. Notice how both datasets have a state column.\nYour turn: Create a new dataset named combined_data that uses inner_join() to merge national_data_2019 and national_libraries_2019.\n   left_join() Again, go to this page in a new tab and stare at the animation.\nLeft joining is less destructive than inner joining. With left joining, any rows in the first dataset that don’t have matches in the second don’t get thrown away and instead are filled with NA:\nleft_join(x, y, by = \u0026quot;id\u0026quot;) ## # A tibble: 3 x 3 ## id some_variable some_other_variable ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 \u0026lt;NA\u0026gt; Notice how the row with id = 4 from the second dataset is gone, but the row with id = 3 from the first is still there, with NA for some_other_variable.\nI find this much more useful when combining data. I often have a larger dataset with all the main variables I care about, perhaps with every combination of country and year over 20 years and 180 countries. If I find another dataset I want to join, and it has missing data for some of the years or countries, I don’t want the combined data to throw away all the rows from the main big dataset that don’t match! I still want those!\n(Look at this for a real life example: I create a dataset I name panel_skeleton that is just all the combinations of countries and years (Afghanistan 1990, Afghanistan 1991, etc.), and then I bring in all sorts of other datasets that match the same countries and years. When there aren’t matches, nothing in the skeleton gets thrown away—R just adds missing values instead.)\nYour turn: Create a new dataset named combined_data that uses left_join() to merge national_data_2019 and national_libraries_2019.\n  Left joining is also often surprisingly helpful for recoding lots of variables. Right now in our fake national data, we have a column for state, but it would be nice if we could have a column for region so we could facet or fill or color by region in a plot. Hunting around on the internet, you find this dataset that has a column for state and a column for abbreviations:\nstate_regions ## # A tibble: 51 x 2 ## region state ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 West AK ## 2 South AL ## 3 South AR ## 4 West AZ ## 5 West CA ## 6 West CO ## 7 Northeast CT ## 8 South DC ## 9 South DE ## 10 South FL ## # … with 41 more rows Your turn: Create a new dataset named national_data_with_region that uses left_join() to combine national_data_2019 with state_regions.\n  Because left_join() only keeps rows from the second dataset that match the first, we don’t actually bring in all 50 rows from the state_regions data—only the rows that match the first dataset (national_data_2019) come over. We could have done with if some massive recoding (mutate(region = ifelse(state == \"GA\" | state == \"NC\", \"South\", ifelse(state == \"CO\"), \"West\", NA))), but that’s awful. Left joining is far easier here.\nYou can also join by multiple columns. So far we’ve been working with just national_data_2019, but if you look at national_data, you’ll see there are rows for different years across these states:\nnational_data ## # A tibble: 9 x 5 ## state year unemployment inflation population ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 GA 2018 5 2 100 ## 2 GA 2019 5.3 1.8 200 ## 3 GA 2020 5.2 2.5 300 ## 4 NC 2018 6.1 1.8 350 ## 5 NC 2019 5.9 1.6 375 ## 6 NC 2020 5.3 1.8 400 ## 7 CO 2018 4.7 2.7 200 ## 8 CO 2019 4.4 2.6 300 ## 9 CO 2020 5.1 2.5 400 Previously, we’ve been specifying the ID column with by = \"state\", but now we have two ID columns: state and year. We can specify both with by = c(\"state\", \"year\").\nYour turn: Create a new dataset named national_data_combined that uses left_join() to combine national_data with national_libraries by state and year.\n  If one dataset has things like state and year, but another only has state, left_join() will still work, but it will only join where the state is the same. For instance, here’s what happens when we join the region data to the yearly national data:\nnational_data_with_region \u0026lt;- national_data %\u0026gt;% left_join(state_regions, by = \u0026quot;state\u0026quot;) national_data_with_region ## # A tibble: 9 x 6 ## state year unemployment inflation population region ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 GA 2018 5 2 100 South ## 2 GA 2019 5.3 1.8 200 South ## 3 GA 2020 5.2 2.5 300 South ## 4 NC 2018 6.1 1.8 350 South ## 5 NC 2019 5.9 1.6 375 South ## 6 NC 2020 5.3 1.8 400 South ## 7 CO 2018 4.7 2.7 200 West ## 8 CO 2019 4.4 2.6 300 West ## 9 CO 2020 5.1 2.5 400 West The “South” region gets added to every row where the state is “GA” and “NC”, even though those rows only appear once in state_regions. left_join() will still match all the values even if states are repeated. Magic!\n Common column names So far, the column names in both datasets have been the same, which has greatly simplified life. In fact, if the columns have the same name, we can technically leave out the by argument and R will guess:\nnational_data %\u0026gt;% left_join(national_libraries) ## Joining, by = c(\u0026quot;state\u0026quot;, \u0026quot;year\u0026quot;) ## # A tibble: 9 x 7 ## state year unemployment inflation population libraries schools ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 GA 2018 5 2 100 NA NA ## 2 GA 2019 5.3 1.8 200 NA NA ## 3 GA 2020 5.2 2.5 300 NA NA ## 4 NC 2018 6.1 1.8 350 200 610 ## 5 NC 2019 5.9 1.6 375 210 590 ## 6 NC 2020 5.3 1.8 400 220 530 ## 7 CO 2018 4.7 2.7 200 230 470 ## 8 CO 2019 4.4 2.6 300 240 440 ## 9 CO 2020 5.1 2.5 400 270 510 It’s good practice to be specific about the columns you want and actually use by, but I will often run left_join() without it and then copy the message that it generates (“by = c(\"state\", \"year\")”) and paste it into my code.\nBut what if the column names don’t match? Let’s rename the state column in our state/region table for fun:\nstate_regions_different \u0026lt;- state_regions %\u0026gt;% rename(ST = state) state_regions_different ## # A tibble: 51 x 2 ## region ST ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 West AK ## 2 South AL ## 3 South AR ## 4 West AZ ## 5 West CA ## 6 West CO ## 7 Northeast CT ## 8 South DC ## 9 South DE ## 10 South FL ## # … with 41 more rows Now watch what happens when we try to join the datasets:\nnational_data %\u0026gt;% left_join(state_regions_different) ## Error: `by` required, because the data sources have no common variables There are no common variables, so we get an error. The state and ST columns really are common variables, but R doesn’t know that.\nWe have two options:\nRename one of the columns so it matches (either change state to ST or change ST to state) Tell left_join() which columns are the same  We can do option two by modifying the by argument like so:\nnational_data %\u0026gt;% left_join(state_regions_different, by = c(\u0026quot;state\u0026quot; = \u0026quot;ST\u0026quot;)) ## # A tibble: 9 x 6 ## state year unemployment inflation population region ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 GA 2018 5 2 100 South ## 2 GA 2019 5.3 1.8 200 South ## 3 GA 2020 5.2 2.5 300 South ## 4 NC 2018 6.1 1.8 350 South ## 5 NC 2019 5.9 1.6 375 South ## 6 NC 2020 5.3 1.8 400 South ## 7 CO 2018 4.7 2.7 200 West ## 8 CO 2019 4.4 2.6 300 West ## 9 CO 2020 5.1 2.5 400 West  right_join() Once again, go to this page in a new tab and watch the animation.\nright_join() works exactly like left_join(), but in reverse. The second dataset is the base data. Any rows in the second dataset that don’t match in the first will be kept, and any rows from the first that don’t match will get thrown away.\nWatch what happens if we right join national_data and state_regions:\nnational_data %\u0026gt;% right_join(state_regions, by = \u0026quot;state\u0026quot;) ## # A tibble: 57 x 6 ## state year unemployment inflation population region ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 AK NA NA NA NA West ## 2 AL NA NA NA NA South ## 3 AR NA NA NA NA South ## 4 AZ NA NA NA NA West ## 5 CA NA NA NA NA West ## 6 CO 2018 4.7 2.7 200 West ## 7 CO 2019 4.4 2.6 300 West ## 8 CO 2020 5.1 2.5 400 West ## 9 CT NA NA NA NA Northeast ## 10 DC NA NA NA NA South ## # … with 47 more rows Yikes. R kept all the rows in state_regions, brought in the columns from national_data and filled most of the new columns with NA, and then repeated Colorado (and NC and GA) three times for each of the years from national_data. That’s a mess.\nIf we reverse the order, we’ll get the correct merged data:\nstate_regions %\u0026gt;% right_join(national_data, by = \u0026quot;state\u0026quot;) ## # A tibble: 9 x 6 ## region state year unemployment inflation population ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 South GA 2018 5 2 100 ## 2 South GA 2019 5.3 1.8 200 ## 3 South GA 2020 5.2 2.5 300 ## 4 South NC 2018 6.1 1.8 350 ## 5 South NC 2019 5.9 1.6 375 ## 6 South NC 2020 5.3 1.8 400 ## 7 West CO 2018 4.7 2.7 200 ## 8 West CO 2019 4.4 2.6 300 ## 9 West CO 2020 5.1 2.5 400 I rarely use right_join() because I find it more intuitive to just use left_join() since in my head, I’m taking a dataset and stacking columns onto the end of it. If you want to right join instead, neat—just remember to order things correctly.\n ","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590537600,"objectID":"05e82a05a3ed4f364a54f9dff01f853b","permalink":"/lesson/12-lesson/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/lesson/12-lesson/","section":"lesson","summary":"Combining datasets vertically Combining datasets horizontally inner_join() left_join() Common column names right_join()   There is a short lesson today! You’ll learn the basics of joining two different datasets together, both vertically and horizontally.\nThere are a few imaginary datasets I’ve created for you to play with:\nx ## # A tibble: 3 x 2 ## id some_variable ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 x1 ## 2 2 x2 ## 3 3 x3 y ## # A tibble: 3 x 2 ## id some_other_variable ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 y1 ## 2 2 y2 ## 3 4 y4 national_data ## # A tibble: 9 x 5 ## state year unemployment inflation population ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 GA 2018 5 2 100 ## 2 GA 2019 5.","tags":null,"title":"Space","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Word frequencies 100% optional bonus fun tasks Turning everything in   Getting started For this exercise, you’ll download some books from Project Gutenberg and visualize patterns in the words.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some helpful starter code. Download that here and include it in your project:\n  13-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 13-exercise.Rmd your-project-name.Rproj To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  13-exercise.zip  The example from today’s session will be incredibly helpful for this exercise.\nThis can be as simple or as complex as you want. You don’t need to make your plots super fancy, but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Word frequencies Download 4+ books by some author on Project Gutenberg. Jane Austen, Victor Hugo, Emily Brontë, Lucy Maud Montgomery, Arthur Conan Doyle, Mark Twain, Henry David Thoreau, Fyodor Dostoyevsky, Leo Tolstoy. Anyone. Just make sure it’s all from the same author.\nMake these two plots and describe what each tell about your author’s books:\nTop 10 most frequent words in each book Top 10 most unique words in each book (i.e. tf-idf)   100% optional bonus fun tasks If you want, do some other things with the text you’ve downloaded. Make a “he verbs vs. she verbs” plot. Tag the parts of speech and find the most common verbs or nouns. Try some sentiment analysis.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"3a3d7547c91fd1692926df876b83befd","permalink":"/assignment/13-exercise/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/assignment/13-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Word frequencies 100% optional bonus fun tasks Turning everything in   Getting started For this exercise, you’ll download some books from Project Gutenberg and visualize patterns in the words.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.","tags":null,"title":"Text","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Get data Clean data Tokens and word counts  Single words Bigrams  Bigrams and probability Term frequency-inverse document frequency (tf-idf) Sentiment analysis Neat extra stuff  Part of speech tagging Topic modeling and fingerprinting Text features     For this example, we’re going to use the text of Little Women by Louisa May Alcott and four Shakespearean tragedies (Romeo and Juliet, King Lear, Macbeth, and Hamlet) to explore how to do some basic text visualization.\nYou can follow along if you want, but don’t feel like you have too. This is mostly just to give you a taste of different methods for visualizing text. It’s by no means comprehensive, but it is well annotated and commented and should (hopefully) be easy to follow.\nIf you want to play with part-of-speech tagging, you can download an already-tagged version of Little Women here (you’ll likely need to right click and choose “Save Link As…”):\n  little_women_tagged.csv  If you want to see other examples of text visualizations with the tidytext package, check out some of these:\n  Harry Potter Sentiment Analysis for Beginners (this uses the harrypotter package, which you can install from GitHub (not from CRAN))  Peer Christensen “Fair is foul, and foul is fair: a tidytext sentiment analysis of Shakespeare’s tragedies”  “Tidy text, parts of speech, and unique words in the Bible”  “Tidy text, parts of speech, and unique words in the Qur’an”  Live coding example    Complete code (This is a highly cleaned up version of the code from the video.)\nGet data First, as always, we’ll load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, etc. library(tidytext) # For neat text things library(gutenbergr) # For downloading books from Project Gutenberg We’re going to use the gutenbergr package to download some books directly from Project Gutenberg. The IDs for these books come from the URLs at their website. For instance, Little Women is book #514. We’ll store these books as `*_raw* and then clean them up later.\n# 514 Little Women little_women_raw \u0026lt;- gutenberg_download(514, meta_fields = \u0026quot;title\u0026quot;) # 1524 - Hamlet # 1532 - King Lear # 1533 - Macbeth # 1513 - Romeo and Juliet tragedies_raw \u0026lt;- gutenberg_download(c(1524, 1532, 1533, 1513), meta_fields = \u0026quot;title\u0026quot;) If you won’t want to redownload the books every time you knit (you don’t), you can do the same trick we’ve used for WDI and FRED data. Put the actual code for getting the books in a chunk with eval=FALSE on it and run it manually in RStudio when you want to get the data. Then you can write the downloaded data as a CSV file, and then load it invisibly from the CSV file when you knit:\nI first download data from Project Gutenberg: ```{r get-book, eval=FALSE} books_raw \u0026lt;- gutenberg_download(...) write_csv(books_raw, \u0026quot;data/books_raw.csv\u0026quot;) ``` ```{r load-book-data-real, include=FALSE} books_raw \u0026lt;- read_csv(\u0026quot;data/books_raw.csv\u0026quot;) ```  Clean data The data you get from Project Gutenberg comes in a tidy format, with a column for the book id, a column for the title, and a column for text. Sometimes this text column will be divided by lines in the book; sometimes it might be an entire page or paragraph or chapter. It all depends on how the book is formatted at Project Gutenberg.\nHere’s what the start of our little_women_raw data looks like:\nhead(little_women_raw) ## # A tibble: 6 x 3 ## gutenberg_id text title ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 514 LITTLE WOMEN Little Women ## 2 514 \u0026lt;NA\u0026gt; Little Women ## 3 514 \u0026lt;NA\u0026gt; Little Women ## 4 514 by Little Women ## 5 514 \u0026lt;NA\u0026gt; Little Women ## 6 514 Louisa May Alcott Little Women If we look at the data in RStudio, we can see that the actual book doesn’t start until row 70 (the first 69 rows are the table of contents and other parts of the front matter).\nIt would be nice if we had a column that indicated what chapter each line is in, since we could then group by chapter and look at patterns within chapters. Since the data doesn’t come with a chapter column, we have to make one ourselves using a fun little trick. Each chapter in the book starts with “CHAPTER ONE” or “CHAPTER TWO”, with “chapter” in ALL CAPS. We can make a variable named chapter_start that will be true if a line starts with “CHAPTER” and false if not. Then we can use the cumsum() function to take the cumulative sum of this column, which will increment up one number ever time there’s a new chapter, thus creating a helpful chapter column.\n# Clean up Little Women little_women \u0026lt;- little_women_raw %\u0026gt;% # The actual book doesn\u0026#39;t start until line 70 slice(70:n()) %\u0026gt;% # Get rid of rows where text is missing drop_na(text) %\u0026gt;% # Chapters start with CHAPTER X, so mark if each row is a chapter start # cumsum() calculates the cumulative sum, so it\u0026#39;ll increase every time there\u0026#39;s # a new chapter and automatically make chapter numbers mutate(chapter_start = str_detect(text, \u0026quot;^CHAPTER\u0026quot;), chapter_number = cumsum(chapter_start)) %\u0026gt;% # Get rid of these columns select(-gutenberg_id, -title, -chapter_start) head(little_women) ## # A tibble: 6 x 2 ## text chapter_number ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 \u0026quot;CHAPTER ONE\u0026quot; 1 ## 2 \u0026quot;PLAYING PILGRIMS\u0026quot; 1 ## 3 \u0026quot;\\\u0026quot;Christmas won\u0026#39;t be Christmas without any presents,\\\u0026quot; grumbled Jo, lying\u0026quot; 1 ## 4 \u0026quot;on the rug.\u0026quot; 1 ## 5 \u0026quot;\\\u0026quot;It\u0026#39;s so dreadful to be poor!\\\u0026quot; sighed Meg, looking down at her old\u0026quot; 1 ## 6 \u0026quot;dress.\u0026quot; 1 The data from Shakespeare is similarly messy, with just three columns:\nhead(tragedies_raw) ## # A tibble: 6 x 3 ## gutenberg_id text title ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1513 ROMEO AND JULIET Romeo and Juliet ## 2 1513 \u0026lt;NA\u0026gt; Romeo and Juliet ## 3 1513 by William Shakespeare Romeo and Juliet ## 4 1513 \u0026lt;NA\u0026gt; Romeo and Juliet ## 5 1513 \u0026lt;NA\u0026gt; Romeo and Juliet ## 6 1513 \u0026lt;NA\u0026gt; Romeo and Juliet The initial text sometimes isn’t the actual text of the book. If you look at the beginning of Hamlet, for instance, there’s a bunch of introductory stuff from editors and transcribers. In real life, we’d want to find a systematic way to get rid of that (perhaps by looking at how many introductory rows there are in each of the four plays and removing those rows), but for now, we’ll just live with it and pretend Shakespeare wrote these notes. 🤷\nWe could also figure out a systematic way to indicate acts and scenes, but that’s tricky, so we won’t for this example. (This guy did though!)\nNow that we have tidy text data, let’s do stuff with it!\n Tokens and word counts Single words One way we can visualize text is to look at word frequencies and find the most common words. This is even more important when looking across documents.\nRight now the text we have is tidy, but it is based on lines of text, not words. In order to count words correctly, we need each token (or text element, whether it be a word or bigram or paragraph or whatever) to be in its own row. The unnest_tokens() functions from tidytext does this for us. The first argument is the name of the column we want to create; the second argument is the name of the column we want to split into tokens.\nLet’s just work with the Shakespeare tragedies:\ntragedies_words \u0026lt;- tragedies_raw %\u0026gt;% drop_na(text) %\u0026gt;% unnest_tokens(word, text) head(tragedies_words) ## # A tibble: 6 x 3 ## gutenberg_id title word ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1513 Romeo and Juliet romeo ## 2 1513 Romeo and Juliet and ## 3 1513 Romeo and Juliet juliet ## 4 1513 Romeo and Juliet by ## 5 1513 Romeo and Juliet william ## 6 1513 Romeo and Juliet shakespeare Now that we have words, we can filter and count the words. Here’s what’s happening in this next chunk:\n We use anti_join() to remove all common stop words like “a” and “the” that are listed in the stop_words dataset that is loaded when you load tidytext We count how many times each word appears in each title/play We only keep the top 15 words  top_words_tragedies \u0026lt;- tragedies_words %\u0026gt;% # Remove stop words anti_join(stop_words) %\u0026gt;% # Get rid of old timey words and stage directions filter(!(word %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;haue\u0026quot;, \u0026quot;thee\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;))) %\u0026gt;% # Count all the words in each play count(title, word, sort = TRUE) %\u0026gt;% # Keep top 15 in each play group_by(title) %\u0026gt;% top_n(15) %\u0026gt;% ungroup() %\u0026gt;% # Make the words an ordered factor so they plot in order mutate(word = fct_inorder(word)) top_words_tragedies ## # A tibble: 63 x 3 ## title word n ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Hamlet, Prince of Denmark ham 358 ## 2 Romeo and Juliet romeo 296 ## 3 Macbeth macbeth 282 ## 4 The Tragedy of King Lear lear 230 ## 5 Hamlet, Prince of Denmark lord 223 ## 6 Hamlet, Prince of Denmark king 197 ## 7 Romeo and Juliet juliet 178 ## 8 The Tragedy of King Lear kent 174 ## 9 Romeo and Juliet nurse 149 ## 10 Romeo and Juliet capulet 145 ## # … with 53 more rows Now we can plot these results, facetting and filling by title:\nggplot(top_words_tragedies, aes(y = fct_rev(word), x = n, fill = title)) + geom_col() + guides(fill = FALSE) + labs(y = \u0026quot;Count\u0026quot;, x = NULL, title = \u0026quot;15 most frequent words in four Shakespearean tragedies\u0026quot;) + facet_wrap(vars(title), scales = \u0026quot;free_y\u0026quot;) + theme_bw() These results aren’t terribly surprising. “lear” is the most common word in King Lear, “macbeth” is the most common word in Macbeth, and so on. But the results are still really neat! This is a wordcloud for grownups!\n(Sharp-eyed readers will notice that the words aren’t actually in perfect order! That’s because some common words are repeated across the plays, like “lord” and “sir”. However, each category in a factor can only have one possible position in the orer, so because “lord” is the second most common word in Hamlet it also appears as #2 in Macbeth and King Lear. You can fix this with the reorder_within() function in tidytext—see Julia Silge’s tutorial here for how to use it.)\n Bigrams We can also look at pairs of words instead of single words. To do this, we need to change a couple arguments in unnest_tokens(), but otherwise everything else stays the same. In order to remove stopwords, we need to split the bigram column into two columns (word1 and word2) with separate(), filter each of those columns, and then combine the word columns back together as bigram with unite()\ntragedies_bigrams \u0026lt;- tragedies_raw %\u0026gt;% drop_na(text) %\u0026gt;% # n = 2 here means bigrams. We could also make trigrams (n = 3) or any type of n-gram unnest_tokens(bigram, text, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;% # Split the bigrams into two words so we can remove stopwords separate(bigram, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %\u0026gt;% filter(!word1 %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;), !word2 %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;)) %\u0026gt;% # Put the two word columns back together unite(bigram, word1, word2, sep = \u0026quot; \u0026quot;) tragedies_bigrams ## # A tibble: 14,237 x 3 ## gutenberg_id title bigram ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1513 Romeo and Juliet william shakespeare ## 2 1513 Romeo and Juliet shakespeare persons ## 3 1513 Romeo and Juliet persons represented ## 4 1513 Romeo and Juliet represented escalus ## 5 1513 Romeo and Juliet escalus prince ## 6 1513 Romeo and Juliet verona paris ## 7 1513 Romeo and Juliet nobleman kinsman ## 8 1513 Romeo and Juliet prince montague ## 9 1513 Romeo and Juliet montague heads ## 10 1513 Romeo and Juliet capulet romeo ## # … with 14,227 more rows top_bigrams \u0026lt;- tragedies_bigrams %\u0026gt;% # Count all the bigrams in each play count(title, bigram, sort = TRUE) %\u0026gt;% # Keep top 15 in each play group_by(title) %\u0026gt;% top_n(15) %\u0026gt;% ungroup() %\u0026gt;% # Make the bigrams an ordered factor so they plot in order mutate(bigram = fct_inorder(bigram)) ## Selecting by n ggplot(top_bigrams, aes(y = fct_rev(bigram), x = n, fill = title)) + geom_col() + guides(fill = FALSE) + labs(y = \u0026quot;Count\u0026quot;, x = NULL, title = \u0026quot;15 most frequent bigrams in four Shakespearean tragedies\u0026quot;) + facet_wrap(vars(title), scales = \u0026quot;free\u0026quot;) + theme_bw() There are some neat trends here. “Lord Hamlet” is the most common pair of words in Hamlet (not surprisingly), but in Macbeth the repeated “knock knock” (the first non-name repeated pair) is a well-known plot point and reoccurring symbolic theme throughout the play.\n  Bigrams and probability We can replicate the “She Giggles, He Gallops” idea by counting the bigrams that match “he X” and “she X”.\nThe log ratio idea shows how much more likely a word is compared to its counterpart (so “he that” is about 5 more likely to appear than “she that”. In this graph, I replaced the x-axis labels with “2x” and “4x”, but without those, you get numbers like 1, 2, and 3 (or -1, -2, -3)). To convert those logged ratio numbers into the multiplicative version (i.e. 2x instead of 1), raise 2 to the power of the log ratio. If the log ratio is 3, the human-readable version is \\(2^3\\), or 8 times.\n# Take the log of 8: log2(8) ## [1] 3 # Reverse log of 3: 2^3 ## [1] 8 The only text wizardry here is tokenizing the words. Pretty much the rest of all this code is just dplyr mutating, filtering, and counting:\npronouns \u0026lt;- c(\u0026quot;he\u0026quot;, \u0026quot;she\u0026quot;) bigram_he_she_counts \u0026lt;- tragedies_raw %\u0026gt;% drop_na(text) %\u0026gt;% # Split into bigrams unnest_tokens(bigram, text, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;% # Find counts of bigrams count(bigram, sort = TRUE) %\u0026gt;% # Split the bigram column into two columns separate(bigram, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% # Only choose rows where the first word is he or she filter(word1 %in% pronouns) %\u0026gt;% count(word1, word2, wt = n, sort = TRUE) %\u0026gt;% rename(total = n) word_ratios \u0026lt;- bigram_he_she_counts %\u0026gt;% # Look at each of the second words group_by(word2) %\u0026gt;% # Only choose rows where the second word appears more than 10 times filter(sum(total) \u0026gt; 10) %\u0026gt;% ungroup() %\u0026gt;% # Spread out the word1 column so that there\u0026#39;s a column named \u0026quot;he\u0026quot; and one named \u0026quot;she\u0026quot; spread(word1, total, fill = 0) %\u0026gt;% # Add 1 to each number so that logs work (just in case any are zero) mutate_if(is.numeric, ~(. + 1) / sum(. + 1)) %\u0026gt;% # Create a new column that is the logged ratio of the she counts to he counts mutate(logratio = log2(she / he)) %\u0026gt;% # Sort by that ratio arrange(desc(logratio)) # Rearrange this data so it\u0026#39;s plottable plot_word_ratios \u0026lt;- word_ratios %\u0026gt;% # This gets the words in the right order---we take the absolute value, select # only rows where the log ratio is bigger than 0, and then take the top 15 words mutate(abslogratio = abs(logratio)) %\u0026gt;% group_by(logratio \u0026lt; 0) %\u0026gt;% top_n(15, abslogratio) %\u0026gt;% ungroup() %\u0026gt;% mutate(word = reorder(word2, logratio)) # Finally we plot this ggplot(plot_word_ratios, aes(y = word, x = logratio, color = logratio \u0026lt; 0)) + geom_segment(aes(y = word, yend = word, x = 0, xend = logratio), size = 1.1, alpha = 0.6) + geom_point(size = 3.5) + labs(x = \u0026quot;How much more/less likely\u0026quot;, y = NULL) + scale_color_discrete(name = \u0026quot;\u0026quot;, labels = c(\u0026quot;More \u0026#39;she\u0026#39;\u0026quot;, \u0026quot;More \u0026#39;he\u0026#39;\u0026quot;)) + scale_x_continuous(breaks = seq(-3, 3), labels = c(\u0026quot;8x\u0026quot;, \u0026quot;4x\u0026quot;, \u0026quot;2x\u0026quot;, \u0026quot;Same\u0026quot;, \u0026quot;2x\u0026quot;, \u0026quot;4x\u0026quot;, \u0026quot;8x\u0026quot;)) + theme_bw() + theme(legend.position = \u0026quot;bottom\u0026quot;) Shakespeare doesn’t use a lot of fancy verbs in his plays, so we’re left with incredibly common verbs like “should” and “comes” and “was”. Oh well.\n Term frequency-inverse document frequency (tf-idf) We can determine which words are the most unique for each book/document in our corpus using by calculating the tf-idf (term frequency-inverse document frequency) score for each term. The tf-idf is the product of the term frequency and the inverse document frequency:\n\\[ \\begin{aligned} tf(\\text{term}) \u0026amp;= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\ idf(\\text{term}) \u0026amp;= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\ tf\\text{-}idf(\\text{term}) \u0026amp;= tf(\\text{term}) \\times idf(\\text{term}) \\end{aligned} \\]\nFortunately you don’t need to remember that formula. The bind_tf_idf() function will calculate this for you. Remember, the higher the tf-idf number, the more unique the term is in the document, but these numbers are meaningless and unitless—you can’t convert them to a percentage or anything.\nHere are the most unique words in these four tragedies, compared to all the tragedies:\ntragedy_words \u0026lt;- tragedies_raw %\u0026gt;% drop_na() %\u0026gt;% # Split into word tokens unnest_tokens(word, text) %\u0026gt;% # Remove stop words and old timey words anti_join(stop_words) %\u0026gt;% filter(!word %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;haue\u0026quot;, \u0026quot;thee\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;)) %\u0026gt;% count(title, word, sort = TRUE) # Add the tf-idf values to the counts tragedy_tf_idf \u0026lt;- tragedy_words %\u0026gt;% bind_tf_idf(word, title, n) # Get the top 10 uniquest words tragedy_tf_idf_plot \u0026lt;- tragedy_tf_idf %\u0026gt;% arrange(desc(tf_idf)) %\u0026gt;% group_by(title) %\u0026gt;% top_n(10) %\u0026gt;% ungroup() %\u0026gt;% mutate(word = fct_inorder(word)) ggplot(tragedy_tf_idf_plot, aes(y = fct_rev(word), x = tf_idf, fill = title)) + geom_col() + guides(fill = FALSE) + labs(x = \u0026quot;tf-idf\u0026quot;, y = NULL) + facet_wrap(~ title, scales = \u0026quot;free\u0026quot;) + theme_bw() Not surprisingly, the most unique words for each play happen to be the names of the characters in those plays.\n Sentiment analysis In the video, I plotted the sentiment of Little Women across the book, but it wasn’t a very interesting plot. We’ll try with Shakespeare here instead.\nAt its core, sentiment analysis involves looking at a big list of words for how negative or positive they are. Some sentiment dictionaries mark if a word is “negative” or “positive”; some give words a score from -3 to 3; some give different emotions like “sadness” or “anger”. You can see what the different dictionaries look like with get_sentiments()\nget_sentiments(\u0026quot;afinn\u0026quot;) # Scoring system ## # A tibble: 2,477 x 2 ## word value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 ## 7 abhor -3 ## 8 abhorred -3 ## 9 abhorrent -3 ## 10 abhors -3 ## # … with 2,467 more rows # get_sentiments(\u0026quot;bing\u0026quot;) # Negative/positive # get_sentiments(\u0026quot;nrc\u0026quot;) # Specific emotions # get_sentiments(\u0026quot;loughran\u0026quot;) # Designed for financial statements; positive/negative Here we split the Shakespearean tragedies into words, join a sentiment dictionary to it, and use dplyr data wrangling to calculate the net number positive words in each chapter. Had we used the AFINN library, we could calculate the average sentiment per chapter, since AFINN uses a scoring system instead of negative/positive labels. Or we could’ve used the NRC library, which has specific emotions like trust and fear.\ntragedy_words \u0026lt;- tragedies_raw %\u0026gt;% drop_na() %\u0026gt;% # Split into word tokens unnest_tokens(word, text) %\u0026gt;% # Remove stop words and old timey words anti_join(stop_words) %\u0026gt;% filter(!word %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;haue\u0026quot;, \u0026quot;thee\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;)) # Join the sentiment dictionary tragedy_sentiment \u0026lt;- tragedy_words %\u0026gt;% inner_join(get_sentiments(\u0026quot;bing\u0026quot;)) tragedy_sentiment ## # A tibble: 7,736 x 4 ## gutenberg_id title word sentiment ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1513 Romeo and Juliet dignity positive ## 2 1513 Romeo and Juliet fair positive ## 3 1513 Romeo and Juliet grudge negative ## 4 1513 Romeo and Juliet break negative ## 5 1513 Romeo and Juliet unclean negative ## 6 1513 Romeo and Juliet fatal negative ## 7 1513 Romeo and Juliet overthrows negative ## 8 1513 Romeo and Juliet death negative ## 9 1513 Romeo and Juliet strife negative ## 10 1513 Romeo and Juliet fearful negative ## # … with 7,726 more rows We can look at these sentiments a few different ways. First we can get a count of total positive and negative words in the four books. We can see that in all four, there are more negative words than positive ones (they’re tragdies, after all):\ntragedy_sentiment_plot \u0026lt;- tragedy_sentiment %\u0026gt;% count(title, sentiment) ggplot(tragedy_sentiment_plot, aes(x = sentiment, y = n, fill = title, alpha = sentiment)) + geom_col(position = position_dodge()) + scale_alpha_manual(values = c(0.5, 1)) + facet_wrap(vars(title)) + theme_bw() Perhaps more usefully, we can divide each of the plays into groups of 100 lines, and then get the net sentiment of each group (number of positive words − number of negative words). By splitting the data into groups of lines, we can show a more granular view of the progression of the plot. To do this we make a column that indicates the row number, and then we use the special %/% operator to perform integer division, which essentially lops off the decimal point when dividing numbers: 150/100 normally is 1.5, but in integer divison, it is 1. This is a helpful trick for putting rows 1-99 in one group, then rows 100-199 in another group, etc.\ntragedies_split_into_lines \u0026lt;- tragedy_sentiment %\u0026gt;% # Divide lines into groups of 100 mutate(line = row_number(), line_chunk = line %/% 100) %\u0026gt;% # Get a count of postiive and negative words in each 100-line chunk in each play count(title, line_chunk, sentiment) %\u0026gt;% # Convert the sentiment column into two columns named \u0026quot;positive\u0026quot; and \u0026quot;negative\u0026quot; pivot_wider(names_from = sentiment, values_from = n) %\u0026gt;% # Calculate net sentiment mutate(sentiment = positive - negative) ggplot(tragedies_split_into_lines, aes(x = line_chunk, y = sentiment, fill = sentiment)) + geom_col() + scale_fill_viridis_c(option = \u0026quot;magma\u0026quot;, end = 0.9) + facet_wrap(vars(title), scales = \u0026quot;free_x\u0026quot;) + theme_bw() Neat. They’re all really sad and negative, except for the beginning of Romeo and Juliet where the two lovers meet and fall in love. Then everyone dies later.\n Neat extra stuff None of this stuff was in the video, but it’s useful to know and see how to do it. It all generally comes from the Tidy Text Mining book by Julia Silge and David Robinson\nPart of speech tagging R has no way of knowing if words are nouns, verbs, or adjectives. You can algorithmically predict what part of speech each word is using a part-of-speech tagger, like spaCy or Stanford’s Natural Langauge Processing (NLP) library.\nThese are external programs that are not written in R and don’t naturally communicate with R (spaCy is written in Python; Stanford’s CoreNLP is written in Java). There is a helpful R package named cleanNLP that helps you interact with these programs from within R, whis is super helpful. cleanNLP also comes with its own R-only tagger so you don’t need to install anything with Python or Java (however, it’s not as powerful as either spaCy, which is faster, and doesn’t deal with foreign languages like Arabic and Chinese like Stanford’s NLP library).\nYou can see other examples of part-of-speech tagging (along with instructions for how to install spaCy and coreNLP) here:\n  “Tidy text, parts of speech, and unique words in the Bible”  “Tidy text, parts of speech, and unique words in the Qur’an”  Here’s the general process for tagging (or “annotating”) text with the cleanNLP package:\nMake a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\n Initialize the NLP tagger. You can use any of these:\n cnlp_init_udpipe(): Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!) cnlp_init_spacy(): Use spaCy (if you’ve installed it on your computer with Python) cnlp_init_corenlp(): Use Stanford’s NLP library (if you’ve installed it on your computer with Java)  Feed the data frame from step 1 into the cnlp_annotate() function and wait.\n Save the tagged data on your computer so you don’t have to re-tag it every time.\n  Here’s an example using the Little Women data:\n# For the tagger to work, each row needs to be unique, which means we need to # combine all the text into individual chapter-based rows. This takes a little # bit of text-wrangling with dplyr: little_women_to_tag \u0026lt;- little_women %\u0026gt;% # Group by chapter number group_by(chapter_number) %\u0026gt;% # Take all the rows in each chapter and collapse them into a single cell nest(data = c(text)) %\u0026gt;% ungroup() %\u0026gt;% # Look at each individual cell full of text lines and paste them together into # one really long string of text per chapter mutate(text = map_chr(data, ~paste(.$text, collapse = \u0026quot; \u0026quot;))) %\u0026gt;% # Get rid of this column select(-data) little_women_to_tag ## # A tibble: 47 x 2 ## chapter_number text ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 \u0026quot;CHAPTER ONE PLAYING PILGRIMS \\\u0026quot;Christmas won\u0026#39;t be Christmas without any presents,\\\u0026quot; grumbled Jo, lying on the rug. \\\u0026quot;It\u0026#39;s so dread… ## 2 2 \u0026quot;CHAPTER TWO A MERRY CHRISTMAS Jo was the first to wake in the gray dawn of Christmas morning. No stockings hung at the fireplace, … ## 3 3 \u0026quot;CHAPTER THREE THE LAURENCE BOY \\\u0026quot;Jo! Jo! Where are you?\\\u0026quot; cried Meg at the foot of the garret stairs. \\\u0026quot;Here!\\\u0026quot; answered a husky… ## 4 4 \u0026quot;CHAPTER FOUR BURDENS \\\u0026quot;Oh, dear, how hard it does seem to take up our packs and go on,\\\u0026quot; sighed Meg the morning after the party, f… ## 5 5 \u0026quot;CHAPTER FIVE BEING NEIGHBORLY \\\u0026quot;What in the world are you going to do now, Jo?\\\u0026quot; asked Meg one snowy afternoon, as her sister came… ## 6 6 \u0026quot;CHAPTER SIX BETH FINDS THE PALACE BEAUTIFUL The big house did prove a Palace Beautiful, though it took some time for all to get in… ## 7 7 \u0026quot;CHAPTER SEVEN AMY\u0026#39;S VALLEY OF HUMILIATION \\\u0026quot;That boy is a perfect cyclops, isn\u0026#39;t he?\\\u0026quot; said Amy one day, as Laurie clattered by on… ## 8 8 \u0026quot;CHAPTER EIGHT JO MEETS APOLLYON \\\u0026quot;Girls, where are you going?\\\u0026quot; asked Amy, coming into their room one Saturday afternoon, and find… ## 9 9 \u0026quot;CHAPTER NINE MEG GOES TO VANITY FAIR \\\u0026quot;I do think it was the most fortunate thing in the world that those children should have the… ## 10 10 \u0026quot;CHAPTER TEN THE P.C. AND P.O. As spring came on, a new set of amusements became the fashion, and the lengthening days gave long af… ## # … with 37 more rows Notice how there’s now a row for each chapter, and the whole chapter is contained in the text column. With the data in this format, we can annotate it. It takes about 3 minutes to run this on my 2016 MacBook Pro with the R-only udpipe tagger (and only 30 seconds if I use the spaCy tagger). Notice how I immediately save the tagged tokens as a CSV file after so I don’t have to do it again.\nlibrary(cleanNLP) # Use the built-in R-based tagger cnlp_init_udpipe() little_women_tagged \u0026lt;- cnlp_annotate(little_women_to_tag, text_name = \u0026quot;text\u0026quot;, doc_name = \u0026quot;chapter_number\u0026quot;) write_csv(little_women_tagged$token, \u0026quot;little_women_tagged.csv\u0026quot;) Here’s what the tagged text looks like:\nlittle_women_tagged ## # A tibble: 232,093 x 10 ## doc_id sid tid token token_with_ws lemma upos xpos tid_source relation ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 1 1 \u0026quot;CHAPTER\u0026quot; \u0026quot;CHAPTER\u0026quot; \u0026quot;chapter\u0026quot; NOUN NN 4 nmod ## 2 1 1 2 \u0026quot;ONE\u0026quot; \u0026quot;ONE\u0026quot; \u0026quot;one\u0026quot; NUM CD 1 nummod ## 3 1 1 3 \u0026quot;PLAYING\u0026quot; \u0026quot;PLAYING\u0026quot; \u0026quot;playing\u0026quot; NOUN NN 4 compound ## 4 1 1 4 \u0026quot;PILGRIMS\u0026quot; \u0026quot;PILGRIMS\u0026quot; \u0026quot;pilgrims\u0026quot; NOUN NN 0 root ## 5 1 1 5 \u0026quot;\\\u0026quot;\u0026quot; \u0026quot;\\\u0026quot;\u0026quot; \u0026quot;\\\u0026quot;\u0026quot; PUNCT `` 4 punct ## 6 1 2 1 \u0026quot;Christmas\u0026quot; \u0026quot;Christmas\u0026quot; \u0026quot;Christmas\u0026quot; PROPN NNP 4 nsubj ## 7 1 2 2 \u0026quot;wo\u0026quot; \u0026quot;wo\u0026quot; \u0026quot;will\u0026quot; VERB MD 4 aux ## 8 1 2 3 \u0026quot;n\u0026#39;t\u0026quot; \u0026quot;n\u0026#39;t\u0026quot; \u0026quot;not\u0026quot; PART RB 4 neg ## 9 1 2 4 \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; AUX VB 0 root ## 10 1 2 5 \u0026quot;Christmas\u0026quot; \u0026quot;Christmas\u0026quot; \u0026quot;Christmas\u0026quot; PROPN NNP 4 attr ## # … with 232,083 more rows There are a bunch of new columns like lemma (or the base stemmed word), and upos and pos for the different parts of speech. These use the Penn Treebank codes.\nNow that everything is tagged, we can do any grouping and summarizing and filtering we want. We could find the most common verbs, or the most common nouns or proper names, for instance. Here’s a fun plot that shows the proportion of mentions of the four main characters (Meg, Jo, Beth, and Amy) in each chapter.\n# Find all proper nouns proper_nouns \u0026lt;- little_women_tagged %\u0026gt;% filter(upos == \u0026quot;PROPN\u0026quot;) main_characters_by_chapter \u0026lt;- proper_nouns %\u0026gt;% # Find only Meg, Jo, Beth, and Amy filter(lemma %in% c(\u0026quot;Meg\u0026quot;, \u0026quot;Jo\u0026quot;, \u0026quot;Beth\u0026quot;, \u0026quot;Amy\u0026quot;)) %\u0026gt;% # Group by chapter and character name group_by(doc_id, lemma) %\u0026gt;% # Get the count of mentions summarize(n = n()) %\u0026gt;% # Make a new column named \u0026quot;name\u0026quot; that is an ordered factor of the girls\u0026#39; names mutate(name = factor(lemma, levels = c(\u0026quot;Meg\u0026quot;, \u0026quot;Jo\u0026quot;, \u0026quot;Beth\u0026quot;, \u0026quot;Amy\u0026quot;), ordered = TRUE)) %\u0026gt;% # Rename this so it\u0026#39;s called chapter rename(chapter = doc_id) %\u0026gt;% # Group by chapter group_by(chapter) %\u0026gt;% # Calculate the proportion of each girl\u0026#39;s mentions in each chapter mutate(prop = n / sum(n)) %\u0026gt;% ungroup() %\u0026gt;% # Make a cleaner chapter name column mutate(chapter_name = paste(\u0026quot;Chapter\u0026quot;, chapter)) %\u0026gt;% mutate(chapter_name = fct_inorder(chapter_name)) main_characters_by_chapter ## # A tibble: 177 x 6 ## chapter lemma n name prop chapter_name ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 Amy 23 Amy 0.195 Chapter 1 ## 2 1 Beth 26 Beth 0.220 Chapter 1 ## 3 1 Jo 43 Jo 0.364 Chapter 1 ## 4 1 Meg 26 Meg 0.220 Chapter 1 ## 5 2 Amy 13 Amy 0.197 Chapter 2 ## 6 2 Beth 12 Beth 0.182 Chapter 2 ## 7 2 Jo 21 Jo 0.318 Chapter 2 ## 8 2 Meg 20 Meg 0.303 Chapter 2 ## 9 3 Amy 2 Amy 0.0202 Chapter 3 ## 10 3 Beth 2 Beth 0.0202 Chapter 3 ## # … with 167 more rows And here’s the polished plot:\nggplot(main_characters_by_chapter, aes(x = prop, y = 1, fill = fct_rev(name))) + geom_col(position = position_stack()) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + scale_fill_viridis_d(option = \u0026quot;plasma\u0026quot;, end = 0.9, name = NULL) + guides(fill = guide_legend(reverse = TRUE)) + labs(x = NULL, y = NULL, title = \u0026quot;Proportion of mentions of each\\nLittle Woman per chapter\u0026quot;, subtitle = \u0026quot;Jo basically dominates the last third of the book\u0026quot;) + facet_wrap(vars(chapter_name), nrow = 6) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;, axis.text = element_blank(), axis.ticks = element_blank(), strip.background = element_rect(fill = \u0026quot;white\u0026quot;), legend.text = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1)), plot.title = element_text(face = \u0026quot;bold\u0026quot;, hjust = 0.5, size = rel(1.7)), plot.subtitle = element_text(hjust = 0.5, size = rel(1.1)))  Topic modeling and fingerprinting If you want to see some examples of topic modeling with Latent Dirichlet Allocation (LDA) or text fingerprinting based on sentence length and counts of hapax legomena (based on this article), see these examples from a previous version of this class: topic modeling and fingerprinting\n Text features Finally, you can use the textfeatures package to find all sorts of interesting numeric statistics about text, like the number of exclamation points, commas, digits, characters per word, uppercase letters, lowercase letters, and more!\n   ","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"9b49beb720db92e02ef300134b02a9dc","permalink":"/example/13-example/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/example/13-example/","section":"example","summary":"Live coding example Complete code  Get data Clean data Tokens and word counts  Single words Bigrams  Bigrams and probability Term frequency-inverse document frequency (tf-idf) Sentiment analysis Neat extra stuff  Part of speech tagging Topic modeling and fingerprinting Text features     For this example, we’re going to use the text of Little Women by Louisa May Alcott and four Shakespearean tragedies (Romeo and Juliet, King Lear, Macbeth, and Hamlet) to explore how to do some basic text visualization.","tags":null,"title":"Text","type":"docs"},{"authors":null,"categories":null,"content":"  There’s no lesson for this session. In your exercise today you’ll visualize text data using tidytext, and the best way to figure that out is to just play with data.\nSo head over to the example to see how it’s done, or the exercise to get started!\n","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"2ea81152b1f77e665a1de070264de2ad","permalink":"/lesson/13-lesson/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/lesson/13-lesson/","section":"lesson","summary":"There’s no lesson for this session. In your exercise today you’ll visualize text data using tidytext, and the best way to figure that out is to just play with data.\nSo head over to the example to see how it’s done, or the exercise to get started!","tags":null,"title":"Text","type":"docs"},{"authors":null,"categories":null,"content":"   Getting started Task 1: Reflection Task 2: Hot dog eating contest winners Turning everything in   Getting started For this exercise, you’ll export a PDF and/or an SVG from R, open it in Adobe Illustrator (free for GSU students) or Gravit Designer (free for the basic version), add annotations and make minor edits, and then export a final polished version.\nI have given you 100% of the R code you need to use. All you have to do is run it. You need to download one CSV file:\n  hot-dog-contest-winners.csv  You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with all the code you’ll need. Download that here and include it in your project:\n  14-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 14-exercise.Rmd your-project-name.Rproj data\\ hot-dog-contest-winners.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  14-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Hot dog eating contest winners Recreate this plot (or something like it):\nCreate and save a basic bar chart of hot dog eating contest winners using the code provided. Open the resulting file in Illustrator or Gravit Designer. Open the PDF in Illustrator; open the SVG in Gravit Designer.\nBe sure that you save your file in Illustrator or Gravit Designer with a different name. You don’t want to accidentally overwrite all your enhancements and updates when you knit this document. That would be so sad.\nYou don’t have data prior to 1980, so don’t worry about recreating that half of the graph. You don’t have to put all the text boxes in exactly the same locations—you can even do a completely different design and add different annotations if you want.\nThe point of this assignment is to help you get familiar with vector editing software, so don’t stress out about R issues or graphic design issues (though try to follow CRAP where possible).\nTo save you some typing, here’s all the text from the original plot. Copy and paste it into your enhanced version (or change the text if you want—again, do whatever you want):\n Winners from Nathan’s Hot Dog Eating Contest It’s that time of year again. Since 1916, the annual eating competition has grown substantially attracting competitors from around the world Frank Dellarosa eats 21 and a half HDBs over 12 minutes, breaking the previous record of 19 and a half Through 2001-2005, Takeru Kobayashi wins by no less than 12 HDBs. In 2006 he only wins by 1.75. After winning 6 years in a row and setting the world record 4 times, Kobayashi places second in 2007. For the first time since 1999, an American reclaims the title when Joey Chestnut consumes 66 HDBs, a new world record. Chestnut repeats in 2008. Source: Wikipedia and Nathan’s Famous   Turning everything in When you’re all done, knit your R Markdown file and use Illustrator or Gravit Designer to export a PDF or PNG version (or both) of your enhanced plot. Upload these files to iCollege.\n ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"bf2d99c2fa80d79a0f5e88bfacf7cf49","permalink":"/assignment/14-exercise/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/assignment/14-exercise/","section":"assignment","summary":"Getting started Task 1: Reflection Task 2: Hot dog eating contest winners Turning everything in   Getting started For this exercise, you’ll export a PDF and/or an SVG from R, open it in Adobe Illustrator (free for GSU students) or Gravit Designer (free for the basic version), add annotations and make minor edits, and then export a final polished version.\nI have given you 100% of the R code you need to use.","tags":null,"title":"Enhancing graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Why enhance graphics? Enhancing graphics in 2020 Abbreviated example   Why enhance graphics? The content from today isn’t really code-based at all. Instead, you’re learning about how to take a plot from R and make it fancy in a vector editing program like Illustrator, Gravit Designer, or Inkscape.\nThis concept comes from a common workflow in the real world, where organizations like news outlets, think tanks, research centers, or nonprofits will publish highly polished plots in annual reports, magazines, and other types of publications. These graphics often have to follow specific in-house style guidelines and use specific colors and fonts and other design elements. Even if you don’t work for a place with in-house style guides, you’ll often want to make some edits to your plots by hand after you create them.\nThe general workflow goes like this:\nCreate a plot in R and ggplot Export that plot as a vector image (either a PDF or an SVG) Edit and enhance the vector image in a vector editor, like Adobe Illustrator Export the polished version from Illustrator as either a PDF or PNG (or both)  Big data-focused organizations have been using a process like this for years. Nathan Yau describes this whole process in his 2011 book Visualize This and the book contains a bunch of tutorials to help you learn how create something in R, export it, and edit it in Illustrator.\nFor instance, in his first chapter, he guides you through the process of creating the skeleton of this chart in R, exporting it as a PDF, and adding all the titles and annotations and arrows and extra lines in Illustrator (original post from 2008):\n Enhancing graphics in 2020 In 2011, that was the best possible workflow because ggplot couldn’t deal with subtitles, captions, repelled labels, embedded fonts, and differently-styled text (like bold in the middle of a title). Illustrator was the only way to do this stuff.\nNowadays in 2020, though, you can do nearly all of this annotating and enhancing with packages like ggtext and patchwork and ggrepel. You can almost perfectly replicate in-house style guides with the theme() function and put text and arrows and labels and text boxes wherever you want with annotate(). It’s a brave exciting new world.\nYou still can’t do everything with R. ggplot can’t create fancy font ligatures like “ﬁ” in words that have an “f” followed by an “i”, and it can’t handle automatic hyphenation and full text justification, among other limitations. But these are the minorest of graphic design issues (and the ggplot team is working on them!).\nThat all said, it’s still often faster and easier to make edits to your graphs in Illustrator rather than fight with a reluctant annotate() layer that just won’t put an arrow exactly where you want. And ggtext is so new (it’s not on CRAN yet) that lots of people haven’t heard of it yet. This is all cutting edge stuff.\nSo it’s still a good idea to understand how to follow the standard workflow of exporting from R and enhancing in Illustrator.\n Abbreviated example In this video I use the code for the hot dog plot that I provide in today’s assignment to create a plot, export it, and make edits to it both in Illustrator and Gravit Designer. It’s not a complete example at all, but I show you the general process for adding text and lines and editing plot elements.\n   ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"e5f768bcbe278964a1cfab6c6674ea2f","permalink":"/example/14-example/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/example/14-example/","section":"example","summary":"Why enhance graphics? Enhancing graphics in 2020 Abbreviated example   Why enhance graphics? The content from today isn’t really code-based at all. Instead, you’re learning about how to take a plot from R and make it fancy in a vector editing program like Illustrator, Gravit Designer, or Inkscape.\nThis concept comes from a common workflow in the real world, where organizations like news outlets, think tanks, research centers, or nonprofits will publish highly polished plots in annual reports, magazines, and other types of publications.","tags":null,"title":"Enhancing graphics","type":"docs"},{"authors":null,"categories":null,"content":"  There’s no lesson for this session. In your exercise today you’ll export a plot from ggplot, open it in a vector editor like Illustrator, Inkscape, or Gravit Designer, and make it extra pretty and well-designed. The best way to learn this is by actually doing it.\nSo head over to the example to see how it’s done, or the exercise to get started!\n","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"ee37ad1d5277682b8171e2f97b60b0a5","permalink":"/lesson/14-lesson/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/lesson/14-lesson/","section":"lesson","summary":"There’s no lesson for this session. In your exercise today you’ll export a plot from ggplot, open it in a vector editor like Illustrator, Inkscape, or Gravit Designer, and make it extra pretty and well-designed. The best way to learn this is by actually doing it.\nSo head over to the example to see how it’s done, or the exercise to get started!","tags":null,"title":"Enhancing graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Publishing your stuff online  Quickest and easiest way: RPubs Great for standalone projects: R Markdown websites More complex blogs and websites: blogdown Books, dissertations, and theses: bookdown Slides: xaringan Code: GitHub and GitHub gists  Telling stories with data   Publishing your stuff online Quickest and easiest way: RPubs The easiest way to get a knitted R Markdown onto the internet is to use RPubs. We talked about this in session 10, and you used it to turn in exercise 10. After knitting an HTML document in RStudio, click on the “Publish” button in the top right corner to upload the document to the RPubs server and get a URL that you can share with others:\nYou don’t have to set up a web server or anything—it’s all pretty automatic and seamless.\n Great for standalone projects: R Markdown websites If you have something slightly more complex, like a collection of R Markdown files that do related things, it’s easy to stitch them all together in an R Markdown website. RStudio supports these automatically—after telling RStudio to consider an RStudio project to be a website, it will knit all the .Rmd files in the root of your project directory every time you click on the “Build Website” button.\nRStudio generates a standalone folder named public with static HTML pages of all your knitted documents. You then have to put that folder on the internet somewhere, either on a web server you have access to, or a free service like Netlify.\nSee this page for complete documentation, or follow these tutorials by Lucy D’Agostino McGowan and Emily Zabor.\nThese websites are especially helpful for standalone projects like research papers and reports. I’ve had students do their master’s capstone projects with these, with specific pages for their introduction, literature review, data cleaning, exploratory data analysis, modeling, and results.\nI typically make a website for each of my research projects and will include pages with IRB details, copies of survey experiments, data cleaning, results, and so on. Here are some examples:\n NGO Crackdowns and Philanthropy Are Donors Really Responding? The Power of Ranking Constraint Closure  You can also make really neat small websites like Desirée De Leon’s Teacup Giraffes for teaching basic statistics.\n More complex blogs and websites: blogdown If you want more control (i.e. total control) over the HTML output and the structure of a website, you can use a package named blogdown to convert R Markdown files into an entire website. This course website is built with blogdown: you can see all the underlying R Markdown files at GitHub.\nLike R Markdown websites, blogdown generates a complete static version of the knitted website and puts it in a folder named public. You’re then responsible for putting that somewhere on the internet, either on your own server or by using a free hosting service like Netlify.\nBlogdown is incredibly well documented, and there are lots of tutorials for how to get started. Alison Hill’s tutorial here is the best place to get started—follow it and you’ll have a basic blog completely free.\n Books, dissertations, and theses: bookdown If you don’t want to create a website, you can use a package named bookdown to stitch a collection of R Markdown files into a PDF, Word, or HTML book. (You could even put all your exercises from this class into a single book!). bookdown is incredibly well documented too (as a bookdown book), and you can get familiar with it fairly quickly.\nDozens of real-world books, dissertations, and theses have been written with bookdown, including both Claus Wilke’s and Kieran Healy’s books from this course. Because of the magic of Markdown, you can create parallel HTML and PDF versions of your book and post one type of output on the internet and print and bind the other one.\n Slides: xaringan R Markdown isn’t just for PDF, Word, and HTML documents. You can also make slides! All the slides for this course were made in R Markdown with a package named xaringan. You can see the documentation here, and see the main example presentation here. You can also see all the R Markdown files I wrote to create the slides for this class here.\n Code: GitHub and GitHub gists And finally, if you want to share code (and keep track of versions of your code), GitHub is one of the best places for that. Posting your code at places like GitHub lets other people see and borrow and adapt and make suggestions to your code. You can see all my different repositories and projects here, for example.\nJenny Bryan has a useful bookdown website explaining how to get started, and GitHub itself has excellent materials for learning how to use git.\nIf you don’t want to go through the process of creating a full-blown git repository, GitHub also lets you make “gists”, which are single shareable files of code. (See all mine here for examples). Gists are excellent ways to share reproducible examples (or reprexes), and the reprex package in R generates output that you can paste directly into a new gist for sharing (see this one, for instance, which I used to show someone how to run and plot logistic regression with R).\n  Telling stories with data If you’re interested in learning more about data storytelling and science communication, check out these resources:\n  BUSM 491R: Telling Stories with Data (BYU, Fall 2017)  Cole Nussbaumer Knaflic, Storytelling with Data: A Data Visualization Guide for Business Professionals (Hoboken, New Jersey: John Wiley \u0026amp; Sons, Inc., 2015).  Alan Alda, If I Understood You, Would I Have This Look on My Face? My Adventures in the Art and Science of Relating and Communicating (New York: Random House, 2017).  Nancy Duarte, Resonate: Present Visual Stories That Transform Audiences (Hoboken, New Jersey: John Wiley \u0026amp; Sons, Inc., 2010).  “Understanding the way scientists speak,” MSNBC Morning Joe, 2013-04-24  “Improvisation for Scientists: Workshops by Alan Alda and the Center for Communicating Science,” Stony Brook Journalism, 2010-03-23   ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"b7eba0e8bc4f8880d263e93aced4e1c5","permalink":"/example/15-example/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/example/15-example/","section":"example","summary":"Publishing your stuff online  Quickest and easiest way: RPubs Great for standalone projects: R Markdown websites More complex blogs and websites: blogdown Books, dissertations, and theses: bookdown Slides: xaringan Code: GitHub and GitHub gists  Telling stories with data   Publishing your stuff online Quickest and easiest way: RPubs The easiest way to get a knitted R Markdown onto the internet is to use RPubs. We talked about this in session 10, and you used it to turn in exercise 10.","tags":null,"title":"Sharing R output online","type":"docs"},{"authors":null,"categories":null,"content":"   Task 1: Storytelling reflection Task 2: Summary reflection Turning everything in   For your final exercise, you won’t do anything with R. You’ll instead have two writing tasks. You can write these in R Markdown if you want, or you can do it in Word or Google Docs or wherever else.\nTask 1: Storytelling reflection Write your standard reflection about the storytelling readings and videos.\n Task 2: Summary reflection Write a longer (400ish words) reflection on what you learned in the course in general. What was new? What was exciting? What will you remember? How has this class changed the way you look at data and graphics?\nYou might explore a few of these summative questions (but definitely don’t just go through and answer each of these!):\n What is truth? How do we find truth? Are facts truth? What’s the difference between content and form? Does beauty matter when describing truth? How does any of this philosophical humanities stuff relate to data visualization?   Turning everything in When you’re all done, upload the document with both reflection to iCollege.\nCongratulations! You did it!\n ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"bad635e8d14ea41006fa04c877289bb2","permalink":"/assignment/15-exercise/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/assignment/15-exercise/","section":"assignment","summary":"Task 1: Storytelling reflection Task 2: Summary reflection Turning everything in   For your final exercise, you won’t do anything with R. You’ll instead have two writing tasks. You can write these in R Markdown if you want, or you can do it in Word or Google Docs or wherever else.\nTask 1: Storytelling reflection Write your standard reflection about the storytelling readings and videos.\n Task 2: Summary reflection Write a longer (400ish words) reflection on what you learned in the course in general.","tags":null,"title":"Truth, beauty, and data revisited","type":"docs"},{"authors":null,"categories":null,"content":"  There’s no lesson for this session. You made it to the end of the course! Congratulations!\n","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"eba25e9e890c63b7e08cf49cf2816105","permalink":"/lesson/15-lesson/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/lesson/15-lesson/","section":"lesson","summary":"There’s no lesson for this session. You made it to the end of the course! Congratulations!","tags":null,"title":"Truth, beauty, and data revisited","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 12 in Alberto Cairo, The Truthful Art1  Chapter 26 in Claus Wilke, Fundamentals of Data Visualization2  Martin Krzywinski and Alberto Cairo, “Storytelling”  Ben Wellington, “Making data mean more through storytelling”  Jonathan Schwabish, “Better Data Communication,” National Bureau of Economic Research  Why People Make Bad Charts (and What to Do When it Happens)  Questions to reflect on  Why are stories so powerful? How are stories related to truth? Is it ethical to emphasize certain aspects of the facts in data more than others? How do you decide which facts to use to convince audiences? When you’re telling a story about data, you’re inherently manipulating audience emotions. Is that okay?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nTruth, beauty, and data revisited  Telling stories with data  Curiosity             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Telling stories with data Curiosity  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n   ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"6328480b5032d9a31a9d7d7d2cb4a78f","permalink":"/content/15-content/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/content/15-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 12 in Alberto Cairo, The Truthful Art1  Chapter 26 in Claus Wilke, Fundamentals of Data Visualization2  Martin Krzywinski and Alberto Cairo, “Storytelling”  Ben Wellington, “Making data mean more through storytelling”  Jonathan Schwabish, “Better Data Communication,” National Bureau of Economic Research  Why People Make Bad Charts (and What to Do When it Happens)  Questions to reflect on  Why are stories so powerful?","tags":null,"title":"Truth, beauty, and data revisited","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos   Readings   Chapter 14, Chapter 15, Chapter 16, Chapter 22, and Chapter 23 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 8 in Kieran Healy, Data Visualization2 Browse through recent visualizations at The Pudding (like this one!), FiveThirtyEight, Vox, Christopher Ingraham’s articles at the Washington Post, WSJ Graphics, and the New York Times’s TheUpshot. Most (if not all) of these graphics were made in R (or something similar) and exported for enhancement in Illustrator or D3.   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nEnhancing graphics       Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Enhancing graphics  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n   ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590710400,"objectID":"6c1d7292f14fb9f873bbf517670d292d","permalink":"/content/14-content/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/content/14-content/","section":"content","summary":"Readings Slides Videos   Readings   Chapter 14, Chapter 15, Chapter 16, Chapter 22, and Chapter 23 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 8 in Kieran Healy, Data Visualization2 Browse through recent visualizations at The Pudding (like this one!), FiveThirtyEight, Vox, Christopher Ingraham’s articles at the Washington Post, WSJ Graphics, and the New York Times’s TheUpshot. Most (if not all) of these graphics were made in R (or something similar) and exported for enhancement in Illustrator or D3.","tags":null,"title":"Enhancing graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Look through some of the chapters in Julia Silge and David Robinson, Tidy Text Mining1 (but definitely don’t read them all! You just need to get a taste of what modern text analysis looks like.) A bunch of (really) short things:   Evangeline Reynolds, “Federalist Papers”  Julia Silge, “She Giggles, He Gallops”  Abby Ohlheiser, “These are the words most associated with men and women, according to Facebook status updates”  Six Ideas for Displaying Qualitative Data  Word clouds considered harmful  Word clouds cause death… or something  When It’s Ok to Use Word Clouds  The Class of 2011  Every time Ford and Kavanaugh dodged a question, in one chart  Tweet by @s_soroka   Questions to reflect on  Why is qualitative data difficult to visualize? Why are word clouds so problematic? When is (not) okay to use them?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nText  Qualitative text-based data  Crash course in computational linguistics             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Qualitative text-based data Crash course in computational linguistics  You can also watch the playlist (and skip around to different sections) here:\n    Julia Silge and David Robinson, Text Mining with R (Sebastopol, California: O’Reilly Media, 2017), https://www.tidytextmining.com/.↩︎\n   ","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590624000,"objectID":"5b7a67b9a0df14e8aac6e2750a796e6e","permalink":"/content/13-content/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/content/13-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Look through some of the chapters in Julia Silge and David Robinson, Tidy Text Mining1 (but definitely don’t read them all! You just need to get a taste of what modern text analysis looks like.) A bunch of (really) short things:   Evangeline Reynolds, “Federalist Papers”  Julia Silge, “She Giggles, He Gallops”  Abby Ohlheiser, “These are the words most associated with men and women, according to Facebook status updates”  Six Ideas for Displaying Qualitative Data  Word clouds considered harmful  Word clouds cause death… or something  When It’s Ok to Use Word Clouds  The Class of 2011  Every time Ford and Kavanaugh dodged a question, in one chart  Tweet by @s_soroka   Questions to reflect on  Why is qualitative data difficult to visualize?","tags":null,"title":"Text","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 10 in Alberto Cairo, The Truthful Art1  Chapter 7 in Kieran Healy, Data Visualization2  It looks like this is a lot of reading, but lots of these are short videos or tweets or interactive websites, so don’t worry!\n  Why all world maps are wrong  The True Size Of…  Map projections  Gall-Peters Projection  “When Maps Lie”  Animated Mercator distortion  “These Twisted Maps Prove That America Isn’t a Red Country”  “The next great fake news threat? Bot-designed maps”  “New World Map That Accurately Shows Earth in 2D Created by Scientists”  Questions to reflect on  How can you know if a map projection is truthful or misleading? What’s wrong (or not wrong) with using points on maps? Choropleths? Lines?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nSpace  Maps and truth  Putting data on maps  GIS in R with sf                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Maps and truth Putting data on maps GIS in R with sf  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n   ","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590537600,"objectID":"9eee4da015dc05af8a4bee2b0e8794b9","permalink":"/content/12-content/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/content/12-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 10 in Alberto Cairo, The Truthful Art1  Chapter 7 in Kieran Healy, Data Visualization2  It looks like this is a lot of reading, but lots of these are short videos or tweets or interactive websites, so don’t worry!\n  Why all world maps are wrong  The True Size Of…  Map projections  Gall-Peters Projection  “When Maps Lie”  Animated Mercator distortion  “These Twisted Maps Prove That America Isn’t a Red Country”  “The next great fake news threat?","tags":null,"title":"Space","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 8 in Alberto Cairo, The Truthful Art1  The Nuclear Threat—The Shadow Peace, part 1  11 Ways to Visualize Changes Over Time – A Guide A bunch of (really) short blog posts:   What a Hundred Million Calls to 311 Reveal About New York (just look at the picture; you don’t need to read this unless you’re really curious about trends in 311 calls)  A century of ocean shipping animated  What is seasonal adjustment and why is it used?  The start-at-zero rule  Keeping one’s appetite after touring the sausage factory  How Common is Your Birthday? This Visualization Might Surprise You   Recommended   The Fallen of World War II  Visualizing Statistical Mix Effects and Simpson’s Paradox2  How To Fix a Toilet (And Other Things We Couldn’t Do Without Search)   Questions to reflect on  When is it okay (or not) to truncate the y-axis? It is remarkably easy to mislead people with many of these chart types. Why? How can you avoid the same mistakes? All these types of charts are good at communicating change over time, but some are more appropriate in different situations. When is it best to use these different types (e.g. line graphs vs. area graphs vs. horizon charts vs. heatmaps, etc.)?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nTime  Axis issues  Visualizing time  Starting, ending, and decomposing time                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Axis issues Visualizing time Starting, ending, and decomposing time  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Zan Armstrong and Martin Wattenberg, “Visualizing Statistical Mix Effects and Simpson’s Paradox,” in Proceedings of IEEE InfoVis 2014, 2014, https://research.google.com/pubs/pub42901.html.↩︎\n   ","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590451200,"objectID":"d9c34011e152a6099f13882fb144b965","permalink":"/content/11-content/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/content/11-content/","section":"content","summary":"Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 8 in Alberto Cairo, The Truthful Art1  The Nuclear Threat—The Shadow Peace, part 1  11 Ways to Visualize Changes Over Time – A Guide A bunch of (really) short blog posts:   What a Hundred Million Calls to 311 Reveal About New York (just look at the picture; you don’t need to read this unless you’re really curious about trends in 311 calls)  A century of ocean shipping animated  What is seasonal adjustment and why is it used?","tags":null,"title":"Time","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Explorable explanations Dashboards Questions to reflect on  Slides Videos   Readings Explorable explanations   Marcel Salathé and Nicky Case, “What Happens Next: COVID-19 Futures, Explained with Playable Situations”  Brett Victor, “Explorable Explanations”  Look at some of the explorable explorations here  Dragicevic, Jansen, Sarma, Kay, and Chevalier, “Explorable Multiverse Analyses”. Use Chrome, open Example 1, scroll to page 2, and click on some of the blue text to change the results of the paper within the paper itself. This is magical. R Markdown can’t quite get this interactive in real-time, but you can knit different versions of a document with slightly different parameters and options.   Dashboards   Look at some of these examples of Shiny apps  Stephanie Evergreen, “How a Dashboard Changes the Conversation”  Stephanie Evergreen, “The Problem with Dashboards (and a Solution)”  Stephen Few, “2012 Perceptual Edge Dashboard Design Competition: A Solution of My Own”  Skim through Stephen Few’s presentation on Information Dashboard Design  Google “dashboard design” and skim through some of the thousands of articles about what makes a good (and bad) dashboard   Questions to reflect on  How helpful (or unhelpful) are explorable explanations? Have you seen examples of good dashboards before this class? Bad dashboards? What makes them good or bad?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nInteractivity  Making interactive graphs  Sharing content             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Making interactive graphics Sharing content  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590105600,"objectID":"71d614e4debdb64f2a26fc68ad217ddd","permalink":"/content/10-content/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/content/10-content/","section":"content","summary":"Readings  Explorable explanations Dashboards Questions to reflect on  Slides Videos   Readings Explorable explanations   Marcel Salathé and Nicky Case, “What Happens Next: COVID-19 Futures, Explained with Playable Situations”  Brett Victor, “Explorable Explanations”  Look at some of the explorable explorations here  Dragicevic, Jansen, Sarma, Kay, and Chevalier, “Explorable Multiverse Analyses”. Use Chrome, open Example 1, scroll to page 2, and click on some of the blue text to change the results of the paper within the paper itself.","tags":null,"title":"Interactivity","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos   Readings   Chapter 17, Chapter 18, Chapter 19, and Chapter 21 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 5 in Kieran Healy, Data Visualization2   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nAnnotations  Fretting the little things  Text in plots  Seeds                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Fretting the little things Text in plots Seeds  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n   ","date":1590019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590019200,"objectID":"35bbbb8e03581e65c2b2e10271df2fce","permalink":"/content/09-content/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/content/09-content/","section":"content","summary":"Readings Slides Videos   Readings   Chapter 17, Chapter 18, Chapter 19, and Chapter 21 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 5 in Kieran Healy, Data Visualization2   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later).","tags":null,"title":"Annotations","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Reproducible examples Questions to reflect on  Slides Videos   Readings   Chapter 9 in Claus Wilke, Fundamentals of Data Visualization1  Mike Bostock, “Methods of Comparison, Compared”. Explanation of the differences between showing relative differences, absolute differences, and log ratios.  Sparkline theory and practice. This is a collection of posts by Edward Tufte about sparklines—scroll down a ways and check out his examples, analysis, and critiques.  Time series sparklines  Comparisons with lollipop charts. If you’re feeling adventurous (and you should!), do this tutorial as you read it.  Reproducible examples Reprexes (or reproducible examples) are the best way to (1) get help online and (2) fix issues on your own.\nMaking a good reprex is tricky, but it’s a very valuable skill to know (regardless of programming language!). Here are some helpful resources for making them:\n  What’s a reproducible example (reprex) and how do I do one?  So you’ve been asked to make a reprex  The reprex package   Questions to reflect on  These readings all show a ton of new ways to present comparisons. Which ones are your favorite? Which ones didn’t quite click with you? In what situations are some more appropriate than others?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nComparisons  Visualizing comparisons  Reproducible examples             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Visualizing comparisons Reproducible examples  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n   ","date":1589932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589932800,"objectID":"d3f7fc26f58a06eab9a68e8656b03674","permalink":"/content/08-content/","publishdate":"2020-05-20T00:00:00Z","relpermalink":"/content/08-content/","section":"content","summary":"Readings  Reproducible examples Questions to reflect on  Slides Videos   Readings   Chapter 9 in Claus Wilke, Fundamentals of Data Visualization1  Mike Bostock, “Methods of Comparison, Compared”. Explanation of the differences between showing relative differences, absolute differences, and log ratios.  Sparkline theory and practice. This is a collection of posts by Edward Tufte about sparklines—scroll down a ways and check out his examples, analysis, and critiques.","tags":null,"title":"Comparisons","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 9 in Alberto Cairo, The Truthful Art1  Chapter 12 in Claus Wilke, Fundamentals of Data Visualization2  Kieran Healy, “Two y-axes”  Two Alternatives to Using a Second Y-Axis \u0026amp; Illusion of success \u0026amp; Dissecting two axes  Dual-Scaled Axes in Graphs: Are They Ever the Best Solution?3  Recommended   “A Study on Dual-Scale Data Charts”4   Questions to reflect on  How can you correctly and honestly communicate relationships between variables? How can you communicate the uncertainty in those relationships? What are the dangers of visualizing two variables? When is it appropriate to use two y-axes?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nRelationships  The dangers of dual y-axes  Visualizing correlations  Visualizing regressions                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction The dangers of dual y-axes Visualizing correlations Visualizing regressions  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Stephen Few, “Dual-Scaled Axes in Graphs: Are They Ever the Best Solution?” Visual Business Intelligence Newsletter, March 2008, http://www.perceptualedge.com/articles/visual_business_intelligence/dual-scaled_axes.pdf.↩︎\n Petra Isenberg et al., “A Study on Dual-Scale Data Charts,” IEEE Transactions on Visualization and Computer Graphics 17, no. 12 (2011): 2469–78, doi:10.1109/tvcg.2011.160.↩︎\n   ","date":1589846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589846400,"objectID":"79f283e309555def72f172af8fa16106","permalink":"/content/07-content/","publishdate":"2020-05-19T00:00:00Z","relpermalink":"/content/07-content/","section":"content","summary":"Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 9 in Alberto Cairo, The Truthful Art1  Chapter 12 in Claus Wilke, Fundamentals of Data Visualization2  Kieran Healy, “Two y-axes”  Two Alternatives to Using a Second Y-Axis \u0026amp; Illusion of success \u0026amp; Dissecting two axes  Dual-Scaled Axes in Graphs: Are They Ever the Best Solution?3  Recommended   “A Study on Dual-Scale Data Charts”4   Questions to reflect on  How can you correctly and honestly communicate relationships between variables?","tags":null,"title":"Relationships","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 7 in Claus Wilke, Fundamentals of Data Visualization1  Chapters 4, 7, and 11 in Alberto Cairo, The Truthful Art2  Why It’s So Hard for Us to Visualize Uncertainty  Amanda Cox’s keynote address at the 2017 OpenVis Conf  Communicating Uncertainty When Lives Are on the Line  Showing uncertainty during the live election forecast \u0026amp; Trolling the uncertainty dial  Questions to reflect on  Why is it important to deal with uncertainty in data? What was good or bad about the New York Times’ 2016 live election guage? Why is it so hard to visualize uncertainty? Why is it so hard to communicate uncertainty to others?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nUncertainty  Communicating uncertainty  Visualizing uncertainty             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Communicating uncertainty Visualizing uncertainty  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n   ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589760000,"objectID":"4fb51266c58f677bf1e1454e5cd9c527","permalink":"/content/06-content/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/content/06-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 7 in Claus Wilke, Fundamentals of Data Visualization1  Chapters 4, 7, and 11 in Alberto Cairo, The Truthful Art2  Why It’s So Hard for Us to Visualize Uncertainty  Amanda Cox’s keynote address at the 2017 OpenVis Conf  Communicating Uncertainty When Lives Are on the Line  Showing uncertainty during the live election forecast \u0026amp; Trolling the uncertainty dial  Questions to reflect on  Why is it important to deal with uncertainty in data?","tags":null,"title":"Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 22 in Claus Wilke, Fundamentals of Data Visualization1  Naomi Robbins, “Are Grid Lines Useful or Chartjunk?”  Stephen Few, “Grid Lines in Graphs are Rarely Useful”  Henry Wang, “ggplot2 Theme Elements Demonstration”  Glance through the documentation for ggplot’s complete themes and theme(), especially the examples near the bottom  Questions to reflect on  How do the principles of CRAP apply to graph design and other theme elements? Should plots use gridlines? Naomi Robbins says yes; Stephen Few says no—what do you say?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nThemes  CRAP and ggplot  The anatomy of a ggplot theme             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction CRAP and ggplot The anatomy of a ggplot theme  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n   ","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589500800,"objectID":"744a9e81e6eae570052294b144f8d401","permalink":"/content/05-content/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/content/05-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 22 in Claus Wilke, Fundamentals of Data Visualization1  Naomi Robbins, “Are Grid Lines Useful or Chartjunk?”  Stephen Few, “Grid Lines in Graphs are Rarely Useful”  Henry Wang, “ggplot2 Theme Elements Demonstration”  Glance through the documentation for ggplot’s complete themes and theme(), especially the examples near the bottom  Questions to reflect on  How do the principles of CRAP apply to graph design and other theme elements?","tags":null,"title":"Themes","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 6 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 6 in Alberto Cairo, The Truthful Art2  Chapter 10 in Claus Wilke, Fundamentals of Data Visualization3  Engaging Readers with Square Pie/Waffle Charts  Understanding Pie Charts  Square pie chart beats out the rest in perception study  Twitter thread from John Burn-Murdoch on why the Financial Times uses log scales in their COVID-19 tracking charts  Tweet and Twitter thread from John Burn-Murdoch on why the Financial Times doesn’t use population-adjusted numbers in their COVID-19 tracking charts  Video from the Financial Times about the design decisions behind their COVID-19 tracking charts  Recommended   See how to create your own COVID-19 tracking chart with R   Questions to reflect on  How do these types of visualizations help or hinder our search for truth in data? What do you think of the Financial Times explanations of their use of absolute numbers (not per capita numbers) and log scales (not regular scales)? How have these decisions affected your perception of the pandemic? How have they affected others’ perceptions?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nAmounts and proportions  Reproducibility  Amounts  Proportions                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Reproducibility Amounts Proportions  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Wilke, Fundamentals of Data Visualization.↩︎\n   ","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589414400,"objectID":"570f902af3e232a725f0155a46b1910b","permalink":"/content/04-content/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/content/04-content/","section":"content","summary":"Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 6 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 6 in Alberto Cairo, The Truthful Art2  Chapter 10 in Claus Wilke, Fundamentals of Data Visualization3  Engaging Readers with Square Pie/Waffle Charts  Understanding Pie Charts  Square pie chart beats out the rest in perception study  Twitter thread from John Burn-Murdoch on why the Financial Times uses log scales in their COVID-19 tracking charts  Tweet and Twitter thread from John Burn-Murdoch on why the Financial Times doesn’t use population-adjusted numbers in their COVID-19 tracking charts  Video from the Financial Times about the design decisions behind their COVID-19 tracking charts  Recommended   See how to create your own COVID-19 tracking chart with R   Questions to reflect on  How do these types of visualizations help or hinder our search for truth in data?","tags":null,"title":"Amounts and proportions","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Hans Rosling, “200 Countries, 200 Years, 4 Minutes”  Chapter 2 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 3 in Kieran Healy, Data Visualization2  Questions to reflect on  Why is it important to visualize variables and data? What does it mean to map data to graph aesthetics? What data was mapped to which aesthetics in Rosling’s video?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nMapping data to graphics  Data, aesthetics, \u0026amp; the grammar of graphics  Grammatical layers  Aesthetics in extra dimensions  Tidy data                   Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Data, aesthetics, and the grammar of graphics Grammatical layers Aesthetics in extra dimensions Tidy data  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n   ","date":1589328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589328000,"objectID":"267a7446c3b4fe31bb41b58ba3b49b11","permalink":"/content/03-content/","publishdate":"2020-05-13T00:00:00Z","relpermalink":"/content/03-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Hans Rosling, “200 Countries, 200 Years, 4 Minutes”  Chapter 2 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 3 in Kieran Healy, Data Visualization2  Questions to reflect on  Why is it important to visualize variables and data? What does it mean to map data to graph aesthetics? What data was mapped to which aesthetics in Rosling’s video?","tags":null,"title":"Mapping data to graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 5 in Alberto Cairo, The Truthful Art1  Chapter 4 and Chapter 22 in Claus Wilke, Fundamentals of Data Visualization2  Summary of CRAP graphic design principles from Garr Reynolds, Presentation Zen.3 These principles are from Robin Williams’ The Non-Designer’s Design \u0026amp; Type Books,4 which you should really get if you’re interested in doing anything design-related ever. Her stuff is life-changing.  Typography in ten minutes. The rest of the Practical Typography book is phenomenal and you’d be remiss if you didn’t read the whole thing and bookmark it for life, but for now just read this quick summary.  “What’s the Difference Between JPG, PNG, and GIF?”  “File formats explained”  Questions to reflect on  Why does graphic design matter when conveying truth? What makes something well designed (vs. poorly designed)?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nGraphic design  Truth, beauty, stories, design  Graphic design and CRAP  Contrast  Repetition  Alignment  Proximity  Image types                            Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Truth, beauty, stories, design Graphic design and CRAP Image types  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Garr Reynolds, Presentation Zen: Simple Ideas on Presentation Design and Delivery, 1st ed. (Berkeley, California: New Riders, 2008).↩︎\n Robin Williams, The Non-Designer’s Design \u0026amp; Type Books: Design and Typographic Principles for the Visual Novice, Deluxe Edition (Berkeley, California: Peachpit Press, 2008).↩︎\n   ","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589241600,"objectID":"f4445f225dbc16ad343f16f0677c881d","permalink":"/content/02-content/","publishdate":"2020-05-12T00:00:00Z","relpermalink":"/content/02-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 5 in Alberto Cairo, The Truthful Art1  Chapter 4 and Chapter 22 in Claus Wilke, Fundamentals of Data Visualization2  Summary of CRAP graphic design principles from Garr Reynolds, Presentation Zen.3 These principles are from Robin Williams’ The Non-Designer’s Design \u0026amp; Type Books,4 which you should really get if you’re interested in doing anything design-related ever.","tags":null,"title":"Graphic design","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings  The syllabus, content, lessons, examples, and assignments pages for this class  Chapter 1 in Kieran Healy, Data Visualization1  Chapters 2 and 3 in Alberto Cairo, The Truthful Art2 (skim the introduction and chapter 1)  Study: Charts change hearts and minds better than words do  Questions to reflect on  How do we know what is true? Are facts truth? Why do we visualize data? What makes a great visualization? How do you choose which kind of visualization to use?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nTruth, beauty, and data  Facts, truth, and beauty  Data, truth, and beauty  Beautiful visualizations  Class details                   Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Facts, truth, and beauty Data, truth, and beauty Beautiful visualizations Class details  You can also watch the playlist (and skip around to different sections) here:\n    Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n   ","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"fbbae951c935dd3d35f82710943d5efd","permalink":"/content/01-content/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/content/01-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings  The syllabus, content, lessons, examples, and assignments pages for this class  Chapter 1 in Kieran Healy, Data Visualization1  Chapters 2 and 3 in Alberto Cairo, The Truthful Art2 (skim the introduction and chapter 1)  Study: Charts change hearts and minds better than words do  Questions to reflect on  How do we know what is true?","tags":null,"title":"Truth, beauty, and data + R and tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"  Here’s your roadmap for the mini-mester!\nEvery class session has four important sections. You should read about the details for each using the main menu at the top of this webpage.\n Content (): This page contains the readings, slides, and recorded lectures for the topic. Read and watch these first each day. Lesson (): This page contains an interactive lesson that teaches you the principles and code you need to know. Go through these after doing the content. Example (): This page contains fully annotated R code that you can use as a reference for creating your own visualizations. This is only a reference page—you don’t have to necessarily do anything here. Each section also contains videos of me live coding the examples so you can see what it looks like to work with R in real time. This page will be very helpful as you work on your assignments. Assignment (): This page contains the instructions for either the session exercise (1–3 brief tasks), or for the two mini projects and final project. Assignments are due by 11:59 PM on the day they’re listed.  tl;dr: You should follow this general process each day:\n Do everything on the content () page Work through the lesson () page Complete the assignment () while referencing the example ()      Foundations Content Lesson Example Assignment   May 11 Truth, beauty, and data + R and tidyverse       May 12 Graphic design       May 13 Mapping data to graphics        Core types of graphics Content Lesson Example Assignment   May 14 Amounts and proportions       May 15 Themes       May 18 Uncertainty       May 19 Relationships       May 20 Comparisons       May 21 Annotations       May 22  Mini project 1 due           Special applications Content Lesson Example Assignment   May 22 Interactivity       May 26 Time       May 27 Space       May 28 Text       May 29 Enhancing graphics       May 29  Mini project 2 due           Conclusions Content Lesson Example Assignment   June 1 Truth, beauty, and data revisited       June 5  Final project due           ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"Here’s your roadmap for the mini-mester!\nEvery class session has four important sections. You should read about the details for each using the main menu at the top of this webpage.\n Content (): This page contains the readings, slides, and recorded lectures for the topic. Read and watch these first each day. Lesson (): This page contains an interactive lesson that teaches you the principles and code you need to know.","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"   Course objectives Important pep talk! Course materials  Books, articles, and other materials R and RStudio Online help  Course policies  Student hours Class conduct and expectations Learning during a pandemic Counseling and Psychological Services (CPS) Basic needs security Lauren’s Promise Academic honesty Special needs  Assignments and grades Star Wars   Instructor  Prof. Ben Bushjong  25A Marshall-Adams Hall  bbushong@msu.edu  @benbushong  Schedule an appointment   Course details  Tuesday and Thursday  September – December, 2020  Whenever  Slack   Contacting me See course syllabus.\n  Course objectives Data rarely speaks for itself. On their own, the facts contained in raw data are difficult to understand, and in the absence of beauty and order, it is impossible to understand the truth that the data shows.\nIn this class, you’ll learn how to use industry-standard graphic and data design techniques to create beautiful, understandable visualizations and uncover truth in data.\nBy the end of this course, you will become (1) literate in data and graphic design principles, and (2) an ethical data communicator, by producing beautiful, powerful, and clear visualizations of your own data. Specifically, you should:\n Understand the principles of data and graphic design Evaluate the credibility, ethics, and aesthetics of data visualizations Create well-designed data visualizations with appropriate tools Share data and graphics in open forums Be curious and confident in consuming and producing data visualizations  This class will expose you to R—one of the most popular, sought-after, and in-demand statistical programming languages. Armed with the foundation of R skills you’ll learn in this class, you’ll know enough to be able to find how to visualize and analyze any sort of data-based question in the future.\n Important pep talk! I promise you can succeed in this class.\nLearning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2—made this wise observation:\n It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n Even experienced programmers find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n\n Course materials All of the readings and software in this class are free. There are free online version of all the textbooks, R and RStudio are inherently free, and GSU provides free access to Adobe Illustrator.\nBooks, articles, and other materials We’ll rely heavily on these books, which are all available online (for free!). I recommend getting the printed versions of these books if you are interested, but it is not required.\n Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).\n $27 used, $32 new at Amazon. A free eBook version is available through GSU’s library through O’Reilly’s Higher Education database. The easiest way to access it is to visit a special URL (http://go.oreilly.com/georgia-state-university), log in with your GSU account, and then search for “The Truthful Art”.\n Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.\n FREE online; $30 used, $36 new at Amazon.\n Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.\n FREE online; $32 new at Amazon. An eBook version is also available through the O’Reilly database, but you can just use the online version.\n  There will occasionally be additional articles and videos to read and watch. When this happens, links to these other resources will be included on the content page for that session.\nI also highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\n R and RStudio You will do all of your analysis with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for examples, exercises, and mini projects.\nRStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis and graphics. You also can’t use your own custom fonts with RStudio.cloud. Over the course of the semester, you’ll probably want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud. This isn’t 100% necessary, but it’s helpful.\nYou can find instructions for installing R, RStudio, and all the tidyverse packages here.\n Online help Data science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nAdditionally, we have a class chatroom at Slack where anyone in the class can ask questions and anyone can answer. I will monitor Slack regularly and will respond quickly. (It’s one of the rare Slack workspaces where I actually have notifications enabled!) Ask questions about the readings, exercises, and mini projects. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too.\n  Course policies Be nice. Be honest. Don’t cheat.\nWe will also follow Georgia State’s Code of Conduct.\nThis syllabus reflects a plan for the mini-mester. Deviations may become necessary as the course progresses.\nStudent hours Please watch this video:\n Student hours are set times dedicated to all of you (most professors call these “office hours”; I don’t1). This means that I will be in my office at home (wistfully) waiting for you to come by talk to me remotely with whatever questions you have. This is the best and easiest way to find me and the best chance for discussing class material and concerns.\nBecause of the pandemic, we cannot meet in person. I can meet you online via Webex. Use this link for regular student hours: https://gsumeetings.webex.com/meet/aheiss. You can also easily [make an appointment You can also find me through e-mail and Slack.\n Class conduct and expectations Here are the rules, expectations, and policies that we came up with collectively:\n Late work: You will lose 1 point per day for each day an assigment or problem set is late. After 1 week, I will send a reminder e-mail. After 2 weeks, you will receive no points. Technology use: Use phones, computers, etc. responsibly. You’re all adults. Participation: Ensure that you are engaged and participate in class. Engagement is defined by you—if that means commenting and answering questions, neat; if it means sitting quietly and being focused, also neat.   Learning during a pandemic Life absolutely sucks right now.\nYou most likely know people who have lost their jobs, have tested positive for COVID-19, have been hospitalized, or perhaps have even died. You all have increased (or possibly decreased) work responsibilities and increased family care responsibilities—you might be caring for extra people (young and/or old!) right now, and you are likely facing uncertain job prospects (or have been laid off!).\nI’m fully committed to making sure that you learn everything you were hoping to learn from this class! I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Under pandemic conditions, that flexibility and leniency is intensified.\nIf you feel like you’re behind or not understanding everything, do not suffer in silence! Talk to me! Please sign up for a time to meet with me during student hours at https://calendly.com/andrewheiss/meeting/. I’m also available through e-mail and Slack. Remember, this is the only Slack account where I’ve enabled notifications!\nI want you to learn lots of things from this class (Graphic design! Fancy charts! R! ggplot!), but I primarily want you to stay healthy, balanced, and grounded during this crisis.\n Counseling and Psychological Services (CPS) Life at GSU can be complicated and challenging (especially during a pandemic!). You might feel overwhelmed, experience anxiety or depression, or struggle with relationships or family responsibilities. Counseling and Psychological Services (CPS) provides free, confidential support for students who are struggling with mental health and emotional challenges. The CPS office is staffed by professional psychologists who are attuned to the needs of all types of college and professional students. Please do not hesitate to contact CPS for assistance—getting help is a smart and courageous thing to do.\n Basic needs security If you have difficulty affording groceries or accessing sufficient food to eat every day, or if you lack a safe and stable place to live, and you believe this may affect your performance in this course, please contact the Dean of Students for support. They can provide a host of services including free groceries from the Panther Pantry and assisting with homelessness with the Embark Network. Additionally, please talk to me if you are comfortable in doing so. This will enable me to provide any resources that I might possess.\n Lauren’s Promise I will listen and believe you if someone is threatening you.\nLauren McCluskey, a 21-year-old honors student athlete, was murdered on October 22, 2018 by a man she briefly dated on the University of Utah campus. We must all take action to ensure that this never happens again.\nIf you are in immediate danger, call 911 or GSU police (404-413-3333).\nIf you are experiencing sexual assault, domestic violence, or stalking, please report it to me and I will connect you to resources or call GSU’s Counseling and Psychological Services (404-413-1640).\nAny form of sexual harassment or violence will not be excused or tolerated at Georgia State. GSU has instituted procedures to respond to violations of these laws and standards, programs aimed at the prevention of such conduct, and intervention on behalf of the victims. Georgia State University Police officers will treat victims of sexual assault, domestic violence, and stalking with respect and dignity. Advocates on campus and in the community can help with victims’ physical and emotional health, reporting options, and academic concerns.\n Academic honesty Violation of GSU’s Policy on Academic Honesty will result in an F in the course and possible disciplinary action.2 All violations will be formally reported to the Dean of Students.\n Special needs Students who wish to request accommodation for a disability may do so by registering with the Office of Disability Services. Students may only be accommodated upon issuance by the Office of Disability Services of a signed Accommodation Plan and are responsible for providing a copy of that plan to instructors of all classes in which accommodations are sought.\nStudents with special needs should then make an appointment with me during the first week of class to discuss any accommodations that need to be made.\n  Assignments and grades You can find descriptions for all the assignments on the assignments page.\n   Assignment Points Percent    Reflections (15 × 10) 150 23%  Exercises (15 × 10) 150 23%  Mini project 1 75 12%  Mini project 2 75 12%  Final project 200 31%  Total 650 —        Grade Range Grade Range    A 93–100% C 73–76%  A− 90–92% C− 70–72%  B+ 87–89% D+ 67–69%  B 83–86% D 63–66%  B− 80–82% D− 60–62%  C+ 77–79% F \u0026lt; 60%      Star Wars Once you have read this entire syllabus and the assignments page, please click here and e-mail me a picture of a cute Star Wars character.3 Brownie points if it’s animated.\n   There’s fairly widespread misunderstanding about what office hours actually are! Many students often think that they are the times I shouldn’t be disturbed, which is the exact opposite of what they’re for!↩︎\n So seriously, just don’t cheat or plagiarize!↩︎\n Baby Yoda, Babu Frik, porgs, etc. are all super fair game.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Course objectives Important pep talk! Course materials  Books, articles, and other materials R and RStudio Online help  Course policies  Student hours Class conduct and expectations Learning during a pandemic Counseling and Psychological Services (CPS) Basic needs security Lauren’s Promise Academic honesty Special needs  Assignments and grades Star Wars   Instructor  Prof. Ben Bushjong  25A Marshall-Adams Hall  bbushong@msu.edu  @benbushong  Schedule an appointment   Course details  Tuesday and Thursday  September – December, 2020  Whenever  Slack   Contacting me See course syllabus.","tags":null,"title":"Syllabus","type":"page"}]