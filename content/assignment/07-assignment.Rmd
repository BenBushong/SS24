---
title: "Advanced Model Building"
linktitle: "8: Linear Regression III"
output:
  blogdown::html_page:
    toc: true
menu:
  assignment:
    parent: Labs
    weight: 2
type: docs
weight: 1
editor_options:
  chunk_output_type: console
---


:::fyi


You must turn in a PDF document of your `R Markdown` code. Submit this to D2L by 11:59 PM Eastern Time on [DATE.]

:::

## Backstory and Set Up
You still work for Zillow as a junior analyst (sorry). But you're hunting a promotion. Your job is to present some more advanced predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.

As always, let's load the data.

```{r, include = F}
library(tidyverse)
```

```{r}

Ames <- read.table('https://raw.githubusercontent.com/ajkirkpatrick/FS20/postS21_rev/classdata/ames.csv', 
                   header = TRUE,
                   sep = ',') 
```

## Data Cleaning
**Do not skip this section. This isn't your kitchen junk drawer -- you can't get away with not cleaning your data.**

Oh, the Ames data yet again. It's given us lots of trouble. Many of you have found a few variables (columns) that should be avoided. The main problem is that some columns have only one value in them, or they have only `NA` and one value, so once `lm(...)` drops the `NA` rows, they are left with only one value. Linear regression by OLS does not like variables that don't vary! So, let's be systematic about figuring out which columns in our data are to be avoided.

The `skimr` package is very helpful for seeing what our data contains. Install it, and then use `skim(Ames)` directly in the console (we're just looking at data at the moment -- do **not** put `skim` output into your RMarkdown output - it will give you an error). Take a look at the "complete rate" column - this tells us the fraction of observations in that column that are `NA`. If it's very small (see `Alley`), then that variable will be problematic. The "n_unique" column tells us if there are few or many different values - a "1" in "n_unique" is definitely going to be a problem and you must drop that variable.

You can make a note of those columns that have extremely low "complete rates" and drop them to start off. Of course, we could keep them, and drop all observations where any of those columns are `NA`, but once we do that, there probably won't be many rows left! There are about 6-7 of them that will drop so many that it will cause an error if we include them in a regression. Let's drop those. (Note: the list of columns to drop in Lab 06 is a good start and should suffice, but feel free to drop others that have low complete rates).

### Many models do not like `NA` values
`predict` has some unusual behavior that can give unexpected results. Thus far, we have mostly used `predict(myOLS)`, which gives the predicted values from a model *using the same data that estimated the model*. When we ask `lm` (or, later, other machine learning models) to estimate a model, it will drop any rows of our data that contain a `NA` value for any of the variables used in the estimation. If your regression is `SalePrice ~ GrLivArea`, then it will check `SalePrice` and `GrLivArea` for NA's. If you add another variable, then you add another possible set of `NA` values that can be dropped, and R will estimate the model on a subset of the data.

This will mess with your measure of $RMSE$ - if you compare two models that use different sets of the data, then you aren't really comparing the fit very well. Because of that, we need to take a moment to check our data.

Ames has a lot of variables, and in this assignment, you're going to be asked to construct 15 regressions of increasing complexity. So we're going to choose 15 variables to be explanatory, plus one variable that *isn't* `SalePrice` that we want to be the target variable we predict (in Exercise 1), plus `SalePrice`, which will be our target variable for Exercise 2. That makes 17 variables. 

**Select your 17 variables** by making a character vector of the variables you think will best predict your chosen variable. Then, make a new version of `Ames` that contains only those 17 variables (you can use `dplyr::select` or any other method). Once you've done that, use `na.omit` to make a new, clean version of Ames that has (1) no NA's in it, and (2) 17 variables of your choice (as long as one is `SalePrice`). Use the help for `na.omit` to see how it works. This way, we know every model we make will have the same number of rows in it as none will be dropped due to NA values.

## Linear Models
When exploring linear models in other classes, we often emphasize asymptotic results under distributional assumptions. That is, we make assumptions about the model in order to derive properties of large samples. This general approach is useful for creating and performing hypothesis tests. Frequently, when developing a linear regression model, part of the goal is to **explain** a relationship. However, this isn't always the case. And it's often not a valid approach, as we discussed in this week's content.

So, we will ignore much of what we have learned in other classes (sorry, EC420) and instead simply use regression as a tool to **predict**. Instead of a model which supposedly explains relationships, we seek a model which minimizes **errors**.

To discuss linear models in the context of prediction, we return to the `Ames` data. Accordingly, you should utilize some of the early code from Lab 2 to hasten your progress in this lab.


### Assesing Model Accuracy

There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.

$$
\text{RMSE}(\hat{f}, \text{Data}) = \sqrt{\frac{1}{n}\displaystyle\sum_{i = 1}^{n}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
$$

While for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be $n$.

For a linear model , the estimate of $f$, $\hat{f}$, is given by the fitted regression line.

$$
\hat{y}({\bf{x}_i}) = \hat{f}({\bf{x}_i})
$$

We can write an `R` function that will be useful for performing this calculation.

```{r}
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```

### Model Complexity

Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, $p$.

We write a simple `R` function to extract this information from a model.

```{r}
get_complexity = function(model) {
  length(coef(model)) - 1
}
```

When deciding how complex of a model to use, we can utilize two techniques: *forward selection* or *backward selection*. Forward selection means that we start with the simplest model (with a single predictor) and then add one at a time until we decide to stop. Backward selection means that we start with the most complex model (with every available predictor) and then remove one at a time until we decide to stop. There are many criteria for "when to stop". Below, we'll try to give you some intuition on the model-building process.


:::fyi

__EXERCISES__

1. Using `skimr::skim`, find the variables that have a low complete rate (below 60\%) and drop them. 60\% isn't a magic number by any means, the "right" number is entirely dependent on your data. It is always standard practice to document the fields you have dropped from the data, so make sure you state which variables have been dropped. Also using `skim`, note the variables with values for "n_unique" equal to 1 and drop them. See *Data Cleaning* (above) for details

2. Then, choose one numeric variable (not SalePrice) to be your target variable (the variable you want to explain or predict), and the 15 variables you want to use to predict it - they can be numeric or categorical, both will work. Then, as described in *Data Cleaning* above, use `na.omit` to make a clean version of `Ames` where every variable is 100% complete. Include `SalePrice` in your cleaning, but don't use it yet -- it will be the target variable for next week.

3. Using **forward selection** (that is, select one variable, then select another) create a series of models up to complexity length 15. While you can code each of the 15 regressions separately, if you really want to be efficient, try to use a loop along with a character vector of your 15 variables to "step" through the 15 regressions. There are multiple ways to specify a regression using a character vector of arguments, but I'll leave the solution to you. Use a list object to hold your results, and use `lapply` along with your RMSE function(s) to get a list of the RMSE's, one for each model.

4. Make a `data.frame` or `tibble` of the RMSE results and the model complexity (the function `unlist` is helpful when you have a list of identical types, as should be the case with your measures of model complexity and RMSE). Create a chart plotting the model complexity as the $x$-axis variable and RMSE as the $y$-axis variable. Describe any patterns you see. Do you think you should use the full-size model? Why or why not? What criterion are you using to make this statement?

:::

