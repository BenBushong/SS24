---
title: "Training, Testing, and Advanced Model Selection"
linktitle: "9: Training and Testing"
output:
  blogdown::html_page:
    toc: true
menu:
  assignment:
    parent: Labs
    weight: 2
type: docs
weight: 1
editor_options:
  chunk_output_type: console
---

```{r, include = F}
library(tidyverse)
```

## Backstory and Set Up
You **still** work for Zillow. Your job is to present some super-duper advanced predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.

As always, we load the data.

```{r, include = F}
library(tidyverse)
get_complexity = function(model) {
  length(coef(model)) - 1
}
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
```

```{r}

Ames <- read.table('https://raw.githubusercontent.com/ajkirkpatrick/FS20/postS21_rev/classdata/ames.csv', 
                   header = TRUE,
                   sep = ',') 
```

The steps from the last lab should be considered prerequisites in order to get this lab to function properly.

## Test-Train Split

Here, we will use `SalePrice` as our target variable. You can re-use your code for cleaning from above (you included `SalePrice` there, right?), and if you want to choose different predictors, feel free to do so.


There is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts: it is essentially cheating. As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down---or, in very specific cases where a new predictor is completely uncorrelated with the target, stay the same. This might seem to suggest that in order to predict well, we should use the largest possible model. However, in reality we have fit to a specific dataset, but as soon as we see new data, a large model may (in fact) predict poorly. This is called **overfitting**.

The most common approach to overfitting is to take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the **training** data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the **test** data. Test data should *never* be used to train a model---its pupose is to evaluate the fitted model once you've settled on something.[^1]

[^1]: Note that sometimes the terms *evaluation set* and *test set* are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.

Here we use the `sample()` function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the `set.seed()` function to allow use to reproduce the same random split each time we perform this analysis. Sometimes we don't want to do this; if we want to run lots of independent splits, then we do not need to set the initial seed.

```{r, include = F}
Amesclean = na.omit(Ames %>% dplyr::select(SalePrice, TotalBsmtSF, Heating, SaleCondition, YrSold))
```

```{r}
set.seed(9)
num_obs = nrow(Amesclean)

train_index = sample(num_obs, size = trunc(0.50 * num_obs))
train_data = Amesclean[train_index, ]
test_data = Amesclean[-train_index, ]
```

Of course, you'll have different results here since you'll have different columns in `Amesclean` and will thus have different numbers after using `na.omit`. We will look at two measures that assess how well a model is predicting: **train RMSE** and **test RMSE**.

$$
\text{RMSE}_\text{Train} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Tr}}}\sum_{i \in \text{Train}}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
$$

Here $n_{Tr}$ is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.

$$
\text{RMSE}_{\text{Test}} = \text{RMSE}(\hat{f}, \text{Test Data}) = \sqrt{\frac{1}{n_{\text{Te}}}\sum_{i \in \text{Test}} \left ( y_i - \hat{f}(\bf{x}_i) \right ) ^2}
$$

Here $n_{Te}$ is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict **in general**, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.

We will start with the simplest possible linear model, that is, a model with no predictors.

```{r}
fit_0 = lm(SalePrice ~ 1, data = train_data)
get_complexity(fit_0)

# train RMSE
sqrt(mean((train_data$SalePrice - predict(fit_0, train_data)) ^ 2))
# test RMSE
sqrt(mean((test_data$SalePrice - predict(fit_0, test_data)) ^ 2))
```

Your results will be different, depending on what 17 variables you selected (and which rows contained `NA` for them). The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.

```{r}
# train RMSE
rmse(actual = train_data$SalePrice, predicted = predict(fit_0, train_data))
# test RMSE
rmse(actual = test_data$SalePrice, predicted = predict(fit_0, test_data))
```

This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.

```{r}
get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}
```

By using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.

```{r}
get_rmse(model = fit_0, data = train_data, response = "SalePrice") # train RMSE
get_rmse(model = fit_0, data = test_data, response = "SalePrice") # test RMSE
```

**Try it:** Apply this basic function with different arguments. Do you understand how we've nested functions within functions?


### Adding Flexibility to Linear Models
We started with the simpliest model including only a constant (which gives us only the `(Intercept)` as a coefficient). This is identical to estimating $\widehat{\bar{y}}$. But we want to let our model be more complex -- we want to add variables, polynomial terms, and interactions. Let's do this.

**Try it:** (This is Exercise 2, Question 1, so probably a good idea to do this) Using `lm()`, predict `SalePrice` with no fewer than five nested models of increasing complexity (each new model must include all of the terms from the previous, plus something new in the predictor). Here, we probably want to use interactions and polynomial terms, but make sure your models are still nested! Call them `fit_1` to `fit_5`. It may be easiest to write out the formulas in your code rather than automating as in Exercise 1.

Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting. To better understand the relationship between train RMSE, test RMSE, and model complexity, we'll explore further.

Hopefully, you tried the in-line excercise above. If so, we can create a list of the models fit.

```{r, eval=FALSE}
model_list = list(fit_0, fit_1, fit_2, fit_3, fit_4, fit_5)
```

We then obtain train RMSE, test RMSE, and model complexity for each using our old friend `sapply()`. 
```{r, eval=FALSE}
train_rmse = sapply(model_list, get_rmse, data = train_data, response = "SalePrice")
test_rmse = sapply(model_list, get_rmse, data = test_data, response = "SalePrice")
model_complexity = sapply(model_list, get_complexity)
```

Once you've done this, you'll notice the following:

```{r, eval=FALSE}
# This is the same as the sapply command above

test_rmse = c(get_rmse(fit_0, test_data, "SalePrice"),
              get_rmse(fit_1, test_data, "SalePrice"),
              get_rmse(fit_2, test_data, "SalePrice"),
              get_rmse(fit_3, test_data, "SalePrice"),
              get_rmse(fit_4, test_data, "SalePrice"),
              get_rmse(fit_5, test_data, "SalePrice"))
```

We can plot the results. If you execute the code below, you'll see the train RMSE in blue, while the test RMSE is given in orange.[^3]

[^3]: The train RMSE is guaranteed to follow this non-increasing pattern as long as no data is being dropped when new variables are added (see [Data Cleaning](#Data Cleaning) above). The same is not true of test RMSE. We often see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next week. We will discuss how we can help create this U-shape much later. Also, we might intuitively expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get (un)lucky and this will not be true.

```{r, eval=FALSE}
plot(model_complexity, train_rmse, type = "b",
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02,
              max(c(train_rmse, test_rmse)) + 0.02),
     col = "dodgerblue",
     xlab = "Model Size",
     ylab = "RMSE")
lines(model_complexity, test_rmse, type = "b", col = "darkorange")
```

We could also summarize the results as a table. `fit_1` is the least flexible, and `fit_5` is the most flexible. If we were to do this (see the exercise below) we would see that Train RMSE decreases as flexibility increases forever. However, this may not be the case for the Test RMSE.

| Model   | Train RMSE        | Test RMSE        | Predictors              |
|---------|-------------------|------------------|-------------------------|
| `fit_1` | RMSE$_{\text{train}}$ for model 1 | RMSE$_{\text{test}}$ for model 1 | put predictors here|
| ...| ... | .... | ... |
| `fit_5` | RMSE$_{\text{train}}$ for model 5  | RMSE$_{\text{train}}$ for model 5  | $p$ predictors |


To summarize:

- **Underfitting models:** In general *High* Train RMSE, *High* Test RMSE.
- **Overfitting models:** In general *Low* Train RMSE, *High* Test RMSE.

Specifically, we say that a model is overfitting if there exists a less complex model with lower **Test** RMSE.[^2] Then a model is underfitting if there exists a more complex model with lower Test RMSE.

[^2]: The labels of under and overfitting are *relative* to the best model we see. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.

:::fyi

__EXERCISES__

1. Using `lm()` and any number of regressors, predict `SalePrice` with no fewer than five models of increasing complexity (as in the try-it above). Complexity can be increased by adding variables *or* by adding interactions or polynomials of existing variables. Put the five models into a list.

2. Calculate the Train and Test RMSE. Your goal is to have a lower *Test* RMSE than others in the class. 

3. Make a table exactly like the table above for the 5 models you just fit. The first column should have the name of the model (e.g. `fit_1`). Hint: you can get the names of the entries in a list using `names(model_list)` provided you named the list items when you added them.

4. In a short paragraph, describe the resulting model. Discuss how you arrived at this model, what interactions you're using (if any) and how confident you are that your prediction will perform well, relative to other people in the class.

5. Use a shrinkage method applied to the approach above to try to find a restricted model that does almost as well as your model. 

:::

A final note on the analysis performed here; we paid no attention whatsoever to the "assumptions" of a linear model. We only sought a model that **predicted** well, and paid no attention to a model for **explaination**. This is especially true if we interacted variables without any theory as to why they might need to be interacted -- "exterior type interacted with number of half-baths" may help with RMSE, but it certainly isn't grounded in a story that a real estate agent might tell. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don't care. Assumptions? Still don't care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)
