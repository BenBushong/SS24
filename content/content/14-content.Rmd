---
title: "Geospatial with R"
linktitle: "14: Geospatial with R"
read_date: "2024-04-16"
menu:
  content:
    parent: Course content
    weight: 2
type: docs
output:
  blogdown::html_page:
    toc: true
---

## Required Reading

- This page.

### Guiding Questions

- What are the building blocks of geospatial data?
- How do we handle uniquely geospatial properties like distance or spatial correlation?


# Geospatial in R

We will need a handful of new packages for our introduction to geospatial analysis in R. The primary package we will interact with is the `sf` package. `sf` stands for "simple features." It has become the standard for geospatial work in R, and relies on the `rgeos` and `rgdal` libraries (which are themselves `R` compilations of the `geos` and `gdal` libraries). Documentation of sf [can be found here](https://r-spatial.github.io/sf/). 

We will also use the `mapview` package, as well as the `tmaptools` package. Plus, we'll use `tigris` to get state boundaries and `tidycensus` to pull down census maps. Install those, and any of the many dependencies that they also install. 


```{r, echo=TRUE, message = F, warning = F}
library(sf)
library(mapview)
library(tigris)
library(tidycensus)
library(tidyverse)
library(tmaptools)
```

## What is spatial data?
Spatial data is any data that contains a reference to a defined area, usually a physical area with a geographic definition The linkages between some data (e.g. "population in 1,000's") and a defined geographic area (e.g. "state") go beyond simply including a column that contains the name of the defined area. The physical properties of that area are included. For instance, if we have the *physical* area corresponding to a state, then we can also find neighboring states. Simply looking at a state name does not tell us which states share boundaries -- this is a spatial property. Of course, we can look up a state's neighbors easily because maps are commonplace, but more obscure geographies (think "small rivers" or areas with common soil types) won't be as easy. Thus, we might want to be able to link different variables via their spatial data.

Spatial data can be stored in three ways: point, line, or polygon. Point data is usually stored as a latitude + longitude pair, usually in decimal degrees (though sometimes in other formats, like meters from a common centerpoint). Line data (think of things like highways or railroad tracks) is stored as a vector of lat+lon pairs, where each point defines a portion of a line segment. Polygons are used to store areas -- e.g. a city boundary, a residential housing lot, or a geographic region (where a rough "region" is represented as a well-defined area).

**R** and `sf` store spatial data in a "tidy" way: each point, line, or polygon is an observation. We may have multiple variables for each observation, each contained in a separate column and each of the referring to the same spatial point, line, or polygon. Finally, for the spatial data, a special additional columnn titled `geometry` will be present, and each row of the `geometry` column will contain the data necessary to identify the point, line, or polygon.

For points, the `geometry` column will simply hold two numbers in each row corresponding to the latitude and longitude. The data itself will contain information about how to interpret those numbers, and where they fall on a map. The definition can be abstract -- it can refer to any "cartesian coordinates" (X and Y coordinates), but usually it'll refer to coordinates that are located somewhere on earth. For lines, the row of the `geometry` column will hold multiple lat+lon points, *all in the same row* and saved in a list. For polygons that refer to an area, each row of `geometry` will contain all the points that define the polygon, and some information on how to draw the polygon (and how to make any "holes" in it, when needed). *All of this happens behind the scenes* and you don't need to know how it works. If you're interested, our course site's Resources page has a short primer titled "Spatial Basics".

As we'll see, holding the "geometry" (points, lines, or polygons) in each row lets us to spatial operations, like summarizing (which, in spatial terms, combines areas or points) and joining (which, in spatial terms, lets us relate points/lines/polygons to other points/lines/polygons). That's what we'll do today.

# Reading spatial data
While it's fun to draw our own shapes (caution: my definition of fun $\neq$ your definition of fun), we're probably most interested in finding and using existing spatial data. Let's talk briefly about the types of spatial data out there:

- Shapefiles
  - Shapefiles are not actually single files - they're usually 4-6 files with similar names and different suffixes like .dbf, .shx, etc. This is because the shapefile format kind of pre-dates our current way of thinking of file storage. The most common program for reading or making shapefiles is ESRI's ArcGIS. It is expensive, cumbersome, and some may say bloated. Our goal in this section is to be able to rescue shapefiles from the clutches of ArcGIS and open them in R
  
- GEOJSON
  - JSON is a way of structuring text data (like a .csv) but with the potential for nests in the data (like our `list` object) where each nest has a different data structure. GEOJSON pairs this with **WKT** or Well-Known Text representations of coordinates and takes care of making sure that each observation in the JSON data has a corresponding polygon in WKT coordinates.
  
- KML
  - Bare-bones storage of coordinates and basic data
  
- .RDS
  - Okay, this is just R's native data type for storage, but it's really helpful for storing `sf` objects
  
- Comma separated values (.csv)
  - Just like the CSV's we've been using, but with Latitude and Longitude columns. Only works for points (one point per .csv line), but is very commonly found. We can use `st_as_sf` to tell R which columns are the latitude and longitude.

We can open and use any one of these filetypes. I will cover Shapefiles and GEOJSON as the latter has become a very popular way of sharing spatial datasets.

### Where to find spatial data
Spatial data is all around us! Try searching google for a topic + "spatial shapefile". One of my favorite sources for spatial data is the DHS [HIFLD Open database](https://hifld-geoplatform.opendata.arcgis.com/), which has lots of government datasets that are well-organized by category. Click through, and when you find something you like, click the "Download" button. If there is a GEOJSON or KML file available, **right-click** and copy the link address, or click on **full details** and scroll down to *I want to...View API Resources* and copy the GEOJSON link. Then, use that with `st_read()`. On many maps (including this one), the GeoJSON link is shown under the API drop-down. Use GeoJSON over KML as some systems have issues importing the data fields in KML. 

Increasingly, it's getting harder and harder to find direct links. You may have to download data, then load it in from your local drive.

### Loading the data
We'll use the `sf` package's `st_read` to open spatial data. Here, I'm loading the Natural Gas Processing Plants data from the Energy section of HIFLD. I'm using the **GeoJSON** option, which `st_read()` knows how to handle:

```{r}
gasplants = st_read('https://opendata.arcgis.com/datasets/ca984888f8154c63bf3a023f0a1f9ac2_0.geojson') %>%
  dplyr::select(name = NAME)

head(gasplants)
```

The `sf` data type holds both the data (which here is just the name of the plant) *and* the "geometry", which are the points. It's tidy data - one row is one observation of one plant, and each row has a set of coordinates telling us where to find the plant. The `geometry` column contains **points** as this is point data. While these gas plants do likely have some area, we are not really interested in the parcel of land they occupy, we think of them (and hold them in data) as points.

We can use ggplot with `geom_sf()` to plot these points. They're just scattered across the country and we don't automatically get a background map, but here are the points

```{r plotgas1}
ggplot(gasplants) + geom_sf() + theme_minimal()
```

Well, we're missing some context here -- we can kind of make out the point of Texas down there, but it's hard to tell anything about where these plants are located. Let's use `tigris` to get a map of the US, then plot it plus the points. Note we use different `data = ` in each of the `geom_sf()` calls:

```{r plotgas2, message = F, warning = F, results = 'hide'}

US <- tigris::states(cb = TRUE, progress_bar = FALSE)  # tigris maps

ggplot() + geom_sf(data = US, col = 'gray50') +
  geom_sf(data = gasplants) + theme_minimal()

```


Getting there. The problem now is that the `tigris` data covers all US territories (and that data covers Canada), which are really spread out! Let's drop down to just Michigan. We can use good old `filter` just like with regular data:

```{r plotgas3}
MI = states(cb = TRUE) %>% dplyr::filter(STUSPS=='MI')

ggplot() + geom_sf(data = MI, col = 'gray50') +
  geom_sf(data = gasplants) + theme_minimal()


```

Well, now we have a different problem. We want to have only the `gasplants` that are `over` the state of Michigan. That requires a **spatial join**. Luckily, our `tidyverse` syntax **works pretty seamlessly on `sf` objects**. First, we have to take care of a little issue with spatial data. The **projection**

### Projections, briefly
The *projection* for spatial data is the translation from a 3-D object (e.g. the globe) to a 2-D space (a map, or the cartesian x-y coordinates of our screen). This is no simple matter! There are entire PhD programs dedicated to forming and processing projections and datum (which refer to the shape of the globe, which is not actually round). It can all be a nightmare. Worst of all, projections determine the definition of your coordinates, so you may be at -81 latitude, +30 longitude, but in another projection, you might be 1245349m above some reference point, and -2452849m to the left of that point. Projections define the distance along the X and Y axis, the scale of the coordinates, and a lot of other stuff about your 2-D polygons.

Luckily, over the last few years, very smart people have been working on regularizing "projections". Now, we really need to know three things:

1. The *projection* of your data's coordinates when you read it in
2. The *projection* you want your data to be in when you map it
3. The *projection* of other spatial data you may want to combine.

#### Importing projected data
GEOJSON, shapefiles, and KML files usually come with embedded projections stored as **EPSG** numbers like '4326' (incidentally, '4326' is the usual projection for GPS coordinates). Thus, the first one is usually already taken care of. If your data doesnt have a PROJ4 or EPSG number but the coordinates are all between -180 and +180, it's likely in EPSG=4326. If none of those, then the data creator should have *metadata* stating the proejction. It might take some googling and some trial and error.

For mapping, you might need to *transform* your data between projections (or "reproject" it, same thing). We use `st_transform` for this. We only need to give `R` the EPSG (Geodetic Parameter Dataset) of the projection you want to end up in. As long as it's already in a known projection, `R` can re-project it. The projection is refered to by the Coordinate Reference System (CRS). `st_crs` will tell us the projection (EPSG number and a lot more) of any spatial object. If they do not match, then `R` will give an error or, worse, plot them on totally different scales - sometimes you end up with points from the US landing in the middle of the Indian Ocean! In fact, look back at our map of gas plants and the US. 


```{r}
st_crs(gasplants)
st_crs(MI)
```
One is in 4326, the other in 4269. We can use `st_transform` on the `gasplants` data, which will reproject the points (and won't change the data at all). The data won't be any different, and the points won't look too much different

```{r plotgas4}
gasplants = gasplants %>%
  st_transform(st_crs(MI))

ggplot(gasplants) + geom_sf() + theme_minimal()
```

#### Importing unprojected data
Sometimes, we have data only in .csv format, but with X and Y coordinates (e.g. Longitude and Latitude). To import this data, we do the following:

- Read in from .csv, .xls, etc.
- Determine the CRS of the data (usually 4326 for gps coordinates)
- Set the spatial coordinates and CRS


We know how to do the first, and the 2nd and 3rd are done in one step. We'll make a data.frame of city names and use the `tmaptools` package's `geocode_OSM` to get the latitudes and longitudes of the city centers. This function uses open-source Open Street Maps instead of the google API (which is used by `ggmap`). This way, we don't need an API key.

```{r plotcity1}

ourCities = geocode_OSM(c('Detroit','Lansing','Grand Rapids','Kalamazoo','Traverse City','Marquette')) %>%
  select(City = query, lat = lat, lon = lon)

head(ourCities)
```

Since these are GPS-type coordinates, we are going to assume the CRS is EPSG=4326. Longitude is the "x" axis, and latitude is the "y" axis.

```{r}
ourCities.spatial = st_as_sf(ourCities, coords = c('lon','lat'), crs = 4326)
head(ourCities.spatial)
```

Now we have the point geometries! We can map this:

```{r plotcity2}
ggplot() +
  geom_sf(data = MI, fill = 'gray90') +
  geom_sf(data = ourCities.spatial, col = 'blue') +
  theme_minimal()
```

## Spatial merges
Combining spatial data is the strength of geospatial analysis. We have our map of MI, and we have out points. Let's "merge" the points to the map, meaning let's connect the elements in our map (the state of MI) to the elements in our points (gas plants). This is a point-to-polygon merge.

### Point-to-polygon merges
We'll use `st_join` to produce an inner join, so we keep only those points that are "in" (spatially) the state of Michigan. I'm specifying `join = st_intersects` though this is the default. Note that all the points that remain in the merged `MI.gasplants` are in Michigan, and note that all the data columns from `MI` are now in `gasplants`. We'll use a county map of MI here so we will have the *county* data for each county containing a gas plant.

```{r plotgas10, warning = F, message = F, result = 'hide'}

MI.counties = counties(state = 'MI', cb = TRUE, progress_bar = FALSE)

MI.gasplants = gasplants %>%
  st_transform(st_crs(MI.counties)) %>%
  st_join(MI.counties, left = FALSE,
          join = st_intersects)

head(MI.gasplants)
```


Now, we can plot the counties map with the gasplants over it. We can even use `aes(...)` to `fill` the counties:

```{r plotgas11}
ggplot() + 
  geom_sf(data = MI.counties, aes(fill = NAME), show.legend = F) +  
  geom_sf(data = MI.gasplants) + 
  theme_minimal()  
```

Huh. Most gas plants in Michigan are to the north of here. Interesting.

### Mapview
Sometimes, we want to be able to zoom in and out. `ggplot` is static, so that won't work too well. Thanks to the `leaflet` engine, the `mapview` packages is stellar for exploration of spatial data. You can specify `zcol = Name` if you want to color by the `Name` field. I can't embed this in the website, but you can run this at home. It will appear in the "Viewer" pane, not in the "Plots" pane. Unlike the static image here, you will be able to zoom and pan.

```
mapview(MI.gasplants)
```

```{r, echo = F, warning=F, message = F}
knitr::include_graphics('img/MIgas.png')
```

In an actual mapview window (not this static image here), clicking on the points or polygons will bring up a pop-up of the data for that row. Mapview is very useful for *exploring* your spatial data. It is **not** useful for presenting your data. Please, never use `mapview` as an output from a markdown code chunk. Use it for exploring, then use `geom_sf()` to present your analysis.

While Mapview has many features that you will likely find interesting, one of the more useful features is that you can set the display color, which helps for data exploration. 
```
mapview(MI.counties, zcol = 'NAME')
```

This will set each county in Michigan to a different color (using the `viridis` colors). Another very useful feature of `mapview` is that you can layer plots with `+`. Again, I'm including a static image. Using this command in Rstudio will open an interactive map:

```
mapview(MI.gasplants) + mapview(MI.counties, zcol = 'NAME')
```

```{r, echo = F, warning=F, message = F}
knitr::include_graphics('img/mapview_ex2.png')
```


### Polygon-to-polygon merges
The gas plants and state merge, above, was very simple because points are always either within or not within a polygon. Worst that can happen is some of your points are not over any polygon at all (resulting in `NA` values). But what if you're merging polygons to polygons?

First, let's load some (overlapping) polygons. We can load up all of our states again (dropping the territories). We'll also use a map of watersheds (which cross state boundaries). This is the HUC-4 map of the Rockies from the [US Geological Survey](https://hub.arcgis.com/datasets/7f8632f3e3114623b4f5c8f97d935dca_1?geometry=-157.095%2C32.305%2C-73.379%2C44.324). The HUC-4 is a definition of a watershed where the area of the HUC-4 is the area drained by a major tributary:

```{r, message =F, warning=F}
US = states(cb=TRUE) %>%
  dplyr::filter(!STUSPS %in% c('PR','GU','VI','MP','AS','AK','HI'))

HUC4 = st_read('https://opendata.arcgis.com/datasets/7f8632f3e3114623b4f5c8f97d935dca_1.kml') %>%
  st_transform(st_crs(US)) %>%
  dplyr::mutate(randomData = rpois(n(), 20))

ggplot() + 
  geom_sf(data = US, col = 'gray50') +
  geom_sf(data = HUC4, aes(fill = Name), show.legend = FALSE) + 
  theme_minimal()


```

These watersheds clearly overlap state boundaries. So what happens if we merge them? `sf` will create a new obsveration (row) for every HUC-4 / State combination

```{r HUC1, message = F, warning = F}
poly.merge = HUC4 %>%
  st_join(US, left = TRUE)

head(poly.merge)
```

Now, every HUC-4 like "Lower Colorado" has multiple observations, one for each STUSPS that it touches. When we plot it, though, each of those observations are still attached to the same HUC-4 polygon. Technically, R is plotting the same HUC multiple times (once for each state) on top of each other, so we don't see them. This is the equivalent of merging your data in a way that duplicates observations.

```{r HUC2, message = F, warning=F}
ggplot(poly.merge) + geom_sf() + 
  theme_minimal()
```

We have another option in our join - we can ask `st_join` to keep just the `largest`:

```{r HUC3, message = F, warning = F}
poly.merge.largest = HUC4 %>%
  st_join(US, left = TRUE, largest = TRUE)

head(poly.merge.largest)
```
Now, there is only *one* observation per HUC-4, and it corresponds to the state that has the most overlap area-wise. For Lower Colorado, Arizona has the most overlap. There are lots of things besides `st_intersect` we can use to call two things "joined". `?st_join` tells you about them. For instance, we can use `join = st_covers` and we will only get a merge when HUC-4 completely covers the state.

```{r HUC4, message = F, warning = F}
HUC4 %>%
  st_join(US, left = TRUE, join = st_covers) %>%
  head()
```

None of the HUC-4's completely cover a state, so we get `NA` for all the state data.

The other thing we can do is ask `R` to create separate polygons - one for every HUC-4 / state combination. That isn't a merge, but it plays a similar role. Note this uses `st_intersection`:

```{r HUC5, message = F, warning = F}
poly.int = HUC4 %>%
  st_intersection(US) %>%
  arrange(Name)

head(poly.int)
```

Now, we have a unique polygon for every combination of HUC-4 and State (with the US):

```{r HUC6}
ggplot() + 
  geom_sf(data = US, fill = 'gray50') +
  geom_sf(data = poly.int, aes(fill = STUSPS), show.legend = FALSE) +
  theme_minimal()
```

Here, I've set the fill to the state, but you can see that the HUC-4's have boundaries at the state line.

### Summarizing 
Our `summarize` function let us collapse by groups and calculate interseting things like average (over a group or region). The neat part is that *it works on spatial data as well*. Let's look at the data again:

```{r}
head(poly.int %>% 
       dplyr::select(Name, randomData, STUSPS) %>%
       arrange(STUSPS))
```

So AZ has two HUC-4's in it - Lower Colorado and Lower Coloardo - Lake Mead (you can see them above). Summarize on geospatial data works just like regular data - we can `group_by(STUSPS)`, and we can `summarize()` any of the data. I threw some random data into `HUC-4` so we can summarize that. 

But how do we combine data specific to each HUC-4 in AZ? We could:

- Just take the average of all of the `randomData` values within the state.

- Take a weighted average of `randomData` where the *area* is the weight

- Take some other function (min, max, etc.) of `randomData`.

We can implement any of these using `sf`. Let's do the second since it nests the first. First, we'll add the area of the State x HUC-4 using `st_area`, which gives a `units` object. We can turn that into a numeric:

```{r HUC7, message = F, warning = F}
poly.int.summary = poly.int %>%
  dplyr::mutate(State.HUC.area = as.numeric(st_area(.))) %>%
  group_by(STUSPS) %>%
  dplyr::summarize(mean.randomData = weighted.mean(randomData, w = State.HUC.area))

head(poly.int.summary)

```

`sf` with the `tidyverse` makes it really easy to apply spatial versions of `summarize` and `mutate`. Very useful.

If we had wanted to just take the average (ignoring area), we'd just leave out the `w = State.HUC.area` or just used `mean`. If we had wanted to take, say, the minimum, we would use `min(randomData)` instead of `weighted.mean`. We can use whatever function we want in `summarize`, just as we did with non-spatial data.


## Cropping vs. merging
Sometimes, we wish to only crop to a region rather than merging. `sf` has the `st_crop` function to do this. Let's crop our `HUC-4` data to just the **bounding box** of the state of Nevada

```{r HUC8, echo=T, warning = F, message = F}

HUC4.nv = HUC4 %>%
  st_crop(US %>% dplyr::filter(STUSPS=='NV'))

ggplot(HUC4.nv) + geom_sf(aes(fill = Name), show.legend = F) +
  geom_sf(data = US %>% dplyr::filter(STUSPS=='NV'),  fill = NA, col = 'gray20', lwd = 3 ) +
  theme_minimal()

```


If we wanted to actually crop to the state of Nevada (`st_crop` only uses the bounding box of Nevada), we'd use `st_intersection` but do the intersection with just the state of Nevada:

```{r HUC9, echo=T, warning = F, message = F}
HUC4.nv = HUC4 %>%
  st_intersection(US %>% dplyr::filter(STUSPS=='NV'))

ggplot(HUC4.nv) + geom_sf(aes(fill = Name), show.legend = F) +
  geom_sf(data = US %>% dplyr::filter(STUSPS=='NV'),  fill = NA, col = 'gray20', lwd = 3 ) +
  theme_minimal()
```

Note that the data for each HUC-4 remains unchanged. If there were something in that data (like "population" or "area") that is specific to the entire polygon representing the HUC, then cropping the HUC outside of Nevada may lead to misleading data. The moral of the story is: be careful and thoughtful!

### Bounding boxes

This introduces a useful concept: the *bounding box*. The bounding box is defined by the closest 4 points that form a box that perfectly encloses the object (even when the object is not a rectangle). The extent of the above plot is the bounding box for Nevada.

```{r NEV1, echo=T}
Nevada.bbox = st_bbox(US %>% dplyr::filter(STUSPS=='NV'))
Nevada.bbox
```
The bounding box can be used to frame a "window" in a `ggplot` using `geom_sf()`. That is, sometimes, we want to *plot* just a subsection of a map, but we still want the data to be the whole map. Here's an example using the HOLC Redlining Maps, which were created in the 1930's and were used to segregate US housing up until the 1970's. They are available at the [University of Richmond's *Mapping Inequality* site](https://dsl.richmond.edu/panorama/redlining). We can load Lansing and Detroit using the code below:

```{r MI1}
lansing = st_read('https://dsl.richmond.edu/panorama/redlining/static/citiesData/MILansing19XX/geojson.json')

detroit = st_read('https://dsl.richmond.edu/panorama/redlining/static/citiesData/MIDetroit1939/geojson.json')


mi.redlining = bind_rows(lansing, detroit) %>%
  st_transform(st_crs(MI))

ggplot(mi.redlining) +
  geom_sf(aes(fill = grade, col = grade)) + 
  geom_sf(data = MI, fill = NA, col = 'gray50') +
  scale_fill_manual(values = c('A' = 'darkgreen', 'B' = 'blue', 'C' = 'yellow', 'D' = 'red'),
                    aesthetics = c('color','fill')) +
  theme_minimal()
```

We can tell that our polygons have plotted, but since we have the whole state of Michigan, they're almost unreadable. We need to set our window over the lower part of the lower peninsula. We'll use `coord_sf` to do this, but first we need to define a window. Since windows are almost always rectangular, we can use the `st_bbox(mi.redlining)`, but we have to pull out the xlim (xmin, xmax) and ylim (ymin, ymax):

```{r MI2}
ggplot(mi.redlining) +
  geom_sf(aes(fill = grade, col = grade)) + 
  geom_sf(data = MI, fill = NA, col = 'gray50') +
  scale_fill_manual(values = c('A' = 'green', 'B' = 'blue', 'C' = 'yellow', 'D' = 'red'),
                    aesthetics = c('color','fill')) +
  theme_minimal() +
  coord_sf(xlim = st_bbox(mi.redlining)[c(1, 3)],
           ylim = st_bbox(mi.redlining)[c(2, 4)])
```


## Distance matrices
One of the most common spatial statistics we'd use in data analytics is the *distance* matrix. If we have a set of points and we think that we can explain some data about those points (unemployment, ag production, murders per capita) based on the distance to some explanatory source (gas plants, superfund site, etc.), then we might want to include *distance to gas plants* in our model as a predictor. Frequently, we'll use inverse distance, $\frac{1}{d}$, so that closer things can have more of an impact. To do this, we need a distance matrix.

Let's combine our `gasplants` and our `ourCities` to get the distance from each of our cities to the nearest gas plant. Maybe we have city-level data on student achievements and we want to see if gas plants lower student achievement. While we would need a lot more information to make this model, we can look at what we have for now.

We will use `st_distance`, which will generate a special type of object that contains the distance information. `MI.gasplants` has `r NROW(MI.gasplants)` observations, and ourCities has `r NROW(ourCities)`, so for each row in `ourCities` we will get `r NROW(MI.gasplants)` distances, one to each gasplant. This forms a *distance matrix* where each row is an object in `ourCities` and each column is an object in `MI.gasplants`. We are going to take only a few `MI.gasplants` so we can easily view the results:

```{r MI3}
ourCities.spatial = ourCities.spatial %>% 
  st_transform(st_crs(MI.gasplants))

MI.gasplants.small = MI.gasplants[1:4,]

ourDistance = st_distance(x = ourCities.spatial, y = MI.gasplants.small)
ourDistance
```
We get a `units` matrix, which has extra properties that allow us to convert the units. The units will be in whatever the CRS of the objects is in - `st_crs(ourCities.spatial)` tells us the units are meters.

What if we wanted to find the closest gas plant to each city? That is akin to looking at each row, and finding the column that is the smallest, right? We will use `apply`, and we will note that the order of the columns is the same as the order in `MI.gasplants.small`, so we can use `MI.gasplants.small$name` to tell us the name of the closest gas plant. We will `apply` over each row (`MAR=1`) and use the `which.min` function, which returns the *index* number of the maximum column.

```{r}
max.index = apply(ourDistance, MAR = 1, which.min)
```

We can combine this index with the `MI.gasplants.small` object to get the names of the closest gas plant for each of the cities. We'll make a nice, neat tibble with the city name (in the order from ourCities.spatial), the closest gas plant name, and the distance to that plant:

```{r}
tibble(City = ourCities.spatial$City,
       Closest.gasplant = MI.gasplants.small$name[max.index],
       Distance.to.closest = ourDistance[cbind(1:length(max.index), max.index)])
  
```

But wait, what is going on in the last line of code there? Well, recall our distance matrix and `max.index`:

```{r}
ourDistance
#
max.index
```

we want to select from our distance matrix the 1st row, 3rd column; the 2nd row, 3rd column; 3rd row, 1st column; 4th row, 3rd column; 5th row, 4th column; and 6th row, 4th column. This means the row index and column index are not ranges, but are paired. Using `cbind(1:6, max.index)` makes them paired entries, and we can select specific row x column combinations that way.

### st_nearest_feature
As is common in R, there is a function that will get the closest points between to spatial objects. `st_nearest_points` takes two geometries, and returns  the neaarest point in `y` for every point in `x`, which is what we did with the MI gas plants.

```{r MI5}
st_nearest_feature(x = ourCities.spatial, y = MI.gasplants.small)
```
This is exactly our `max.index` and can be used on the `y` object, `MI.gasplants.small`, to pull the names, subset, get distances etc.

`st_nearest_feature` also works for points and polygons, or polygons and polygons, where it returns the index of the polygon that contains the nearest point to the features in `x`. Let's find the nearest Great Lake for each of our cities using a [KML shapefile of the Great Lakes from WI DNR](https://hub.arcgis.com/datasets/wi-dnr::great-lakes?geometry=-127.301%2C37.038%2C-43.585%2C48.298) Note that the GeoJSON link is under "API" on this site. We run into a complex geometry problem, and add `st_make_valid()` to fix it: 

```{r MI6}
GL = st_read('https://opendata.arcgis.com/datasets/a8bb79fc10e64eee8c3a9db97cc5dc80_4.geojson') %>%
  st_transform(st_crs(ourCities.spatial)) %>% st_make_valid()

closest.GL.index = st_nearest_feature(x = ourCities.spatial, y = GL)
ourCities %>% dplyr::mutate(Closest.GreatLake = GL$FEAT_NAME[closest.GL.index])
```


## Other resources
- Claudia Engel's "[Using Spatial Data with R](https://cengel.github.io/R-spatial/)" is a very useful resource. It covers `sf` and an older geospatial library called `sp` that has similar functionality but was not tidyverse-friendly. 

- The [Rstudio Spatial Cheat Sheet](https://github.com/rstudio/cheatsheets/raw/master/sf.pdf).

  - There are [lots of useful RStudio cheat sheets, actually.](https://www.rstudio.com/resources/cheatsheets/)

- If you want to remove the Great Lakes from your map, here's how you do it

  - First, retrieve the geojson file of the great lakes. Only WI DNR seems to have a public shapefile of the Great Lakes (an informal complaint has been registered with MI-EGLE, grrrr) which you can download as a [geojson from here](https://data-wi-dnr.opendata.arcgis.com/datasets/great-lakes/explore) -- see the little cloud download button, and select geojson. This one doesn't have a link to take it directly, so download the file to your local drive, keep it with your .RMD file, and load it direction from your local path.
  
  - Second, use the code below as an example. You may have a different shapefile you want to clip to the Great Lakes -- put it in place of `MI` below. Note that we use a spatial version of the `difference` function, which gives us "everything in the first that isn't in the second". 
  
```
library(sf)
library(tidyverse)
library(tigris)


MI = counties(state = 'MI', year = 2019) # your school district map here
GreatLakes = st_read('/PUT YOUR LOCAL FILEPATH TO THE GEOJSON HERE.geojson') %>%  # put your local filepath in here
  st_transform(st_crs(MI)) %>%  # need them to be in the same projection
  st_make_valid() %>%  # this fixes an issue with the shapefile after projection
  st_union()  # we will put all the lakes together as we don't care which lake is being used to erase county/district boundaries


MIx = st_difference(MI, GreatLakes)  # spatial version of the difference function
```

