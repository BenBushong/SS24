[{"authors":["Ben"],"categories":null,"content":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron. Prior to coming to Michigan State University, I worked with the U.S. Army to help soldiers become more psychologically resilient.\nI hold a Ph.D. in Social Science (Economics) from the California Institute of Technology (Caltech), and a B.S. in Economics from the University of Oregon.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592849563,"objectID":"bf008f22d9b0754cde4f6972811c28b7","permalink":"https://ssc442kirkpatrick.netlify.app/authors/ben/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ben/","section":"authors","summary":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron.","tags":null,"title":"Ben Bushong","type":"authors"},{"authors":["Justin"],"categories":null,"content":"I am an Assistant Professor at Michigan State University who has not yet filled out this description.\nI hold a Ph.D. in Environmental Economics from Duke University, an M.E.M. in Environmental Policy and Economics from Duke University, and a B.S. in Environmental Policy Analysis and City Planning from the University of California at Davis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1607463658,"objectID":"8422260f0f3251af15c00666a8df9838","permalink":"https://ssc442kirkpatrick.netlify.app/authors/justin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/justin/","section":"authors","summary":"I am an Assistant Professor at Michigan State University who has not yet filled out this description.\nI hold a Ph.D. in Environmental Economics from Duke University, an M.E.M. in Environmental Policy and Economics from Duke University, and a B.S. in Environmental Policy Analysis and City Planning from the University of California at Davis.","tags":null,"title":"Justin Kirkpatrick","type":"authors"},{"authors":null,"categories":null,"content":"\rIn these (modestly) secret pages, I’ve included some resources for those who read the syllabus closely.\nIf you’re stuck with anything or want help with, say, using markdown, you’ll find some basic guidance here. Additionally, there are links throughout to outside resources.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1641494647,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"https://ssc442kirkpatrick.netlify.app/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"In these (modestly) secret pages, I’ve included some resources for those who read the syllabus closely.\nIf you’re stuck with anything or want help with, say, using markdown, you’ll find some basic guidance here. Additionally, there are links throughout to outside resources.","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":"\rThis section contains the content covered in Thursday lectures and some annotated R code that you can use as a reference for creating your own work. The intention is that in the Content section, you will sequentially build up your understanding of R and data analytics; here, you can see how all the pieces work together.\nVisit this section after you have finished the readings in the Content section and any supplemental lecture videos.\n](https://mediaspace.msu.edu/channel/SSC442+-+Spring+2021+-+KIRKPATRICK/199607633/subscribe). --\r","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1661170210,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"https://ssc442kirkpatrick.netlify.app/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"This section contains the content covered in Thursday lectures and some annotated R code that you can use as a reference for creating your own work. The intention is that in the Content section, you will sequentially build up your understanding of R and data analytics; here, you can see how all the pieces work together.\nVisit this section after you have finished the readings in the Content section and any supplemental lecture videos.","tags":null,"title":"Examples","type":"docs"},{"authors":null,"categories":null,"content":"\rEach week has a set of required readings that you should complete before coming to the (online) Tuesday lecture. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due). Don’t forget your weekly writing in between, due Saturday at 11:59pm.\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts. These questions can serve as helpful starting places for your thinking; they are not representative of the totality of the content and are not intended to be limiting. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read.\n](https://mediaspace.msu.edu/channel/SSC442+-+Spring+20221+-+KIRKPATRICK/199607633/subscribe). --\r","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1661811829,"objectID":"1413f960ec974b9863bc45d887efa8bd","permalink":"https://ssc442kirkpatrick.netlify.app/content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Each week has a set of required readings that you should complete before coming to the (online) Tuesday lecture. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due). Don’t forget your weekly writing in between, due Saturday at 11:59pm.","tags":null,"title":"Readings, lectures, and videos","type":"docs"},{"authors":null,"categories":null,"content":"\r\rIntroduction to Examples\r\rGetting started with R and RStudio\rThe R console\rScripts\rRStudio\r\rThe panes\rKey bindings\rRunning commands while editing scripts\r\rInstalling R packages\rRmarkdown\rLecture Video\r\r\r\rIntroduction to Examples\rExamples in this class are designed to be presented in-class. Accordingly, the notes here are not comprehensive. Instead, they are intended to guide students through\nI’m also aware that my writing is dry and lifeless. If you’re reading this online without the advantage of seeing it in person, don’t worry—I’ll be “funnier” in class.1\nGetting started with R and RStudio\rR is not a programming language like C or Java. It was not created by software engineers for software development. Instead, it was developed by statisticians as an interactive environment for data analysis. You can read the full history in the paper A Brief History of S2. The interactivity is an indispensable feature in data science because, as you will soon learn, the ability to quickly explore data is a necessity for success in this field. However, like in other programming languages, you can save your work as scripts that can be easily executed at any moment. These scripts serve as a record of the analysis you performed, a key feature that facilitates reproducible work. If you are an expert programmer, you should not expect R to follow the conventions you are used—assuming this will leave you disappointed. If you are patient, you will come to appreciate the unequal power of R when it comes to data analysis and data visualization.\nOther attractive features of R are:\nR is free and open source3.\rIt runs on all major platforms: Windows, Mac OS, UNIX/Linux.\rScripts and data objects can be shared seamlessly across platforms.\rThere is a large, growing, and active community of R users and, as a result, there are numerous resources for learning and asking questions4 5 6.\rIt is easy for others to contribute add-ons which enables developers to share software implementations of new data science methodologies. The latest methods and tools are developed in R for a wide variety of disciplines and since social science is so broad, R is one of the few tools that spans the varied social sciences.\r\r\rThe R console\rInteractive data analysis usually occurs on the R console that executes commands as you type them. There are several ways to gain access to an R console. One way is to simply start R on your computer. The console looks something like this:\nAs a quick example, try using the console to calculate a 15% tip on a meal that cost $19.71:7\n0.15 * 19.71 \r## [1] 2.9565\rNote that in this course (at least, on most browsers), grey boxes are used to show R code typed into the R console. The symbol ## is used to denote what the R console outputs.\n\rScripts\rOne of the great advantages of R over point-and-click analysis software is that you can save your work as scripts. You can edit and save these scripts using a text editor. The material in this course was developed using the interactive integrated development environment (IDE) RStudio8. RStudio includes an editor with many R specific features, a console to execute your code, and other useful panes, including one to show figures.\nMost web-based R consoles also provide a pane to edit scripts, but not all permit you to save the scripts for later use. On the upper-right part of this webpage you’ll see a little button with the R logo. You can access a web-based console there.\n\rRStudio\rRStudio will be our launching pad for data science projects. It not only provides an editor for us to create and edit our scripts but also provides many other useful tools. In this section, we go over some of the basics.\nThe panes\rWhen you start RStudio for the first time, you will see three panes. The left pane shows the R console. On the right, the top pane includes tabs such as Environment and History, while the bottom pane shows five tabs: File, Plots, Packages, Help, and Viewer (these tabs may change in new versions). You can click on each tab to move across the different features.\nTo start a new script, you can click on File, then New File, then R Script.\nThis starts a new pane on the left and it is here where you can start writing your script.\n\rKey bindings\rMany tasks we perform with the mouse can be achieved with a combination of key strokes instead. These keyboard versions for performing tasks are referred to as key bindings. For example, we just showed how to use the mouse to start a new script, but you can also use a key binding: Ctrl+Shift+N on Windows and command+shift+N on the Mac.\nAlthough in this tutorial we often show how to use the mouse, we highly recommend that you memorize key bindings for the operations you use most. RStudio provides a useful cheat sheet with the most widely used commands. You might want to keep this handy so you can look up key-bindings when you find yourself performing repetitive point-and-clicking.\n\rRunning commands while editing scripts\rThere are many editors specifically made for coding. These are useful because color and indentation are automatically added to make code more readable. RStudio is one of these editors, and it was specifically developed for R. One of the main advantages provided by RStudio over other editors is that we can test our code easily as we edit our scripts. Below we show an example.\nLet’s start by opening a new script as we did before. A next step is to give the script a name. We can do this through the editor by saving the current new unnamed script. To do this, click on the save icon or use the key binding Ctrl+S on Windows and command+S on the Mac.\nWhen you ask for the document to be saved for the first time, RStudio will prompt you for a name. A good convention is to use a descriptive name, with lower case letters, no spaces, only hyphens to separate words, and then followed by the suffix .R. We will call this script my-first-script.R.\nNow we are ready to start editing our first script. The first lines of code in an R script are dedicated to loading the libraries we will use. Another useful RStudio feature is that once we type library() it starts auto-completing with libraries that we have installed. Note what happens when we type library(ti):\nAnother feature you may have noticed is that when you type library( the second parenthesis is automatically added. This will help you avoid one of the most common errors in coding: forgetting to close a parenthesis.\nNow we can continue to write code. As an example, we will make a graph showing murder totals versus population totals by state. Once you are done writing the code needed to make this plot, you can try it out by executing the code. To do this, click on the Run button on the upper right side of the editing pane. You can also use the key binding: Ctrl+Shift+Enter on Windows or command+shift+return on the Mac.\nOnce you run the code, you will see it appear in the R console and, in this case, the generated plot appears in the plots console. Note that the plot console has a useful interface that permits you to click back and forward across different plots, zoom in to the plot, or save the plots as files.\nTo run one line at a time instead of the entire script, you can use Control-Enter on Windows and command-return on the Mac.\nSETUP TIP\nChange the option Save workspace to .RData on exit to Never and uncheck the Restore .RData into workspace at start. By default, when you exit R saves all the objects you have created into a file called .RData. This is done so that when you restart the session in the same folder, it will load these objects. I find that this causes confusion especially when sharing code with colleagues or peers.\n\r\r\rInstalling R packages\rThe functionality provided by a fresh install of R is only a small fraction of what is possible. In fact, we refer to what you get after your first install as base R. The extra functionality comes from add-ons available from developers. There are currently hundreds of these available from CRAN and many others shared via other repositories such as GitHub. However, because not everybody needs all available functionality, R instead makes different components available via packages. R makes it very easy to install packages from within R. For example, to install the dslabs package, which we use to share datasets and code related to this book, you would type:\ninstall.packages(\u0026quot;dslabs\u0026quot;)\rIn RStudio, you can navigate to the Tools tab and select install packages. We can then load the package into our R sessions using the library function:\nlibrary(dslabs)\r## ## Attaching package: \u0026#39;dslabs\u0026#39;\r## The following object is masked from \u0026#39;package:gapminder\u0026#39;:\r## ## gapminder\rAs you go through this book, you will see that we load packages without installing them. This is because once you install a package, it remains installed and only needs to be loaded with library. The package remains loaded until we quit the R session. If you try to load a package and get an error, it probably means you need to\rinstall it first.\nWe can install more than one package at once by feeding a character vector to this function:\ninstall.packages(c(\u0026quot;tidyverse\u0026quot;, \u0026quot;dslabs\u0026quot;))\rOne advantage of using RStudio is that it auto-completes package names once you start typing, which is helpful when you do not remember the exact spelling of the package. Once you select your package, we recommend selecting all the defaults. Note that installing tidyverse actually installs several packages. This commonly occurs when a package has dependencies, or uses functions from other packages. When you load a package using library, you also load its dependencies.\nOnce packages are installed, you can load them into R and you do not need to install them again, unless you install a fresh version of R. Remember packages are installed in R not RStudio.\nIt is helpful to keep a list of all the packages you need for your work in a script because if you need to perform a fresh install of R, you can re-install all your packages by simply running a script.\nYou can see all the packages you have installed using the following function:\ninstalled.packages()\rAs we move through this course, we will constantly be adding to our toolbox of packages. Accordingly, you will need to keep track to ensure you have the requisite package for any given lecture.\n\rRmarkdown\rMarkdown is a general-purpose syntax for laying out documents. Rmarkdown is a combination of R and markdown, as the name implies. When using markdown, one can define headers and tables using specific notation, and depending on the rendering engine, the headers and tables (and a whole lot more) are customized. In fact, this whole website is built in R using Rmarkdown (and a lot of add-ons like Hugo and blogdown). In other contexts, the rendering engine may recognize that your headers are likely to be entries in a table of contents, and does so for you. The table of contents at the top of this document is built from the markdown headers.\nThe power of Rmarkdown is that it lets us mix formatted text with R code. That is, you can have a section of the document that understands R code, and a separate section right after that discusses the results from the R code.\nTry it out using the Weekly Writing Template. If it opens in your web browser, just right-click the link and select Save As…. Make sure you save the file to its own folder on your hard drive. In converting your Rmarkdown .Rmd file to a .pdf, your system will make multiple interim files9. It also creates folders to store the output of any plots or graphics you create with your R code.\nIf we have time today, let’s open the template linked above and see what happens when we select “knit to pdf”.\n\rLecture Video\rVideo from lecture hosted on Mediaspace \n\r\r\rComments from previous classes indicate that I am not, in fact, funny.↩︎\n\rhttps://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf↩︎\n\rhttps://opensource.org/history↩︎\n\rhttps://stats.stackexchange.com/questions/138/free-resources-for-learning-r↩︎\n\rhttps://www.r-project.org/help.html↩︎\n\rhttps://stackoverflow.com/documentation/r/topics↩︎\n\rBut probably tip more than 15%. Times are tough, man.↩︎\n\rhttps://www.rstudio.com/↩︎\n\rSpecifically, knitr will create an intermediate .md file which is then processed with Pandoc using Latex to create a pdf. Whew!↩︎\n\r\r\r","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661811829,"objectID":"bbf45ee74dc37731d7fd26186d3a77a6","permalink":"https://ssc442kirkpatrick.netlify.app/example/00-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/00-example/","section":"example","summary":"Introduction to Examples\r\rGetting started with R and RStudio\rThe R console\rScripts\rRStudio\r\rThe panes\rKey bindings\rRunning commands while editing scripts\r\rInstalling R packages\rRmarkdown\rLecture Video\r\r\r\rIntroduction to Examples\rExamples in this class are designed to be presented in-class. Accordingly, the notes here are not comprehensive. Instead, they are intended to guide students through\nI’m also aware that my writing is dry and lifeless.","tags":null,"title":"Working with R and RStudio","type":"docs"},{"authors":null,"categories":null,"content":"\r\rVisualizing data distributions\r\rVariable types\rCase study: describing student heights\r\rDistribution function\rCumulative distribution functions\r\rGeometries for describing distributions\r\rHistograms\rSmoothed density\rInterpreting the y-axis\rDensities permit stratification\r\rThe normal distribution\r\rStandard units\rQuantile-quantile plots\rPercentiles\r\rggplot2 geometries\r\rBarplots\rHistograms\rDensity plots\rBoxplots\r\rLecture Videos\rTry it!\r\r\r\rVisualizing data distributions\rThroughout your education, you may have noticed that numerical data is often summarized with the average value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the standard deviation. For example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list?\nOur first data visualization building block is learning to summarize lists of factors or numeric vectors—the two primary data types that we encounter in data analytics. More often than not, the best way to share or explore this summary is through data visualization. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as a distribution, there are several data visualization techniques to effectively relay this information.\nIn this section, we first discuss properties of a variety of distributions and how to visualize distributions using a motivating example of student heights. We then discuss some principles of data visualizations more broadly, and introduce new ggplot geometries to help us along the way.\nVariable types\rWe will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.\nWhen each entry in a vector comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as ordinal data. In psychology, a number of different terms are used for this same idea.\nExamples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as population sizes, are discrete because they have to be integers—that’s how we count.1\n\rCase study: describing student heights\rHere we consider an artificial problem to help us illustrate the underlying concepts.\nPretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the heights data frame:\nlibrary(tidyverse)\rlibrary(dslabs)\rdata(heights)\rOne way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights.\nDistribution function\rIt turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.\nThe most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:\n## ## Female Male ## 0.2266667 0.7733333\rThis two-category frequency table is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:\nThis particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.\n\rCumulative distribution functions\rNumerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters, respectively.\nStatistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below \\(a\\) for all possible values of \\(a\\). This function is called the cumulative distribution function (CDF). In statistics, the following notation is used:\n\\[ F(a) = \\mbox{Pr}(x \\leq a) \\]\nHere is a plot of \\(F\\) for the male height data:\nSimilar to what the frequency table does for categorical data, the CDF\rdefines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since \\(F(66)=\\) 0.1637931, or that 84% of the values are below 72, since \\(F(72)=\\) 0.841133,\rand so on. In fact, we can report the proportion of values between any two heights, say \\(a\\) and \\(b\\), by computing \\(F(b) - F(a)\\). This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.\nA final note: because CDFs can be defined mathematically—and absent any data—the word empirical is added to make the distinction when data is used. We therefore use the term empirical CDF (eCDF).\n\r\rGeometries for describing distributions\rNow, we’ll introduce ggplot geometries useful for describing distributions (or for many other things).\nHistograms\rAlthough the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? I doubt you can figure these out from glancing at the plot above. Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret.\nThe simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: \\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\\)\nheights %\u0026gt;%\rfilter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;black\u0026quot;)\rIf we send this histogram plot to some uninformed reader, she will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, this reader could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.\nWhat information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.\nThe geom_histogram layer only requires one aesthetic mapping - the x-axis. This is because the y-axis is computed from counts of the x-axis. Giving an aesthetic mapping to an additional variable for y will result in an error. Using an aesthetic mapping like fill will work - it’ll give you two histograms on top of each other. Try it! Try setting the alpha aesthetic to .5 (not an aesthetic mapping) so you can see both layers when they overlap.\n\rSmoothed density\rSmooth density plots are aesthetically more appealing than histograms. geom_density is the geometry that gives a smoothed density. Here is what a smooth density plot looks like for our heights data:\nheights %\u0026gt;%\rfilter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;%\rggplot(aes(height)) +\rgeom_density(alpha = .2, fill= \u0026quot;#00BFC4\u0026quot;, color = \u0026#39;gray50\u0026#39;) \rIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density. That is, the area under the curve will add up to 1, so we can read it like a probability density.\nTo understand the smooth densities, we have to understand estimates, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool.\nThe main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.\nHowever, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:\nThe smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:\nThe smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts. We do this by using the double-dot object ..density... Objects surrounded by .. are objects that are calculated by ggplot. If we look at ?geom_histogram, and go down to “Computed variables”, we see that we could use ..count.. to get “number of points in a bin”; ..ncount.. for the count scaled to a max of 1; or ..ndensity.. which scales the density to a max of 1 (which is a strange one). We can manually set the y aesthetic mapping, which defaults to ..count.., to ..density..:\nx %\u0026gt;% ggplot(aes(x = height)) +\rgeom_histogram(aes(y=..density..), binwidth = 0.1, color = \u0026quot;black\u0026quot;) \rNow, back to reality. We don’t have millions of measurements. In this concrete example, we have 812 and we can’t make a histogram with very small bins.\nWe therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots (loosely) demonstrate the steps that the computer goes through to ultimately create a smooth density:\n\rInterpreting the y-axis\rNote that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:\nThe proportion of this area is about\r0.3,\rmeaning that about\r30%\rof male heights are between 65 and 68 inches.\nBy understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:\nheights %\u0026gt;%\rfilter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) +\rgeom_density(alpha=.2, fill= \u0026quot;#00BFC4\u0026quot;, color = \u0026#39;black\u0026#39;) \rNote that the only aesthetic mapping is x = height, while the fill and color are set as un-mapped aesthetics.\n\rDensities permit stratification\rAs a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities make it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:\nheights %\u0026gt;%\rggplot(aes(height, fill=sex)) +\rgeom_density(alpha = 0.2, color = \u0026#39;black\u0026#39;)\rWith the right argument, ggplot automatically shades the intersecting region with a different color.\n\r\rThe normal distribution\rHistograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.\nThe normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.\nRather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula:\n\\[\\mbox{Pr}(a \u0026lt; x \u0026lt; b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}s} e^{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2} \\, dx\\]\nYou don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: \\(m\\) and \\(s\\). The rest of the symbols in the formula represent the interval ends that we determine, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(e\\). These two parameters, \\(m\\) and \\(s\\), are referred to as the average (also called the mean) and the standard deviation (SD) of the distribution, respectively.\nThe distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:\nThe fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.\nFor a list of numbers contained in a vector x, the average is defined as:\nm \u0026lt;- sum(x) / length(x)\rand the SD is defined as:\ns \u0026lt;- sqrt(sum((x-mu)^2) / length(x))\rwhich can be interpreted as the average distance between values and their average.\nLet’s compute the values for the height for males which we will store in the object \\(x\\):\nindex \u0026lt;- heights$sex == \u0026quot;Male\u0026quot;\rx \u0026lt;- heights$height[index]\rThe pre-built functions mean and sd (note that for reasons explained later, sd divides by length(x)-1 rather than length(x)) can be used here:\nm \u0026lt;- mean(x)\rs \u0026lt;- sd(x)\rc(average = m, sd = s)\r## average sd ## 69.314755 3.611024\rHere is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:\nNow, we can ask the question “is our height data approximately normally distributed?”. The normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.\nStandard units\rFor data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value x from a vector X, we define the value of x in standard units as z = (x - m)/s with m and s the average and standard deviation of X, respectively. Why is this convenient?\nFirst look back at the formula for the normal distribution and note that what is being exponentiated is \\(-z^2/2\\) with \\(z\\) equivalent to \\(x\\) in standard units. Because the maximum of \\(e^{-z^2/2}\\) is when \\(z=0\\), this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since \\(- z^2/2\\) is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (\\(z=0\\)), one of the largest (\\(z \\approx 2\\)), one of the smallest (\\(z \\approx -2\\)), or an extremely rare occurrence (\\(z \u0026gt; 3\\) or \\(z \u0026lt; -3\\)). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.\nIn R, we can obtain standard units using the function scale:\nz \u0026lt;- scale(x)\rNow to see how many men are within 2 SDs from the average, we simply type:\nmean(abs(z) \u0026lt; 2)\r## [1] 0.9495074\rThe proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.\n\rQuantile-quantile plots\rA systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot). If our heights distribution is really normal, then the 10th percentile of our heights data should be the same as the 10th percentile of a theoretical normal, as should the 20th, 30th, 33rd, 37.5th, etc. percentiles.\nFirst let’s define the theoretical quantiles (percentiles) for the normal distribution. In statistics books we use the symbol \\(\\Phi(x)\\) to define the function that gives us the probability of a standard normal distribution being smaller than \\(x\\). So, for example, \\(\\Phi(-1.96) = 0.025\\) and \\(\\Phi(1.96) = 0.975\\). In R, we can evaluate \\(\\Phi\\) using the pnorm function:\npnorm(-1.96)\r## [1] 0.0249979\rThe inverse function \\(\\Phi^{-1}(x)\\) gives us the theoretical quantiles for the normal distribution. So, for example, \\(\\Phi^{-1}(0.975) = 1.96\\). In R, we can evaluate the inverse of \\(\\Phi\\) using the qnorm function.\nqnorm(0.975)\r## [1] 1.959964\rNote that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution. We can do this using the mean and sd arguments in the pnorm and qnorm function. For example, we can use qnorm to determine quantiles of a distribution with a specific average and standard deviation\nqnorm(0.975, mean = 5, sd = 2)\r## [1] 8.919928\rFor the normal distribution, all the calculations related to quantiles are done without data, thus the name theoretical quantiles. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector \\(x\\), we can define the quantile associated with any proportion \\(p\\) as the \\(q\\) for which the proportion of values below \\(q\\) is \\(p\\). Using R code, we can define q as the value for which mean(x \u0026lt;= q) = p. Notice that not all \\(p\\) have a \\(q\\) for which the proportion is exactly \\(p\\). There are several ways of defining the best \\(q\\) as discussed in the help for the quantile function.\nTo give a quick example, for the male heights data, we have that:\nmean(x \u0026lt;= 69.5)\r## [1] 0.5147783\rSo about 50% are shorter or equal to 69 inches. This implies that if \\(p=0.50\\) then \\(q=69.5\\).\nThe idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:\nDefine a vector of \\(m\\) proportions \\(p_1, p_2, \\dots, p_m\\).\rDefine a vector of quantiles \\(q_1, \\dots, q_m\\) for your data for the proportions \\(p_1, \\dots, p_m\\). We refer to these as the sample quantiles.\rDefine a vector of theoretical quantiles for the proportions \\(p_1, \\dots, p_m\\) for a normal distribution with the same average and standard deviation as the data.\rPlot the sample quantiles versus the theoretical quantiles.\r\rLet’s construct a QQ-plot using R code. Start by defining the vector of proportions.\np \u0026lt;- seq(0.005, 0.995, 0.01)\rTo obtain the quantiles from the data, we can use the quantile function like this:\nsample_quantiles \u0026lt;- quantile(x, p)\rTo obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the qnorm function:\ntheoretical_quantiles \u0026lt;- qnorm(p, mean = mean(x), sd = sd(x))\rdf = data.frame(sample_quantiles, theoretical_quantiles)\rTo see if they match or not, we plot them against each other and draw the identity line:\nggplot(data = df, aes(x = theoretical_quantiles, y = sample_quantiles)) + geom_point() + geom_abline() # a 45-degree line \rNotice that this code becomes much cleaner if we use standard units:\nsample_quantiles \u0026lt;- quantile(z, p)\rtheoretical_quantiles \u0026lt;- qnorm(p)\rdf2 = data.frame(sample_quantiles, theoretical_quantiles)\rggplot(data = df2, aes(x = theoretical_quantiles, y = sample_quantiles)) + geom_point() + geom_abline()\rThe above code is included to help describe QQ-plots. However, in practice it is easier to use the ggplot geometry geom_qq:\nheights %\u0026gt;% filter(sex == \u0026quot;Male\u0026quot;) %\u0026gt;%\rggplot(aes(sample = scale(height))) +\rgeom_qq() +\rgeom_abline()\rWhile for the illustration above we used 100 quantiles, the default from the geom_qq function is to use as many quantiles as data points.\n\rPercentiles\rBefore we move on, let’s define some terms that are commonly used in exploratory data analysis.\nPercentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). We call, for example, the case of \\(p=0.25\\) the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median.\nFor the normal distribution the median and average are the same, but this is generally not the case.\nAnother special case that receives a name are the quartiles, which are obtained when setting \\(p=0.25,0.50\\), and \\(0.75\\).\n\r\rggplot2 geometries\rAlhough we haven’t gone into detain about the ggplot2 package for data visualization, we now will briefly discuss some of the geometries involved in the plots above. We will discuss ggplot2 in (excruciating) detail later this week. For now, we will briefly demonstrate how to generate plots related to distributions.\nBarplots\rTo generate a barplot we can use the geom_bar geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.\nmurders %\u0026gt;% ggplot(aes(region)) + geom_bar()\rWe often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:\ndata(murders)\rtab \u0026lt;- murders %\u0026gt;%\rcount(region) %\u0026gt;%\rmutate(proportion = n/sum(n))\rtab\r## region n proportion\r## 1 Northeast 9 0.1764706\r## 2 South 17 0.3333333\r## 3 North Central 12 0.2352941\r## 4 West 13 0.2549020\rWe no longer want geom_bar to count, but rather just plot a bar to the height provided by the proportion variable. For this we need to provide x (the categories) and y (the values) and use the stat=\"identity\" option. This tells R to just use the actual value in proportion for the y aesthetic. This is only necessary when you’re telling R that you have your own field (proportion) that you want to use instead of just the count.\ntab %\u0026gt;% ggplot(aes(x = region, y = proportion)) + geom_bar(stat = \u0026quot;identity\u0026quot;)\r\rHistograms\rTo generate histograms we use geom_histogram. By looking at the help file for this function, we learn that the only required argument is x, the variable for which we will construct a histogram. We dropped the x because we know it is the first argument.\rThe code looks like this:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram()\rIf we run the code above, it gives us a message:\n\rstat_bin() using bins = 30. Pick better value with\rbinwidth.\n\rWe previously used a bin size of 1 inch (of observed height), so the code looks like this:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram(binwidth = 1)\rFinally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram(binwidth = 1, fill = \u0026quot;blue\u0026quot;, col = \u0026quot;black\u0026quot;) +\rlabs(x = \u0026quot;Male heights in inches\u0026quot;, title = \u0026quot;Histogram\u0026quot;)\r\rDensity plots\rTo create a smooth density, we use the geom_density. To make a smooth density plot with the data previously shown as a histogram we can use this code:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) +\rgeom_density()\rTo fill in with color, we can use the fill argument.\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) +\rgeom_density(fill=\u0026quot;blue\u0026quot;)\rTo change the smoothness of the density, we use the adjust argument to multiply the default value by that adjust. For example, if we want the bandwidth to be twice as big we use:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) + geom_density(fill=\u0026quot;blue\u0026quot;, adjust = 2)\r\rBoxplots\rThe geometry for boxplot is geom_boxplot. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments x as the categories, and y as the values.\nNote that our x-axis is a categorical variable. The order is determined by either the factor variable levels in heights or, if no levels are set, in the order in which the sex variable first encounters them. Later on, we’ll learn how to change the ordering.\nWe can do much more with boxplots when we have more data. Right now, our heights data has only two variables - sex and height. Let’s say we took the measurements over two different years - 2010 and 2020. That’s not in our data, so purely for exposition, we’ll add it by randomly drawing a year for each observation. We’ll do this with sample\nheights = heights %\u0026gt;%\rdplyr::mutate(year = sample(x = c(2010, 2020), size = n(), replace = TRUE, prob = c(.5, .5)))\rhead(heights)\r## sex height year\r## 1 Male 75 2020\r## 2 Male 70 2010\r## 3 Male 68 2020\r## 4 Male 74 2010\r## 5 Male 61 2020\r## 6 Female 65 2010\rNow, let’s look at the boxplot of heights by sex, but broken out by year. We can do this by adding year as an aesthetic mapping. Because our year variable is an integer, R will start by thinking it’s a continuous numeric, but we want to treat it as a discrete variable. So, we wrap it in as.factor() to force R to recognize it as a discrete variable.\nheights %\u0026gt;% ggplot(aes(x = sex, y = height, fill = as.factor(year))) +\rgeom_boxplot() +\rlabs(fill = \u0026#39;Year\u0026#39;)\rNow we have each sex broken out by year! Since we randomly assigned year to our data (and didn’t actually take samples in two different decades), the distribution between years and within sex is nearly identical.\nWhat if we wanted to have year on the x-axis, but then put the sex boxplots next to each other. This would let us compare the difference in heights by sex over the two sample years.\nheights %\u0026gt;% ggplot(aes(x = year, y = height, fill = sex)) + geom_boxplot() +\rlabs(fill = \u0026#39;Sex\u0026#39;)\rWoah. Wait. What? Remember, in our data, class(heights$year) is numeric, so when we ask R to put year on the x-axis, it thinks it’s plotting a number. It gives us a nonsense x-axis. How do we fix this? We force as.factor(year) to tell R that yes, year is a categorical variable. Note that we didn’t have to use as.factor(sex) - that’s because sex is already a categorical variable.\nheights %\u0026gt;% ggplot(aes(x = as.factor(year), y = height, fill = sex)) + geom_boxplot() +\rlabs(fill = \u0026#39;Sex\u0026#39;)\rNow we can see the height difference by sex, by year.\nWe will explore more with boxplots and colors in our next lecture.\n\r\rLecture Videos\rThe video from this lecture will be available here around 3-6 days after class \n\rTry it!\rTry it!\nTRY IT\nStart by loading the dplyr and ggplot2 library as well as the murders and heights data.\nlibrary(dplyr)\rlibrary(ggplot2)\rlibrary(dslabs)\rdata(heights)\rdata(murders)\rFirst, create a new variable in murders that has murders_per_capita.\r\rmurders = murders %\u0026gt;%\rmutate(........)\rMake a histogram of murders per capita. Use the default values for color and fill, but make sure you label the x-axis with a meaningful label.\n\rMake the same histogram, but set the fill aesthetic to MSU Green and the color to black.\n\rDo the same, but make it a smooth density plot\n\rFinally, plot the smooth density but use a fill aesthetic mapping so that each region’s density is shown. Set a meaningful title on the legend, and make sure you make the density transparent so we can see all of the region’s densities (see alpha aesthetic).\n\rNow, try making a boxplot to show the same data - the distribution across states of murders per capita by region. What is the average Northeastern state’s murder rate? What about the average Southern state?\n\r\r\r\r\r\rKeep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data.↩︎\n\r\r\r","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661170210,"objectID":"ce879375c9ab42490f4d0d112e48c07c","permalink":"https://ssc442kirkpatrick.netlify.app/example/02-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/02-example/","section":"example","summary":"Visualizing data distributions\r\rVariable types\rCase study: describing student heights\r\rDistribution function\rCumulative distribution functions\r\rGeometries for describing distributions\r\rHistograms\rSmoothed density\rInterpreting the y-axis\rDensities permit stratification\r\rThe normal distribution\r\rStandard units\rQuantile-quantile plots\rPercentiles\r\rggplot2 geometries\r\rBarplots\rHistograms\rDensity plots\rBoxplots\r\rLecture Videos\rTry it!\r\r\r\rVisualizing data distributions\rThroughout your education, you may have noticed that numerical data is often summarized with the average value.","tags":null,"title":"ggplot2: Everything you ever wanted to know","type":"docs"},{"authors":null,"categories":null,"content":"\r\rInstall R\rInstall RStudio\rInstall tidyverse\rInstall tinytex\r\r\rAs mentioned in the syllabus, you will do all of your work in this class with the open source programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—–R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nHopefully you’re well-versed in dealing with these things, but if you’re lost, here’s how you install the required software for the course.\nInstall R\rFirst you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n\rClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\rIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.\n\rIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\r\rDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n\rIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\r\r\rInstall RStudio\rNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n\rThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n\rDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\r\rDouble click on RStudio to run it (check your applications folder or start menu).\n\rInstall tidyverse\rR packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including the ever-present ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel. Hopefully you’ve experienced installing packages before now; if not, consider this a crash course!\n\rInstall tinytex\rWhen you knit to PDF, R uses a special scientific typesetting program named LaTeX.2\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB. To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console.\rRun tinytex::install_tinytex() in the console.\rWait for a bit while R downloads and installs everything you need.\rThe end! You should now be able to knit to PDF.\r\r\r\rIt’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n\rPronounced “lay-tek” for those who are correct; or “lah-tex” to those who love goofy nerdy pronunciation. Technically speaking, the x is the “ch” sound in “Bach”, but most people just say it as “k”. While either saying “lay” or “lah” is correct, “layteks” is frowned upon because it clearly shows you’re not cool.↩︎\n\r\r\r","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641494647,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"https://ssc442kirkpatrick.netlify.app/resource/install/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"Install R\rInstall RStudio\rInstall tidyverse\rInstall tinytex\r\r\rAs mentioned in the syllabus, you will do all of your work in this class with the open source programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—–R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"\r\rAccessibility\rColors\rFonts\rGraphic assets\rImages\rVectors\rVectors, photos, videos, and other assets\r\r\r\rAccessibility\r\rVischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\rColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)\r\r\rColors\r\rAdobe Color: Create, share, and explore rule-based and custom color palettes.\rColourLovers: Like Facebook for color palettes.\rviridis: Percetually uniform color scales.\rScientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with scico.\rColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account.\rColorgorical: Create color palettes based on fancy mathematical rules for perceptual distance.\rColorpicker for data: More fancy mathematical rules for color palettes (explanation).\riWantHue: Yet another perceptual distance-based color palette builder.\rPhotochrome: Word-based color pallettes.\rPolicyViz Design Color Tools: Large collection of useful color resources\r\r\rFonts\r\rGoogle Fonts: Huge collection of free, well-made fonts.\rThe Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).\r\r\rGraphic assets\rImages\r\rUse the Creative Commons filters on Google Images or Flickr\rUnsplash\rPexels\rPixabay\rStockSnap.io\rBurst\rfreephotos.cc\r\r\rVectors\r\rNoun Project: Thousands of free simple vector images\raiconica: 1,000+ vector icons\rVecteezy: Thousands of free vector images\r\r\rVectors, photos, videos, and other assets\r\rStockio\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641494647,"objectID":"16fd04c4714e3d096bffcf19e6c524ca","permalink":"https://ssc442kirkpatrick.netlify.app/resource/design/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/design/","section":"resource","summary":"Accessibility\rColors\rFonts\rGraphic assets\rImages\rVectors\rVectors, photos, videos, and other assets\r\r\r\rAccessibility\r\rVischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\rColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)\r\r\rColors\r\rAdobe Color: Create, share, and explore rule-based and custom color palettes.\rColourLovers: Like Facebook for color palettes.","tags":null,"title":"Design","type":"docs"},{"authors":null,"categories":null,"content":"\r\rBasic Markdown formatting\rMath\rTables\rFootnotes\rFront matter\rCitations\rOther references\r\r\rMarkdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting\r\r\r\r\rType…\r…or…\r…to get\r\r\r\rSome text in a paragraph.\rMore text in the next paragraph. Always\ruse empty lines between paragraphs.\r\rSome text in a paragraph.\nMore text in the next paragraph. Always\ruse empty lines between paragraphs.\n\r\r*Italic*\r_Italic_\rItalic\r\r**Bold**\r__Bold__\rBold\r\r# Heading 1\r\rHeading 1\r\r\r## Heading 2\r\rHeading 2\r\r\r### Heading 3\r\rHeading 3\r\r\r(Go up to heading level 6 with ######)\r\r\r\r[Link text](http://www.example.com)\r\rLink text\r\r![Image caption](/path/to/image.png)\r\r\r\r`Inline code` with backticks\r\rInline code with backticks\r\r\u0026gt; Blockquote\r\r\rBlockquote\n\r\r- Things in\r- an unordered\r- list\r* Things in\r* an unordered\r* list\r\rThings in\ran unordered\rlist\r\r\r1. Things in\r2. an ordered\r3. list\r1) Things in\r2) an ordered\r3) list\rThings in\ran ordered\rlist\r\r\rHorizontal line\r---\rHorizontal line\r***\rHorizontal line\n\r\r\r\r\rMath\rMarkdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n\r\r\r\rType…\r…to get\r\r\r\rBased on the DAG, the regression model for\restimating the effect of education on wages\ris $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or\r$\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$.\rBased on the DAG, the regression model for\restimating the effect of education on wages\ris \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or\r\\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).\r\r\r\rTo put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math:\r$$\rx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\r$$\rBut now we just use computers to solve for $x$.\r…to get…\n\rThe quadratic equation was an important part of high school math:\n\\[\rx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\r\\]\nBut now we just use computers to solve for \\(x\\).\n\rBecause dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n\rTables\rThere are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default\r------- ------ ---------- -------\r12 12 12 12\r123 123 123 123\r1 1 1 1\rTable: Caption goes here\r…to get…\n\rCaption goes here\r\rRight\rLeft\rCenter\rDefault\r\r\r\r12\r12\r12\r12\r\r123\r123\r123\r123\r\r1\r1\r1\r1\r\r\r\rFor pipe tables, type…\n| Right | Left | Default | Center |\r|------:|:-----|---------|:------:|\r| 12 | 12 | 12 | 12 |\r| 123 | 123 | 123 | 123 |\r| 1 | 1 | 1 | 1 |\rTable: Caption goes here\r…to get…\n\rCaption goes here\r\rRight\rLeft\rDefault\rCenter\r\r\r\r12\r12\r12\r12\r\r123\r123\r123\r123\r\r1\r1\r1\r1\r\r\r\r\rFootnotes\rThere are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags].\r[^1]: This is a note.\r[^note-on-dags]: DAGs are neat.\rAnd here\u0026#39;s more of the document.\r…to get…\n\rHere is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n\r\rThis is a note.↩︎\r\r\rDAGs are neat.↩︎\r\r\r\r\rYou can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!]\r…to get…\n\rCausal inference is neat.1\n\r\rBut it can be hard too!↩︎\r\r\r\r\r\rFront matter\rYou can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n---\rtitle: Title of your document\rdate: \u0026quot;January 13, 2020\u0026quot;\rauthor: \u0026quot;Your name\u0026quot;\r---\rYou can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n---\rtitle: \u0026quot;My cool title: a subtitle\u0026quot;\r---\rIf you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n---\rtitle: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39;\r---\r\rCitations\rOne of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n---\rtitle: Title of your document\rdate: \u0026quot;January 13, 2020\u0026quot;\rauthor: \u0026quot;Your name\u0026quot;\rbibliography: name_of_file.bib\r---\rChoose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n---\rtitle: Title of your document\rdate: \u0026quot;January 13, 2020\u0026quot;\rauthor: \u0026quot;Your name\u0026quot;\rbibliography: name_of_file.bib\rcsl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot;\r---\rSome of the most common CSLs are:\n\rChicago author-date\rChicago note-bibliography\rChicago full note-bibliography (no shortened notes or ibids)\rAPA 7th edition\rMLA 8th edition\r\rCite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n\r\r\r\rType…\r…to get…\r\r\r\rCausal inference is neat [@Rohrer:2018; @AngristPischke:2015].\rCausal inference is neat (Rohrer 2018; Angrist and Pischke 2015).\r\rCausal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1].\rCausal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).\r\rAngrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018].\rAngrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).\r\r@AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees.\rAngrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.\r\r\r\rAfter compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n\rAngrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n\r\r\rOther references\rThese websites have additional details and examples and practice tools:\n\rCommonMark’s Markdown tutorial: A quick interactive Markdown tutorial.\rMarkdown tutorial: Another interactive tutorial to practice using Markdown.\rMarkdown cheatsheet: Useful one-page reminder of Markdown syntax.\rThe Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.\r\r\r","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641494647,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"https://ssc442kirkpatrick.netlify.app/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Basic Markdown formatting\rMath\rTables\rFootnotes\rFront matter\rCitations\rOther references\r\r\rMarkdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting\r\r\r\r\rType…\r…or…\r…to get\r\r\r\rSome text in a paragraph.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"\r\rInteresting and excellent real world examples\rHow to select the appropriate chart type\rGeneral resources\rVisualization in Excel\rVisualization in Tableau\r\r\rInteresting and excellent real world examples\r\rThe Stories Behind a Line\rAustralia as 100 people: You can make something like this with d3 and the potato project.\rMarrying Later, Staying Single Longer\r\r\rHow to select the appropriate chart type\rMany people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:\n\rThe Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations.\rThe Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.).\rFrom Data to Viz: A decision tree for dozens of chart types with links to R and Python code.\rThe Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R.\rR Graph Catalog: R code for 124 ggplot graphs.\rEmery’s Essentials: Descriptions and examples of 26 different chart types.\r\r\rGeneral resources\r\rStorytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic.\rAnn K. Emery’s blog: Blog and tutorials by Ann Emery.\rEvergreen Data: Helful resources by Stephanie Evergreen.\rPolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch.\rVisualising Data: Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk.\rInfo We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field.\rFlowingData: Blog by Nathan Yau.\rInformation is Beautiful: Blog by David McCandless.\rJunk Charts: Blog by Kaiser Fung.\rWTF Visualizations: Visualizations that make you ask “wtf?”\rThe Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic.\rData Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway.\rSeeing Data: A series of research projects about perceptions and visualizations.\r\r\rVisualization in Excel\r\rHow to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel.\rAnn Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel.\r\r\rVisualization in Tableau\rBecause it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641494647,"objectID":"ca403ba352e0871f06b445d2470037b3","permalink":"https://ssc442kirkpatrick.netlify.app/resource/visualization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/visualization/","section":"resource","summary":"Interesting and excellent real world examples\rHow to select the appropriate chart type\rGeneral resources\rVisualization in Excel\rVisualization in Tableau\r\r\rInteresting and excellent real world examples\r\rThe Stories Behind a Line\rAustralia as 100 people: You can make something like this with d3 and the potato project.\rMarrying Later, Staying Single Longer\r\r\rHow to select the appropriate chart type\rMany people have created many useful tools for selecting the correct chart type for a given dataset or question.","tags":null,"title":"Visualization","type":"docs"},{"authors":null,"categories":null,"content":"\r\rKey terms\rAdd chunks\rChunk names\rChunk options\rInline chunks\rOutput formats\r\r\rR Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms\r\rDocument: A Markdown file where you type stuff\n\rChunk: A piece of R code that is included in your document. It looks like this:\n```{r}\r# Code goes here\r```\rThere must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n\rKnit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n\r\r\rAdd chunks\rThere are three ways to insert chunks:\n\rPress ⌘⌥I on macOS or control + alt + I on Windows\n\rClick on the “Insert” button at the top of the editor window\n\rManually type all the backticks and curly braces (don’t do this)\n\r\r\rChunk names\rYou can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk}\r# Code goes here\r```\r\rChunk options\rThere are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE}\r# Code goes here\r```\rThe most common chunk options are these:\n\rfig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures\recho=FALSE: The code is not shown in the final document, but the results are\rmessage=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted\rwarning=FALSE: Any warnings that R generates are omitted\rinclude=FALSE: The chunk still runs, but the code and results are not included in the final document\r\rYou can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n\rInline chunks\rYou can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE}\ravg_mpg \u0026lt;- mean(mtcars$mpg)\r```\rThe average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon.\r… would knit into this:\n\rThe average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n\r\rOutput formats\rYou can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot;\routput:\rhtml_document: default\rpdf_document: default\rword_document: default\rYou can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n---\rtitle: \u0026quot;My document\u0026quot;\rauthor: \u0026quot;My name\u0026quot;\rdate: \u0026quot;January 13, 2020\u0026quot;\routput: html_document: toc: yes\rfig_caption: yes\rfig_height: 8\rfig_width: 10\rpdf_document: latex_engine: xelatex # More modern PDF typesetting engine\rtoc: yes\rword_document: toc: yes\rfig_caption: yes\rfig_height: 4\rfig_width: 5\r---\r\r","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641494647,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"https://ssc442kirkpatrick.netlify.app/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"Key terms\rAdd chunks\rChunk names\rChunk options\rInline chunks\rOutput formats\r\r\rR Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"\r\rR style conventions\rMain style things to pay attention to for this class\rSpacing\rLong lines\rPipes (%\u0026gt;%) and ggplot layers (+)\rComments\r\r\r\rR style conventions\rR is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;)\rfilter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rfilter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; )\rBut you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n\rMain style things to pay attention to for this class\r\rImportant note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n\rSpacing\r\rSee the “Spacing” section in the tidyverse style guide.\n\rPut spaces after commas (like in regular English):\n# Good\rfilter(mpg, cty \u0026gt; 10)\r# Bad\rfilter(mpg , cty \u0026gt; 10)\rfilter(mpg ,cty \u0026gt; 10)\rfilter(mpg,cty \u0026gt; 10)\rPut spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good\rfilter(mpg, cty \u0026gt; 10)\r# Bad\rfilter(mpg, cty\u0026gt;10)\rfilter(mpg, cty\u0026gt; 10)\rfilter(mpg, cty \u0026gt;10)\rDon’t put spaces around parentheses that are parts of functions:\n# Good\rfilter(mpg, cty \u0026gt; 10)\r# Bad\rfilter (mpg, cty \u0026gt; 10)\rfilter ( mpg, cty \u0026gt; 10)\rfilter( mpg, cty \u0026gt; 10 )\r\rLong lines\r\rSee the “Long lines” section in the tidyverse style guide.\n\rIt’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good\rfilter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\r# Good\rfilter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\r# Good\rfilter(mpg,\rcty \u0026gt; 10,\rclass == \u0026quot;compact\u0026quot;)\r# Bad\rfilter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))\r# Good\rfilter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))\r\rPipes (%\u0026gt;%) and ggplot layers (+)\rPut each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good\rggplot(mpg, aes(x = cty, y = hwy, color = class)) +\rgeom_point() +\rgeom_smooth() +\rtheme_bw()\r# Bad\rggplot(mpg, aes(x = cty, y = hwy, color = class)) +\rgeom_point() + geom_smooth() +\rtheme_bw()\r# Super bad\rggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw()\r# Super bad and won\u0026#39;t even work\rggplot(mpg, aes(x = cty, y = hwy, color = class))\r+ geom_point()\r+ geom_smooth() + theme_bw()\rPut each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good\rmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))\r# Bad\rmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))\r# Super bad\rmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))\r# Super bad and won\u0026#39;t even work\rmpg %\u0026gt;% filter(cty \u0026gt; 10)\r%\u0026gt;% group_by(class)\r%\u0026gt;% summarize(avg_hwy = mean(hwy))\r\rComments\r\rSee the “Comments” section in the tidyverse style guide.\n\rComments should start with a comment symbol and a single space: #\n# Good\r#Bad\r#Bad\rIf the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 +\rgroup_by(class) %\u0026gt;% # Divide into class groups\rsummarize(avg_hwy = mean(hwy)) # Find the average hwy in each group\rYou can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 +\rgroup_by(class) %\u0026gt;% # Divide into class groups\rsummarize(avg_hwy = mean(hwy)) # Find the average hwy in each group\rIf the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good\r# Happy families are all alike; every unhappy family is unhappy in its own way.\r# Everything was in confusion in the Oblonskys’ house. The wife had discovered\r# that the husband was carrying on an intrigue with a French girl, who had been\r# a governess in their family, and she had announced to her husband that she\r# could not go on living in the same house with him. This position of affairs\r# had now lasted three days, and not only the husband and wife themselves, but\r# all the members of their family and household, were painfully conscious of it.\r# Bad\r# Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.\rThough, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641494647,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"https://ssc442kirkpatrick.netlify.app/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions\rMain style things to pay attention to for this class\rSpacing\rLong lines\rPipes (%\u0026gt;%) and ggplot layers (+)\rComments\r\r\r\rR style conventions\rR is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;)\rfilter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rfilter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; )\rBut you’ll notice that only a few of those iterations (the first three) are easily readable.","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"\rBecause RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS\rDouble click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n\rUnzipping files on Windows\rtl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n\r","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641494647,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"https://ssc442kirkpatrick.netlify.app/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"\rThere are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n\rData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n\rGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\n\rKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n\r360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\n\rUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n\rPolitical science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n\rFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\rThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\rErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641494647,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"https://ssc442kirkpatrick.netlify.app/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n\rData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n\rGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rReadings\r\rGuiding Question\r\rA Brief Introduction to SSC442\r\rAbout Me\rAbout You\rThis Course\rMore About This Course\rAnd finally…\r\rWhat is “Data Analytics”?\r\rStarting point for this course\r\rOutline of the Course\r\rNon-Social Science Approaches to Statistical Learning\rThe Pros and Cons of Correlation\rA Case Study in Prediction\rMore Recent Examples of Prediction\rAn Aside: Nomenclature\rLearning from Data\r\rR basics\r\rCase study: US homicides by firearm\rThe (very) basics\r\rObjects\rThe workspace\rFunctions\rOther prebuilt objects\rVariable names\rSaving your workspace\rMotivating scripts\rCommenting your code\r\rData types\r\rData frames\rExamining an object\rThe accessor: $\rVectors: numerics, characters, and logical\rFactors\rLists\rMatrices\r\rVectors\r\rCreating vectors\rNames\rSequences\rSubsetting\r\rCoercion\r\rNot availables (NA)\r\rSorting\r\rsort\rorder\rmax and which.max\rrank\rBeware of recycling\r\rVector arithmetics\r\rRescaling a vector\rTwo vectors\r\rIndexing\r\rSubsetting with logicals\rLogical operators\rwhich\rmatch\r%in%\r\rRmarkdown\r\r\r\rReadings\rAs noted in the syllabus, your readings will be assigned each week in this area. For this initial week, please read the course content. Read closely the following:\n\rThe syllabus, content, examples, and labs pages for this class.\rThis page. Yes, the whole thing.\r\rGuiding Question\rFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\rDo you remember anything about R?\rWhat are the different data types in R?\rHow do you index specific elements of a vector? Why might you want to do that?\r\r\r\rA Brief Introduction to SSC442\r\rI keep saying that the sexy job in the next 10 years will be statisticians. And I’m not kidding.\n\rHal Varian, Chief Economist, Google\r\rAbout Me\rMe: My primary area of expertise is environmental and energy (applied) economics.\nThis class is totally, unapologetically a work in progress. It was developed mainly by Prof. Bushong with refinements by myself.\nMaterial is a mish-mash of stuff from courses offered at Caltech, Stanford, Harvard, and Duke…so, yeah, it will be challenging. Hopefully, you’ll find it fun!\nMy research: occasionally touches the topics in the course, but mostly utilizes things in the course as tools.\n\rAbout You\rNew phone who dis? Please email me jkirk@msu.edu your\n\rname (with pronunciation guide)\n\rmajor\n\rdesired graduation year and semester\n\rinterest in this course on a 10-point scale (1: not at all interested; 10: helllllll yeah)\n\r\r\rYou must spend 5 minutes emailing me a little bit about your interests before the next class.\n\rThis Course\rThe syllabus is posted on the course website. I’ll walk through highlights now, but read it later – it’s long.\r- But eventually, please read it. It is “required.”\nSyllabus highlights:\n\rGrade is composed of weekly writings, labs, and projects (see syllabus page for exact points)\r\rWeekly writings: 19%\rParticipation: 4%\rLabs: 32%\rProjects: 45%\r\rThis structure is designed to give ~55% “for free”. Success on the projects will require real work.\rLabs consist of a practical implementation of something we’ve covered in the course (e.g., code your own Recommender System).\r\rGrading\rGrading: come to class.\nIf you complete all assignments and attend all class dates, I suspect you will do very well. Given the way the syllabus is structured, I conjecture that the following is a loose guide to grades:\n4.0 Turned in all assignments with good effort, worked hard on the projects and was proud of final product.\n3.5 Turned in all assignments with good effort, worked a bit on the projects and was indifferent to final product.\n3.0 Turned in all assignments with some effort, worked a bit on the projects and was shy about final product.\n\u0026lt; 3.0 Very little effort, or did not turn in all assignments, worked very little on the projects and was embarassed by final product.\n…of course, failing to turn in assignments can lead to a grade dramatically lower than just a 3.0.\n\r\rMore About This Course\rThere are sort of three texts for this course and sort of zero.\nThe “main text” is free and available online. The secondary text is substantially more difficult, but also free online. The third text costs about $25. Assigned readings can be found on the course website under “Content”.\nPlease please please please please: Ask questions during class via chat.\r- Most ideas will be new.\r- Sometimes (often?) the material itself will be confusing or interesting—or both!\r- Teaching online is incredibly challenging (no feedback) and chat is vital to success.\r- Note: If I find that attendance is terrible, I may have to start incorporating attendance into participation.\nReturn of the Please: If there is some topic that you really want to learn about, ask. If you are uncomfortable asking in front of the whole group, please see me during office hours.\nBecause this is a new course:\n\rSome of the lectures will be way too long or too short.\n\rSome (most?) of the lectures won’t make sense.\n\rSome of the time I’ll forget what I intended to say and awkwardly stare at you for a few moments (sorry).\n\r\rComment throughout the course, not just at the end.\nThe material will improve with time and feedback.\nI encourage measured feedback and thoughtful responses to questions. If I call on you and you don’t know immediately, don’t freak out. If you don’t know, it’s totally okay to say you don’t know.\nSUPER BIG IMPORTANT EXPLANATION OF THE COURSE\rI teach using ``math’’.\n…Don’t be afraid. The math won’t hurt you.\nI fundamentally believe that true knowledge of how we learn from data depends on a basic understanding of the underlying mathematics.\n-Good news: no black boxes.\r- You’ll actually learn stuff. (Probably. Hopefully?)\r- Also good news: level of required math is reasonably low. High-school algebra or equivalent should be fine.\n-Bad news: notation-heavy slides and reading.\n\r\rAnd finally…\rFinally: I cannot address field-specific questions in areas outside economics to any satisfying degree.\nGood news: I’m good at knowing what I don’t know and have a very small ego, which means that I’m much less likely to blow smoke up your ass than other professors.\nBad news: I can’t help with certain types of questions.\nThis course should be applicable broadly, but many of the examples will lean on my personal expertise (sorry).\nYour assignment: read syllabus the content from Week 0.\nThings to stress from syllabus:\n\rE-mail isn’t the ideal solution for technical problems\rNo appointments necessary for regularly scheduled office hours; or by appointment.\rTA office hours are great as well. Our TA has experience in this course.\rCan only reschedule exams (with good reason) if you tell me before the exam that you have a conflict.\r\rNotify me immediately if you need accommodations because of RCPD or religious convictions; If you approach me at the last minute, I may not be able to help.\r\r\rDespite my hard-assness in these intro slides: I’m here to help and I am not in the business of giving bad grades for no reason.\n\r\rWhat is “Data Analytics”?\rHow do you define “data analytics”? (Not a rhetorical question!)\n\rThis course will avoid this nomenclature. It is confusing and imprecise. But you signed up (suckers) and I owe an explanation of what this course will cover.\r\rSome “data analytics” topics we will cover:\n\rLinear regression: il classico.\rModels of classification or discrete choice.\rAnalysis of ``wide’’ data.\rDecision trees and other non-linear models.\r\rStarting point for this course\rBetter utilizing existing data can improve our predictive power whilst providing interpretable outputs for considering new policies.\nWARNING: Causation is tough and we will spend the entire course warning you to avoid making causal claims!\n\r\rOutline of the Course\rNon-Social Science Approaches to Statistical Learning\rA Brief History\nSuppose you are a researcher and you want to teach a computer to recognize images of a tree.\nNote: this is an ``easy” problem. If you show pictures to a 3-year-old, that child will probably be able to tell you if there is a tree in the picture.\nComputer scientists spent about 20 years on this problem because they thought about the problem like nerds and tried to write down a series of rules.\nRules are difficult to form, and simply writing rules misses the key insight: the data can tell you something.\nSocial Science Approaches to Statistical Learning\rA Brief History\nSuppose you are a researcher and you want to know whether prisons reduce crime.\nfrom ``A Call for a Moratorium on Prison Building’’ (1976)\n\rBetween 1955 and 1975, fifteen states increased the collective capacity of their adult prison systems by 56% (from, on average, 63,100 to 98,649).\rFifteen other states increased capacity by less than 4% (from 49,575 to 51,440).\rIn “heavy-construction” states the crime rate increased by 167%; in “low-construction” states the crime rate increased by 145%.\r\r\r\r\rPrison Capacity\rCrime Rate\r\r\r\rHigh construction\r\\(\\uparrow\\)~56%\r\\(\\uparrow\\)~167%\r\rLow construction\r\\(\\uparrow\\)~4%\r\\(\\uparrow\\)~145%\r\r\r\r\r\rThe Pros and Cons of Correlation\rPros:\r- Nature gives you correlations for free.\r- In principle, everyone can agree on the facts.\nCons:\r- Correlations are not very helpful.\r- They show what has happened, but not why.\r- For many things, we care about why.\nWhy a Correlation Exists Between X and Y\r\\(X \\rightarrow Y\\)\rX causes Y (causality)\n\r\\(X \\leftarrow Y\\)\rY causes X (reverse causality)\n\r\\(Z \\rightarrow X\\); \\(Z \\rightarrow Y\\)\rZ causes X and Y (common cause)\n\r\\(X \\rightarrow Y\\); \\(Y \\rightarrow X\\)\rX causes Y and Y causes X (simultaneous equations)\n\r\r\rUniting Social Science and Computer Science\rWe will start in this course by examining situations where we do not care about why something has happened, but instead we care about our ability to predict its occurrence from existing data.\n(But of course keep in back of mind that if you are making policy, you must care about why something happened).\nWe will also borrow a few other ideas from CS:\r- Anything is data\r+ Satellite data\r+ Unstructured text or audio\r+ Facial expressions or vocal intonations\r- Subtle improvements on existing techniques\r- An eye towards practical implementability over ``cleanliness”\n\r\rA Case Study in Prediction\rExample: a firm wishes to predict user behavior based on previous purchases or interactions.\nSmall margins \\(\\rightarrow\\) huge payoffs. \\(10\\% \\rightarrow\\) $1 million.\nNot obvious why this was true for Netflix; quite obvious why this is true in financial markets.\n\rMore Recent Examples of Prediction\r\rIdentify the risk factors for prostate cancer.\rClassify a tissue sample into one of several cancer classes, based on a gene expression profile.\rClassify a recorded phoneme based on a log-periodogram.\rPredict whether someone will have a heart attack on the basis of demographic, diet and clinical measurements.\rCustomize an email spam detection system.\rIdentify a hand-drawn object.\rDetermine which oscillations of stellar luminosity are likely due to exoplanets.\rEstablish the relationship between salary and demographic variables in population survey data.\r\r\rAn Aside: Nomenclature\rMachine learning arose as a subfield of Artificial Intelligence.\nStatistical learning arose as a subfield of Statistics.\nThere is much overlap; however, a few points of distinction:\n\rMachine learning has a greater emphasis on large scale applications and prediction accuracy.\n\rStatistical learning emphasizes models and their interpretability, and precision and uncertainty.\n\rBut the distinction has become more and more blurred, and there is a great deal of “cross-fertilization”.\r\r\rObviously true: machine learning has the upper hand in marketing.\n\rLearning from Data\rThe following are the basic requirements for statistical learning:\nA pattern exists.\rThis pattern is not easily expressed in a closed mathematical form.\rYou have data.\r\rALERT\nThe course content below should be considered a prerequisite for success. For those concerned about basics of R, you absolutely must read this content and attempt the coding exercises. If you struggle to follow the content, please contact the professor or TA.\n\r\r\rR basics\rIn this class, we will be using R software environment for all our analyses. You will learn R and data analysis techniques simultaneously. To follow along you will therefore need access to R. We also recommend the use of an integrated development environment (IDE), such as RStudio, to save your work.\rNote that it is common for a course or workshop to offer access to an R environment and an IDE through your web browser, as done by RStudio cloud1. If you have access to such a resource, you don’t need to install R and RStudio. However, if you intend on becoming a practicing data analyst, we highly recommend installing these tools on your computer2. This is not hard.\nBoth R and RStudio are free and available online.\nCase study: US homicides by firearm\rImagine you live in Europe (if only!) and are offered a job in a US company with many locations in every state. It is a great job, but headlines such as US Gun Homicide Rate Higher Than Other Developed Countries3 have you worried. Fox News runs a scary looking graphic, and charts like the one below only add to that concern:\nOr even worse, this version from everytown.org:\rBut then you remember that (1) this is a hypothetical exercise; (2) you’ll take literally any job at this point; and (3) Geographic diversity matters – the United States is a large and diverse country with 50 very different states (plus the District of Columbia and some lovely territories).4\n## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please\r## use `guide = \u0026quot;none\u0026quot;` instead.\rCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial.\nThis is a relatively simple and straightforward problem in social science: you have options of where to live, and want to determine the safety of the various states. Your “research” is clearly policy-relevant: you will eventually have to live somewhere. We will begin to tackle the problem by examining data related to gun homicides in the US during 2010 using R.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Ideally, this is a refresher. However, we are aware that your preparation in previously courses varies greatly from student to student. Moreover, we want you to be aware that the usefulness of some of these early building blocks may not be immediately obvious. Later in the class you will appreciate having these skills. Mastery will be rewarded both in this class and (of course) in life.\n\rThe (very) basics\rBefore we get started with the motivating dataset, we need to cover the very basics of R.\nObjects\rSuppose a relatively math unsavvy student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). You—a savvy student—recall that the quadratic formula gives us the solutions:\n\\[\r\\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}\r\\]\nwhich of course depend on the values of \\(a\\), \\(b\\), and \\(c\\). That is, the quadratic equation represents a function with three arguments.\nOne advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define:\na \u0026lt;- 1\rb \u0026lt;- 1\rc \u0026lt;- -1\rwhich stores the values for later use. We use \u0026lt;- to assign values to the variables.\nWe can also assign values using = instead of \u0026lt;-, but we recommend against using = to avoid confusion.5\nTRY IT\nCopy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Throughout these written notes, you’ll have the most success if you continue to copy code into your own console.\n\rTo see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\na\r## [1] 1\rA more explicit way to ask R to show us the value stored in a is using print like this:\nprint(a)\r## [1] 1\rWe use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\n\rThe workspace\rAs we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\nls()\r## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;dat\u0026quot; \u0026quot;filter\u0026quot; \u0026quot;murders\u0026quot; \u0026quot;select\u0026quot;\r(Note that one of my variables listed above comes from generating the graphs above). In RStudio, the Environment tab shows the values:\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )\r## [1] 0.618034\r(-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )\r## [1] -1.618034\r\rFunctions\rOnce you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several zillion predefined functions and most of the analysis pipelines we construct make extensive use of the built-in functions. But R’s power comes from its scalability. We have access to (nearly) infinite functions via install.packages and library. As we go through the course, we will carefully note new functions we bring to each problem. For now, though, we will stick to the basics.\nNote that you’ve used a function already: you used the function sqrt to solve the quadratic equation above. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\nlog(8)\r## [1] 2.079442\rlog(a)\r## [1] 0\rYou can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\nhelp(\u0026quot;log\u0026quot;)\rFor most functions, we can also use this shorthand:\n?log\rThe help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional.6 For example, the base of the function log defaults to base = exp(1)—that is, log evaluates the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\nargs(log)\r## function (x, base = exp(1)) ## NULL\rYou can change the default values by simply assigning another object:\nlog(8, base = 2)\r## [1] 3\rNote that we have not been specifying the argument x as such:\nlog(x = 8, base = 2)\r## [1] 3\rThe above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\nlog(8,2)\r## [1] 3\rIf using the arguments’ names, then we can include them in whatever order we want:\nlog(base = 2, x = 8)\r## [1] 3\rTo specify arguments, we must use =, and cannot use \u0026lt;-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n2 ^ 3\r## [1] 8\rYou can see the arithmetic operators by typing:\nhelp(\u0026quot;+\u0026quot;)\ror\n?\u0026quot;+\u0026quot;\rand the relational operators by typing:\nhelp(\u0026quot;\u0026gt;\u0026quot;)\ror\n?\u0026quot;\u0026gt;\u0026quot;\r\rOther prebuilt objects\rThere are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\ndata()\rThis shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\nco2\rR will show you Mauna Loa atmospheric \\(CO^2\\) concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant \\(\\pi\\) and \\(\\infty\\):\npi\r## [1] 3.141593\rInf+1\r## [1] Inf\r\rVariable names\rWe have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in R. For example, don’t name one of your variables install.packages by typing something like install.packages \u0026lt;- 2. Usually, R is smart enough to prevent you from doing such nonsense, but it’s important to develop good habits.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\nsolution_1 \u0026lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a)\rsolution_2 \u0026lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a)\rFor more advice, we highly recommend studying (Hadley Wickham’s style guide)[http://adv-r.had.co.nz/Style.html].\n\rSaving your workspace\rValues remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab.\rYou can read the help pages on save, save.image, and load to learn more.\n\rMotivating scripts\rTo solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\na \u0026lt;- 3\rb \u0026lt;- 2\rc \u0026lt;- -1\r(-b + sqrt(b^2 - 4*a*c)) / (2*a)\r(-b - sqrt(b^2 - 4*a*c)) / (2*a)\rBy creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\n\rCommenting your code\rIf a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n## Code to compute solution to quadratic equation of the form ax^2 + bx + c\r## define the variables\ra \u0026lt;- 3\rb \u0026lt;- 2\rc \u0026lt;- -1\r## now compute the solution\r(-b + sqrt(b^2 - 4*a*c)) / (2*a)\r(-b - sqrt(b^2 - 4*a*c)) / (2*a)\rTRY IT\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\n\rNow use the same formula to compute the sum of the integers from 1 through 1,000.\n\rLook at the result of typing the following code into R:\n\r\rn \u0026lt;- 1000\rx \u0026lt;- seq(1, n)\rsum(x)\rBased on the result, what do you think the functions seq and sum do? You can use help.\nsum creates a list of numbers and seq adds them up.\rseq creates a list of numbers and sum adds them up.\rseq creates a random list and sum computes the sum of 1 through 1,000.\rsum always returns the same number.\r\rIn math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\n\rWhich of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\n\r\rlog(10^x)\rlog10(x^10)\rlog(exp(x))\rexp(log(x, base = 2))\r\r\r\r\rData types\rVariables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\na \u0026lt;- 2\rclass(a)\r## [1] \u0026quot;numeric\u0026quot;\rTo work efficiently in R, it is important to learn the different types of variables and what we can do with these.\nData frames\rUp to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\nlibrary(dslabs)\rdata(murders)\rTo see that this is in fact a data frame, we type:\nclass(murders)\r## [1] \u0026quot;data.frame\u0026quot;\r\rExamining an object\rThe function str is useful for finding out more about the structure of an object:\nstr(murders)\r## \u0026#39;data.frame\u0026#39;: 51 obs. of 5 variables:\r## $ state : chr \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; ...\r## $ abb : chr \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; ...\r## $ region : Factor w/ 4 levels \u0026quot;Northeast\u0026quot;,\u0026quot;South\u0026quot;,..: 2 4 4 2 4 4 1 2 2 2 ...\r## $ population: num 4779736 710231 6392017 2915918 37253956 ...\r## $ total : num 135 19 232 93 1257 ...\rThis tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\nhead(murders)\r## state abb region population total\r## 1 Alabama AL South 4779736 135\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\r## 4 Arkansas AR South 2915918 93\r## 5 California CA West 37253956 1257\r## 6 Colorado CO West 5029196 65\rIn this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let’s learn more about the components of this object.\n\rThe accessor: $\rFor our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\nmurders$population\r## [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 897934\r## [9] 601723 19687653 9920000 1360301 1567582 12830632 6483802 3046355\r## [17] 2853118 4339367 4533372 1328361 5773552 6547629 9883640 5303925\r## [25] 2967297 5988927 989415 1826341 2700551 1316470 8791894 2059179\r## [33] 19378102 9535483 672591 11536504 3751351 3831074 12702379 1052567\r## [41] 4625364 814180 6346105 25145561 2763885 625741 8001024 6724540\r## [49] 1852994 5686986 563626\rBut how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\nnames(murders)\r## [1] \u0026quot;state\u0026quot; \u0026quot;abb\u0026quot; \u0026quot;region\u0026quot; \u0026quot;population\u0026quot; \u0026quot;total\u0026quot;\rIt is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\nTip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\n\rVectors: numerics, characters, and logical\rThe object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\npop \u0026lt;- murders$population\rlength(pop)\r## [1] 51\rThis particular vector is numeric since population sizes are numbers:\nclass(pop)\r## [1] \u0026quot;numeric\u0026quot;\rIn a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\nclass(murders$state)\r## [1] \u0026quot;character\u0026quot;\rAs with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\nz \u0026lt;- 3 == 2\rz\r## [1] FALSE\rclass(z)\r## [1] \u0026quot;logical\u0026quot;\rHere the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality. Yet another reason to avoid assigning via =… it can get confusing and typos can really mess things up.\nYou can see the other relational operators by typing:\n?Comparison\rIn future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\nAdvanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\n\rFactors\rIn the murders dataset, we might expect the region to also be a character vector. However, it is not:\nclass(murders$region)\r## [1] \u0026quot;factor\u0026quot;\rIt is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\nlevels(murders$region)\r## [1] \u0026quot;Northeast\u0026quot; \u0026quot;South\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot;\rIn the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. It is also useful for computational reasons we’ll explore later.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\nregion \u0026lt;- murders$region\rvalue \u0026lt;- murders$total\rregion \u0026lt;- reorder(region, value, FUN = sum)\rlevels(region)\r## [1] \u0026quot;Northeast\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot; \u0026quot;South\u0026quot;\rThe new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\nWarning: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\n\rLists\rData frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:\nrecord\r## $name\r## [1] \u0026quot;John Doe\u0026quot;\r## ## $student_id\r## [1] 1234\r## ## $grades\r## [1] 95 82 91 97 93\r## ## $final_grade\r## [1] \u0026quot;A\u0026quot;\rclass(record)\r## [1] \u0026quot;list\u0026quot;\rAs with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list.\nrecord$student_id\r## [1] 1234\rWe can also use double square brackets ([[) like this:\nrecord[[\u0026quot;student_id\u0026quot;]]\r## [1] 1234\rYou should get used to the fact that in R there are often several ways to do the same thing. such as accessing entries.7\nYou might also encounter lists without variable names.\nrecord2\r## [[1]]\r## [1] \u0026quot;John Doe\u0026quot;\r## ## [[2]]\r## [1] 1234\rIf a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\nrecord2[[1]]\r## [1] \u0026quot;John Doe\u0026quot;\rWe won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\n\rMatrices\rMatrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this class, but much of what happens in the background when you perform a data analysis involves matrices. We describe them briefly here since some of the functions we will learn return matrices.\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\nmat \u0026lt;- matrix(1:12, 4, 3)\rmat\r## [,1] [,2] [,3]\r## [1,] 1 5 9\r## [2,] 2 6 10\r## [3,] 3 7 11\r## [4,] 4 8 12\rYou can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\nmat[2, 3]\r## [1] 10\rIf you want the entire second row, you leave the column spot empty:\nmat[2, ]\r## [1] 2 6 10\rNotice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\nmat[, 3]\r## [1] 9 10 11 12\rThis is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\nmat[, 2:3]\r## [,1] [,2]\r## [1,] 5 9\r## [2,] 6 10\r## [3,] 7 11\r## [4,] 8 12\rYou can subset both rows and columns:\nmat[1:2, 2:3]\r## [,1] [,2]\r## [1,] 5 9\r## [2,] 6 10\rWe can convert matrices into data frames using the function as.data.frame:\nas.data.frame(mat)\r## V1 V2 V3\r## 1 1 5 9\r## 2 2 6 10\r## 3 3 7 11\r## 4 4 8 12\rYou can also use single square brackets ([) to access rows and columns of a data frame:\ndata(\u0026quot;murders\u0026quot;)\rmurders[25, 1]\r## [1] \u0026quot;Mississippi\u0026quot;\rmurders[2:3, ]\r## state abb region population total\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\rTRY IT\nLoad the US murders dataset.\r\rlibrary(dslabs)\rdata(murders)\rUse the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\nThe 51 states.\rThe murder rates for all 50 states and DC.\rThe state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010.\rstr shows no relevant information.\r\rWhat are the column names used by the data frame for these five variables?\n\rUse the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\n\rNow use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\n\rWe saw that the region column stores a factor. You can corroborate this by typing:\n\r\rclass(murders$region)\rWith one line of code, use the function levels and length to determine the number of regions defined by this dataset.\nThe function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.\r\r\r\r\rVectors\rIn R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\nCreating vectors\rWe can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\ncodes \u0026lt;- c(380, 124, 818)\rcodes\r## [1] 380 124 818\rWe can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\ncountry \u0026lt;- c(\u0026quot;italy\u0026quot;, \u0026quot;canada\u0026quot;, \u0026quot;egypt\u0026quot;)\rIn R you can also use single quotes:\ncountry \u0026lt;- c(\u0026#39;italy\u0026#39;, \u0026#39;canada\u0026#39;, \u0026#39;egypt\u0026#39;)\rBut be careful not to confuse the single quote ’ with the back quote, which shares a keyboard key with ~.\nBy now you should know that if you type:\ncountry \u0026lt;- c(italy, canada, egypt)\ryou receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\n\rNames\rSometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\ncodes \u0026lt;- c(italy = 380, canada = 124, egypt = 818)\rcodes\r## italy canada egypt ## 380 124 818\rThe object codes continues to be a numeric vector:\nclass(codes)\r## [1] \u0026quot;numeric\u0026quot;\rbut with names:\nnames(codes)\r## [1] \u0026quot;italy\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;egypt\u0026quot;\rIf the use of strings without quotes looks confusing, know that you can use the quotes as well:\ncodes \u0026lt;- c(\u0026quot;italy\u0026quot; = 380, \u0026quot;canada\u0026quot; = 124, \u0026quot;egypt\u0026quot; = 818)\rcodes\r## italy canada egypt ## 380 124 818\rThere is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\nWe can also assign names using the names functions:\ncodes \u0026lt;- c(380, 124, 818)\rcountry \u0026lt;- c(\u0026quot;italy\u0026quot;,\u0026quot;canada\u0026quot;,\u0026quot;egypt\u0026quot;)\rnames(codes) \u0026lt;- country\rcodes\r## italy canada egypt ## 380 124 818\r\rSequences\rAnother useful function for creating vectors generates sequences:\nseq(1, 10)\r## [1] 1 2 3 4 5 6 7 8 9 10\rThe first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\nseq(1, 10, 2)\r## [1] 1 3 5 7 9\rIf we want consecutive integers, we can use the following shorthand:\n1:10\r## [1] 1 2 3 4 5 6 7 8 9 10\rWhen we use these functions, R produces integers, not numerics, because they are typically used to index something:\nclass(1:10)\r## [1] \u0026quot;integer\u0026quot;\rHowever, if we create a sequence including non-integers, the class changes:\nclass(seq(1, 10, 0.5))\r## [1] \u0026quot;numeric\u0026quot;\r\rSubsetting\rWe use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\ncodes[2]\r## canada ## 124\rYou can get more than one entry by using a multi-entry vector as an index:\ncodes[c(1,3)]\r## italy egypt ## 380 818\rThe sequences defined above are particularly useful if we want to access, say, the first two elements:\ncodes[1:2]\r## italy canada ## 380 124\rIf the elements have names, we can also access the entries using these names. Below are two examples.\ncodes[\u0026quot;canada\u0026quot;]\r## canada ## 124\rcodes[c(\u0026quot;egypt\u0026quot;,\u0026quot;italy\u0026quot;)]\r## egypt italy ## 818 380\r\r\rCoercion\rIn general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\nx \u0026lt;- c(1, \u0026quot;canada\u0026quot;, 3)\rBut we don’t get one, not even a warning! What happened? Look at x and its class:\nx\r## [1] \u0026quot;1\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;3\u0026quot;\rclass(x)\r## [1] \u0026quot;character\u0026quot;\rR coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \"1\" and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\nx \u0026lt;- 1:5\ry \u0026lt;- as.character(x)\ry\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5\u0026quot;\rYou can turn it back with as.numeric:\nas.numeric(y)\r## [1] 1 2 3 4 5\rThis function is actually quite useful since datasets that include numbers as character strings are common.\nNot availables (NA)\rThis “topic” seems to be wholly unappreciated and it has been our experience that students often panic when encountering an NA. This often happens when a function tries to coerce one type to another and encounters an impossible case. In such circumstances, R usually gives us a warning and turns the entry into a special value called an NA (for “not available”). For example:\nx \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;3\u0026quot;)\ras.numeric(x)\r## Warning: NAs introduced by coercion\r## [1] 1 NA 3\rR does not have any guesses for what number you want when you type b, so it does not try.\nWhile coercion is a common case leading to NAs, you’ll see them in nearly every real-world dataset. Most often, you will encounter the NAs as a stand-in for missing data. Again, this a common problem in real-world datasets and you need to be aware that it will come up.\n\r\rSorting\rNow that we have mastered some basic R knowledge (ha!), let’s try to gain some insights into the safety of different states in the context of gun murders.\nsort\rSay we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\nlibrary(dslabs)\rdata(murders)\rsort(murders$total)\r## [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32\r## [16] 36 38 53 63 65 67 84 93 93 97 97 99 111 116 118\r## [31] 120 135 142 207 219 232 246 250 286 293 310 321 351 364 376\r## [46] 413 457 517 669 805 1257\rHowever, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.\n\rorder\rThe function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:\nx \u0026lt;- c(31, 4, 15, 92, 65)\rsort(x)\r## [1] 4 15 31 65 92\rRather than sort the input vector, the function order returns the index that sorts input vector:\nindex \u0026lt;- order(x)\rx[index]\r## [1] 4 15 31 65 92\rThis is the same output as that returned by sort(x). If we look at this index, we see why it works:\nx\r## [1] 31 4 15 92 65\rorder(x)\r## [1] 2 3 1 5 4\rThe second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\nmurders$state[1:6]\r## [1] \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; \u0026quot;California\u0026quot;\r## [6] \u0026quot;Colorado\u0026quot;\rmurders$abb[1:6]\r## [1] \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;CO\u0026quot;\rThis means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\nind \u0026lt;- order(murders$total)\rmurders$abb[ind]\r## [1] \u0026quot;VT\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;WV\u0026quot; \u0026quot;NE\u0026quot;\r## [16] \u0026quot;OR\u0026quot; \u0026quot;DE\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;WA\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;DC\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;MA\u0026quot;\r## [31] \u0026quot;MS\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;IL\u0026quot; \u0026quot;GA\u0026quot;\r## [46] \u0026quot;MI\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;CA\u0026quot;\rAccording to the above, California had the most murders.\n\rmax and which.max\rIf we are only interested in the entry with the largest value, we can use max for the value:\nmax(murders$total)\r## [1] 1257\rand which.max for the index of the largest value:\ni_max \u0026lt;- which.max(murders$total)\rmurders$state[i_max]\r## [1] \u0026quot;California\u0026quot;\rFor the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\n\rrank\rAlthough not as frequently used as order and sort, the function rank is also related to order and can be useful.\rFor any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\nx \u0026lt;- c(31, 4, 15, 92, 65)\rrank(x)\r## [1] 3 1 2 5 4\rTo summarize, let’s look at the results of the three functions we have introduced:\n\r\roriginal\r\rsort\r\rorder\r\rrank\r\r\r\r\r\r31\r\r4\r\r2\r\r3\r\r\r\r4\r\r15\r\r3\r\r1\r\r\r\r15\r\r31\r\r1\r\r2\r\r\r\r92\r\r65\r\r5\r\r5\r\r\r\r65\r\r92\r\r4\r\r4\r\r\r\r\r\rBeware of recycling\rAnother common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:\nx \u0026lt;- c(1,2,3)\ry \u0026lt;- c(10, 20, 30, 40, 50, 60, 70)\rx+y\r## Warning in x + y: longer object length is not a multiple of shorter object\r## length\r## [1] 11 22 33 41 52 63 71\rWe do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\nTRY IT\nFor these exercises we will use the US murders dataset. Make sure you load it prior to starting.\nlibrary(dslabs)\rdata(\u0026quot;murders\u0026quot;)\rUse the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\n\rNow instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\n\rWe can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\n\rNow we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\n\rYou can create a data frame using the data.frame function. Here is a quick example:\n\r\rtemp \u0026lt;- c(35, 88, 42, 84, 81, 30)\rcity \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;,\r\u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;)\rcity_temps \u0026lt;- data.frame(name = city, temperature = temp)\rUse the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\nRepeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\n\rThe na_example vector represents a series of counts. You can quickly examine the object using:\n\r\rdata(\u0026quot;na_example\u0026quot;)\rstr(na_example)\r## int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...\rHowever, when we compute the average with the function mean, we obtain an NA:\nmean(na_example)\r## [1] NA\rThe is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have.\nNow compute the average again, but only for the entries that are not NA. Hint: remember the ! operator.\r\r\r\r\rVector arithmetics\rCalifornia had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\nlibrary(dslabs)\rdata(\u0026quot;murders\u0026quot;)\rmurders$state[which.max(murders$population)]\r## [1] \u0026quot;California\u0026quot;\rwith over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\nRescaling a vector\rIn R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\ninches \u0026lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\rand want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\ninches * 2.54\r## [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80\rIn the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\ninches - 69\r## [1] 0 -7 -3 1 1 4 -2 4 -2 1\r\rTwo vectors\rIf we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n\\[\r\\begin{pmatrix}\ra\\\\\rb\\\\\rc\\\\\rd\r\\end{pmatrix}\r+\r\\begin{pmatrix}\re\\\\\rf\\\\\rg\\\\\rh\r\\end{pmatrix}\r=\r\\begin{pmatrix}\ra +e\\\\\rb + f\\\\\rc + g\\\\\rd + h\r\\end{pmatrix}\r\\]\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000\rOnce we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\nmurders$abb[order(murder_rate)]\r## [1] \u0026quot;VT\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;OR\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;WA\u0026quot;\r## [16] \u0026quot;WV\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;NE\u0026quot; \u0026quot;MA\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IL\u0026quot;\r## [31] \u0026quot;OK\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;GA\u0026quot; \u0026quot;MS\u0026quot; \u0026quot;MI\u0026quot;\r## [46] \u0026quot;DE\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;DC\u0026quot;\rTRY IT\nPreviously we created this data frame:\r\rtemp \u0026lt;- c(35, 88, 42, 84, 81, 30)\rcity \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;,\r\u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;)\rcity_temps \u0026lt;- data.frame(name = city, temperature = temp)\rRemake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is \\(C = \\frac{5}{9} \\times (F - 32)\\).\nWrite code to compute the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler, we know it should be close to \\(\\pi^2/6\\).\n\rCompute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?\n\r\r\r\r\rIndexing\rIndexing is a boring name for an important tool. R provides a powerful and convenient way of referencing specific elements of vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:\nlibrary(dslabs)\rdata(\u0026quot;murders\u0026quot;)\rSubsetting with logicals\rWe have now calculated the murder rate using:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000\rImagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\nind \u0026lt;- murder_rate \u0026lt; 0.71\rIf we instead want to know if a value is less or equal, we can use:\nind \u0026lt;- murder_rate \u0026lt;= 0.71\rNote that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\nmurders$state[ind]\r## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Iowa\u0026quot; \u0026quot;New Hampshire\u0026quot; \u0026quot;North Dakota\u0026quot; ## [5] \u0026quot;Vermont\u0026quot;\rIn order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\nsum(ind)\r## [1] 5\r\rLogical operators\rSuppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with \u0026amp;. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\nTRUE \u0026amp; TRUE\r## [1] TRUE\rTRUE \u0026amp; FALSE\r## [1] FALSE\rFALSE \u0026amp; FALSE\r## [1] FALSE\rFor our example, we can form two logicals:\nwest \u0026lt;- murders$region == \u0026quot;West\u0026quot;\rsafe \u0026lt;- murder_rate \u0026lt;= 1\rand we can use the \u0026amp; to get a vector of logicals that tells us which states satisfy both conditions:\nind \u0026lt;- safe \u0026amp; west\rmurders$state[ind]\r## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Idaho\u0026quot; \u0026quot;Oregon\u0026quot; \u0026quot;Utah\u0026quot; \u0026quot;Wyoming\u0026quot;\r\rwhich\rSuppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\nind \u0026lt;- which(murders$state == \u0026quot;California\u0026quot;)\rmurder_rate[ind]\r## [1] 3.374138\r\rmatch\rIf instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function match. This function tells us which indexes of a second vector match each of the entries of a first vector:\nind \u0026lt;- match(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state)\rind\r## [1] 33 10 44\rNow we can look at the murder rates:\nmurder_rate[ind]\r## [1] 2.667960 3.398069 3.201360\r\r%in%\rIf rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\nc(\u0026quot;Boston\u0026quot;, \u0026quot;Dakota\u0026quot;, \u0026quot;Washington\u0026quot;) %in% murders$state\r## [1] FALSE FALSE TRUE\rNote that we will be using %in% often throughout the book.\nAdvanced: There is a connection between match and %in% through which. To see this, notice that the following two lines produce the same index (although in different order):\nmatch(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state)\r## [1] 33 10 44\rwhich(murders$state%in%c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;))\r## [1] 10 33 44\r\r\rRmarkdown\rIf you’re new to Rmarkdown, I have made a short video on how to use it . This video is for my EC420 course, but works for us as well.\nEXERCISES\nStart by loading the library and data.\nlibrary(dslabs)\rdata(murders)\rCompute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\n\rNow use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\n\rUse the results from the previous exercise to report the names of the states with murder rates lower than 1.\n\rNow extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator \u0026amp;.\n\rIn a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\n\rUse the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\n\rUse the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\n\rExtend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index.\n\r\r\r\r\r\rhttps://rstudio.cloud↩︎\n\rhttps://rafalab.github.io/dsbook/installing-r-rstudio.html↩︎\n\rhttp://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/↩︎\n\rI’m especially partial to Puerto Rico.↩︎\n\rThis is, without a doubt, my least favorite aspect of R. I’d even venture to call it stupid. The logic behind this pesky \u0026lt;- is a total mystery to me, but there is logic to avoiding =. But, you do you.↩︎\n\rThis equals sign is the reasons we assign values with \u0026lt;-; then when arguments of a function are assigned values, we don’t end up with multiple equals signs. But… who cares.↩︎\n\rWhether you view this as a feature or a bug is a good indicator whether you’ll enjoy working with R.↩︎\n\r\r\r","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661811829,"objectID":"9be773bd8dbb1773f9326846c039d666","permalink":"https://ssc442kirkpatrick.netlify.app/content/00-content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/content/00-content/","section":"content","summary":"Readings\r\rGuiding Question\r\rA Brief Introduction to SSC442\r\rAbout Me\rAbout You\rThis Course\rMore About This Course\rAnd finally…\r\rWhat is “Data Analytics”?\r\rStarting point for this course\r\rOutline of the Course\r\rNon-Social Science Approaches to Statistical Learning\rThe Pros and Cons of Correlation\rA Case Study in Prediction\rMore Recent Examples of Prediction\rAn Aside: Nomenclature\rLearning from Data\r\rR basics\r\rCase study: US homicides by firearm\rThe (very) basics\r\rObjects\rThe workspace\rFunctions\rOther prebuilt objects\rVariable names\rSaving your workspace\rMotivating scripts\rCommenting your code\r\rData types\r\rData frames\rExamining an object\rThe accessor: $\rVectors: numerics, characters, and logical\rFactors\rLists\rMatrices\r\rVectors\r\rCreating vectors\rNames\rSequences\rSubsetting\r\rCoercion\r\rNot availables (NA)\r\rSorting\r\rsort\rorder\rmax and which.","tags":null,"title":"Welcome Back to R","type":"docs"},{"authors":null,"categories":null,"content":"\r\rReadings\r\rGuiding Question\r\rGroup Projects\r\rTeams\r\rRandomness and Data Analytics\r\rLearning From Data\rFormalization\rThe Target Function\rWhy Estimate an Unknown Function?\rThe Parable of the Marbles\rOutside the Data\rHoeffding’s Inequality\rAn example of Hoeffding’s Inequality\r\rThe tidyverse\r\rTidy data\rManipulating data frames\r\rAdding a column with mutate\rSubsetting with filter\rSelecting columns with select\r\rThe pipe: %\u0026gt;%\rSummarizing data\r\rsummarize\rpull\rGroup then summarize with group_by\r\rSorting data frames\r\rNested sorting\rThe top \\(n\\)\r\rTibbles\r\rTibbles display better\rSubsets of tibbles are tibbles\rTibbles can have complex entries\rTibbles can be grouped\rCreate a tibble using tibble instead of data.frame\r\rThe dot operator\rdo\rThe purrr package\rTidyverse conditionals\r\rcase_when\rbetween\r\r\r\r\rReadings\r\rThis page.\rChapter 1 of Introduction to Statistical Learning, available here.\rOptional: The “Tidy Your Data” tutorial on Rstudio Clould Primers\r\rGuiding Question\rFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\rWhy do we want tidy data?\rWhat are the challenges associated with shaping things into a tidy format?\r\rOverview\r\r\r\r\r\r\r--\r\r\rGroup Projects\rYour final is a group project.\nYou need to start planning soon.\nTo aid in your planning, here are the required elements of your project (note: the project assignment that currently exists on this site, if you find it, is old and will change a lot between now and next week).\nYou must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.\n\rYou must visualize 3 intersting features of that data.\n\rYou must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.\n\rYou must think critically about your analysis and be able to identify potential issues/\n\rYou must present your analysis as if presenting to a C-suite executive.\n\r\rTeams\rPlease form teams of 2-4 people. Once all agree to be on a team, have one person email our TA, Xueshi, wangxu36@msu.edu and cc all of the members of the team so that nobody is surprised to be included on a team. Title the email [SSC442] - Group Formation. Tell us your team name, and list in the email the names of all of the team members and their email address (in addition to cc-ing those team members on the email).\nIf you opt to not form a team, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with another 2-3 persons.\nSend this email by Sunday Jan 23rd and I will assign un-teamed folks at the beginning of next week.\nMore on your team\r\rYou should strongly consider coordinating your work via Github\n\rYour team will earn the same scores on all projects\n\rTeams will submit only one write-up for the mini-projects and the final\n\r\rTo combat additional freeloading, we will use a reporting system for work contributed. More details will follow.\n\r\r\rRandomness and Data Analytics\rAnd the fabulous importance of probabilistic inference…\nThis lecture is very “high-level,” which means it is talking about abstract concepts. It is also quite important.\nWe want to discuss why we eventually will need ot utilize tons of difficult mathematics. Why do we care so much about hypothesis tests and the like?\nMoreover, we can highlight why we want our data structured to behave nicely.\nLearning From Data\rThe following are the baisc requirements for statistical learning\nA pattern exists\n\rThis pattern is not easily expressed in a closed mathematical form\n\rYou have data\n\r\r\rFormalization\rWe think of our outcome-of-interest as a reponse or target that we wish to predict or wish to learn something about.\nWe generically refer to the response as \\(Y\\)\nOther aspects of the data are known as features, inputs, predictors, or regressors. We call one of these \\(X_i\\).\n\rThe subscript \\(i\\) indicates that we have an \\(X\\) realized for every individual in our data\r\rWe can refer to the input vector collectively as:\n\\[X = \\begin{bmatrix} x_{11} \u0026amp; x_{12} \\\\\rx_{21} \u0026amp; x_{22} \\\\\r\\vdots \u0026amp; \\vdots \\\\\rx_{N1} \u0026amp; x_{N2}\r\\end{bmatrix}\\]\nWe are seeking some unknown function that maps \\(X\\) to \\(Y\\)\nPut another way, we are seeking to explain \\(Y\\) as follows:\n\\[Y = f(X) + e\\]\n\rThe Target Function\rWe call the function \\(f: \\mathcal{X} \\rightarrow \\mathcal{Y}\\) the target function\nHow do we find the function? We don’t! We get as close as we can, though:\n\rObserve data \\((\\mathbf{x}_1, y_1), \\cdots, (\\mathbf{x}_N, y_N)\\)\n\rUse some algorithm to approximate \\(f\\)\n\rProduce final hypothesis function \\(g \\approx f\\)\n\rEvaluate how well \\(g\\) approximates \\(f\\) and iterate as needed.\n\r\r\rWhy Estimate an Unknown Function?\rWith a good estimate of \\(f\\) we can make predictions of \\(Y\\) at new points \\(X = x\\)\nWe can also understand which components of \\(X = (X_1, X_2, \\cdots, X_m)\\) are important in explaining \\(Y\\), and which are (potentially) irrelevant\n\re.g. GDP and yearsindustrialized have a big impact on emissions but hydroutilization typically does not.\r\rDepending on the complexity of \\(f\\), we may be able to meaningfully understand how each component of \\(X\\) affects \\(Y\\).\n(But we should be careful about assigning causal interpretations, more on this later)\n\rThe Parable of the Marbles\r(Courtesy of Prof. Bushong)\nImagine a bag of marbles with two types of marbles: ♣️ and ♦️.\nWe are going to pick a sample of \\(n\\) marbles (with replacement).\nWe want to learn something about \\(\\mu\\), the objective probability to pick a ♣️.\nIn addition to defining the objective probability of picking a ♣️, we have an observed fraction \\(\\eta\\), which will define as the fraction of ♣️ in the sample.\nQuestion: Can we say anything exact and for-sure about \\(\\mu\\) (outside the data) after observing \\(\\eta\\) (the data)?\n\rNo. It is possible for the sample to be all ♣️, ♣️, ♣️, ♣️, ♣️ even when the bag is is 50/50 ♦️\n\rNo matter what we draw, we can’t (based on that draw alone) eliminate the possibility of drawing a ♦️.\n\rAnd unless we assume that the only two values in the world are ♦️ and ♣️, we can’t rule out 💩1\n\r\r\rQuestion: Then why do we do things like polling (e.g. to predict the outcome of a presidential election)?\n\rThe bad case, that we draw something that has is completely misleading, is possible but not probable.\r\r\rOutside the Data\rPut another way, since \\(f\\) is unknown, it can take on any value outside the data we have, no matter how large the data.\n\rThis is called No Free Lunch\r\rYou cannot know anything for sure about \\(f\\) outside the data without making assumptions.\nIs there any hope to know anything about \\(f\\) outside the data set without making assumptions about \\(f\\)?\nYes, if we are willing to give up the “for sure”\n\rHoeffding’s Inequality\rHoeffding’s Inequality states, loosely, that \\(\\eta\\) cannot be too far from \\(\\mu\\).\n\\[\\mathbb{P}\\left[|\\eta - \\mu| \u0026gt; \\epsilon \\right] \\leq 2e^{-2\\epsilon^2n}\\]\r\\(\\eta \\approx \\mu\\) is called probably approximately correct (PAC) learning.\n\rAn example of Hoeffding’s Inequality\rExample: n = 1,000. Draw a sample and observe \\(\\eta\\)\n\r\\(\\sim\\) 99% of the time, \\(\\mu - .05 \\leq \\eta \\leq \\mu+.05\\)\n\rThis is implied by setting \\(\\epsilon = 0.05\\) and using \\(n=1,000\\)\r\r99.9999996% of the time \\(\\mu - .10 \\leq \\eta \\leq \\mu + .10\\)\n\r\rWhat does this mean?\nIf I repeatedly pick a sample of size 1,000, observe \\(\\eta\\) and claim that \\(\\mu \\in \\left[\\eta - .05, \\eta + .05\\right]\\) (or that the error bar is \\(\\pm 0.05\\)), I will be right 99% of the time.\nOn any particular sample you may be wrong, but not often.\nNOTE\nThis week’s content is split into two “halves”: the critical data manipulation information contained below and a more-entertaining discussion of visualization included in the Exercises.\n\r\r\rThe tidyverse\rIn the first weeks’ content, we demonstrated how to manipulate vectors by reordering and subsetting them through indexing. However, once we start more advanced analyses, the preferred unit for data storage is not the vector but the data frame. In this lecture, we learn to work directly with data frames, which greatly facilitate the organization of information. We will be using data frames for the majority of this class and you will use them for the majority of your data science life (however long that might be). We will focus on a specific data format referred to as tidy and on specific collection of packages that are particularly helpful for working with tidy data referred to as the tidyverse.\nWe can load all the tidyverse packages at once by installing and loading the tidyverse package:2\nlibrary(tidyverse)\rWe will learn how to implement the tidyverse approach throughout the book, but before delving into the details, in this chapter we introduce some of the most widely used tidyverse functionality, starting with the dplyr package for manipulating data frames and the purrr package for working with functions. Note that the tidyverse also includes a graphing package, ggplot2, the readr package, and many others. In this lesson, we first introduce the concept of tidy data and then demonstrate how we use the tidyverse to work with data frames in this format.\nTidy data\rWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\nlibrary(dslabs)\rdata(murders)\rhead(murders)\r## state abb region population total\r## 1 Alabama AL South 4779736 135\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\r## 4 Arkansas AR South 2915918 93\r## 5 California CA West 37253956 1257\r## 6 Colorado CO West 5029196 65\rEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\nlibrary(dslabs)\rdata(\u0026quot;gapminder\u0026quot;)\rtidy_data \u0026lt;- gapminder %\u0026gt;%\rfilter(country %in% c(\u0026quot;South Korea\u0026quot;, \u0026quot;Germany\u0026quot;) \u0026amp; !is.na(fertility)) %\u0026gt;%\rselect(country, year, fertility)\rhead(tidy_data, 6)\r## country year fertility\r## 1 Germany 1960 2.41\r## 2 South Korea 1960 6.16\r## 3 Germany 1961 2.44\r## 4 South Korea 1961 5.99\r## 5 Germany 1962 2.47\r## 6 South Korea 1962 5.79\rThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n## country 1960 1961 1962\r## 1 Germany 2.41 2.44 2.47\r## 2 South Korea 6.16 5.99 5.79\rThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\nTRY IT\nExamine the built-in dataset co2. Which of the following is true:\r\rco2 is tidy data: it has one year for each row.\rco2 is not tidy: we need at least one column with a character vector.\rco2 is not tidy: it is a matrix instead of a data frame.\rco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\r\rExamine the built-in dataset ChickWeight. Which of the following is true:\r\rChickWeight is not tidy: each chick has more than one row.\rChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\rChickWeight is not tidy: we are missing the year column.\rChickWeight is tidy: it is stored in a data frame.\r\rExamine the built-in dataset BOD. Which of the following is true:\r\rBOD is not tidy: it only has six rows.\rBOD is not tidy: the first column is just an index.\rBOD is tidy: each row is an observation with two values (time and demand)\rBOD is tidy: all small datasets are tidy by definition.\r\rWhich of the following built-in datasets is tidy (you can pick more than one):\r\rBJsales\rEuStockMarkets\rDNase\rFormaldehyde\rOrange\rUCBAdmissions\r\r\r\rManipulating data frames\rThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\nAdding a column with mutate\rWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\nlibrary(dslabs)\rdata(\u0026quot;murders\u0026quot;)\rmurders \u0026lt;- mutate(murders, rate = total / population * 100000)\rNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\nhead(murders)\r## state abb region population total rate\r## 1 Alabama AL South 4779736 135 2.824424\r## 2 Alaska AK West 710231 19 2.675186\r## 3 Arizona AZ West 6392017 232 3.629527\r## 4 Arkansas AR South 2915918 93 3.189390\r## 5 California CA West 37253956 1257 3.374138\r## 6 Colorado CO West 5029196 65 1.292453\rAlthough we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\rSubsetting with filter\rNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\nfilter(murders, rate \u0026lt;= 0.71)\r## state abb region population total rate\r## 1 Hawaii HI West 1360301 7 0.5145920\r## 2 Iowa IA North Central 3046355 21 0.6893484\r## 3 New Hampshire NH Northeast 1316470 5 0.3798036\r## 4 North Dakota ND North Central 672591 4 0.5947151\r## 5 Vermont VT Northeast 625741 2 0.3196211\r\rSelecting columns with select\rAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\nnew_table \u0026lt;- select(murders, state, region, rate)\rfilter(new_table, rate \u0026lt;= 0.71)\r## state region rate\r## 1 Hawaii West 0.5145920\r## 2 Iowa North Central 0.6893484\r## 3 New Hampshire Northeast 0.3798036\r## 4 North Dakota North Central 0.5947151\r## 5 Vermont Northeast 0.3196211\rIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\nTRY IT\nLoad the dplyr package and the murders dataset.\r\rlibrary(dplyr)\rlibrary(dslabs)\rdata(murders)\rYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\nmurders \u0026lt;- mutate(murders, population_in_millions = population / 10^6)\rWe can write population rather than murders$population. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders \u0026lt;- [your code]) so we can keep using this variable.\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\n\rWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\r\rselect(murders, state, population) %\u0026gt;% head()\rUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\r\rfilter(murders, state == \u0026quot;New York\u0026quot;)\rYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\r\rno_florida \u0026lt;- filter(murders, state != \u0026quot;Florida\u0026quot;)\rCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\r\rfilter(murders, state %in% c(\u0026quot;New York\u0026quot;, \u0026quot;Texas\u0026quot;))\rCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\r\rfilter(murders, population \u0026lt; 5000000 \u0026amp; region == \u0026quot;Northeast\u0026quot;)\rMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank.\n\r\r\rThe pipe: %\u0026gt;%\rWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %\u0026gt;%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\r\\rightarrow \\mbox{ select }\r\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %\u0026gt;%. The code looks like this:\nmurders %\u0026gt;% select(state, region, rate) %\u0026gt;% filter(rate \u0026lt;= 0.71)\r## state region rate\r## 1 Hawaii West 0.5145920\r## 2 Iowa North Central 0.6893484\r## 3 New Hampshire Northeast 0.3798036\r## 4 North Dakota North Central 0.5947151\r## 5 Vermont Northeast 0.3196211\rThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n16 %\u0026gt;% sqrt()\r## [1] 4\rWe can continue to pipe values along:\n16 %\u0026gt;% sqrt() %\u0026gt;% log2()\r## [1] 2\rThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n16 %\u0026gt;% sqrt() %\u0026gt;% log(base = 2)\r## [1] 2\rTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\nmurders %\u0026gt;% select(state, region, rate) %\u0026gt;% filter(rate \u0026lt;= 0.71)\rmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe. It’s worth noting that as of R 4.1, there is a base-R version of the pipe |\u0026gt;, though this has its disadvantages. We’ll stick with %\u0026gt;% for now.\nTRY IT\nThe pipe %\u0026gt;% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\r\rmurders \u0026lt;- mutate(murders, rate = total / population * 100000,\rrank = rank(-rate))\rIn the solution to the previous exercise, we did the following:\nmy_states \u0026lt;- filter(murders, region %in% c(\u0026quot;Northeast\u0026quot;, \u0026quot;West\u0026quot;) \u0026amp;\rrate \u0026lt; 1)\rselect(my_states, state, rate, rank)\rThe pipe %\u0026gt;% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\nmutate(murders, rate = total / population * 100000,\rrank = rank(-rate)) %\u0026gt;%\rselect(state, rate, rank)\rNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %\u0026gt;%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %\u0026gt;% to do this in just one line.\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %\u0026gt;%. The code should look something like this:\r\rmy_states \u0026lt;- murders %\u0026gt;%\rmutate SOMETHING %\u0026gt;%\rfilter SOMETHING %\u0026gt;%\rselect SOMETHING\r\r\rSummarizing data\rAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\nsummarize\rThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\nlibrary(dplyr)\rlibrary(dslabs)\rdata(heights)\rThe following code computes the average and standard deviation for females:\ns \u0026lt;- heights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rsummarize(average = mean(height), standard_deviation = sd(height))\rs\r## average standard_deviation\r## 1 64.93942 3.760656\rThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\ns$average\r## [1] 64.93942\rs$standard_deviation\r## [1] 3.760656\rAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rsummarize(median = median(height), minimum = min(height),\rmaximum = max(height))\r## median minimum maximum\r## 1 64.98031 51 79\rWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rsummarize(range = quantile(height, c(0, 0.5, 1)))\rwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In later sections, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\nmurders \u0026lt;- murders %\u0026gt;% mutate(rate = total/population*100000)\rRemember that the US murder rate is not the average of the state murder rates:\nsummarize(murders, mean(rate))\r## mean(rate)\r## 1 2.779125\rThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\nus_murder_rate \u0026lt;- murders %\u0026gt;%\rsummarize(rate = sum(total) / sum(population) * 100000)\rus_murder_rate\r## rate\r## 1 3.034555\rThis computation counts larger states proportionally to their size which results in a larger value.\n\rpull\rThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\nclass(us_murder_rate)\r## [1] \u0026quot;data.frame\u0026quot;\rsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\nus_murder_rate %\u0026gt;% pull(rate)\r## [1] 3.034555\rThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\nus_murder_rate \u0026lt;- murders %\u0026gt;%\rsummarize(rate = sum(total) / sum(population) * 100000) %\u0026gt;%\rpull(rate)\rus_murder_rate\r## [1] 3.034555\rwhich is now a numeric:\nclass(us_murder_rate)\r## [1] \u0026quot;numeric\u0026quot;\r\rGroup then summarize with group_by\rA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\nheights %\u0026gt;% group_by(sex)\r## # A tibble: 1,050 × 2\r## # Groups: sex [2]\r## sex height\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Male 75\r## 2 Male 70\r## 3 Male 68\r## 4 Male 74\r## 5 Male 61\r## 6 Female 65\r## 7 Female 66\r## 8 Female 62\r## 9 Female 66\r## 10 Male 67\r## # … with 1,040 more rows\rThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rsummarize(average = mean(height), standard_deviation = sd(height))\r## # A tibble: 2 × 3\r## sex average standard_deviation\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Female 64.9 3.76\r## 2 Male 69.3 3.61\rThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\nmurders %\u0026gt;%\rgroup_by(region) %\u0026gt;%\rsummarize(median_rate = median(rate))\r## # A tibble: 4 × 2\r## region median_rate\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Northeast 1.80\r## 2 South 3.40\r## 3 North Central 1.97\r## 4 West 1.29\r\r\rSorting data frames\rWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\nmurders %\u0026gt;%\rarrange(population) %\u0026gt;%\rhead()\r## state abb region population total rate\r## 1 Wyoming WY West 563626 5 0.8871131\r## 2 District of Columbia DC South 601723 99 16.4527532\r## 3 Vermont VT Northeast 625741 2 0.3196211\r## 4 North Dakota ND North Central 672591 4 0.5947151\r## 5 Alaska AK West 710231 19 2.6751860\r## 6 South Dakota SD North Central 814180 8 0.9825837\rWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\nmurders %\u0026gt;%\rarrange(rate) %\u0026gt;%\rhead()\r## state abb region population total rate\r## 1 Vermont VT Northeast 625741 2 0.3196211\r## 2 New Hampshire NH Northeast 1316470 5 0.3798036\r## 3 Hawaii HI West 1360301 7 0.5145920\r## 4 North Dakota ND North Central 672591 4 0.5947151\r## 5 Iowa IA North Central 3046355 21 0.6893484\r## 6 Idaho ID West 1567582 12 0.7655102\rNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\nmurders %\u0026gt;%\rarrange(desc(rate))\rNested sorting\rIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\nmurders %\u0026gt;%\rarrange(region, rate) %\u0026gt;%\rhead()\r## state abb region population total rate\r## 1 Vermont VT Northeast 625741 2 0.3196211\r## 2 New Hampshire NH Northeast 1316470 5 0.3798036\r## 3 Maine ME Northeast 1328361 11 0.8280881\r## 4 Rhode Island RI Northeast 1052567 16 1.5200933\r## 5 Massachusetts MA Northeast 6547629 118 1.8021791\r## 6 New York NY Northeast 19378102 517 2.6679599\r\rThe top \\(n\\)\rIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\nmurders %\u0026gt;% top_n(5, rate)\r## state abb region population total rate\r## 1 District of Columbia DC South 601723 99 16.452753\r## 2 Louisiana LA South 4533372 351 7.742581\r## 3 Maryland MD South 5773552 293 5.074866\r## 4 Missouri MO North Central 5988927 321 5.359892\r## 5 South Carolina SC South 4625364 207 4.475323\rNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange.\rNote that if the third argument is left blank, top_n filters by the last column.\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\nlibrary(NHANES)\rdata(NHANES)\rThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\nlibrary(dslabs)\rdata(na_example)\rmean(na_example)\r## [1] NA\rsd(na_example)\r## [1] NA\rTo ignore the NAs we can use the na.rm argument:\nmean(na_example, na.rm = TRUE)\r## [1] 2.301754\rsd(na_example, na.rm = TRUE)\r## [1] 1.22338\rLet’s now explore the NHANES data.\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\r\rHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\n\rNow report the min and max values for the same group.\n\rCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\n\rRepeat exercise 4 for males.\n\rWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\n\rFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure.\n\r\r\r\r\rTibbles\rTidy data must be stored in data frames. We introduced the data frame in Section ?? and have been using the murders data frame throughout the book. In Section ?? we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\nmurders %\u0026gt;% group_by(region)\r## # A tibble: 51 × 6\r## # Groups: region [4]\r## state abb region population total rate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Alabama AL South 4779736 135 2.82\r## 2 Alaska AK West 710231 19 2.68\r## 3 Arizona AZ West 6392017 232 3.63\r## 4 Arkansas AR South 2915918 93 3.19\r## 5 California CA West 37253956 1257 3.37\r## 6 Colorado CO West 5029196 65 1.29\r## 7 Connecticut CT Northeast 3574097 97 2.71\r## 8 Delaware DE South 897934 38 4.23\r## 9 District of Columbia DC South 601723 99 16.5 ## 10 Florida FL South 19687653 669 3.40\r## # … with 41 more rows\rNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followd by dimensions. We can learn the class of the returned object using:\nmurders %\u0026gt;% group_by(region) %\u0026gt;% class()\r## [1] \u0026quot;grouped_df\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\rThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble. For example, in Chapter ?? we will see that tidyverse functions used to import data create tibbles.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\nTibbles display better\rThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\rSubsets of tibbles are tibbles\rIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\nclass(murders[,4])\r## [1] \u0026quot;numeric\u0026quot;\ris not a data frame. With tibbles this does not happen:\nclass(as_tibble(murders)[,4])\r## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\rThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\nclass(as_tibble(murders)$population)\r## [1] \u0026quot;numeric\u0026quot;\rA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\nmurders$Population\r## NULL\rreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\nas_tibble(murders)$Population\r## Warning: Unknown or uninitialised column: `Population`.\r## NULL\r\rTibbles can have complex entries\rWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\r## # A tibble: 3 × 2\r## id func ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt;\r## 1 1 \u0026lt;fn\u0026gt; ## 2 2 \u0026lt;fn\u0026gt; ## 3 3 \u0026lt;fn\u0026gt;\r\rTibbles can be grouped\rThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\rCreate a tibble using tibble instead of data.frame\rIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\ngrades \u0026lt;- tibble(names = c(\u0026quot;John\u0026quot;, \u0026quot;Juan\u0026quot;, \u0026quot;Jean\u0026quot;, \u0026quot;Yao\u0026quot;),\rexam_1 = c(95, 80, 90, 85),\rexam_2 = c(90, 85, 85, 90))\rNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\ngrades \u0026lt;- data.frame(names = c(\u0026quot;John\u0026quot;, \u0026quot;Juan\u0026quot;, \u0026quot;Jean\u0026quot;, \u0026quot;Yao\u0026quot;),\rexam_1 = c(95, 80, 90, 85),\rexam_2 = c(90, 85, 85, 90))\rclass(grades$names)\r## [1] \u0026quot;character\u0026quot;\rTo avoid this, we use the rather cumbersome argument stringsAsFactors:\ngrades \u0026lt;- data.frame(names = c(\u0026quot;John\u0026quot;, \u0026quot;Juan\u0026quot;, \u0026quot;Jean\u0026quot;, \u0026quot;Yao\u0026quot;),\rexam_1 = c(95, 80, 90, 85),\rexam_2 = c(90, 85, 85, 90),\rstringsAsFactors = FALSE)\rclass(grades$names)\r## [1] \u0026quot;character\u0026quot;\rTo convert a regular data frame to a tibble, you can use the as_tibble function.\nas_tibble(grades) %\u0026gt;% class()\r## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\r\r\rThe dot operator\rOne of the advantages of using the pipe %\u0026gt;% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\ntab_1 \u0026lt;- filter(murders, region == \u0026quot;South\u0026quot;)\rtab_2 \u0026lt;- mutate(tab_1, rate = total / population * 10^5)\rrates \u0026lt;- tab_2$rate\rmedian(rates)\r## [1] 3.398069\rWe can avoid defining any new intermediate objects by instead typing:\nfilter(murders, region == \u0026quot;South\u0026quot;) %\u0026gt;%\rmutate(rate = total / population * 10^5) %\u0026gt;%\rsummarize(median = median(rate)) %\u0026gt;%\rpull(median)\r## [1] 3.398069\rWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\nrates \u0026lt;- filter(murders, region == \u0026quot;South\u0026quot;) %\u0026gt;%\rmutate(rate = total / population * 10^5) %\u0026gt;%\r.$rate\rmedian(rates)\r## [1] 3.398069\rIn the next section, we will see other instances in which using the . is useful.\n\rdo\rThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %\u0026gt;%, tidyverse functions consistently return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described earlier. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn the summarize section (above), we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive an error: Error: expecting result of length one, got : 2.\ndata(heights)\rheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rsummarize(range = quantile(height, c(0, 0.5, 1)))\rWe can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame. Note that it returns a single-row data frame.\nmy_summary \u0026lt;- function(dat){\rx \u0026lt;- quantile(dat$height, c(0, 0.5, 1))\rtibble(min = x[1], median = x[2], max = x[3])\r}\rWe can now apply the function to the heights dataset to obtain the summaries:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rmy_summary\r## # A tibble: 1 × 3\r## min median max\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 50 68.5 82.7\rBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rdo(my_summary(.))\r## # A tibble: 2 × 4\r## # Groups: sex [2]\r## sex min median max\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Female 51 65.0 79 ## 2 Male 50 69 82.7\rNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rdo(my_summary())\rIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rdo(my_summary)\r\rThe purrr package\rIn previous sections (and labs) we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\ncompute_s_n \u0026lt;- function(n){\rx \u0026lt;- 1:n\rsum(x)\r}\rn \u0026lt;- 1:25\rs_n \u0026lt;- sapply(n, compute_s_n)\rs_n\r## [1] 1 3 6 10 15 21 28 36 45 55 66 78 91 105 120 136 153 171 190\r## [20] 210 231 253 276 300 325\rThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\nlibrary(purrr) # or library(tidyverse)\rn \u0026lt;- 1:25\rs_n \u0026lt;- map(n, compute_s_n)\rclass(s_n)\r## [1] \u0026quot;list\u0026quot;\rIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\ns_n \u0026lt;- map_dbl(n, compute_s_n)\rclass(s_n)\r## [1] \u0026quot;numeric\u0026quot;\rThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\ns_n \u0026lt;- map_df(n, compute_s_n)\rWe need to change the function to make this work:\ncompute_s_n \u0026lt;- function(n){\rx \u0026lt;- 1:n\rtibble(sum = sum(x))\r}\rs_n \u0026lt;- map_df(n, compute_s_n)\rhead(s_n)\r## # A tibble: 6 × 1\r## sum\r## \u0026lt;int\u0026gt;\r## 1 1\r## 2 3\r## 3 6\r## 4 10\r## 5 15\r## 6 21\rThe purrr package provides much more functionality not covered here. For more details you can consult this online resource.\n\rTidyverse conditionals\rA typical data analysis will often involve one or more conditional operations. In the section on Conditionals, we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\ncase_when\rThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\nx \u0026lt;- c(-2, -1, 0, 1, 2)\rcase_when(x \u0026lt; 0 ~ \u0026quot;Negative\u0026quot;,\rx \u0026gt; 0 ~ \u0026quot;Positive\u0026quot;,\rx == 0 ~ \u0026quot;Zero\u0026quot;)\r## [1] \u0026quot;Negative\u0026quot; \u0026quot;Negative\u0026quot; \u0026quot;Zero\u0026quot; \u0026quot;Positive\u0026quot; \u0026quot;Positive\u0026quot;\rA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\nmurders %\u0026gt;%\rmutate(group = case_when(\rabb %in% c(\u0026quot;ME\u0026quot;, \u0026quot;NH\u0026quot;, \u0026quot;VT\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;RI\u0026quot;, \u0026quot;CT\u0026quot;) ~ \u0026quot;New England\u0026quot;,\rabb %in% c(\u0026quot;WA\u0026quot;, \u0026quot;OR\u0026quot;, \u0026quot;CA\u0026quot;) ~ \u0026quot;West Coast\u0026quot;,\rregion == \u0026quot;South\u0026quot; ~ \u0026quot;South\u0026quot;,\rTRUE ~ \u0026quot;Other\u0026quot;)) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rsummarize(rate = sum(total) / sum(population) * 10^5)\r## # A tibble: 4 × 2\r## group rate\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 New England 1.72\r## 2 Other 2.71\r## 3 South 3.63\r## 4 West Coast 2.90\r\rbetween\rA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\nx \u0026gt;= a \u0026amp; x \u0026lt;= b\rHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\nbetween(x, a, b)\rTRY IT\nLoad the murders dataset. Which of the following is true?\r\rmurders is in tidy format and is stored in a tibble.\rmurders is in tidy format and is stored in a data frame.\rmurders is not in tidy format and is stored in a tibble.\rmurders is not in tidy format and is stored in a data frame.\r\rUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\n\rUse the group_by function to convert murders into a tibble that is grouped by region.\n\rWrite tidyverse code that is equivalent to this code:\n\r\rexp(mean(log(murders$population)))\rWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %\u0026gt;%.\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number.\r\r\r  ```\r--\r\r\r\r\rI discovered the emo::ji() function at 8:55am. My wife joked that I would find a way to use the poop emoji by 9:00am. It is now 8:59am. She was right.↩︎\n\rIf you have not installed this package already, you must use install.packages(\"tidyverse\") prior to the library() call you see below.↩︎\n\r\r\r","date":1661731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661811829,"objectID":"fbbae951c935dd3d35f82710943d5efd","permalink":"https://ssc442kirkpatrick.netlify.app/content/01-content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/content/01-content/","section":"content","summary":"Readings\r\rGuiding Question\r\rGroup Projects\r\rTeams\r\rRandomness and Data Analytics\r\rLearning From Data\rFormalization\rThe Target Function\rWhy Estimate an Unknown Function?\rThe Parable of the Marbles\rOutside the Data\rHoeffding’s Inequality\rAn example of Hoeffding’s Inequality\r\rThe tidyverse\r\rTidy data\rManipulating data frames\r\rAdding a column with mutate\rSubsetting with filter\rSelecting columns with select\r\rThe pipe: %\u0026gt;%\rSummarizing data\r\rsummarize\rpull\rGroup then summarize with group_by\r\rSorting data frames\r\rNested sorting\rThe top \\(n\\)\r\rTibbles\r\rTibbles display better\rSubsets of tibbles are tibbles\rTibbles can have complex entries\rTibbles can be grouped\rCreate a tibble using tibble instead of data.","tags":null,"title":"Introduction to the tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"\r\rReadings\r\rGuiding Questions\rStarting up\r\rScales and transformations\r\rLog transformations\rTransforming data vs. transforming using scale_...\r\rAxis labels, legends, and titles\r\rChanging axis titles\rTitles\r\rAxis ticks\rAdditional geometries\r\rgeom_line\rUsing different data with different geometries\rMultiple geometries\r\rTry it!\r\r\rReadings\r\rThis page.\r\rGuiding Questions\r\rNo guiding questions today as we’re mostly learning some technical aspects of ggplot\rWe have largely reversed our content and example lectures for data viz.\r\r\rStarting up\rLoad up our murders data\nlibrary(dslabs)\rlibrary(ggplot2)\rlibrary(dplyr)\rdata(murders)\rp \u0026lt;- ggplot(data = murders, aes(x = population, y = total, label = abb))\r\r\rScales and transformations\rLog transformations\rLast lecture, we re-scaled our population by 10^6 (millions), but still had a lot of variation because some states are tiny and some are huge. Sometimes, we want to have one (or both) of our axes scaled non-linearly. For instance, if we wanted to have our x-axis be in log base 10, then each major tick would represent a factor of 10 over the last. This is not the default, so this change needs to be added through a scales layer. A quick look at the cheat sheet reveals the scale_x_continuous function lets us control the behavior of scales. We use them like this:\np + geom_point(size = 3) +\rgeom_text(nudge_x = 0.05) +\rscale_x_continuous(trans = \u0026quot;log10\u0026quot;) +\rscale_y_continuous(trans = \u0026quot;log10\u0026quot;)\rA couple of things here: adding things like scale_x_continuous(...) operates on the whole plot. In some cases, order matters, but it doesn’t here, so we can throw scale_x_continuous anywhere. Because we have altered the whole plot’s scale to be in the log-scale now, the nudge must be made smaller. It is in log-base-10 units. Using ?scale_x_continuous brings us to the help for both scale_x_continuous and scale_y_continuous, which shows us the options for transformations trans = ...\nThis particular transformation is so common that ggplot2 provides the specialized functions scale_x_log10 and scale_y_log10 which “inherit” (take the place of) the scale_x_continuous functions but have log base 10 as default. We can use these to rewrite the code like this:\np + geom_point(size = 3) +\rgeom_text(nudge_x = 0.05) +\rscale_x_log10() +\rscale_y_log10()\rThis can make a plot much easier to read, though one has to be sure to pay attention to the values on the axes. Plotting anything with very large outliers will almost always be better if done in log-scale. Adding the scale layer is an easy way to fix this.\nWe can also use one of many built-in transformations. Of note: reverse just inverts the scale, which can be helpful, log uses the natural log, sqrt takes the square root (dropping anything with a negative value), reciprocal takes 1/x. If your x-axis is in a date format, you can also scale to hms (hour-minute-second) or date.\n\rTransforming data vs. transforming using scale_...\rWe could simply take the log of population and log of total in the call and we’d get something very similar. Note that we had to override the aesthetic mapping set in p in each of the geometries:\np + geom_point(aes(x = log(population, base=10), y = log(total, base=10)), size = 3) +\rgeom_text(aes(x = log(population, base=10), y = log(total, base=10)), nudge_x = 0.05) \rThis avoids using scale_x_continuous or it’s child function scale_x_log10. One advantage to using scale_x... is that the axes are correctly labeled. When we transform the data directly, the axis labels only show the transformed values, so 7,000,000 becomes 7.0. This could be confusing! We could update the axis labels to say “total murders (log base 10)” and “total population (log base 10)”, but that’s cumbersome. Using scale_x... is a lot more refined and easy.\n\r\rAxis labels, legends, and titles\rBut let’s say we did want to re-name our x-axis label. Or maybe we don’t like that the variable column name is lower-case “p”.\nAs with many things in ggplot, there are many ways to get the same result. We’ll go over one way of changing titles and labels, but know that there are many more.\nChanging axis titles\rWe’ll use the labs(...) annotation layer to do this, which is pretty straightforward. ?labs shows us what we can change, and while it looks pretty basic, the real meat is in the ... argument, which the help says is “A list of new name-value pairs”. This means we can re-define the label on anything that is an aesthetic mapping. X and Y are aesthetic mappings, so…\np + geom_point(size = 3) +\rgeom_text(nudge_x = 0.05) +\rscale_x_log10() +\rscale_y_log10() + labs(x = \u0026#39;Population\u0026#39;, y = \u0026#39;Total murders\u0026#39;)\rNow, let’s use an aesthetic mapping that generates a legend, like color, and see what labs renames:\np + geom_point(aes(color = region), size = 3) +\rgeom_text(nudge_x = 0.05) +\rscale_x_log10() +\rscale_y_log10() + labs(x = \u0026#39;Population\u0026#39;, y = \u0026#39;Total murders\u0026#39;, color = \u0026#39;US Region\u0026#39;)\rWe can rename the aesthetic mapping-relevant label using labs. Even if there are multiple mapped aesthetics:\np + geom_point(aes(color = region, size = total/population)) +\rgeom_text(nudge_x = 0.05) +\rscale_x_log10() +\rscale_y_log10() + labs(x = \u0026#39;Population\u0026#39;, y = \u0026#39;Total murders\u0026#39;, color = \u0026#39;US Region\u0026#39;, size = \u0026#39;Murder rate\u0026#39;)\r\rTitles\rIn ?labs, we also see some things that look like titles and captions. We can include those:\np + geom_point(aes(color = region, size = total/population)) +\rgeom_text(nudge_x = 0.05) +\rscale_x_log10() +\rscale_y_log10() + labs(x = \u0026#39;Population\u0026#39;, y = \u0026#39;Total murders\u0026#39;, color = \u0026#39;US Region\u0026#39;, size = \u0026#39;Murder rate\u0026#39;,\rtitle = \u0026#39;This is a title\u0026#39;, subtitle = \u0026#39;This is a subtitle\u0026#39;, caption = \u0026#39;This is a caption\u0026#39;, tag = \u0026#39;This is a tag\u0026#39;)\rNow that you know how, always label your plots with at least a title and have meaningful axis and legend labels.\n\r\rAxis ticks\rIn addition to the axis labels, we may want to format or change the axis tick labels (like “1e+06” above) or even where the tick marks and lines are drawn. If we don’t specify anything, the axis labels and tick marks are drawn as best as ggplot can do, but we can change this. This might be especially useful if our data has some meaningful cutoffs that aren’t found by the default, or we just don’t like where the marks fall or how they are labeled. This is easy to fix with ggplot.\nTo change the tick mark labels, we have to set the tick mark locations. Then we can set a label for each tick mark. Let’s go back to our murders data and, for simplicity, take the log transformation off the Y axis. We’ll use scale_y_continuous to tell R where to put the breaks (breaks =) and what to label the breaks. We have to give it one label for every break. Let’s say we just want a line at the 500’s and let’s say we want to (absurdly) use written numerics for each of the Y-axis lines. Since scale_y_log10 inherits from scale_y_continuous, we can just use that and add the breaks and labels:\np + geom_point(aes(color = region), size = 3) +\rgeom_text(nudge_x = .05) +\rscale_x_log10() +\rscale_y_log10(breaks = c(0,50, 100, 500,1000,1500), labels = c(\u0026#39;Zero\u0026#39;,\u0026#39;Fifty\u0026#39;,\u0026#39;One hundred\u0026#39;,\u0026#39;Five hundred\u0026#39;,\u0026#39;One thousand\u0026#39;,\u0026#39;Fifteen hundred\u0026#39;)) +\rlabs(x = \u0026#39;Population\u0026#39;, y = \u0026#39;Total murders\u0026#39;, color = \u0026#39;US Region\u0026#39;)\rWe have manually set both the location and the label for the y-axis. Note that R filled in the in-between “minor” tick lines, but we can take those out. Since we are setting the location of the lines, we can do anything we want:\np + geom_point(aes(color = region), size = 3) +\rgeom_text(nudge_x = .05) +\rscale_x_log10() +\rscale_y_log10(breaks = c(0,50, 100, 721, 1000,1500), labels = c(\u0026#39;Zero\u0026#39;,\u0026#39;Fifty\u0026#39;,\u0026#39;One hundred\u0026#39;,\u0026#39;Seven hundred twenty one\u0026#39;,\u0026#39;One thousand\u0026#39;,\u0026#39;Fifteen hundred\u0026#39;),\rminor_breaks = NULL) +\rlabs(x = \u0026#39;Population\u0026#39;, y = \u0026#39;Total murders\u0026#39;, color = \u0026#39;US Region\u0026#39;)\rSo we can now define where axis tick lines should lie and how they should be labeled.\n\rAdditional geometries\rLet’s say we are happy with our axis tick locations, but we want to add a single additional line. Maybe we want to divide at 1,000,000 population (a vertical line at 1,000,000) becuase we think those over 1,000,000 are somehow different, and we want to call attention to the data around that point. As a more general example, if we were to plot, say, car accidents by age, we would maybe want to label age 21, when people can legally purchase alcohol (and subsequently cause car accidents).\nThis brings us to our first additional geometry beyond geom_point (OK, we used geom_text, but that’s more of an annotation). geom_vline lets us add a single vertical line (without aesthetic mappings). If we look at ?geom_vline we see that it requires ones aesthetic:xintercept. It also takes aesthetics like color and size, and introduces the linetype aesthetic:\np + geom_point(aes(color = region), size = 3) +\rgeom_text(nudge_x = .05) +\rgeom_vline(aes(xintercept = 1000000), col = \u0026#39;red\u0026#39;, size = 2, linetype = 2) +\rscale_x_log10() +\rscale_y_log10(breaks = c(0,50, 100, 721, 1000,1500), labels = c(\u0026#39;Zero\u0026#39;,\u0026#39;Fifty\u0026#39;,\u0026#39;One hundred\u0026#39;,\u0026#39;Seven hundred twenty one\u0026#39;,\u0026#39;One thousand\u0026#39;,\u0026#39;Fifteen hundred\u0026#39;),\rminor_breaks = NULL) +\rlabs(x = \u0026#39;Population\u0026#39;, y = \u0026#39;Total murders\u0026#39;, color = \u0026#39;US Region\u0026#39;)\rCombining geometries is as easy as adding the layers with +.\ngeom_line\rFor a good old line plot, we use the line geometry at geom_line. The help for ?geom_line tells us that we need an x and a y aesthetic (much like geom_points). Since our murders data isn’t really suited to a line graph, we’ll use a daily stock price. We’ll get this using tidyquant, which pulls stock prices from Yahoo Finance and maintains the “tidy” format. You’ll need to install.packages('tidyquant') before you run this the first time.\nlibrary(tidyquant)\rAAPL = tq_get(\u0026quot;AAPL\u0026quot;, from = \u0026#39;2009-01-01\u0026#39;, to = \u0026#39;2021-08-01\u0026#39;, get = \u0026#39;stock.prices\u0026#39;)\rhead(AAPL)\r## # A tibble: 6 × 8\r## symbol date open high low close volume adjusted\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 AAPL 2009-01-02 3.07 3.25 3.04 3.24 746015200 2.77\r## 2 AAPL 2009-01-05 3.33 3.44 3.31 3.38 1181608400 2.88\r## 3 AAPL 2009-01-06 3.43 3.47 3.30 3.32 1289310400 2.84\r## 4 AAPL 2009-01-07 3.28 3.30 3.22 3.25 753048800 2.78\r## 5 AAPL 2009-01-08 3.23 3.33 3.22 3.31 673500800 2.83\r## 6 AAPL 2009-01-09 3.33 3.34 3.22 3.24 546845600 2.76\rNow, we can plot a line graph of the Apple closing stock price over the requested date range. We want this to be a time series, so the x-axis will be the date and the y-axis will be the closing price.\nggplot(AAPL, aes(x = date, y = close)) +\rgeom_line() +\rlabs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;Closing price\u0026#39;, title = \u0026#39;Apple stock price\u0026#39;)\rIn geom_line, R will automatically sort on the x-variable. If you don’t want this, then geom_path will use whatever order the data is in. Either way, if you have multiple observations for the same value on the x-axis, then you’ll get something pretty messy because R will try to connect, in some order, all the points. Let’s see an example with two stocks:\nAAPLNFLX = tq_get(c(\u0026quot;AAPL\u0026quot;,\u0026quot;NFLX\u0026quot;), from = \u0026#39;2021-01-01\u0026#39;, to = \u0026#39;2021-08-01\u0026#39;, get = \u0026#39;stock.prices\u0026#39;)\rggplot(AAPLNFLX, aes(x = date, y = close)) +\rgeom_line() +\rlabs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;Closing price\u0026#39;, title = \u0026#39;Apple and Netflix stock price\u0026#39;)\rThat looks kinda strange. That’s because, for every date, we have two values - the NFLX and the AAPL value, so each day has a vertical line drawn between the two prices. This is nonsense, especially since what we want to see is the history of NFLX and AAPL over time.\nAesthetics to the rescue! Remember, when we use an aesthetic mapping, we are able to separate out data by things like color or linetype. Let’s use color as the aesthetic here, and map it to the stock ticker:\nAAPLNFLX = tq_get(c(\u0026quot;AAPL\u0026quot;,\u0026quot;NFLX\u0026quot;), from = \u0026#39;2021-01-01\u0026#39;, to = \u0026#39;2021-08-01\u0026#39;, get = \u0026#39;stock.prices\u0026#39;)\rggplot(AAPLNFLX, aes(x = date, y = close, color = symbol)) +\rgeom_line() +\rlabs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;Closing price\u0026#39;, title = \u0026#39;Apple and Netflix stock price\u0026#39;)\rWell there we go! We can now see each stock price over time, with a convenient legend. Later on, we’ll learn how to change the color palatte. If we don’t necessarily want a different color but we do want to separate the lines, we can use the group aesthetic.\nAAPLNFLX = tq_get(c(\u0026quot;AAPL\u0026quot;,\u0026quot;NFLX\u0026quot;), from = \u0026#39;2021-01-01\u0026#39;, to = \u0026#39;2021-08-01\u0026#39;, get = \u0026#39;stock.prices\u0026#39;)\rggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\rgeom_line() + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;Closing price\u0026#39;, title = \u0026#39;Apple and Netflix stock price\u0026#39;)\rSimilar result as geom_line, but without the color difference (which makes it rather hard to tell what you’re looking at). But if we add labels using geom_label, we’ll get one label for every point, which will be overwhelming. The solution? Use some filtered data so that there is only one point for each label. But that means replacing the data in ggplot. Here’s how.\n\rUsing different data with different geometries\rJust as we can use different aesthetic mappings on each geometry, we can use different data entirely. This is useful when we want one geometry to have one set of data (like the stock prices above), but another geometry to only have a subset of the data. Why would we want that? Well, we’d like to label just one part of each of the lines in our plot, right? That means we want to label a subset of the stock data.\nTo replace data in a geometry, we just need to specify the data = argument separately:\nggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\rgeom_line() +\rgeom_label(data = AAPLNFLX %\u0026gt;% group_by(symbol) %\u0026gt;% slice(100),\raes(label = symbol),\rnudge_y = 20) + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;Closing price\u0026#39;, title = \u0026#39;Apple and Netflix stock price\u0026#39;)\rIn geom_label, we specified we wanted the 100th observation from each symbol to be the label location. Then, we nudged it up along y by 20 so that it’s clear of the line.\nR also has a very useful ggrepel package that gives us geom_label_repel which takes care of the nudging for us, even in complicated situations (lots of points, lines, etc.). It does a decent job here of moving the label to a point where it doesn’t cover a lot of data.\nlibrary(ggrepel)\rggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\rgeom_line() +\rgeom_label_repel(data = AAPLNFLX %\u0026gt;% group_by(symbol) %\u0026gt;% slice(100),\raes(label = symbol)) + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;Closing price\u0026#39;, title = \u0026#39;Apple and Netflix stock price\u0026#39;)\rNow, we don’t lose a lot of space to a legend, and we haven’t had to use color to separate the stock symbols.\n\rMultiple geometries\rSince this section is about adding geometries, we can combine points and lines. Since lines connect points, it will look like a giant connect-the-dots.\nlibrary(ggrepel)\rggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +\rgeom_line() +\rgeom_point() + geom_label_repel(data = AAPLNFLX %\u0026gt;% group_by(symbol) %\u0026gt;% slice(100),\raes(label = symbol)) + labs(x = \u0026#39;Date\u0026#39;, y = \u0026#39;Closing price\u0026#39;, title = \u0026#39;Apple and Netflix stock price\u0026#39;)\r\r\rTry it!\rTRY IT\nStart by loading the dplyr and ggplot2 library as well as the murders and heights data.\nlibrary(dplyr)\rlibrary(ggplot2)\rlibrary(dslabs)\rdata(heights)\rdata(murders)\rWith ggplot2 plots can be saved as objects. For example we can associate a dataset with a plot object like this\r\rp \u0026lt;- ggplot(data = murders)\rBecause data is the first argument we don’t need to spell it out\np \u0026lt;- ggplot(murders)\rand we can also use the pipe:\np \u0026lt;- murders %\u0026gt;% ggplot()\rWhat is class of the object p?\nRemember that to print an object you can use the command print or simply type the object.\rPrint the object p defined in exercise one and describe what you see.\r\rNothing happens.\rA blank slate plot.\rA scatterplot.\rA histogram.\r\rUsing the pipe %\u0026gt;%, create an object p but this time associated with the heights dataset instead of the murders dataset.\n\rWhat is the class of the object p you have just created?\n\rNow we are going to add a layer and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the murders data frame to remind yourself what are the names for these two variables and select the correct answer. Hint: Look at ?murders.\n\r\rstate and abb.\rtotal_murders and population_size.\rtotal and population.\rmurders and size.\r\rTo create the scatterplot we add a layer with geom_point. The aesthetic mappings require us to define the x-axis and y-axis variables, respectively. So the code looks like this:\r\rmurders %\u0026gt;% ggplot(aes(x = , y = )) +\rgeom_point()\rexcept we have to define the two variables x and y. Fill this out with the correct variable names.\nNote that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this:\r\rmurders %\u0026gt;% ggplot(aes(population, total)) +\rgeom_point()\rRemake the plot but now with total in the x-axis and population in the y-axis.\nIf instead of points we want to add text, we can use the geom_text() or geom_label() geometries. The following code\r\rmurders %\u0026gt;% ggplot(aes(population, total)) + geom_label()\rwill give us the error message: Error: geom_label requires the following missing aesthetics: label\nWhy is this?\nWe need to map a character to each point through the label argument in aes.\rWe need to let geom_label know what character to use in the plot.\rThe geom_label geometry does not require x-axis and y-axis values.\rgeom_label is not a ggplot2 command.\r\rRewrite the code above to use abbreviation as the label through aes\n\rChange the color of the labels to blue. How will we do this?\n\r\rAdding a column called blue to murders.\rBecause each label needs a different color we map the colors through aes.\rUse the color argument in ggplot.\rBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\r\rRewrite the code above to make the labels blue.\n\rNow suppose we want to use color to represent the different regions. In this case which of the following is most appropriate:\n\r\rAdding a column called color to murders with the color we want to use.\rBecause each label needs a different color we map the colors through the color argument of aes .\rUse the color argument in ggplot.\rBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\r\rRewrite the code above to make the labels’ color be determined by the state’s region.\n\rNow we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by defining an object p holding the plot we have made up to now\n\r\rp \u0026lt;- murders %\u0026gt;%\rggplot(aes(population, total, label = abb, color = region)) +\rgeom_label()\rTo change the y-axis to a log scale we learned about the scale_x_log10() function. Add this layer to the object p to change the scale and render the plot.\nRepeat the previous exercise but now change both axes to be in the log scale.\n\rNow edit the code above to add the title “Gun murder data” to the plot. Hint: use the labs function or the ggtitle function.\n\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661170210,"objectID":"f4445f225dbc16ad343f16f0677c881d","permalink":"https://ssc442kirkpatrick.netlify.app/content/02-content/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/content/02-content/","section":"content","summary":"Readings\r\rGuiding Questions\rStarting up\r\rScales and transformations\r\rLog transformations\rTransforming data vs. transforming using scale_...\r\rAxis labels, legends, and titles\r\rChanging axis titles\rTitles\r\rAxis ticks\rAdditional geometries\r\rgeom_line\rUsing different data with different geometries\rMultiple geometries\r\rTry it!\r\r\rReadings\r\rThis page.\r\rGuiding Questions\r\rNo guiding questions today as we’re mostly learning some technical aspects of ggplot\rWe have largely reversed our content and example lectures for data viz.","tags":null,"title":"Effective Visualizations","type":"docs"}]