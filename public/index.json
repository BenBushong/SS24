[{"authors":["Ben"],"categories":null,"content":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron. Prior to coming to Michigan State University, I worked with the U.S. Army to help soldiers become more psychologically resilient.\nI hold a Ph.D. in Social Science (Economics) from the California Institute of Technology (Caltech), and a B.S. in Economics from the University of Oregon.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592849563,"objectID":"bf008f22d9b0754cde4f6972811c28b7","permalink":"https://ssc442kirkpatrick.netlify.app/authors/ben/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ben/","section":"authors","summary":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron.","tags":null,"title":"Ben Bushong","type":"authors"},{"authors":["Justin"],"categories":null,"content":"I am an Assistant Professor at Michigan State University who has not yet filled out this description.\nI hold a Ph.D. in Environmental Economics from Duke University, an M.E.M. in Environmental Policy and Economics from Duke University, and a B.S. in Environmental Policy Analysis and City Planning from the University of California at Davis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1607443236,"objectID":"8422260f0f3251af15c00666a8df9838","permalink":"https://ssc442kirkpatrick.netlify.app/authors/justin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/justin/","section":"authors","summary":"I am an Assistant Professor at Michigan State University who has not yet filled out this description.\nI hold a Ph.D. in Environmental Economics from Duke University, an M.E.M. in Environmental Policy and Economics from Duke University, and a B.S. in Environmental Policy Analysis and City Planning from the University of California at Davis.","tags":null,"title":"Justin Kirkpatrick","type":"authors"},{"authors":null,"categories":null,"content":"\rIn these (modestly) secret pages, I’ve included some resources for those who read the syllabus closely.\nIf you’re stuck with anything or want help with, say, using markdown, you’ll find some basic guidance here. Additionally, there are links throughout to outside resources.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1594409288,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"https://ssc442kirkpatrick.netlify.app/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"In these (modestly) secret pages, I’ve included some resources for those who read the syllabus closely.\nIf you’re stuck with anything or want help with, say, using markdown, you’ll find some basic guidance here. Additionally, there are links throughout to outside resources.","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":"\rEach week has a set of required readings that you should complete before coming to the (online) Tuesday lecture. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due). Don’t forget your weekly writing in between, due Saturday at 11:59pm.\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts. These questions can serve as helpful starting places for your thinking; they are not representative of the totality of the content and are not intended to be limiting. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read.\nAll lecture videos are posted to our SSC442 channel on Mediaspace .\n","date":1640995200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1641419217,"objectID":"1413f960ec974b9863bc45d887efa8bd","permalink":"https://ssc442kirkpatrick.netlify.app/content/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Each week has a set of required readings that you should complete before coming to the (online) Tuesday lecture. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due). Don’t forget your weekly writing in between, due Saturday at 11:59pm.","tags":null,"title":"Readings, lectures, and videos","type":"docs"},{"authors":null,"categories":null,"content":"\r\rWeekly Writings\rWeekly Writing Template\r\rLabs\rProjects\rFinal project\r\r\rThis course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nEngage with the readings and lecture materials\rRegularly use R (aka engage daily or almost every day in some way)\r\rEach type of assignment in this class helps with one of these strategies.\nDownload and save the following files (right-click to Save Link As…)\n\r Weekly writing template\n\r Lab assignment template\n\r\rWeekly Writings\rTo encourage you to actively engage with the course content, you will write a ≈150 word memorandum about the reading or lecture each week. That’s fairly short: there are ≈250 words on a typical double-spaced page. You must complete eleven of these in the course. I will drop your one lowest weekly writing score. Your actual prompt will be assigned in class, so you must login each day to ensure you get these assignments. To keep you on your toes, we will vary whether these are assigned on Tuesdays or Thursdays. Each week’s weekly writing will be due on D2L by 11:59pm on Saturday\nYou can do a lot of different things with this memo: discuss something you learned from the course content, write about the best or worst data visualization you saw recently, connect the course content to your own work, etc. These reflections let you explore and answer some of the key questions of this course, including:\n\rWhen is a link correlational vs causal? How can we still make useful statements about non-causal things?\rWhy do we visualize data?\rWhat makes a great data analysis? What makes a bad analysis?\rHow do you choose which kind of analysis method to use?\rWhat is the role of the data structure in choosing an analysis? Can we be flexible?\r\rThe course content for each day will also include a set of questions specific to that topic. You do not have to answer all (or any) of these questions. That would be impossible. They exist to guide your thinking and to make complex reading more digestible. The specific topic for each week will be assigned in class. (We can’t emphasize this enough.)\nThe TA will grade these mini-exercises using a very simple system:\n\r✔+: (11.5 points (115%) in gradebook) Work shows phenomenal thought and engagement with the course content. We will not assign these often.\r✔: (10 points (100%) in gradebook) Work is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance.\r✔−: (5 points (50%) in gradebook) Work is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.\r\r(There is an implicit 0 above for work that is not turned in by Saturday at 11:59pm). Notice that this is essentially a pass/fail or completion-based system. We’re not grading your writing ability; we’re not counting the exact number of words you’re writing; and we’re not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. We are looking for thoughtful engagement. Read the material, engage with the work and you’ll get a ✓.\nWeekly Writing Template\rYou will turn these reflections in via D2L. You will write them using R Markdown and this weekly writing template (right-click to Save Link As…) . You must knit your work to a PDF document (this will be what you turn in). D2L will have eleven weekly writing assignments available. Upload your first weekly writing assignment to number 1, your second (regardless of which week you are writing on) to number 2, etc.\n\r\rLabs\rEach week of the course has fully annotated examples of code that teach and demonstrate how to do specific tasks in R. However, without practicing these principles and making graphics on your own, you won’t remember what you learn.\nPlease do not do labs ahead of time. I am updating the assignments as the semester proceeds, and you may do an entire assignment that is completely changed.\nFor example, to practice working with ggplot2 and making data-based graphics, you will complete a brief set of exercises over a few class sessions. These exercises will have 1–3 short tasks that are directly related to the topic for the week. You need to show that you made a good faith effort to work each question. There will also be a final question which requires significantly more thought and work. This will be where you get to show some creativity and stretch your abilities. Overall, labs will be graded the same check system:\n\r✔+: (17.5 points (115%) in gradebook) Exercises are complete. Every task was attempted and answered, and most answers are correct. Knitted document is clean and easy to follow. Work on the final problem shows creativity or is otherwise exceptional. We will not assign these often.\r✔: (15 points (100%) in gradebook) Exercises are complete and most answers are correct. This is the expected level of performance.\r✔−: (7.5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. We will hopefully not assign these often, but subpar work can expect a ✔−.\r\rThere is an implicit 0 for any assignment not turned in on time. If you have only partial work, then turn that in for partial credit. As noted in the syllabus, we are not grading your coding ability. We are not checking each line of code to make sure it produces some exact final figure, and we do not expect perfection. Also note that a ✓ does not require 100% success. You will sometimes get stuck with weird errors that you can’t solve, or the demands of pandemic living might occasionally become overwhelming. We are looking for good faith effort. Try hard, engage with the task, and you’ll get a ✓.\nYou may work together on the labs, but you must turn in your own answers. You will turn these labs in via D2L. You will write them using R Markdown and must knit your work to a PDF document.\n\rProjects\rTo give you practice with the data and design principles you’ll learn in this class, you will complete two projects en route to the overarching final project of the course. Both these mini projects and the final project must be completed in groups.\nThe two (mini) projects are checkpoints to ensure you’re working on your project seriously. They will be graded using a check system:\n\r✔+: (55 points (≈115%) in gradebook) Project is phenomenally well-designed and uses advanced R techniques. The project uncovers an important story that is not readily apparent from just looking at the raw data. I will not assign these often.\r✔: (50 points (100%) in gradebook) Project is fine, follows most design principles, answers a question from the data, and uses R correctly. This is the expected level of performance.\r✔−: (25 points (50%) in gradebook) Project is missing large components, is poorly designed, does not answer a relevant question, and/or uses R incorrectly. This indicates that you need to improve next time. I will hopefully not assign these often.\r\rBecause these mini projects give you practice for the final project, we will provide you with substantial feedback on your design and code.\n\rFinal project\rAt the end of the course, you will demonstrate your skills by completing a final project. Complete details for the final project (including past examples of excellent projects) are here. In brief, the final project has the following elements:\nYou must find existing data to analyze.1 Aggregating data from multiple sources is encouraged, but is not required.\r\rYou must visualize (at least) three interesting features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.2\r\rYou must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.3\r\rYou must write your analysis as if presenting to a C-suite executive. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.\r\rThere is no final exam. This project is your final exam.\nThe project will not be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:\nTechnical skills: Was the project easy? Does it showcase mastery of data analysis?\rVisual design: Was the information smartly conveyed and usable? Was it beautiful?\rAnalytic design: Was the analysis appropriate? Was it sensible, given the dataset?\rStory: Did we learn something?\r\rIf you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.\n\r\rNote that existing is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged.↩︎\n\rPie charts of any kind will result in a 25% grade deduction.↩︎\n\rThis is an extremely dumb idea for a number of reasons. Moreover, it’s worth mentioning that sports data, while rich, can be overwhelming due to its sheer magnitude and the variety of approaches that can be applied. Use with caution.↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1610654899,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"https://ssc442kirkpatrick.netlify.app/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/","section":"assignment","summary":"Weekly Writings\rWeekly Writing Template\r\rLabs\rProjects\rFinal project\r\r\rThis course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nEngage with the readings and lecture materials\rRegularly use R (aka engage daily or almost every day in some way)\r\rEach type of assignment in this class helps with one of these strategies.","tags":null,"title":"Assignments and Evaluations","type":"docs"},{"authors":null,"categories":null,"content":"\rThis section contains the content covered in Thursday lectures and some annotated R code that you can use as a reference for creating your own work. The intention is that in the Content section, you will sequentially build up your understanding of R and data analytics; here, you can see how all the pieces work together.\nVisit this section after you have finished the readings in the Content section and any supplemental lecture videos.\n](https://mediaspace.msu.edu/channel/SSC442+-+Spring+2021+-+KIRKPATRICK/199607633/subscribe). --\r","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1641470874,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"https://ssc442kirkpatrick.netlify.app/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"This section contains the content covered in Thursday lectures and some annotated R code that you can use as a reference for creating your own work. The intention is that in the Content section, you will sequentially build up your understanding of R and data analytics; here, you can see how all the pieces work together.\nVisit this section after you have finished the readings in the Content section and any supplemental lecture videos.","tags":null,"title":"Examples","type":"docs"},{"authors":null,"categories":null,"content":"\r\rVisualizing data distributions\rVariable types\rCase study: describing student heights\rDistribution function\rCumulative distribution functions\r\rGeometries for describing distributions\rHistograms\rSmoothed density\rInterpreting the y-axis\rDensities permit stratification\r\rThe normal distribution\rStandard units\rQuantile-quantile plots\rPercentiles\r\rggplot2 geometries\rBarplots\rHistograms\rDensity plots\rBoxplots\r\rTry it!\r\r\r\rVisualizing data distributions\rThroughout your education, you may have noticed that numerical data is often summarized with the average value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the standard deviation. For example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list?\nOur first data visualization building block is learning to summarize lists of factors or numeric vectors—the two primary data types that we encounter in data analytics. More often than not, the best way to share or explore this summary is through data visualization. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as a distribution, there are several data visualization techniques to effectively relay this information.\nIn this section, we first discuss properties of a variety of distributions and how to visualize distributions using a motivating example of student heights. We then discuss some principles of data visualizations more broadly, and introduce new ggplot geometries to help us along the way.\nVariable types\rWe will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.\nWhen each entry in a vector comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as ordinal data. In psychology, a number of different terms are used for this same idea.\nExamples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as population sizes, are discrete because they have to be integers—that’s how we count.1\n\rCase study: describing student heights\rHere we consider an artificial problem to help us illustrate the underlying concepts.\nPretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the heights data frame:\nlibrary(tidyverse)\rlibrary(dslabs)\rdata(heights)\rOne way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights.\nDistribution function\rIt turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.\nThe most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:\n## ## Female Male ## 0.2266667 0.7733333\rThis two-category frequency table is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:\nThis particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.\n\rCumulative distribution functions\rNumerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters, respectively.\nStatistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below \\(a\\) for all possible values of \\(a\\). This function is called the cumulative distribution function (CDF). In statistics, the following notation is used:\n\\[ F(a) = \\mbox{Pr}(x \\leq a) \\]\nHere is a plot of \\(F\\) for the male height data:\nSimilar to what the frequency table does for categorical data, the CDF\rdefines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since \\(F(66)=\\) 0.1637931, or that 84% of the values are below 72, since \\(F(72)=\\) 0.841133,\rand so on. In fact, we can report the proportion of values between any two heights, say \\(a\\) and \\(b\\), by computing \\(F(b) - F(a)\\). This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.\nA final note: because CDFs can be defined mathematically—and absent any data—the word empirical is added to make the distinction when data is used. We therefore use the term empirical CDF (eCDF).\n\r\rGeometries for describing distributions\rNow, we’ll introduce ggplot geometries useful for describing distributions (or for many other things).\nHistograms\rAlthough the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? I doubt you can figure these out from glancing at the plot above. Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret.\nThe simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: \\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\\)\nheights %\u0026gt;%\rfilter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;black\u0026quot;)\rIf we send this histogram plot to some uninformed reader, she will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, this reader could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.\nWhat information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.\nThe geom_histogram layer only requires one aesthetic mapping - the x-axis. This is because the y-axis is computed from counts of the x-axis. Giving an aesthetic mapping to an additional variable for y will result in an error. Using an aesthetic mapping like fill will work - it’ll give you two histograms on top of each other. Try it! Try setting the alpha aesthetic to .5 (not an aesthetic mapping) so you can see both layers when they overlap.\n\rSmoothed density\rSmooth density plots are aesthetically more appealing than histograms. geom_density is the geometry that gives a smoothed density. Here is what a smooth density plot looks like for our heights data:\nheights %\u0026gt;%\rfilter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;%\rggplot(aes(height)) +\rgeom_density(alpha = .2, fill= \u0026quot;#00BFC4\u0026quot;, color = \u0026#39;gray50\u0026#39;) \rIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density. That is, the area under the curve will add up to 1, so we can read it like a probability density.\nTo understand the smooth densities, we have to understand estimates, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool.\nThe main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.\nHowever, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:\nThe smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:\nThe smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts. We do this by using the double-dot object ..density... Objects surrounded by .. are objects that are calculated by ggplot. If we look at ?geom_histogram, and go down to “Computed variables”, we see that we could use ..count.. to get “number of points in a bin”; ..ncount.. for the count scaled to a max of 1; or ..ndensity.. which scales the density to a max of 1 (which is a strange one). We can manually set the y aesthetic mapping, which defaults to ..count.., to ..density..:\nx %\u0026gt;% ggplot(aes(x = height)) +\rgeom_histogram(aes(y=..density..), binwidth = 0.1, color = \u0026quot;black\u0026quot;) \rNow, back to reality. We don’t have millions of measurements. In this concrete example, we have 812 and we can’t make a histogram with very small bins.\nWe therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots (loosely) demonstrate the steps that the computer goes through to ultimately create a smooth density:\n\rInterpreting the y-axis\rNote that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:\nThe proportion of this area is about\r0.3,\rmeaning that about\r30%\rof male heights are between 65 and 68 inches.\nBy understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:\nheights %\u0026gt;%\rfilter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) +\rgeom_density(alpha=.2, fill= \u0026quot;#00BFC4\u0026quot;, color = \u0026#39;black\u0026#39;) \rNote that the only aesthetic mapping is x = height, while the fill and color are set as un-mapped aesthetics.\n\rDensities permit stratification\rAs a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities make it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:\nheights %\u0026gt;%\rggplot(aes(height, fill=sex)) +\rgeom_density(alpha = 0.2, color = \u0026#39;black\u0026#39;)\rWith the right argument, ggplot automatically shades the intersecting region with a different color.\n\r\rThe normal distribution\rHistograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.\nThe normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.\nRather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula:\n\\[\\mbox{Pr}(a \u0026lt; x \u0026lt; b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}s} e^{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2} \\, dx\\]\nYou don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: \\(m\\) and \\(s\\). The rest of the symbols in the formula represent the interval ends that we determine, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(e\\). These two parameters, \\(m\\) and \\(s\\), are referred to as the average (also called the mean) and the standard deviation (SD) of the distribution, respectively.\nThe distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:\nThe fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.\nFor a list of numbers contained in a vector x, the average is defined as:\nm \u0026lt;- sum(x) / length(x)\rand the SD is defined as:\ns \u0026lt;- sqrt(sum((x-mu)^2) / length(x))\rwhich can be interpreted as the average distance between values and their average.\nLet’s compute the values for the height for males which we will store in the object \\(x\\):\nindex \u0026lt;- heights$sex == \u0026quot;Male\u0026quot;\rx \u0026lt;- heights$height[index]\rThe pre-built functions mean and sd (note that for reasons explained later, sd divides by length(x)-1 rather than length(x)) can be used here:\nm \u0026lt;- mean(x)\rs \u0026lt;- sd(x)\rc(average = m, sd = s)\r## average sd ## 69.314755 3.611024\rHere is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:\nNow, we can ask the question “is our height data approximately normally distributed?”. The normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.\nStandard units\rFor data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value x from a vector X, we define the value of x in standard units as z = (x - m)/s with m and s the average and standard deviation of X, respectively. Why is this convenient?\nFirst look back at the formula for the normal distribution and note that what is being exponentiated is \\(-z^2/2\\) with \\(z\\) equivalent to \\(x\\) in standard units. Because the maximum of \\(e^{-z^2/2}\\) is when \\(z=0\\), this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since \\(- z^2/2\\) is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (\\(z=0\\)), one of the largest (\\(z \\approx 2\\)), one of the smallest (\\(z \\approx -2\\)), or an extremely rare occurrence (\\(z \u0026gt; 3\\) or \\(z \u0026lt; -3\\)). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.\nIn R, we can obtain standard units using the function scale:\nz \u0026lt;- scale(x)\rNow to see how many men are within 2 SDs from the average, we simply type:\nmean(abs(z) \u0026lt; 2)\r## [1] 0.9495074\rThe proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.\n\rQuantile-quantile plots\rA systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot). If our heights distribution is really normal, then the 10th percentile of our heights data should be the same as the 10th percentile of a theoretical normal, as should the 20th, 30th, 33rd, 37.5th, etc. percentiles.\nFirst let’s define the theoretical quantiles (percentiles) for the normal distribution. In statistics books we use the symbol \\(\\Phi(x)\\) to define the function that gives us the probability of a standard normal distribution being smaller than \\(x\\). So, for example, \\(\\Phi(-1.96) = 0.025\\) and \\(\\Phi(1.96) = 0.975\\). In R, we can evaluate \\(\\Phi\\) using the pnorm function:\npnorm(-1.96)\r## [1] 0.0249979\rThe inverse function \\(\\Phi^{-1}(x)\\) gives us the theoretical quantiles for the normal distribution. So, for example, \\(\\Phi^{-1}(0.975) = 1.96\\). In R, we can evaluate the inverse of \\(\\Phi\\) using the qnorm function.\nqnorm(0.975)\r## [1] 1.959964\rNote that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution. We can do this using the mean and sd arguments in the pnorm and qnorm function. For example, we can use qnorm to determine quantiles of a distribution with a specific average and standard deviation\nqnorm(0.975, mean = 5, sd = 2)\r## [1] 8.919928\rFor the normal distribution, all the calculations related to quantiles are done without data, thus the name theoretical quantiles. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector \\(x\\), we can define the quantile associated with any proportion \\(p\\) as the \\(q\\) for which the proportion of values below \\(q\\) is \\(p\\). Using R code, we can define q as the value for which mean(x \u0026lt;= q) = p. Notice that not all \\(p\\) have a \\(q\\) for which the proportion is exactly \\(p\\). There are several ways of defining the best \\(q\\) as discussed in the help for the quantile function.\nTo give a quick example, for the male heights data, we have that:\nmean(x \u0026lt;= 69.5)\r## [1] 0.5147783\rSo about 50% are shorter or equal to 69 inches. This implies that if \\(p=0.50\\) then \\(q=69.5\\).\nThe idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:\nDefine a vector of \\(m\\) proportions \\(p_1, p_2, \\dots, p_m\\).\rDefine a vector of quantiles \\(q_1, \\dots, q_m\\) for your data for the proportions \\(p_1, \\dots, p_m\\). We refer to these as the sample quantiles.\rDefine a vector of theoretical quantiles for the proportions \\(p_1, \\dots, p_m\\) for a normal distribution with the same average and standard deviation as the data.\rPlot the sample quantiles versus the theoretical quantiles.\r\rLet’s construct a QQ-plot using R code. Start by defining the vector of proportions.\np \u0026lt;- seq(0.005, 0.995, 0.01)\rTo obtain the quantiles from the data, we can use the quantile function like this:\nsample_quantiles \u0026lt;- quantile(x, p)\rTo obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the qnorm function:\ntheoretical_quantiles \u0026lt;- qnorm(p, mean = mean(x), sd = sd(x))\rdf = data.frame(sample_quantiles, theoretical_quantiles)\rTo see if they match or not, we plot them against each other and draw the identity line:\nggplot(data = df, aes(x = theoretical_quantiles, y = sample_quantiles)) + geom_point() + geom_abline() # a 45-degree line \rNotice that this code becomes much cleaner if we use standard units:\nsample_quantiles \u0026lt;- quantile(z, p)\rtheoretical_quantiles \u0026lt;- qnorm(p)\rdf2 = data.frame(sample_quantiles, theoretical_quantiles)\rggplot(data = df2, aes(x = theoretical_quantiles, y = sample_quantiles)) + geom_point() + geom_abline()\rThe above code is included to help describe QQ-plots. However, in practice it is easier to use the ggplot geometry geom_qq:\nheights %\u0026gt;% filter(sex == \u0026quot;Male\u0026quot;) %\u0026gt;%\rggplot(aes(sample = scale(height))) +\rgeom_qq() +\rgeom_abline()\rWhile for the illustration above we used 100 quantiles, the default from the geom_qq function is to use as many quantiles as data points.\n\rPercentiles\rBefore we move on, let’s define some terms that are commonly used in exploratory data analysis.\nPercentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). We call, for example, the case of \\(p=0.25\\) the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median.\nFor the normal distribution the median and average are the same, but this is generally not the case.\nAnother special case that receives a name are the quartiles, which are obtained when setting \\(p=0.25,0.50\\), and \\(0.75\\).\n\r\rggplot2 geometries\rAlhough we haven’t gone into detain about the ggplot2 package for data visualization, we now will briefly discuss some of the geometries involved in the plots above. We will discuss ggplot2 in (excruciating) detail later this week. For now, we will briefly demonstrate how to generate plots related to distributions.\nBarplots\rTo generate a barplot we can use the geom_bar geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.\nmurders %\u0026gt;% ggplot(aes(region)) + geom_bar()\rWe often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:\ndata(murders)\rtab \u0026lt;- murders %\u0026gt;%\rcount(region) %\u0026gt;%\rmutate(proportion = n/sum(n))\rtab\r## region n proportion\r## 1 Northeast 9 0.1764706\r## 2 South 17 0.3333333\r## 3 North Central 12 0.2352941\r## 4 West 13 0.2549020\rWe no longer want geom_bar to count, but rather just plot a bar to the height provided by the proportion variable. For this we need to provide x (the categories) and y (the values) and use the stat=\"identity\" option. This tells R to just use the actual value in proportion for the y aesthetic. This is only necessary when you’re telling R that you have your own field (proportion) that you want to use instead of just the count.\ntab %\u0026gt;% ggplot(aes(x = region, y = proportion)) + geom_bar(stat = \u0026quot;identity\u0026quot;)\r\rHistograms\rTo generate histograms we use geom_histogram. By looking at the help file for this function, we learn that the only required argument is x, the variable for which we will construct a histogram. We dropped the x because we know it is the first argument.\rThe code looks like this:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram()\rIf we run the code above, it gives us a message:\n\rstat_bin() using bins = 30. Pick better value with\rbinwidth.\n\rWe previously used a bin size of 1 inch (of observed height), so the code looks like this:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram(binwidth = 1)\rFinally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram(binwidth = 1, fill = \u0026quot;blue\u0026quot;, col = \u0026quot;black\u0026quot;) +\rlabs(x = \u0026quot;Male heights in inches\u0026quot;, title = \u0026quot;Histogram\u0026quot;)\r\rDensity plots\rTo create a smooth density, we use the geom_density. To make a smooth density plot with the data previously shown as a histogram we can use this code:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) +\rgeom_density()\rTo fill in with color, we can use the fill argument.\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) +\rgeom_density(fill=\u0026quot;blue\u0026quot;)\rTo change the smoothness of the density, we use the adjust argument to multiply the default value by that adjust. For example, if we want the bandwidth to be twice as big we use:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rggplot(aes(x = height)) + geom_density(fill=\u0026quot;blue\u0026quot;, adjust = 2)\r\rBoxplots\rThe geometry for boxplot is geom_boxplot. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments x as the categories, and y as the values.\nNote that our x-axis is a categorical variable. The order is determined by either the factor variable levels in heights or, if no levels are set, in the order in which the sex variable first encounters them. Later on, we’ll learn how to change the ordering.\nWe can do much more with boxplots when we have more data. Right now, our heights data has only two variables - sex and height. Let’s say we took the measurements over two different years - 2010 and 2020. That’s not in our data, so purely for exposition, we’ll add it by randomly drawing a year for each observation. We’ll do this with sample\nheights = heights %\u0026gt;%\rdplyr::mutate(year = sample(x = c(2010, 2020), size = n(), replace = TRUE, prob = c(.5, .5)))\rhead(heights)\r## sex height year\r## 1 Male 75 2020\r## 2 Male 70 2010\r## 3 Male 68 2020\r## 4 Male 74 2010\r## 5 Male 61 2020\r## 6 Female 65 2010\rNow, let’s look at the boxplot of heights by sex, but broken out by year. We can do this by adding year as an aesthetic mapping. Because our year variable is an integer, R will start by thinking it’s a continuous numeric, but we want to treat it as a discrete variable. So, we wrap it in as.factor() to force R to recognize it as a discrete variable.\nheights %\u0026gt;% ggplot(aes(x = sex, y = height, fill = as.factor(year))) +\rgeom_boxplot() +\rlabs(fill = \u0026#39;Year\u0026#39;)\rNow we have each sex broken out by year! Since we randomly assigned year to our data (and didn’t actually take samples in two different decades), the distribution between years and within sex is nearly identical.\nWhat if we wanted to have year on the x-axis, but then put the sex boxplots next to each other. This would let us compare the difference in heights by sex over the two sample years.\nheights %\u0026gt;% ggplot(aes(x = year, y = height, fill = sex)) + geom_boxplot() +\rlabs(fill = \u0026#39;Sex\u0026#39;)\rWoah. Wait. What? Remember, in our data, class(heights$year) is numeric, so when we ask R to put year on the x-axis, it thinks it’s plotting a number. It gives us a nonsense x-axis. How do we fix this? We force as.factor(year) to tell R that yes, year is a categorical variable. Note that we didn’t have to use as.factor(sex) - that’s because sex is already a categorical variable.\nheights %\u0026gt;% ggplot(aes(x = as.factor(year), y = height, fill = sex)) + geom_boxplot() +\rlabs(fill = \u0026#39;Sex\u0026#39;)\rNow we can see the height difference by sex, by year.\nWe will explore more with boxplots and colors in our next lecture.\n\r\rTry it!\rTry it!\nTRY IT\nStart by loading the dplyr and ggplot2 library as well as the murders and heights data.\nlibrary(dplyr)\rlibrary(ggplot2)\rlibrary(dslabs)\rdata(heights)\rdata(murders)\rFirst, create a new variable in murders that has murders_per_capita.\r\rmurders = murders %\u0026gt;%\rmutate(........)\rMake a histogram of murders per capita. Use the default values for color and fill, but make sure you label the x-axis with a meaningful label.\n\rMake the same histogram, but set the fill aesthetic to MSU Green and the color to black.\n\rDo the same, but make it a smooth density plot\n\rFinally, plot the smooth density but use a fill aesthetic mapping so that each region’s density is shown. Set a meaningful title on the legend, and make sure you make the density transparent so we can see all of the region’s densities (see alpha aesthetic).\n\rNow, try making a boxplot to show the same data - the distribution across states of murders per capita by region. What is the average Northeastern state’s murder rate? What about the average Southern state?\n\r\r\r\r\r\rKeep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data.↩︎\n\r\r\r","date":1641427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641470874,"objectID":"ce879375c9ab42490f4d0d112e48c07c","permalink":"https://ssc442kirkpatrick.netlify.app/example/02-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/02-example/","section":"example","summary":"Visualizing data distributions\rVariable types\rCase study: describing student heights\rDistribution function\rCumulative distribution functions\r\rGeometries for describing distributions\rHistograms\rSmoothed density\rInterpreting the y-axis\rDensities permit stratification\r\rThe normal distribution\rStandard units\rQuantile-quantile plots\rPercentiles\r\rggplot2 geometries\rBarplots\rHistograms\rDensity plots\rBoxplots\r\rTry it!\r\r\r\rVisualizing data distributions\rThroughout your education, you may have noticed that numerical data is often summarized with the average value.","tags":null,"title":"ggplot2: Everything you ever wanted to know","type":"docs"},{"authors":null,"categories":null,"content":"\r\rPart 1: Statistical Inference and Polls\rPolls\rThe sampling model for polls\r\rPopulations, samples, parameters, and estimates\rThe sample average\rParameters\rPolling versus forecasting\rProperties of our estimate: expected value and standard error\r\rCentral Limit Theorem\rA Monte Carlo simulation\rThe spread\rBias: why not run a very large poll?\r\r\rPart 2: (Supplemental) Additional Visualization Techniques\rCode\rLoad and clean data\rHistograms\rDensity plots\rBox, violin, and rain cloud plots\r\r\r\r\rProbabilistic thinking is central in the human experience. How we describe that thinking is mixed, but most of the time we use (rather imprecise) language. With only a few moments of searching, one can find thousands of articles that use probabilistic words to describe events. Here are some examples:\n\r“‘Highly unlikely’ State of the Union will happen amid shutdown” – The Hill\n\r\r“Tiger Woods makes Masters 15th and most improbable major” – Fox\n\r\r“Trump predicts ‘very good chance’ of China trade deal” – CNN\n\rYet people don’t have a good sense of what these things mean. Uncertainty is key to data analytics: if we were certain of things, A study in the 1960s explored the perception of probabilistic words like these among NATO officers. A more modern replication of this found the following basic pattern:\nA deep, basic fact about humans is that we struggle to understand probabilities. But visualizing things can help. The graphic above shows the uncertainty about uncertainty (very meta). We can convey all manner of uncertainty with clever graphics. Today’s practical example works through some of the myriad of ways to visualize variable data. We’ll cover some territory that we’ve already hit, in hopes of locking in some key concepts.\nPart 1: Statistical Inference and Polls\rIn this Example we will describe, in some detail, how poll aggregators such as FiveThirtyEight use data to predict election outcomes. To understand how they do this, we first need to learn the basics of Statistical Inference, the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.\nPolls\rOpinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as inference and it is the main topic of this chapter.\nPerhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.\nElections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.\nAlthough typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.\nReal Clear Politics1 is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election2:\nAlthough in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we will go into some detail on this later.\nLet’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different spread: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled MoE which stands for margin of error.\nIn this example, we will show how the probability concepts we learned in the previous content can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define estimates and margins of errors, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.\nWe start by connecting probability theory to the task of using polls to learn about a population.\nThe sampling model for polls\rTo help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this hypothetical urn.\nBefore making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you 10 cents per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay \\$25 to collect your \\$25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.\nThe dslabs package includes a function that shows a random draw from this urn:\nlibrary(tidyverse)\rlibrary(dslabs)\rtake_poll(25)\rThink about how you would construct your interval based on the data shown above.\nWe have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.\n\r\rPopulations, samples, parameters, and estimates\rWe want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\).\nIn statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter \\(p\\) using the observed data in the sample.\nCan we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that \\(p\\) \u0026gt; .9 or \\(p\\) \u0026lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?\nWe want to construct an estimate of \\(p\\) using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.\nNote that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.\nThe sample average\rConducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate, we can easily report an estimate for the spread \\(2p-1\\), but for simplicity we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion \\(p\\).\nWe start by defining the random variable \\(X\\) as: \\(X=1\\) if we pick a blue bead at random and \\(X=0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads and dividing this count by the total \\(N\\) is equivalent to computing a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant \\(1/N\\):\n\\[\\bar{X} = 1/N \\times \\sum_{i=1}^N X_i\\]\nFor simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads.\nHere we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\).\n\rParameters\rJust like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. \\(p\\) is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is \\(p\\), we are going to estimate this parameter.\nThe ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.\n\rPolling versus forecasting\rBefore we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment and not for election day. The \\(p\\) for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.\n\rProperties of our estimate: expected value and standard error\rTo understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws so the rules we covered in the probability chapter apply.\nUsing what we have learned, the expected value of the sum \\(N\\bar{X}\\) is \\(N \\times\\) the average of the urn, \\(p\\). So dividing by the non-random constant \\(N\\) gives us that the expected value of the average \\(\\bar{X}\\) is \\(p\\). We can write it using our mathematical notation:\n\\[\r\\mbox{E}(\\bar{X}) = p\r\\]\nWe can also use what we learned to figure out the standard error: the standard error of the sum is \\(\\sqrt{N} \\times\\) the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is \\((1-0) \\sqrt{p (1-p)}\\) = \\(\\sqrt{p (1-p)}\\). Because we are dividing the sum by \\(N\\), we arrive at the following formula for the standard error of the average:\n\\[\r\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\r\\]\nThis result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\) and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough poll, our estimate converges to \\(p\\).\nIf we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?\nOne problem is that we do not know \\(p\\), so we can’t compute the standard error. However, for illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\):\nFrom the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is:\nsqrt(p*(1-p))/sqrt(1000)\r## [1] 0.01580823\ror 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the \\(p\\).\n\r\rCentral Limit Theorem\rThe Central Limit Theorem (CLT) tells us that the distribution function for a sum of draws is approximately normal. You also may recall that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of \\(\\bar{X}\\) is approximately normal.\nIn summary, we have that \\(\\bar{X}\\) has an approximately normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\nNow how does this help us? Suppose we want to know what is the probability that we are within 1% from \\(p\\). We are basically asking what is\n\\[\r\\mbox{Pr}(| \\bar{X} - p| \\leq .01)\r\\]\rwhich is the same as:\n\\[\r\\mbox{Pr}(\\bar{X}\\leq p + .01) - \\mbox{Pr}(\\bar{X} \\leq p - .01)\r\\]\nCan we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it \\(Z\\), on the left. Since \\(p\\) is the expected value and \\(\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\\) is the standard error we get:\n\\[\r\\mbox{Pr}\\left(Z \\leq \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right) -\r\\mbox{Pr}\\left(Z \\leq - \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right)\r\\]\nOne problem we have is that since we don’t know \\(p\\), we don’t know \\(\\mbox{SE}(\\bar{X})\\). But it turns out that the CLT still works if we estimate the standard error by using \\(\\bar{X}\\) in place of \\(p\\). We say that we plug-in the estimate. Our estimate of the standard error is therefore:\n\\[\r\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N}\r\\]\rIn statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and \\(N\\).\nNow we continue with our calculation, but dividing by \\(\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N})\\) instead. In our first sample we had 12 blue and 13 red so \\(\\bar{X} = 0.48\\) and our estimate of standard error is:\nx_hat \u0026lt;- 0.48\rse \u0026lt;- sqrt(x_hat*(1-x_hat)/25)\rse\r## [1] 0.09991997\rAnd now we can answer the question of the probability of being close to \\(p\\). The answer is:\npnorm(0.01/se) - pnorm(-0.01/se)\r## [1] 0.07971926\rTherefore, there is a small chance that we will be close. A poll of only \\(N=25\\) people is not really very useful, at least not for a close election.\nEarlier we mentioned the margin of error. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:\n1.96*se\r## [1] 0.1958431\rWhy do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from \\(p\\), we get:\n\\[\r\\mbox{Pr}\\left(Z \\leq \\, 1.96\\,\\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right) -\r\\mbox{Pr}\\left(Z \\leq - 1.96\\, \\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right)\r\\]\rwhich is:\n\\[\r\\mbox{Pr}\\left(Z \\leq 1.96 \\right) -\r\\mbox{Pr}\\left(Z \\leq - 1.96\\right)\r\\]\nwhich we know is about 95%:\npnorm(1.96)-pnorm(-1.96)\r## [1] 0.9500042\rHence, there is a 95% probability that \\(\\bar{X}\\) will be within \\(1.96\\times \\hat{SE}(\\bar{X})\\), in our case within about 0.2, of \\(p\\). Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.\nIn summary, the CLT tells us that our poll based on a sample size of \\(25\\) is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.\nFrom the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a \\(\\bar{X}\\)=0.48 with a sample size of 2,000, our standard error \\(\\hat{\\mbox{SE}}(\\bar{X})\\) would have been 0.0111714. So our result is an estimate of 48% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.\nA Monte Carlo simulation\rSuppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:\nB \u0026lt;- 10000\rN \u0026lt;- 1000\rx_hat \u0026lt;- replicate(B, {\rx \u0026lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\rmean(x)\r})\rThe problem is, of course, we don’t know p. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.\nOne thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll:\np \u0026lt;- 0.45\rN \u0026lt;- 1000\rx \u0026lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\rx_hat \u0026lt;- mean(x)\rIn this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:\nB \u0026lt;- 10000\rx_hat \u0026lt;- replicate(B, {\rx \u0026lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\rmean(x)\r})\rTo review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45 and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.0157321. The simulation confirms this:\nmean(x_hat)\r## [1] 0.4500761\rsd(x_hat)\r## [1] 0.01579523\rA histogram and qq-plot confirm that the normal approximation is accurate as well:\nOf course, in real life we would never be able to run such an experiment because we don’t know \\(p\\). But we could run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N.\n\rThe spread\rThe competition is to predict the spread, not the proportion \\(p\\). However, because we are assuming there are only two parties, we know that the spread is \\(p - (1-p) = 2p - 1\\). As a result, everything we have done can easily be adapted to an estimate of \\(2p - 1\\). Once we have our estimate \\(\\bar{X}\\) and \\(\\hat{\\mbox{SE}}(\\bar{X})\\), we estimate the spread with \\(2\\bar{X} - 1\\) and, since we are multiplying by 2, the standard error is \\(2\\hat{\\mbox{SE}}(\\bar{X})\\). Note that subtracting 1 does not add any variability so it does not affect the standard error.\nFor our 25 item sample above, our estimate \\(p\\) is .48 with margin of error .20 and our estimate of the spread is 0.04 with margin of error .40. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for \\(p\\), we have it for the spread \\(2p-1\\).\n\rBias: why not run a very large poll?\rFor realistic values of \\(p\\), say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:\nOne reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is \\(p\\). We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this shortly.\n\r\r\rPart 2: (Supplemental) Additional Visualization Techniques\rFor this second part of the example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n\r atl-weather-2019.csv\r\rCode\rLoad and clean data\rFirst, we load the libraries we’ll be using:\nlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggridges)\rlibrary(gghalves)\rThen we load the data with read_csv(). Here we assume that the CSV file lives in a subfolder in my project named data. Naturally, you’ll need to point this to wherever you stashed the data.\nweather_atl_raw \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;)\rWe’ll add a couple columns that we can use for faceting and filling using the month() and wday() functions from lubridate for extracting parts of the date:\nweather_atl \u0026lt;- weather_atl_raw %\u0026gt;%\rmutate(Month = month(time, label = TRUE, abbr = FALSE),\rDay = wday(time, label = TRUE, abbr = FALSE))\rNow we’re ready to go!\n\rHistograms\rWe can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:\nggplot(weather_atl, aes(x = windSpeed)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;)\rThis is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the boundary argument for that. We also add scale_x_continuous() to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:\nggplot(weather_atl, aes(x = windSpeed)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) +\rscale_x_continuous(breaks = seq(0, 12, by = 1))\rWe can show the distribution of wind speed by month if we map the Month column we made onto the fill aesthetic:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) +\rscale_x_continuous(breaks = seq(0, 12, by = 1))\rThis is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) +\rscale_x_continuous(breaks = seq(0, 12, by = 1)) +\rguides(fill = FALSE) +\rfacet_wrap(vars(Month))\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rNeat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.\n\rDensity plots\rThe code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the geom layer:\nggplot(weather_atl, aes(x = windSpeed)) +\rgeom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;)\rIf we want, we can mess with some of the calculus options like the kernel and bandwidth:\nggplot(weather_atl, aes(x = windSpeed)) +\rgeom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;,\rbw = 0.1, kernel = \u0026quot;epanechnikov\u0026quot;)\rWe can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\rgeom_density(alpha = 0.5)\rEven with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\rgeom_density(alpha = 0.5) +\rguides(fill = FALSE) +\rfacet_wrap(vars(Month))\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rOr we can stack the density plots behind each other with ggridges. For that to work, we also need to map Month to the y-axis. We can reverse the y-axis so that January is at the top if we use the fct_rev() function:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\rgeom_density_ridges() +\rguides(fill = FALSE)\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rWe can add some extra information to geom_density_ridges() with some other arguments like quantile_lines. We can use the quantiles argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\rgeom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\rguides(fill = FALSE)\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rNow that we have good working code, we can easily substitute in other variables by changing the x mapping:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) +\rgeom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\rguides(fill = FALSE)\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rWe can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use geom_density_ridges_gradient(), and we need to change the fill mapping to the strange looking ..x.., which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, fill = temperatureHigh doesn’t work 🤷:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) +\rgeom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\rscale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) +\rlabs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;)\rAnd finally, we can get extra fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: temperatureLow and temperatureHigh. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like linetype), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using pivot_longer() from tidyr, which was already loaded with library(tidyverse). In the RStudio primers, you did this same thing with gather()—pivot_longer() is the newer version of gather():\nweather_atl_long \u0026lt;- weather_atl %\u0026gt;%\rpivot_longer(cols = c(temperatureLow, temperatureHigh),\rnames_to = \u0026quot;temp_type\u0026quot;,\rvalues_to = \u0026quot;temp\u0026quot;) %\u0026gt;%\r# Clean up the new temp_type column so that \u0026quot;temperatureHigh\u0026quot; becomes \u0026quot;High\u0026quot;, etc.\rmutate(temp_type = recode(temp_type,\rtemperatureHigh = \u0026quot;High\u0026quot;,\rtemperatureLow = \u0026quot;Low\u0026quot;)) %\u0026gt;%\r# This is optional—just select a handful of columns\rselect(time, temp_type, temp, Month)\r# Show the first few rows\rhead(weather_atl_long)\r## # A tibble: 6 x 4\r## time temp_type temp Month ## \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; ## 1 2019-01-01 05:00:00 Low 50.6 January\r## 2 2019-01-01 05:00:00 High 63.9 January\r## 3 2019-01-02 05:00:00 Low 49.0 January\r## 4 2019-01-02 05:00:00 High 57.4 January\r## 5 2019-01-03 05:00:00 Low 53.1 January\r## 6 2019-01-03 05:00:00 High 55.3 January\rNow we have a column for the temperature (temp) and a column indicating if it is high or low (temp_type). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the linetype aesthetic to show high/low in the border of the plots:\nggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month),\rfill = ..x.., linetype = temp_type)) +\rgeom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\rscale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) +\rlabs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;)\rWe can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.\n\rBox, violin, and rain cloud plots\rFinally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the Day variable we made indicating weekday:\nggplot(weather_atl,\raes(y = windSpeed, fill = Day)) +\rgeom_boxplot()\rWe can switch this to a violin plot by just changing the geom layer and mapping Day to the x-axis:\nggplot(weather_atl,\raes(y = windSpeed, x = Day, fill = Day)) +\rgeom_violin()\rWith violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:\nggplot(weather_atl,\raes(y = windSpeed, x = Day, fill = Day)) +\rgeom_violin() +\rgeom_point(size = 0.5, position = position_jitter(width = 0.1)) +\rguides(fill = FALSE)\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rWe can also add larger points for the daily averages. We’ll use a special layer for this: stat_summary(). It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here \"mean\") and then plotting that result:\nggplot(weather_atl,\raes(y = windSpeed, x = Day, fill = Day)) +\rgeom_violin() +\rstat_summary(geom = \u0026quot;point\u0026quot;, fun = \u0026quot;mean\u0026quot;, size = 5, color = \u0026quot;white\u0026quot;) +\rgeom_point(size = 0.5, position = position_jitter(width = 0.1)) +\rguides(fill = FALSE)\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rWe can also show the mean and confidence interval at the same time by changing the summary function:\nggplot(weather_atl,\raes(y = windSpeed, x = Day, fill = Day)) +\rgeom_violin() +\rstat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, size = 1, color = \u0026quot;white\u0026quot;) +\rgeom_point(size = 0.5, position = position_jitter(width = 0.1)) +\rguides(fill = FALSE)\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rOverlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use the gghalves package, we can use special halved versions of some of these geoms like so:\nggplot(weather_atl,\raes(x = fct_rev(Day), y = temperatureHigh)) +\rgeom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) +\rgeom_half_boxplot(aes(fill = Day), side = \u0026quot;r\u0026quot;) +\rguides(color = FALSE, fill = FALSE)\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rNote the side argument for specifying which half of the column the geom goes. We can also use geom_half_violin():\nggplot(weather_atl,\raes(x = fct_rev(Day), y = temperatureHigh)) +\rgeom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) +\rgeom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) +\rguides(color = FALSE, fill = FALSE)\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\rIf we flip the plot, we can make a rain cloud plot:\nggplot(weather_atl,\raes(x = fct_rev(Day), y = temperatureHigh)) +\rgeom_half_boxplot(aes(fill = Day), side = \u0026quot;l\u0026quot;, width = 0.5, nudge = 0.1) +\rgeom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) +\rgeom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) +\rguides(color = FALSE, fill = FALSE) +\rcoord_flip()\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; =\r## \u0026quot;none\u0026quot;)` instead.\r\r\r\r\rhttp://www.realclearpolitics.com↩︎\n\rhttp://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html↩︎\n\r\r\r","date":1641427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641470874,"objectID":"c977a82d5f9c9c6bf14f43f56de0b41e","permalink":"https://ssc442kirkpatrick.netlify.app/example/04-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/04-example/","section":"example","summary":"Part 1: Statistical Inference and Polls\rPolls\rThe sampling model for polls\r\rPopulations, samples, parameters, and estimates\rThe sample average\rParameters\rPolling versus forecasting\rProperties of our estimate: expected value and standard error\r\rCentral Limit Theorem\rA Monte Carlo simulation\rThe spread\rBias: why not run a very large poll?\r\r\rPart 2: (Supplemental) Additional Visualization Techniques\rCode\rLoad and clean data\rHistograms\rDensity plots\rBox, violin, and rain cloud plots\r\r\r\r\rProbabilistic thinking is central in the human experience.","tags":null,"title":"Visualizing Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"\r\rInstall R\rInstall RStudio\rInstall tidyverse\rInstall tinytex\r\r\rAs mentioned in the syllabus, you will do all of your work in this class with the open source programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—–R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nHopefully you’re well-versed in dealing with these things, but if you’re lost, here’s how you install the required software for the course.\nInstall R\rFirst you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n\rClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\rIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.\n\rIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\r\rDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n\rIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\r\r\rInstall RStudio\rNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n\rThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n\rDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\r\rDouble click on RStudio to run it (check your applications folder or start menu).\n\rInstall tidyverse\rR packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including the ever-present ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel. Hopefully you’ve experienced installing packages before now; if not, consider this a crash course!\n\rInstall tinytex\rWhen you knit to PDF, R uses a special scientific typesetting program named LaTeX.2\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB. To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console.\rRun tinytex::install_tinytex() in the console.\rWait for a bit while R downloads and installs everything you need.\rThe end! You should now be able to knit to PDF.\r\r\r\rIt’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n\rPronounced “lay-tek” for those who are correct; or “lah-tex” to those who love goofy nerdy pronunciation. Technically speaking, the x is the “ch” sound in “Bach”, but most people just say it as “k”. While either saying “lay” or “lah” is correct, “layteks” is frowned upon because it clearly shows you’re not cool.↩︎\n\r\r\r","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594409288,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"https://ssc442kirkpatrick.netlify.app/resource/install/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"Install R\rInstall RStudio\rInstall tidyverse\rInstall tinytex\r\r\rAs mentioned in the syllabus, you will do all of your work in this class with the open source programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—–R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"\rToday’s example will come from the “Content” tab.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641473121,"objectID":"0d7091da7131dcaeb0a7a2758ca2db8e","permalink":"https://ssc442kirkpatrick.netlify.app/example/10-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/10-example/","section":"example","summary":"\rToday’s example will come from the “Content” tab.\n\r","tags":null,"title":"Illustrating Classification","type":"docs"},{"authors":null,"categories":null,"content":"\r\rData to be wrangled\r\r\rData to be wrangled\rYou work for a travel booking website as a data analyst. A hotel has asked your company for data on corporate bookings at the hotel via your site. Specifically, they have five corporations that are frequent customers of the hotel, and they want to know who spends the most with them. They’ve asked you to help out. Most of the corporate spending is in the form of room reservations, but there are also parking fees that the hotel wants included in the analysis. Your goal: total up spending by corporation and report the biggest and smallest spenders inclusive of rooms and parking.\nUnfortunately, you only have the following data:\n\r booking.csv - Contains the corporation name, the room type, and the dates someone from the corporation stayed at the hoted.\n\r roomrates.csv - Contains the price of each room on each day\n\r parking.csv - Contains the corporations who negotiated free parking for employees\n\rParking at the hotel is $60 if you don’t have free parking. This hotel is in California, so everyone drives and parks when they stay.\n\r\rSome tips:\n\rRight-click on each of the links, copy the address, and read the URL in using read.csv or read_csv or whatever you prefer to read .csv’s\n\rYou’ll find you need to use most of the tools we covered on Tuesday including gather, separate and more.\n\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617824247,"objectID":"04fd6e11389955617e7779fefe3c7a53","permalink":"https://ssc442kirkpatrick.netlify.app/example/11-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/11-example/","section":"example","summary":"Data to be wrangled\r\r\rData to be wrangled\rYou work for a travel booking website as a data analyst. A hotel has asked your company for data on corporate bookings at the hotel via your site. Specifically, they have five corporations that are frequent customers of the hotel, and they want to know who spends the most with them. They’ve asked you to help out. Most of the corporate spending is in the form of room reservations, but there are also parking fees that the hotel wants included in the analysis.","tags":null,"title":"Data Wrangling","type":"docs"},{"authors":null,"categories":null,"content":"\rToday’s example will come from the “Content” tab.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617824247,"objectID":"8f294a1c92be1a918ba3fa24cc427a78","permalink":"https://ssc442kirkpatrick.netlify.app/example/12-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/12-example/","section":"example","summary":"\rToday’s example will come from the “Content” tab.\n\r","tags":null,"title":"Text as Data","type":"docs"},{"authors":null,"categories":null,"content":"\r\rPreliminaries\rCode\rLoad and clean data\rLegal dual y-axes\rCombining plots\rScatterplot matrices\rCorrelograms\rSimple regression\rCoefficient plots\rMarginal effects plots\r\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again (in addition to the Atlanta weather data). The Ames data is linked below:\n\r Ames.csv\r\r\rPreliminaries\rFor this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n\r atl-weather-2019.csv\r\r\rCode\rLoad and clean data\rFirst, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends\rlibrary(patchwork) # For combining ggplot plots\rlibrary(GGally) # For scatterplot matrices\rlibrary(broom) # For converting model objects to data frames\rThen we load the data with read_csv(). Here we assume that the CSV file lives in a subfolder named data:\nweather_atl \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;)\r\rLegal dual y-axes\rIt is fine (and often helpful) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.\nTo do this, you need to add an argument (sec.axis) to scale_y_continuous() to tell it to use a second axis. This sec.axis argument takes a sec_axis() function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a ~, which indicates that it’s a function, and it needs to use . to stand in for the original value in the original axis.\nSince the equation for converting Fahrenheit to Celsius is this…\n\\[\r\\text{C} = (32 - \\text{F}) \\times -\\frac{5}{9}\r\\]\n…we can specify this with code like so (where . stands for the Fahrenheit value):\n~ (32 - .) * -5 / 9\rHere’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\rgeom_line() +\rscale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\rname = \u0026quot;Celsius\u0026quot;)) +\rlabs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) +\rtheme_minimal()\rFor fun, we could also convert it to Kelvin, which uses this formula:\n\\[\r\\text{K} = (\\text{F} - 32) \\times \\frac{5}{9} + 273.15\r\\]\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\rgeom_line() +\rscale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15,\rname = \u0026quot;Kelvin\u0026quot;)) +\rlabs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) +\rtheme_minimal()\r\rCombining plots\rA good alternative to using two y-axes is to use two plots instead. The patchwork package makes this really easy to do with R. There are other similar packages that do this, like cowplot and gridExtra, but I’ve found that patchwork is the easiest to use and it actually aligns the different plot elements like axis lines and legends (yay alignment in CRAP!). The documentation for patchwork is really great and full of examples—you should check it out to see all the things you can do with it!\nTo use patchwork, we need to (1) save our plots as objects and (2) add them together with +.\nFor instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:\n# Temperature in Atlanta\rtemp_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\rgeom_line() +\rgeom_smooth() +\rscale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\rname = \u0026quot;Celsius\u0026quot;)) +\rlabs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) +\rtheme_minimal()\rtemp_plot\r# Humidity in Atlanta\rhumidity_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = humidity)) +\rgeom_line() +\rgeom_smooth() +\rlabs(x = NULL, y = \u0026quot;Humidity\u0026quot;) +\rtheme_minimal()\rhumidity_plot\rRight now, these are two separate plots, but we can combine them with + if we load patchwork:\nlibrary(patchwork)\rtemp_plot + humidity_plot\rBy default, patchwork will put these side-by-side, but we can change that with the plot_layout() function:\ntemp_plot + humidity_plot +\rplot_layout(ncol = 1)\rWe can also play with other arguments in plot_layout(). If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:\ntemp_plot + humidity_plot +\rplot_layout(ncol = 1, heights = c(0.7, 0.3))\r\rScatterplot matrices\rWe can visualize the correlations between pairs of variables with the ggpairs() function in the GGally package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into ggpairs() to see all the correlation information:\nlibrary(GGally)\rweather_correlations \u0026lt;- weather_atl %\u0026gt;%\rselect(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability)\rggpairs(weather_correlations)\rIt looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind spped and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).\nEven though ggpairs() doesn’t use the standard ggplot(...) + geom_whatever() syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:\nggpairs(weather_correlations) +\rlabs(title = \u0026quot;Correlations!\u0026quot;) +\rtheme_dark()\rTRY IT\nMake a ggpairs plot for some of the Ames data.\n\r\rCorrelograms\rScatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In the readings for today, Claus Wilke showed a type of plot called a correlogram which is more appropriate for publication.\nThese are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.\nFirst we need to build a correlation matrix of the main variables we care about. Ordinarily the cor() function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into cor() though, it’ll calculate the correlation between each pair of columns\n# Create a correlation matrix\rthings_to_correlate \u0026lt;- weather_atl %\u0026gt;%\rselect(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %\u0026gt;%\rcor()\rthings_to_correlate\r## temperatureHigh temperatureLow humidity windSpeed precipProbability\r## temperatureHigh 1.00 0.920 -0.030 -0.377 -0.124\r## temperatureLow 0.92 1.000 0.112 -0.450 -0.026\r## humidity -0.03 0.112 1.000 0.011 0.722\r## windSpeed -0.38 -0.450 0.011 1.000 0.196\r## precipProbability -0.12 -0.026 0.722 0.196 1.000\rThe two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to NA):\n# Get rid of the lower triangle\rthings_to_correlate[lower.tri(things_to_correlate)] \u0026lt;- NA\rthings_to_correlate\r## temperatureHigh temperatureLow humidity windSpeed precipProbability\r## temperatureHigh 1 0.92 -0.03 -0.377 -0.124\r## temperatureLow NA 1.00 0.11 -0.450 -0.026\r## humidity NA NA 1.00 0.011 0.722\r## windSpeed NA NA NA 1.000 0.196\r## precipProbability NA NA NA NA 1.000\rFinally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the things_to_correlate matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named measure1, and take all the correlation numbers and put them in a column named cor In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)\nthings_to_correlate_long \u0026lt;- things_to_correlate %\u0026gt;%\r# Convert from a matrix to a data frame\ras.data.frame() %\u0026gt;%\r# Matrixes have column names that don\u0026#39;t get converted to columns when using\r# as.data.frame(), so this adds those names as a column\rrownames_to_column(\u0026quot;measure2\u0026quot;) %\u0026gt;%\r# Make this long. Take all the columns except measure2 and put their names in\r# a column named measure1 and their values in a column named cor\rpivot_longer(cols = -measure2,\rnames_to = \u0026quot;measure1\u0026quot;,\rvalues_to = \u0026quot;cor\u0026quot;) %\u0026gt;%\r# Make a new column with the rounded version of the correlation value\rmutate(nice_cor = round(cor, 2)) %\u0026gt;%\r# Remove rows where the two measures are the same (like the correlation\r# between humidity and humidity)\rfilter(measure2 != measure1) %\u0026gt;%\r# Get rid of the empty triangle\rfilter(!is.na(cor)) %\u0026gt;%\r# Put these categories in order\rmutate(measure1 = fct_inorder(measure1),\rmeasure2 = fct_inorder(measure2))\rthings_to_correlate_long\r## # A tibble: 10 x 4\r## measure2 measure1 cor nice_cor\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 temperatureHigh temperatureLow 0.920 0.92\r## 2 temperatureHigh humidity -0.0301 -0.03\r## 3 temperatureHigh windSpeed -0.377 -0.38\r## 4 temperatureHigh precipProbability -0.124 -0.12\r## 5 temperatureLow humidity 0.112 0.11\r## 6 temperatureLow windSpeed -0.450 -0.45\r## 7 temperatureLow precipProbability -0.0255 -0.03\r## 8 humidity windSpeed 0.0108 0.01\r## 9 humidity precipProbability 0.722 0.72\r## 10 windSpeed precipProbability 0.196 0.2\rPhew. With the data all tidied like that, we can make a correlogram with a heatmap. We have manipulated the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.\nggplot(things_to_correlate_long,\raes(x = measure2, y = measure1, fill = cor)) +\rgeom_tile() +\rgeom_text(aes(label = nice_cor)) +\rscale_fill_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;,\rlimits = c(-1, 1)) +\rlabs(x = NULL, y = NULL) +\rcoord_equal() +\rtheme_minimal() +\rtheme(panel.grid = element_blank())\rInstead of using a heatmap, we can also use points, which encode the correlation information both as color and as size. To do that, we just need to switch geom_tile() to geom_point() and set the size = cor mapping:\nggplot(things_to_correlate_long,\raes(x = measure2, y = measure1, color = cor)) +\r# Size by the absolute value so that -0.7 and 0.7 are the same size\rgeom_point(aes(size = abs(cor))) +\rscale_color_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;,\rlimits = c(-1, 1)) +\rscale_size_area(max_size = 15, limits = c(-1, 1), guide = FALSE) +\rlabs(x = NULL, y = NULL) +\rcoord_equal() +\rtheme_minimal() +\rtheme(panel.grid = element_blank())\r## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please use `guide = \u0026quot;none\u0026quot;` instead.\r\rSimple regression\rWe finally get to this week’s content. Although correlation is nice, we eventually may want to visualize regression. The lecture has shown us some very intuitive, straightforward ways to think about regression (aka, a line). Simple regression is easy to visualize, since you’re only working with an \\(X\\) and a \\(Y\\). For instance, what’s the relationship between humidity and high temperatures during the summer?\nFirst, let’s filter the data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.\nweather_atl_summer \u0026lt;- weather_atl %\u0026gt;%\rfilter(time \u0026gt;= \u0026quot;2019-05-01\u0026quot;, time \u0026lt;= \u0026quot;2019-09-30\u0026quot;) %\u0026gt;%\rmutate(humidity_scaled = humidity * 100,\rmoonPhase_scaled = moonPhase * 100,\rprecipProbability_scaled = precipProbability * 100,\rcloudCover_scaled = cloudCover * 100)\rThen we can build a simple regression model:\nmodel_simple \u0026lt;- lm(temperatureHigh ~ humidity_scaled,\rdata = weather_atl_summer)\rtidy(model_simple, conf.int = TRUE)\r## # A tibble: 2 x 7\r## term estimate std.error statistic p.value conf.low conf.high\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 104. 2.35 44.3 1.88e-88 99.5 109. ## 2 humidity_scaled -0.241 0.0358 -6.74 3.21e-10 -0.312 -0.170\rWe can interpret these coefficients like so:\n\rThe intercept shows that the average temperature when there’s 0% humidity is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line.\rThe coefficient for humidity_scaled shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.\r\rVisualizing this model is simple, since there are only two variables:\nggplot(weather_atl_summer,\raes(x = humidity_scaled, y = temperatureHigh)) +\rgeom_point() +\rgeom_smooth(method = \u0026quot;lm\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rAnd indeed, as humidity increases, temperatures decrease.\n\rCoefficient plots\rBut if we use multiple variables in the model (and we will do this a lot going forward), it gets really hard to visualize the results since we’re working with multiple dimensions. Instead, we can use coefficient plots to see the individual coefficients in the model.\nFirst, let’s build a more complex model:\nmodel_complex \u0026lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled +\rprecipProbability_scaled + windSpeed + pressure + cloudCover_scaled,\rdata = weather_atl_summer)\rtidy(model_complex, conf.int = TRUE)\r## # A tibble: 7 x 7\r## term estimate std.error statistic p.value conf.low conf.high\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 262. 125. 2.09 0.0380 14.8 510. ## 2 humidity_scaled -0.111 0.0757 -1.47 0.143 -0.261 0.0381\r## 3 moonPhase_scaled 0.0116 0.0126 0.917 0.360 -0.0134 0.0366\r## 4 precipProbability_scaled 0.0356 0.0203 1.75 0.0820 -0.00458 0.0758\r## 5 windSpeed -1.78 0.414 -4.29 0.0000326 -2.59 -0.958 ## 6 pressure -0.157 0.122 -1.28 0.203 -0.398 0.0854\r## 7 cloudCover_scaled -0.0952 0.0304 -3.14 0.00207 -0.155 -0.0352\rWe can interpret these coefficients like so:\n\rHolding everything else constant, a 1% increase in humidity is associated with a 0.11° decrease in the high temperature, on average, but the effect is not statistically significant\rHolding everything else constant, a 1% increase in moon visibility is associated with a 0.01° increase in the high temperature, on average, and the effect is not statistically significant\rHolding everything else constant, a 1% increase in the probability of precipitation is associated with a 0.04° increase in the high temperature, on average, and the effect is not statistically significant\rHolding everything else constant, a 1 mph increase in the wind speed is associated with a 1.8° decrease in the high temperature, on average, and the effect is statistically significant\rHolding everything else constant, a 1 unit increase in barometric pressure is associated with a 0.15° decrease in the high temperature, on average, and the effect is not statistically significant\rHolding everything else constant, a 1% increase in cloud cover is associated with a 0.01° decrease in the high temperature, on average, and the effect is statistically significant\rThe intercept is pretty useless. It shows that the predicted temperature will be 262° when humidity is 0%, the moon is invisible, there’s no chance of precipitation, no wind, no barometric pressure, and no cloud cover. Yikes.\r\rTo plot all these things at once, we’ll store the results of tidy(model_complex) as a data frame, remove the useless intercept, and plot it using geom_pointrange():\nmodel_tidied \u0026lt;- tidy(model_complex, conf.int = TRUE) %\u0026gt;%\rfilter(term != \u0026quot;(Intercept)\u0026quot;)\rggplot(model_tidied,\raes(x = estimate, y = term)) +\rgeom_vline(xintercept = 0, color = \u0026quot;red\u0026quot;, linetype = \u0026quot;dotted\u0026quot;) +\rgeom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\rlabs(x = \u0026quot;Coefficient estimate\u0026quot;, y = NULL) +\rtheme_minimal()\rNeat! Now we can see how big these different coefficients are and how close they are to zero. Wind speed has a big significant effect on temperature. The others are all very close to zero.\n\rMarginal effects plots\rInstead of just looking at the coefficients, we can also see the effect of moving different variables up and down like sliders and switches. Remember that regression coefficients allow us to build actual mathematical formulas that predict the value of Y. The coefficients from model_compex yield the following big hairy ugly equation:\n\\[\r\\begin{aligned}\r\\hat{\\text{High temperature}} =\u0026amp; 262 - 0.11 \\times \\text{humidity_scaled } \\\\\r\u0026amp; + 0.01 \\times \\text{moonPhase_scaled } + 0.04 \\times \\text{precipProbability_scaled } \\\\\r\u0026amp; - 1.78 \\times \\text{windSpeed} - 0.16 \\times \\text{pressure} - 0.095 \\times \\text{cloudCover_scaled}\r\\end{aligned}\r\\]\nIf we plug in values for each of the explanatory variables, we can get the predicted value of high temperature, or \\(\\hat{y}\\).\nThe augment() function in the broom library allows us to take a data frame of explanatory variable values, plug them into the model equation, and get predictions out. For example, let’s set each of the variables to some arbitrary values (50% for humidity, moon phase, chance of rain, and cloud cover; 1000 for pressure, and 1 MPH for wind speed).\nnewdata_example \u0026lt;- tibble(humidity_scaled = 50, moonPhase_scaled = 50,\rprecipProbability_scaled = 50, windSpeed = 1,\rpressure = 1000, cloudCover_scaled = 50)\rnewdata_example\r## # A tibble: 1 x 6\r## humidity_scaled moonPhase_scaled precipProbability_scaled windSpeed pressure cloudCover_scaled\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 50 50 50 1 1000 50\rWe can plug these values into the model with augment():\n# I use select() here because augment() returns columns for all the explanatory\r# variables, and the .fitted column with the predicted value is on the far right\r# and gets cut off\raugment(model_complex, newdata = newdata_example, se_fit=TRUE) %\u0026gt;%\rselect(.fitted, .se.fit)\rGiven these weather conditions, the predicted high temperature is 96.2°. Now you’re an armchair meteorologist!\nWe can follow the same pattern to show how the predicted temperature changes as specific variables change across a whole range. Here, we create a data frame of possible wind speeds and keep all the other explanatory variables at their means:\nnewdata \u0026lt;- tibble(windSpeed = seq(0, 8, 0.5),\rpressure = mean(weather_atl_summer$pressure),\rprecipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),\rmoonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),\rhumidity_scaled = mean(weather_atl_summer$humidity_scaled),\rcloudCover_scaled = mean(weather_atl_summer$cloudCover_scaled))\rnewdata\r## # A tibble: 17 x 6\r## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 1016. 40.2 50.7 64.8 29.5\r## 2 0.5 1016. 40.2 50.7 64.8 29.5\r## 3 1 1016. 40.2 50.7 64.8 29.5\r## 4 1.5 1016. 40.2 50.7 64.8 29.5\r## 5 2 1016. 40.2 50.7 64.8 29.5\r## 6 2.5 1016. 40.2 50.7 64.8 29.5\r## 7 3 1016. 40.2 50.7 64.8 29.5\r## 8 3.5 1016. 40.2 50.7 64.8 29.5\r## 9 4 1016. 40.2 50.7 64.8 29.5\r## 10 4.5 1016. 40.2 50.7 64.8 29.5\r## 11 5 1016. 40.2 50.7 64.8 29.5\r## 12 5.5 1016. 40.2 50.7 64.8 29.5\r## 13 6 1016. 40.2 50.7 64.8 29.5\r## 14 6.5 1016. 40.2 50.7 64.8 29.5\r## 15 7 1016. 40.2 50.7 64.8 29.5\r## 16 7.5 1016. 40.2 50.7 64.8 29.5\r## 17 8 1016. 40.2 50.7 64.8 29.5\rIf we feed this big data frame into augment(), we can get the predicted high temperature for each row. We can also use the .se.fit column to calculate the 95% confidence interval for each predicted value. We take the standard error, multiply it by -1.96 and 1.96 (or the quantile of the normal distribution at 2.5% and 97.5%), and add that value to the estimate.\npredicted_values \u0026lt;- augment(model_complex, newdata = newdata, se_fit=TRUE) %\u0026gt;%\rmutate(conf.low = .fitted + (-1.96 * .se.fit),\rconf.high = .fitted + (1.96 * .se.fit))\rpredicted_values %\u0026gt;%\rselect(windSpeed, .fitted, .se.fit, conf.low, conf.high) %\u0026gt;%\rhead()\r## # A tibble: 6 x 5\r## windSpeed .fitted .se.fit conf.low conf.high\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 95.3 1.63 92.2 98.5\r## 2 0.5 94.5 1.42 91.7 97.2\r## 3 1 93.6 1.22 91.2 96.0\r## 4 1.5 92.7 1.03 90.7 94.7\r## 5 2 91.8 0.836 90.1 93.4\r## 6 2.5 90.9 0.653 89.6 92.2\rCool! Just looking at the data in the table, we can see that predicted temperature drops as windspeed increases. We can plot this to visualize the effect:\nggplot(predicted_values, aes(x = windSpeed, y = .fitted)) +\rgeom_ribbon(aes(ymin = conf.low, ymax = conf.high),\rfill = \u0026quot;#BF3984\u0026quot;, alpha = 0.5) +\rgeom_line(size = 1, color = \u0026quot;#BF3984\u0026quot;) +\rlabs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) +\rtheme_minimal()\rWe just manipulated one of the model coefficients and held everything else at its mean. We can manipulate multiple variables too and encode them all on the graph. For instance, what is the effect of windspeed and cloud cover on the temperature?\nWe’ll follow the same process, but vary both windSpeed and cloudCover_scaled. Instead of using tibble(), we use exapnd_grid(), which creates every combination of the variables we specify. Instead of varying cloud cover by every possible value (like from 0 to 100), we’ll choose four possible cloud cover types: 0%, 33%, 66%, and 100%. Everything else will be at its mean.\nnewdata_fancy \u0026lt;- expand_grid(windSpeed = seq(0, 8, 0.5),\rpressure = mean(weather_atl_summer$pressure),\rprecipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),\rmoonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),\rhumidity_scaled = mean(weather_atl_summer$humidity_scaled),\rcloudCover_scaled = c(0, 33, 66, 100))\rnewdata_fancy\r## # A tibble: 68 x 6\r## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 1016. 40.2 50.7 64.8 0\r## 2 0 1016. 40.2 50.7 64.8 33\r## 3 0 1016. 40.2 50.7 64.8 66\r## 4 0 1016. 40.2 50.7 64.8 100\r## 5 0.5 1016. 40.2 50.7 64.8 0\r## 6 0.5 1016. 40.2 50.7 64.8 33\r## 7 0.5 1016. 40.2 50.7 64.8 66\r## 8 0.5 1016. 40.2 50.7 64.8 100\r## 9 1 1016. 40.2 50.7 64.8 0\r## 10 1 1016. 40.2 50.7 64.8 33\r## # ... with 58 more rows\rNotice now that windSpeed repeats four times (0, 0, 0, 0, 0.5, 0.5, etc.), since there are four possible cloudCover_scaled values (0, 33, 66, 100).\nWe can plot this now, just like before, with wind speed on the x-axis, the predicted temperature on the y-axis, and colored and faceted by cloud cover:\npredicted_values_fancy \u0026lt;- augment(model_complex, newdata = newdata_fancy, se_fit=TRUE) %\u0026gt;%\rmutate(conf.low = .fitted + (-1.96 * .se.fit),\rconf.high = .fitted + (1.96 * .se.fit)) %\u0026gt;%\r# Make cloud cover a categorical variable\rmutate(cloudCover_scaled = factor(cloudCover_scaled))\rggplot(predicted_values_fancy, aes(x = windSpeed, y = .fitted)) +\rgeom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled),\ralpha = 0.5) +\rgeom_line(aes(color = cloudCover_scaled), size = 1) +\rlabs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) +\rtheme_minimal() +\rguides(fill = FALSE, color = FALSE) +\rfacet_wrap(vars(cloudCover_scaled), nrow = 1)\r## Warning: `guides(\u0026lt;scale\u0026gt; = FALSE)` is deprecated. Please use `guides(\u0026lt;scale\u0026gt; = \u0026quot;none\u0026quot;)` instead.\rNice. Temperatures go down slightly as cloud cover increases. If we wanted to improve the model, we’d add an interaction term between cloud cover and windspeed so that each line would have a different slope in addition to a different intercept, but that’s beyond the scope of this class.\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641470874,"objectID":"ac11eb4b300887ac6e4fc5c1906670ba","permalink":"https://ssc442kirkpatrick.netlify.app/example/05-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/05-example/","section":"example","summary":"Preliminaries\rCode\rLoad and clean data\rLegal dual y-axes\rCombining plots\rScatterplot matrices\rCorrelograms\rSimple regression\rCoefficient plots\rMarginal effects plots\r\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again (in addition to the Atlanta weather data). The Ames data is linked below:\n\r Ames.csv\r\r\rPreliminaries\rFor this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.","tags":null,"title":"Introduction to Regression","type":"docs"},{"authors":null,"categories":null,"content":"\r\rPreliminaries\rDummy Variables\rInteractions\rFactor Variables\rFactors with More Than Two Levels\r\rParameterization\rBuilding Larger Models\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\rPreliminaries\rSo far in each of our analyses, we have only used numeric variables as predictors. We have also only used additive models, meaning the effect any predictor had on the response was not dependent on the other predictors. In this chapter, we will remove both of these restrictions. We will fit models with categorical predictors, and use models that allow predictors to interact. The mathematics of multiple regression will remain largely unchanging, however, we will pay close attention to interpretation, as well as some difference in R usage.\n\rDummy Variables\rFor this example and discussion, we will briefly use the built in dataset mtcars before returning to our favorite autompg dataset. During the in-class lecture / example, I will also use much more interesting datasets. The reason to use these easy, straightforward datasets is that they make visualization of the entire dataset trivially easy. Accordingly, the mtcars dataset is small, so we’ll quickly take a look at the entire dataset.\nmtcars\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4\r## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4\r## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1\r## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1\r## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2\r## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1\r## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4\r## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2\r## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2\r## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4\r## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4\r## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3\r## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3\r## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3\r## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4\r## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4\r## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4\r## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1\r## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2\r## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1\r## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1\r## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2\r## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2\r## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4\r## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2\r## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1\r## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2\r## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2\r## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4\r## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6\r## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8\r## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2\rWe will be interested in three of the variables: mpg, hp, and am.\n\rmpg: fuel efficiency, in miles per gallon.\rhp: horsepower, in foot-pounds per second.\ram: transmission. Automatic or manual.\r\rAs we often do, we will start by plotting the data. We are interested in mpg as the response variable, and hp as a predictor.\nplot(mpg ~ hp, data = mtcars, cex = 2)\rSince we are also interested in the transmission type, we could also label the points accordingly.\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Automatic\u0026quot;, \u0026quot;Manual\u0026quot;), col = c(1, 2), pch = c(1, 2))\rWe now fit the SLR model\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\epsilon,\r\\]\nwhere \\(Y\\) is mpg and \\(x_1\\) is hp. For notational brevity, we drop the index \\(i\\) for observations.\nmpg_hp_slr = lm(mpg ~ hp, data = mtcars)\rWe then re-plot the data and add the fitted line to the plot.\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\rabline(mpg_hp_slr, lwd = 3, col = \u0026quot;grey\u0026quot;)\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Automatic\u0026quot;, \u0026quot;Manual\u0026quot;), col = c(1, 2), pch = c(1, 2))\rWe should notice a pattern here. The red, manual observations largely fall above the line, while the black, automatic observations are mostly below the line. This means our model underestimates the fuel efficiency of manual transmissions, and overestimates the fuel efficiency of automatic transmissions. To correct for this, we will add a predictor to our model, namely, am as \\(x_2\\).\nOur new model is\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\r\\]\nwhere \\(x_1\\) and \\(Y\\) remain the same, but now\n\\[\rx_2 =\r\\begin{cases}\r1 \u0026amp; \\text{manual transmission} \\\\\r0 \u0026amp; \\text{automatic transmission}\r\\end{cases}.\r\\]\nIn this case, we call \\(x_2\\) a dummy variable. A dummy variable (also called an “indicator” variable) is somewhat unfortunately named, as it is in no way “dumb”. In fact, it is actually somewhat clever. A dummy variable is a numerical variable that is used in a regression analysis to “code” for a binary categorical variable. Let’s see how this works.\nFirst, note that am is already a dummy variable, since it uses the values 0 and 1 to represent automatic and manual transmissions. Often, a variable like am would store the character values auto and man and we would either have to convert these to 0 and 1, or, as we will see later, R will take care of creating dummy variables for us.\nSo, to fit the above model, we do so like any other multiple regression model we have seen before.\nmpg_hp_add = lm(mpg ~ hp + am, data = mtcars)\rBriefly checking the output, we see that R has estimated the three \\(\\beta\\) parameters.\nmpg_hp_add\r## ## Call:\r## lm(formula = mpg ~ hp + am, data = mtcars)\r## ## Coefficients:\r## (Intercept) hp am ## 26.58491 -0.05889 5.27709\rSince \\(x_2\\) can only take values 0 and 1, we can effectively write two different models, one for manual and one for automatic transmissions.\nFor automatic transmissions, that is \\(x_2 = 0\\), we have,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\epsilon.\r\\]\nThen for manual transmissions, that is \\(x_2 = 1\\), we have,\n\\[\rY = (\\beta_0 + \\beta_2) + \\beta_1 x_1 + \\epsilon.\r\\]\nNotice that these models share the same slope, \\(\\beta_1\\), but have different intercepts, differing by \\(\\beta_2\\). So the change in mpg is the same for both models, but on average mpg differs by \\(\\beta_2\\) between the two transmission types.\nWe’ll now calculate the estimated slope and intercept of these two models so that we can add them to a plot. Note that:\n\r\\(\\hat{\\beta}_0\\) = coef(mpg_hp_add)[1] = 26.5849137\r\\(\\hat{\\beta}_1\\) = coef(mpg_hp_add)[2] = -0.0588878\r\\(\\hat{\\beta}_2\\) = coef(mpg_hp_add)[3] = 5.2770853\r\rWe can then combine these to calculate the estimated slope and intercepts.\nint_auto = coef(mpg_hp_add)[1]\rint_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3]\rslope_auto = coef(mpg_hp_add)[2]\rslope_manu = coef(mpg_hp_add)[2]\rRe-plotting the data, we use these slopes and intercepts to add the “two” fitted models to the plot.\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\rabline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # add line for auto\rabline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # add line for manual\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Automatic\u0026quot;, \u0026quot;Manual\u0026quot;), col = c(1, 2), pch = c(1, 2))\rWe notice right away that the points are no longer systematically incorrect. The red, manual observations vary about the red line in no particular pattern without underestimating the observations as before. The black, automatic points vary about the black line, also without an obvious pattern.\nThey say a picture is worth a thousand words, but as a statistician, sometimes a picture is worth an entire analysis. The above picture makes it plainly obvious that \\(\\beta_2\\) is significant, but let’s verify mathematically. Essentially we would like to test:\n\\[\rH_0: \\beta_2 = 0 \\quad \\text{vs} \\quad H_1: \\beta_2 \\neq 0.\r\\]\nThis is nothing new. Again, the math is the same as the multiple regression analyses we have seen before. We could perform either a \\(t\\) or \\(F\\) test here. The only difference is a slight change in interpretation. We could think of this as testing a model with a single line (\\(H_0\\)) against a model that allows two lines (\\(H_1\\)).\nTo obtain the test statistic and p-value for the \\(t\\)-test, we would use\nsummary(mpg_hp_add)$coefficients[\u0026quot;am\u0026quot;,]\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## 5.277085e+00 1.079541e+00 4.888270e+00 3.460318e-05\rTo do the same for the \\(F\\) test, we would use\nanova(mpg_hp_slr, mpg_hp_add)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ hp\r## Model 2: mpg ~ hp + am\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 30 447.67 ## 2 29 245.44 1 202.24 23.895 3.46e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rNotice that these are indeed testing the same thing, as the p-values are exactly equal. (And the \\(F\\) test statistic is the \\(t\\) test statistic squared.)\nRecapping some interpretations:\n\r\\(\\hat{\\beta}_0 = 26.5849137\\) is the estimated average mpg for a car with an automatic transmission and 0 hp.\n\r\\(\\hat{\\beta}_0 + \\hat{\\beta}_2 = 31.8619991\\) is the estimated average mpg for a car with a manual transmission and 0 hp.\n\r\\(\\hat{\\beta}_2 = 5.2770853\\) is the estimated difference in average mpg for cars with manual transmissions as compared to those with automatic transmission, for any hp.\n\r\\(\\hat{\\beta}_1 = -0.0588878\\) is the estimated change in average mpg for an increase in one hp, for either transmission types.\n\r\rWe should take special notice of those last two. In the model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\r\\]\nwe see \\(\\beta_1\\) is the average change in \\(Y\\) for an increase in \\(x_1\\), no matter the value of \\(x_2\\). Also, \\(\\beta_2\\) is always the difference in the average of \\(Y\\) for any value of \\(x_1\\). These are two restrictions we won’t always want, so we need a way to specify a more flexible model.\nHere we restricted ourselves to a single numerical predictor \\(x_1\\) and one dummy variable \\(x_2\\). However, the concept of a dummy variable can be used with larger multiple regression models. We only use a single numerical predictor here for ease of visualization since we can think of the “two lines” interpretation. But in general, we can think of a dummy variable as creating “two models,” one for each category of a binary categorical variable.\n\rInteractions\rTo remove the “same slope” restriction, we will now discuss interaction. To illustrate this concept, we will return to the autompg dataset we created in the last chapter, with a few more modifications.\n# read data frame from the web\rautompg = read.table(\r\u0026quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\u0026quot;,\rquote = \u0026quot;\\\u0026quot;\u0026quot;,\rcomment.char = \u0026quot;\u0026quot;,\rstringsAsFactors = FALSE)\r# give the dataframe headers\rcolnames(autompg) = c(\u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;acc\u0026quot;, \u0026quot;year\u0026quot;, \u0026quot;origin\u0026quot;, \u0026quot;name\u0026quot;)\r# remove missing data, which is stored as \u0026quot;?\u0026quot;\rautompg = subset(autompg, autompg$hp != \u0026quot;?\u0026quot;)\r# remove the plymouth reliant, as it causes some issues\rautompg = subset(autompg, autompg$name != \u0026quot;plymouth reliant\u0026quot;)\r# give the dataset row names, based on the engine, year and name\rrownames(autompg) = paste(autompg$cyl, \u0026quot;cylinder\u0026quot;, autompg$year, autompg$name)\r# remove the variable for name\rautompg = subset(autompg, select = c(\u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;acc\u0026quot;, \u0026quot;year\u0026quot;, \u0026quot;origin\u0026quot;))\r# change horsepower from character to numeric\rautompg$hp = as.numeric(autompg$hp)\r# create a dummary variable for foreign vs domestic cars. domestic = 1.\rautompg$domestic = as.numeric(autompg$origin == 1)\r# remove 3 and 5 cylinder cars (which are very rare.)\rautompg = autompg[autompg$cyl != 5,]\rautompg = autompg[autompg$cyl != 3,]\r# the following line would verify the remaining cylinder possibilities are 4, 6, 8\r#unique(autompg$cyl)\r# change cyl to a factor variable\rautompg$cyl = as.factor(autompg$cyl)\rstr(autompg)\r## \u0026#39;data.frame\u0026#39;: 383 obs. of 9 variables:\r## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ...\r## $ cyl : Factor w/ 3 levels \u0026quot;4\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;8\u0026quot;: 3 3 3 3 3 3 3 3 3 3 ...\r## $ disp : num 307 350 318 304 302 429 454 440 455 390 ...\r## $ hp : num 130 165 150 150 140 198 220 215 225 190 ...\r## $ wt : num 3504 3693 3436 3433 3449 ...\r## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\r## $ year : int 70 70 70 70 70 70 70 70 70 70 ...\r## $ origin : int 1 1 1 1 1 1 1 1 1 1 ...\r## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ...\rWe’ve removed cars with 3 and 5 cylinders , as well as created a new variable domestic which indicates whether or not a car was built in the United States. Removing the 3 and 5 cylinders is simply for ease of demonstration later in the chapter and would not be done in practice. The new variable domestic takes the value 1 if the car was built in the United States, and 0 otherwise, which we will refer to as “foreign.” (We are arbitrarily using the United States as the reference point here.) We have also made cyl and origin into factor variables, which we will discuss later.\nWe’ll now be concerned with three variables: mpg, disp, and domestic. We will use mpg as the response. We can fit a model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x_1\\) is disp, the displacement in cubic inches,\r\\(x_2\\) is domestic as described above, which is a dummy variable.\r\r\\[\rx_2 =\r\\begin{cases}\r1 \u0026amp; \\text{Domestic} \\\\\r0 \u0026amp; \\text{Foreign}\r\\end{cases}\r\\]\nWe will fit this model, extract the slope and intercept for the “two lines,” plot the data and add the lines.\nmpg_disp_add = lm(mpg ~ disp + domestic, data = autompg)\rint_for = coef(mpg_disp_add)[1]\rint_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3]\rslope_for = coef(mpg_disp_add)[2]\rslope_dom = coef(mpg_disp_add)[2]\rplot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)\rabline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # add line for foreign cars\rabline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # add line for domestic cars\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Foreign\u0026quot;, \u0026quot;Domestic\u0026quot;), pch = c(1, 2), col = c(1, 2))\rThis is a model that allows for two parallel lines, meaning the mpg can be different on average between foreign and domestic cars of the same engine displacement, but the change in average mpg for an increase in displacement is the same for both. We can see this model isn’t doing very well here. The red line fits the red points fairly well, but the black line isn’t doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes.\nConsider the following model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\r\\]\nwhere \\(x_1\\), \\(x_2\\), and \\(Y\\) are the same as before, but we have added a new interaction term \\(x_1 x_2\\) which multiplies \\(x_1\\) and \\(x_2\\), so we also have an additional \\(\\beta\\) parameter \\(\\beta_3\\).\nThis model essentially creates two slopes and two intercepts, \\(\\beta_2\\) being the difference in intercepts and \\(\\beta_3\\) being the difference in slopes. To see this, we will break down the model into the two “sub-models” for foreign and domestic cars.\nFor foreign cars, that is \\(x_2 = 0\\), we have\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\epsilon.\r\\]\nFor domestic cars, that is \\(x_2 = 1\\), we have\n\\[\rY = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) x_1 + \\epsilon.\r\\]\nThese two models have both different slopes and intercepts.\n\r\\(\\beta_0\\) is the average mpg for a foreign car with 0 disp.\r\\(\\beta_1\\) is the change in average mpg for an increase of one disp, for foreign cars.\r\\(\\beta_0 + \\beta_2\\) is the average mpg for a domestic car with 0 disp.\r\\(\\beta_1 + \\beta_3\\) is the change in average mpg for an increase of one disp, for domestic cars.\r\rHow do we fit this model in R? There are a number of ways.\nOne method would be to simply create a new variable, then fit a model like any other.\nautompg$x3 = autompg$disp * autompg$domestic # THIS CODE NOT RUN!\rdo_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # THIS CODE NOT RUN!\rYou should only do this as a last resort. We greatly prefer not to have to modify our data simply to fit a model. Instead, we can tell R we would like to use the existing data with an interaction term, which it will create automatically when we use the : operator.\nmpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg)\rAn alternative method, which will fit the exact same model as above would be to use the * operator. This method automatically creates the interaction term, as well as any “lower order terms,” which in this case are the first order terms for disp and domestic\nmpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg)\rWe can quickly verify that these are doing the same thing.\ncoef(mpg_disp_int)\r## (Intercept) disp domestic disp:domestic ## 46.0548423 -0.1569239 -12.5754714 0.1025184\rcoef(mpg_disp_int2)\r## (Intercept) disp domestic disp:domestic ## 46.0548423 -0.1569239 -12.5754714 0.1025184\rWe see that both the variables, and their coefficient estimates are indeed the same for both models.\nsummary(mpg_disp_int)\r## ## Call:\r## lm(formula = mpg ~ disp + domestic + disp:domestic, data = autompg)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -10.8332 -2.8956 -0.8332 2.2828 18.7749 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 46.05484 1.80582 25.504 \u0026lt; 2e-16 ***\r## disp -0.15692 0.01668 -9.407 \u0026lt; 2e-16 ***\r## domestic -12.57547 1.95644 -6.428 3.90e-10 ***\r## disp:domestic 0.10252 0.01692 6.060 3.29e-09 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 4.308 on 379 degrees of freedom\r## Multiple R-squared: 0.7011, Adjusted R-squared: 0.6987 ## F-statistic: 296.3 on 3 and 379 DF, p-value: \u0026lt; 2.2e-16\rWe see that using summary() gives the usual output for a multiple regression model. We pay close attention to the row for disp:domestic which tests,\n\\[\rH_0: \\beta_3 = 0.\r\\]\nIn this case, testing for \\(\\beta_3 = 0\\) is testing for two lines with parallel slopes versus two lines with possibly different slopes. The disp:domestic line in the summary() output uses a \\(t\\)-test to perform the test.\nWe could also use an ANOVA \\(F\\)-test. The additive model, without interaction is our null model, and the interaction model is the alternative.\nanova(mpg_disp_add, mpg_disp_int)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp + domestic\r## Model 2: mpg ~ disp + domestic + disp:domestic\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 380 7714.0 ## 2 379 7032.6 1 681.36 36.719 3.294e-09 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rAgain we see this test has the same p-value as the \\(t\\)-test. Also the p-value is extremely low, so between the two, we choose the interaction model.\nint_for = coef(mpg_disp_int)[1]\rint_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3]\rslope_for = coef(mpg_disp_int)[2]\rslope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4]\rHere we again calculate the slope and intercepts for the two lines for use in plotting.\nplot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)\rabline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # line for foreign cars\rabline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # line for domestic cars\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Foreign\u0026quot;, \u0026quot;Domestic\u0026quot;), pch = c(1, 2), col = c(1, 2))\rWe see that these lines fit the data much better, which matches the result of our tests.\nSo far we have only seen interaction between a categorical variable (domestic) and a numerical variable (disp). While this is easy to visualize, since it allows for different slopes for two lines, it is not the only type of interaction we can use in a model. We can also consider interactions between two numerical variables.\nConsider the model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x_1\\) is disp, the displacement in cubic inches,\r\\(x_2\\) is hp, the horsepower, in foot-pounds per second.\r\rHow does mpg change based on disp in this model? We can rearrange some terms to see how.\n\\[\rY = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon\r\\]\nSo, for a one unit increase in \\(x_1\\) (disp), the mean of \\(Y\\) (mpg) increases \\(\\beta_1 + \\beta_3 x_2\\), which is a different value depending on the value of \\(x_2\\) (hp)!\nSince we’re now working in three dimensions, this model can’t be easily justified via visualizations like the previous example. Instead, we will have to rely on a test.\nmpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg)\rmpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg)\rsummary(mpg_disp_int_hp)\r## ## Call:\r## lm(formula = mpg ~ disp * hp, data = autompg)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -10.7849 -2.3104 -0.5699 2.1453 17.9211 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.241e+01 1.523e+00 34.42 \u0026lt;2e-16 ***\r## disp -1.002e-01 6.638e-03 -15.09 \u0026lt;2e-16 ***\r## hp -2.198e-01 1.987e-02 -11.06 \u0026lt;2e-16 ***\r## disp:hp 5.658e-04 5.165e-05 10.96 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 3.896 on 379 degrees of freedom\r## Multiple R-squared: 0.7554, Adjusted R-squared: 0.7535 ## F-statistic: 390.2 on 3 and 379 DF, p-value: \u0026lt; 2.2e-16\rUsing summary() we focus on the row for disp:hp which tests,\n\\[\rH_0: \\beta_3 = 0.\r\\]\nAgain, we see a very low p-value so we reject the null (additive model) in favor of the interaction model. Again, there is an equivalent \\(F\\)-test.\nanova(mpg_disp_add_hp, mpg_disp_int_hp)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp + hp\r## Model 2: mpg ~ disp * hp\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 380 7576.6 ## 2 379 5754.2 1 1822.3 120.03 \u0026lt; 2.2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rWe can take a closer look at the coefficients of our fitted interaction model.\ncoef(mpg_disp_int_hp)\r## (Intercept) disp hp disp:hp ## 52.4081997848 -0.1001737655 -0.2198199720 0.0005658269\r\r\\(\\hat{\\beta}_0 = 52.4081998\\) is the estimated average mpg for a car with 0 disp and 0 hp.\r\\(\\hat{\\beta}_1 = -0.1001738\\) is the estimated change in average mpg for an increase in 1 disp, for a car with 0 hp.\r\\(\\hat{\\beta}_2 = -0.21982\\) is the estimated change in average mpg for an increase in 1 hp, for a car with 0 disp.\r\\(\\hat{\\beta}_3 = 5.658269\\times 10^{-4}\\) is an estimate of the modification to the change in average mpg for an increase in disp, for a car of a certain hp (or vice versa).\r\rThat last coefficient needs further explanation. Recall the rearrangement we made earlier\n\\[\rY = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon.\r\\]\nSo, our estimate for \\(\\beta_1 + \\beta_3 x_2\\), is \\(\\hat{\\beta}_1 + \\hat{\\beta}_3 x_2\\), which in this case is\n\\[\r-0.1001738 + 5.658269\\times 10^{-4} x_2.\r\\]\nThis says that, for an increase of one disp we see an estimated change in average mpg of \\(-0.1001738 + 5.658269\\times 10^{-4} x_2\\). So how disp and mpg are related, depends on the hp of the car.\nSo for a car with 50 hp, the estimated change in average mpg for an increase of one disp is\n\\[\r-0.1001738 + 5.658269\\times 10^{-4} \\cdot 50 = -0.0718824\r\\]\nAnd for a car with 350 hp, the estimated change in average mpg for an increase of one disp is\n\\[\r-0.1001738 + 5.658269\\times 10^{-4} \\cdot 350 = 0.0978657\r\\]\nNotice the sign changed!\n\rFactor Variables\rSo far in this chapter, we have limited our use of categorical variables to binary categorical variables. Specifically, we have limited ourselves to dummy variables which take a value of 0 or 1 and represent a categorical variable numerically.\nWe will now discuss factor variables, which is a special way that R deals with categorical variables. With factor variables, a human user can simply think about the categories of a variable, and R will take care of the necessary dummy variables without any 0/1 assignment being done by the user.\nis.factor(autompg$domestic)\r## [1] FALSE\rEarlier when we used the domestic variable, it was not a factor variable. It was simply a numerical variable that only took two possible values, 1 for domestic, and 0 for foreign. Let’s create a new variable origin that stores the same information, but in a different way.\nautompg$origin[autompg$domestic == 1] = \u0026quot;domestic\u0026quot;\rautompg$origin[autompg$domestic == 0] = \u0026quot;foreign\u0026quot;\rhead(autompg$origin)\r## [1] \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot;\rNow the origin variable stores \"domestic\" for domestic cars and \"foreign\" for foreign cars.\nis.factor(autompg$origin)\r## [1] FALSE\rHowever, this is simply a vector of character values. A vector of car models is a character variable in R. A vector of Vehicle Identification Numbers (VINs) is a character variable as well. But those don’t represent a short list of levels that might influence a response variable. We will want to coerce this origin variable to be something more: a factor variable.\nautompg$origin = as.factor(autompg$origin)\rNow when we check the structure of the autompg dataset, we see that origin is a factor variable.\nstr(autompg)\r## \u0026#39;data.frame\u0026#39;: 383 obs. of 9 variables:\r## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ...\r## $ cyl : Factor w/ 3 levels \u0026quot;4\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;8\u0026quot;: 3 3 3 3 3 3 3 3 3 3 ...\r## $ disp : num 307 350 318 304 302 429 454 440 455 390 ...\r## $ hp : num 130 165 150 150 140 198 220 215 225 190 ...\r## $ wt : num 3504 3693 3436 3433 3449 ...\r## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\r## $ year : int 70 70 70 70 70 70 70 70 70 70 ...\r## $ origin : Factor w/ 2 levels \u0026quot;domestic\u0026quot;,\u0026quot;foreign\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ...\r## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ...\rFactor variables have levels which are the possible values (categories) that the variable may take, in this case foreign or domestic.\nlevels(autompg$origin)\r## [1] \u0026quot;domestic\u0026quot; \u0026quot;foreign\u0026quot;\rRecall that previously we have fit the model\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x_1\\) is disp, the displacement in cubic inches,\r\\(x_2\\) is domestic a dummy variable where 1 indicates a domestic car.\r\r(mod_dummy = lm(mpg ~ disp * domestic, data = autompg))\r## ## Call:\r## lm(formula = mpg ~ disp * domestic, data = autompg)\r## ## Coefficients:\r## (Intercept) disp domestic disp:domestic ## 46.0548 -0.1569 -12.5755 0.1025\rSo here we see that\n\\[\r\\hat{\\beta}_0 + \\hat{\\beta}_2 = 46.0548423 + -12.5754714 = 33.4793709\r\\]\nis the estimated average mpg for a domestic car with 0 disp.\nNow let’s try to do the same, but using our new factor variable.\n(mod_factor = lm(mpg ~ disp * origin, data = autompg))\r## ## Call:\r## lm(formula = mpg ~ disp * origin, data = autompg)\r## ## Coefficients:\r## (Intercept) disp originforeign disp:originforeign ## 33.47937 -0.05441 12.57547 -0.10252\rIt seems that it doesn’t produce the same results. Right away we notice that the intercept is different, as is the the coefficient in front of disp. We also notice that the remaining two coefficients are of the same magnitude as their respective counterparts using the domestic variable, but with a different sign. Why is this happening?\nIt turns out, that by using a factor variable, R is automatically creating a dummy variable for us. However, it is not the dummy variable that we had originally used ourselves.\nR is fitting the model\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x_1\\) is disp, the displacement in cubic inches,\r\\(x_2\\) is a dummy variable created by R. It uses 1 to represent a foreign car.\r\rSo now,\n\\[\r\\hat{\\beta}_0 = 33.4793709\r\\]\nis the estimated average mpg for a domestic car with 0 disp, which is indeed the same as before.\nWhen R created \\(x_2\\), the dummy variable, it used domestic cars as the reference level, that is the default value of the factor variable. So when the dummy variable is 0, the model represents this reference level, which is domestic. (R makes this choice because domestic comes before foreign alphabetically.)\nSo the two models have different estimated coefficients, but due to the different model representations, they are actually the same model.\nFactors with More Than Two Levels\rLet’s now consider a factor variable with more than two levels. In this dataset, cyl is an example.\nis.factor(autompg$cyl)\r## [1] TRUE\rlevels(autompg$cyl)\r## [1] \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rHere the cyl variable has three possible levels: 4, 6, and 8. You may wonder, why not simply use cyl as a numerical variable? You certainly could.\nHowever, that would force the difference in average mpg between 4 and 6 cylinders to be the same as the difference in average mpg between 6 and 8 cylinders. That usually make senses for a continuous variable, but not for a discrete variable with so few possible values. In the case of this variable, there is no such thing as a 7-cylinder engine or a 6.23-cylinder engine in personal vehicles. For these reasons, we will simply consider cyl to be categorical. This is a decision that will commonly need to be made with ordinal variables. Often, with a large number of categories, the decision to treat them as numerical variables is appropriate because, otherwise, a large number of dummy variables are then needed to represent these variables.\nLet’s define three dummy variables related to the cyl factor variable.\n\\[\rv_1 =\r\\begin{cases}\r1 \u0026amp; \\text{4 cylinder} \\\\\r0 \u0026amp; \\text{not 4 cylinder}\r\\end{cases}\r\\]\n\\[\rv_2 =\r\\begin{cases}\r1 \u0026amp; \\text{6 cylinder} \\\\\r0 \u0026amp; \\text{not 6 cylinder}\r\\end{cases}\r\\]\n\\[\rv_3 =\r\\begin{cases}\r1 \u0026amp; \\text{8 cylinder} \\\\\r0 \u0026amp; \\text{not 8 cylinder}\r\\end{cases}\r\\]\nNow, let’s fit an additive model in R, using mpg as the response, and disp and cyl as predictors. This should be a model that uses “three regression lines” to model mpg, one for each of the possible cyl levels. They will all have the same slope (since it is an additive model), but each will have its own intercept.\n(mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg))\r## ## Call:\r## lm(formula = mpg ~ disp + cyl, data = autompg)\r## ## Coefficients:\r## (Intercept) disp cyl6 cyl8 ## 34.99929 -0.05217 -3.63325 -2.03603\rThe question is, what is the model that R has fit here? It has chosen to use the model\n\\[\rY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x\\) is disp, the displacement in cubic inches,\r\\(v_2\\) and \\(v_3\\) are the dummy variables define above.\r\rWhy doesn’t R use \\(v_1\\)? Essentially because it doesn’t need to. To create three lines, it only needs two dummy variables since it is using a reference level, which in this case is a 4 cylinder car. The three “sub models” are then:\n\r4 Cylinder: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\)\r6 Cylinder: \\(Y = (\\beta_0 + \\beta_2) + \\beta_1 x + \\epsilon\\)\r8 Cylinder: \\(Y = (\\beta_0 + \\beta_3) + \\beta_1 x + \\epsilon\\)\r\rNotice that they all have the same slope. However, using the two dummy variables, we achieve the three intercepts.\n\r\\(\\beta_0\\) is the average mpg for a 4 cylinder car with 0 disp.\r\\(\\beta_0 + \\beta_2\\) is the average mpg for a 6 cylinder car with 0 disp.\r\\(\\beta_0 + \\beta_3\\) is the average mpg for a 8 cylinder car with 0 disp.\r\rSo because 4 cylinder is the reference level, \\(\\beta_0\\) is specific to 4 cylinders, but \\(\\beta_2\\) and \\(\\beta_3\\) are used to represent quantities relative to 4 cylinders.\nAs we have done before, we can extract these intercepts and slopes for the three lines, and plot them accordingly.\nint_4cyl = coef(mpg_disp_add_cyl)[1]\rint_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3]\rint_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4]\rslope_all_cyl = coef(mpg_disp_add_cyl)[2]\rplot_colors = c(\u0026quot;Darkorange\u0026quot;, \u0026quot;Darkgrey\u0026quot;, \u0026quot;Dodgerblue\u0026quot;)\rplot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))\rabline(int_4cyl, slope_all_cyl, col = plot_colors[1], lty = 1, lwd = 2)\rabline(int_6cyl, slope_all_cyl, col = plot_colors[2], lty = 2, lwd = 2)\rabline(int_8cyl, slope_all_cyl, col = plot_colors[3], lty = 3, lwd = 2)\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;4 Cylinder\u0026quot;, \u0026quot;6 Cylinder\u0026quot;, \u0026quot;8 Cylinder\u0026quot;),\rcol = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))\rOn this plot, we have\n\r4 Cylinder: orange dots, solid orange line.\r6 Cylinder: grey dots, dashed grey line.\r8 Cylinder: blue dots, dotted blue line.\r\rThe odd result here is that we’re estimating that 8 cylinder cars have better fuel efficiency than 6 cylinder cars at any displacement! The dotted blue line is always above the dashed grey line. That doesn’t seem right. Maybe for very large displacement engines that could be true, but that seems wrong for medium to low displacement.\nTo attempt to fix this, we will try using an interaction model, that is, instead of simply three intercepts and one slope, we will allow for three slopes. Again, we’ll let R take the wheel, (no pun intended) then figure out what model it has applied.\n(mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg))\r## ## Call:\r## lm(formula = mpg ~ disp * cyl, data = autompg)\r## ## Coefficients:\r## (Intercept) disp cyl6 cyl8 disp:cyl6 disp:cyl8 ## 43.59052 -0.13069 -13.20026 -20.85706 0.08299 0.10817\r# could also use mpg ~ disp + cyl + disp:cyl\rR has again chosen to use 4 cylinder cars as the reference level, but this also now has an effect on the interaction terms. R has fit the model.\n\\[\rY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon\r\\]\nWe’re using \\(\\gamma\\) like a \\(\\beta\\) parameter for simplicity, so that, for example \\(\\beta_2\\) and \\(\\gamma_2\\) are both associated with \\(v_2\\).\nNow, the three “sub models” are:\n\r4 Cylinder: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\).\r6 Cylinder: \\(Y = (\\beta_0 + \\beta_2) + (\\beta_1 + \\gamma_2) x + \\epsilon\\).\r8 Cylinder: \\(Y = (\\beta_0 + \\beta_3) + (\\beta_1 + \\gamma_3) x + \\epsilon\\).\r\rInterpreting some parameters and coefficients then:\n\r\\((\\beta_0 + \\beta_2)\\) is the average mpg of a 6 cylinder car with 0 disp\r\\((\\hat{\\beta}_1 + \\hat{\\gamma}_3) = -0.1306935 + 0.1081714 = -0.0225221\\) is the estimated change in average mpg for an increase of one disp, for an 8 cylinder car.\r\rSo, as we have seen before \\(\\beta_2\\) and \\(\\beta_3\\) change the intercepts for 6 and 8 cylinder cars relative to the reference level of \\(\\beta_0\\) for 4 cylinder cars.\nNow, similarly \\(\\gamma_2\\) and \\(\\gamma_3\\) change the slopes for 6 and 8 cylinder cars relative to the reference level of \\(\\beta_1\\) for 4 cylinder cars.\nOnce again, we extract the coefficients and plot the results.\nint_4cyl = coef(mpg_disp_int_cyl)[1]\rint_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3]\rint_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4]\rslope_4cyl = coef(mpg_disp_int_cyl)[2]\rslope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5]\rslope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6]\rplot_colors = c(\u0026quot;Darkorange\u0026quot;, \u0026quot;Darkgrey\u0026quot;, \u0026quot;Dodgerblue\u0026quot;)\rplot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))\rabline(int_4cyl, slope_4cyl, col = plot_colors[1], lty = 1, lwd = 2)\rabline(int_6cyl, slope_6cyl, col = plot_colors[2], lty = 2, lwd = 2)\rabline(int_8cyl, slope_8cyl, col = plot_colors[3], lty = 3, lwd = 2)\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;4 Cylinder\u0026quot;, \u0026quot;6 Cylinder\u0026quot;, \u0026quot;8 Cylinder\u0026quot;),\rcol = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))\rThis looks much better! We can see that for medium displacement cars, 6 cylinder cars now perform better than 8 cylinder cars, which seems much more reasonable than before.\nTo completely justify the interaction model (i.e., a unique slope for each cyl level) compared to the additive model (single slope), we can perform an \\(F\\)-test. Notice first, that there is no \\(t\\)-test that will be able to do this since the difference between the two models is not a single parameter.\nWe will test,\n\\[\rH_0: \\gamma_2 = \\gamma_3 = 0\r\\]\nwhich represents the parallel regression lines we saw before,\n\\[\rY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon.\r\\]\nAgain, this is a difference of two parameters, thus no \\(t\\)-test will be useful.\nanova(mpg_disp_add_cyl, mpg_disp_int_cyl)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp + cyl\r## Model 2: mpg ~ disp * cyl\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 379 7299.5 ## 2 377 6551.7 2 747.79 21.515 1.419e-09 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rAs expected, we see a very low p-value, and thus reject the null. We prefer the interaction model over the additive model.\nRecapping a bit:\n\rNull Model: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon\\)\r\rNumber of parameters: \\(q = 4\\)\r\rFull Model: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon\\)\r\rNumber of parameters: \\(p = 6\\)\r\r\rlength(coef(mpg_disp_int_cyl)) - length(coef(mpg_disp_add_cyl))\r## [1] 2\rWe see there is a difference of two parameters, which is also displayed in the resulting ANOVA table from R. Notice that the following two values also appear on the ANOVA table.\nnrow(autompg) - length(coef(mpg_disp_int_cyl))\r## [1] 377\rnrow(autompg) - length(coef(mpg_disp_add_cyl))\r## [1] 379\r\r\rParameterization\rSo far we have been simply letting R decide how to create the dummy variables, and thus R has been deciding the parameterization of the models. To illustrate the ability to use alternative parameterizations, we will recreate the data, but directly creating the dummy variables ourselves.\nnew_param_data = data.frame(\ry = autompg$mpg,\rx = autompg$disp,\rv1 = 1 * as.numeric(autompg$cyl == 4),\rv2 = 1 * as.numeric(autompg$cyl == 6),\rv3 = 1 * as.numeric(autompg$cyl == 8))\rhead(new_param_data, 20)\r## y x v1 v2 v3\r## 1 18 307 0 0 1\r## 2 15 350 0 0 1\r## 3 18 318 0 0 1\r## 4 16 304 0 0 1\r## 5 17 302 0 0 1\r## 6 15 429 0 0 1\r## 7 14 454 0 0 1\r## 8 14 440 0 0 1\r## 9 14 455 0 0 1\r## 10 15 390 0 0 1\r## 11 15 383 0 0 1\r## 12 14 340 0 0 1\r## 13 15 400 0 0 1\r## 14 14 455 0 0 1\r## 15 24 113 1 0 0\r## 16 22 198 0 1 0\r## 17 18 199 0 1 0\r## 18 21 200 0 1 0\r## 19 27 97 1 0 0\r## 20 26 97 1 0 0\rNow,\n\ry is mpg\rx is disp, the displacement in cubic inches,\rv1, v2, and v3 are dummy variables as defined above.\r\rFirst let’s try to fit an additive model using x as well as the three dummy variables.\nlm(y ~ x + v1 + v2 + v3, data = new_param_data)\r## ## Call:\r## lm(formula = y ~ x + v1 + v2 + v3, data = new_param_data)\r## ## Coefficients:\r## (Intercept) x v1 v2 v3 ## 32.96326 -0.05217 2.03603 -1.59722 NA\rWhat is happening here? Notice that R is essentially ignoring v3, but why? Well, because R uses an intercept, it cannot also use v3. This is because\n\\[\r\\boldsymbol{1} = v_1 + v_2 + v_3\r\\]\nwhich means that \\(\\boldsymbol{1}\\), \\(v_1\\), \\(v_2\\), and \\(v_3\\) are linearly dependent. This would make the \\(X^\\top X\\) matrix singular, but we need to be able to invert it to solve the normal equations and obtain \\(\\hat{\\beta}.\\) With the intercept, v1, and v2, R can make the necessary “three intercepts”. So, in this case v3 is the reference level.\nIf we remove the intercept, then we can directly obtain all “three intercepts” without a reference level.\nlm(y ~ 0 + x + v1 + v2 + v3, data = new_param_data)\r## ## Call:\r## lm(formula = y ~ 0 + x + v1 + v2 + v3, data = new_param_data)\r## ## Coefficients:\r## x v1 v2 v3 ## -0.05217 34.99929 31.36604 32.96326\rHere, we are fitting the model\n\\[\rY = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta x +\\epsilon.\r\\]\nThus we have:\n\r4 Cylinder: \\(Y = \\mu_1 + \\beta x + \\epsilon\\)\r6 Cylinder: \\(Y = \\mu_2 + \\beta x + \\epsilon\\)\r8 Cylinder: \\(Y = \\mu_3 + \\beta x + \\epsilon\\)\r\rWe could also do something similar with the interaction model, and give each line an intercept and slope, without the need for a reference level.\nlm(y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)\r## ## Call:\r## lm(formula = y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)\r## ## Coefficients:\r## v1 v2 v3 v1:x v2:x v3:x ## 43.59052 30.39026 22.73346 -0.13069 -0.04770 -0.02252\r\\[\rY = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta_1 x v_1 + \\beta_2 x v_2 + \\beta_3 x v_3 +\\epsilon\r\\]\n\r4 Cylinder: \\(Y = \\mu_1 + \\beta_1 x + \\epsilon\\)\r6 Cylinder: \\(Y = \\mu_2 + \\beta_2 x + \\epsilon\\)\r8 Cylinder: \\(Y = \\mu_3 + \\beta_3 x + \\epsilon\\)\r\rUsing the original data, we have (at least) three equivalent ways to specify the interaction model with R.\nlm(mpg ~ disp * cyl, data = autompg)\r## ## Call:\r## lm(formula = mpg ~ disp * cyl, data = autompg)\r## ## Coefficients:\r## (Intercept) disp cyl6 cyl8 disp:cyl6 disp:cyl8 ## 43.59052 -0.13069 -13.20026 -20.85706 0.08299 0.10817\rlm(mpg ~ 0 + cyl + disp : cyl, data = autompg)\r## ## Call:\r## lm(formula = mpg ~ 0 + cyl + disp:cyl, data = autompg)\r## ## Coefficients:\r## cyl4 cyl6 cyl8 cyl4:disp cyl6:disp cyl8:disp ## 43.59052 30.39026 22.73346 -0.13069 -0.04770 -0.02252\rlm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg)\r## ## Call:\r## lm(formula = mpg ~ 0 + disp + cyl + disp:cyl, data = autompg)\r## ## Coefficients:\r## disp cyl4 cyl6 cyl8 disp:cyl6 disp:cyl8 ## -0.13069 43.59052 30.39026 22.73346 0.08299 0.10817\rThey all fit the same model, importantly each using six parameters, but the coefficients mean slightly different things in each. However, once they are interpreted as slopes and intercepts for the “three lines” they will have the same result.\nUse ?all.equal to learn about the all.equal() function, and think about how the following code verifies that the residuals of the two models are the same.\nall.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)),\rfitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)))\r## [1] TRUE\r\rBuilding Larger Models\rNow that we have seen how to incorporate categorical predictors as well as interaction terms, we can start to build much larger, much more flexible models which can potentially fit data better.\nLet’s define a “big” model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon.\r\\]\nHere,\n\r\\(Y\\) is mpg.\r\\(x_1\\) is disp.\r\\(x_2\\) is hp.\r\\(x_3\\) is domestic, which is a dummy variable we defined, where 1 is a domestic vehicle.\r\rFirst thing to note here, we have included a new term \\(x_1 x_2 x_3\\) which is a three-way interaction. Interaction terms can be larger and larger, up to the number of predictors in the model.\nSince we are using the three-way interaction term, we also use all possible two-way interactions, as well as each of the first order (main effect) terms. This is the concept of a hierarchy. Any time a “higher-order” term is in a model, the related “lower-order” terms should also be included. Mathematically their inclusion or exclusion is sometimes irrelevant, but from an interpretation standpoint, it is best to follow the hierarchy rules.\nLet’s do some rearrangement to obtain a “coefficient” in front of \\(x_1\\).\n\\[\rY = \\beta_0 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_6 x_2 x_3 + (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)x_1 + \\epsilon.\r\\]\nSpecifically, the “coefficient” in front of \\(x_1\\) is\n\\[\r(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3).\r\\]\nLet’s discuss this “coefficient” to help us understand the idea of the flexibility of a model. Recall that,\n\r\\(\\beta_1\\) is the coefficient for a first order term,\r\\(\\beta_4\\) and \\(\\beta_5\\) are coefficients for two-way interactions,\r\\(\\beta_7\\) is the coefficient for the three-way interaction.\r\rIf the two and three way interactions were not in the model, the whole “coefficient” would simply be\n\\[\r\\beta_1.\r\\]\nThus, no matter the values of \\(x_2\\) and \\(x_3\\), \\(\\beta_1\\) would determine the relationship between \\(x_1\\) (disp) and \\(Y\\) (mpg).\nWith the addition of the two-way interactions, now the “coefficient” would be\n\\[\r(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3).\r\\]\nNow, changing \\(x_1\\) (disp) has a different effect on \\(Y\\) (mpg), depending on the values of \\(x_2\\) and \\(x_3\\).\nLastly, adding the three-way interaction gives the whole “coefficient”\n\\[\r(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)\r\\]\nwhich is even more flexible. Now changing \\(x_1\\) (disp) has a different effect on \\(Y\\) (mpg), depending on the values of \\(x_2\\) and \\(x_3\\), but in a more flexible way which we can see with some more rearrangement. Now the “coefficient” in front of \\(x_3\\) in this “coefficient” is dependent on \\(x_2\\).\n\\[\r(\\beta_1 + \\beta_4 x_2 + (\\beta_5 + \\beta_7 x_2) x_3)\r\\]\nIt is so flexible, it is becoming hard to interpret!\nLet’s fit this three-way interaction model in R.\nbig_model = lm(mpg ~ disp * hp * domestic, data = autompg)\rsummary(big_model)\r## ## Call:\r## lm(formula = mpg ~ disp * hp * domestic, data = autompg)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -11.9410 -2.2147 -0.4008 1.9430 18.4094 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 6.065e+01 6.600e+00 9.189 \u0026lt; 2e-16 ***\r## disp -1.416e-01 6.344e-02 -2.232 0.0262 * ## hp -3.545e-01 8.123e-02 -4.364 1.65e-05 ***\r## domestic -1.257e+01 7.064e+00 -1.780 0.0759 . ## disp:hp 1.369e-03 6.727e-04 2.035 0.0426 * ## disp:domestic 4.933e-02 6.400e-02 0.771 0.4414 ## hp:domestic 1.852e-01 8.709e-02 2.126 0.0342 * ## disp:hp:domestic -9.163e-04 6.768e-04 -1.354 0.1766 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 3.88 on 375 degrees of freedom\r## Multiple R-squared: 0.76, Adjusted R-squared: 0.7556 ## F-statistic: 169.7 on 7 and 375 DF, p-value: \u0026lt; 2.2e-16\rDo we actually need this large of a model? Let’s first test for the necessity of the three-way interaction term. That is,\n\\[\rH_0: \\beta_7 = 0.\r\\]\nSo,\n\rFull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon\\)\rNull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\)\r\rWe fit the null model in R as two_way_int_mod, then use anova() to perform an \\(F\\)-test as usual.\ntwo_way_int_mod = lm(mpg ~ disp * hp + disp * domestic + hp * domestic, data = autompg)\r#two_way_int_mod = lm(mpg ~ (disp + hp + domestic) ^ 2, data = autompg)\ranova(two_way_int_mod, big_model)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp * hp + disp * domestic + hp * domestic\r## Model 2: mpg ~ disp * hp * domestic\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F)\r## 1 376 5673.2 ## 2 375 5645.6 1 27.599 1.8332 0.1766\rWe see the p-value is somewhat large, so we would fail to reject. We prefer the smaller, less flexible, null model, without the three-way interaction.\nA quick note here: the full model does still “fit better.” Notice that it has a smaller RMSE than the null model, which means the full model makes smaller (squared) errors on average.\nmean(resid(big_model) ^ 2)\r## [1] 14.74053\rmean(resid(two_way_int_mod) ^ 2)\r## [1] 14.81259\rHowever, it is not much smaller. We could even say that, the difference is insignificant. This is an idea we will return to later in greater detail.\nNow that we have chosen the model without the three-way interaction, can we go further? Do we need the two-way interactions? Let’s test\n\\[\rH_0: \\beta_4 = \\beta_5 = \\beta_6 = 0.\r\\]\nRemember we already chose \\(\\beta_7 = 0\\), so,\n\rFull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\)\rNull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\)\r\rWe fit the null model in R as additive_mod, then use anova() to perform an \\(F\\)-test as usual.\nadditive_mod = lm(mpg ~ disp + hp + domestic, data = autompg)\ranova(additive_mod, two_way_int_mod)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp + hp + domestic\r## Model 2: mpg ~ disp * hp + disp * domestic + hp * domestic\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 379 7369.7 ## 2 376 5673.2 3 1696.5 37.478 \u0026lt; 2.2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rHere the p-value is small, so we reject the null, and we prefer the full (alternative) model. Of the models we have considered, our final preference is for\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon.\r\\]\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613518807,"objectID":"a66b8951081722f5755747f3314e83f0","permalink":"https://ssc442kirkpatrick.netlify.app/example/06-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/06-example/","section":"example","summary":"Preliminaries\rDummy Variables\rInteractions\rFactor Variables\rFactors with More Than Two Levels\r\rParameterization\rBuilding Larger Models\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\rPreliminaries\rSo far in each of our analyses, we have only used numeric variables as predictors.","tags":null,"title":"Linear Regression: Interpreting Coefficients","type":"docs"},{"authors":null,"categories":null,"content":"\r\rModel Selection\rAssesing Model Accuracy\rModel Complexity\rTest-Train Split\rAdding Flexibility to Linear Models\rChoosing a Model\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\rModel Selection\rOften when we are developing a linear regression model, part of our goal is to explain a relationship. Now, we will ignore much of what we have learned and instead simply use regression as a tool to predict. Instead of a model which explains relationships, we seek a model which minimizes errors.\nFirst, note that a linear model is one of many methods used in regression.\nTo discuss linear models in the context of prediction, we introduce the (very boring) Advertising data that is discussed in the ISL text (see supplemental readings).\nAdvertising\r## # A tibble: 200 x 4\r## TV Radio Newspaper Sales\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 230. 37.8 69.2 22.1\r## 2 44.5 39.3 45.1 10.4\r## 3 17.2 45.9 69.3 9.3\r## 4 152. 41.3 58.5 18.5\r## 5 181. 10.8 58.4 12.9\r## 6 8.7 48.9 75 7.2\r## 7 57.5 32.8 23.5 11.8\r## 8 120. 19.6 11.6 13.2\r## 9 8.6 2.1 1 4.8\r## 10 200. 2.6 21.2 10.6\r## # ... with 190 more rows\rlibrary(caret)\rfeaturePlot(x = Advertising[ , c(\u0026quot;TV\u0026quot;, \u0026quot;Radio\u0026quot;, \u0026quot;Newspaper\u0026quot;)], y = Advertising$Sales)\r\rAssesing Model Accuracy\rThere are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.\n\\[\r\\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\r\\]\nWhile for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be \\(n\\).\nFor a linear model , the estimate of \\(f\\), \\(\\hat{f}\\), is given by the fitted regression line.\n\\[\r\\hat{y}({\\bf{x}_i}) = \\hat{f}({\\bf{x}_i})\r\\]\nWe can write an R function that will be useful for performing this calculation.\nrmse = function(actual, predicted) {\rsqrt(mean((actual - predicted) ^ 2))\r}\r\rModel Complexity\rAside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, \\(p\\).\nWe write a simple R function to extract this information from a model.\nget_complexity = function(model) {\rlength(coef(model)) - 1\r}\r\rTest-Train Split\rThere is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts. It is essentially cheating! As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down, or in very specific cases, stay the same.\nThis would suggest that to predict well, we should use the largest possible model! However, in reality we have hard fit to a specific dataset, but as soon as we see new data, a large model may in fact predict poorly. This is called overfitting.\nFrequently we will take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data. Test data should never be used to train a model.\nNote that sometimes the terms evaluation set and test set are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.\nHere we use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to allow use to reproduce the same random split each time we perform this analysis.\nset.seed(9)\rnum_obs = nrow(Advertising)\rtrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\rtrain_data = Advertising[train_index, ]\rtest_data = Advertising[-train_index, ]\rWe will look at two measures that assess how well a model is predicting, the train RMSE and the test RMSE.\n\\[\r\\text{RMSE}_{\\text{Train}} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\displaystyle\\sum_{i \\in \\text{Train}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\r\\]\nHere \\(n_{Tr}\\) is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.\n\\[\r\\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\displaystyle\\sum_{i \\in \\text{Test}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\r\\]\nHere \\(n_{Te}\\) is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.\nWe will start with the simplest possible linear model, that is, a model with no predictors.\nfit_0 = lm(Sales ~ 1, data = train_data)\rget_complexity(fit_0)\r## [1] 0\r# train RMSE\rsqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))\r## [1] 5.529258\r# test RMSE\rsqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2))\r## [1] 4.914163\rThe previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.\n# train RMSE\rrmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))\r## [1] 5.529258\r# test RMSE\rrmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))\r## [1] 4.914163\rThis function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.\nget_rmse = function(model, data, response) {\rrmse(actual = subset(data, select = response, drop = TRUE),\rpredicted = predict(model, data))\r}\rBy using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.\nget_rmse(model = fit_0, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 5.529258\rget_rmse(model = fit_0, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 4.914163\r\rAdding Flexibility to Linear Models\rEach successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.\nfit_1 = lm(Sales ~ ., data = train_data)\rget_complexity(fit_1)\r## [1] 3\rget_rmse(model = fit_1, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 1.888488\rget_rmse(model = fit_1, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 1.461661\rfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\rget_complexity(fit_2)\r## [1] 7\rget_rmse(model = fit_2, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 1.016822\rget_rmse(model = fit_2, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 0.9117228\rfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\rget_complexity(fit_3)\r## [1] 8\rget_rmse(model = fit_3, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 0.6553091\rget_rmse(model = fit_3, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 0.6633375\rfit_4 = lm(Sales ~ Radio * Newspaper * TV +\rI(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\rget_complexity(fit_4)\r## [1] 10\rget_rmse(model = fit_4, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 0.6421909\rget_rmse(model = fit_4, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 0.7465957\rfit_5 = lm(Sales ~ Radio * Newspaper * TV +\rI(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\rget_complexity(fit_5)\r## [1] 14\rget_rmse(model = fit_5, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 0.6120887\rget_rmse(model = fit_5, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 0.7864181\r\rChoosing a Model\rTo better understand the relationship between train RMSE, test RMSE, and model complexity, we summarize our results, as the above is somewhat cluttered.\nFirst, we recap the models that we have fit.\nfit_1 = lm(Sales ~ ., data = train_data)\rfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\rfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\rfit_4 = lm(Sales ~ Radio * Newspaper * TV +\rI(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\rfit_5 = lm(Sales ~ Radio * Newspaper * TV +\rI(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\rNext, we create a list of the models fit.\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\rWe then obtain train RMSE, test RMSE, and model complexity for each.\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \u0026quot;Sales\u0026quot;)\rtest_rmse = sapply(model_list, get_rmse, data = test_data, response = \u0026quot;Sales\u0026quot;)\rmodel_complexity = sapply(model_list, get_complexity)\rWe then plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\nplot(model_complexity, train_rmse, type = \u0026quot;b\u0026quot;,\rylim = c(min(c(train_rmse, test_rmse)) - 0.02,\rmax(c(train_rmse, test_rmse)) + 0.02),\rcol = \u0026quot;dodgerblue\u0026quot;,\rxlab = \u0026quot;Model Size\u0026quot;,\rylab = \u0026quot;RMSE\u0026quot;)\rlines(model_complexity, test_rmse, type = \u0026quot;b\u0026quot;, col = \u0026quot;darkorange\u0026quot;)\rWe also summarize the results as a table. fit_1 is the least flexible, and fit_5 is the most flexible. We see the Train RMSE decrease as flexibility increases. We see that the Test RMSE is smallest for fit_3, thus is the model we believe will perform the best on future data not used to train the model. Note this may not be the best model, but it is the best model of the models we have seen in this example.\n\r\rModel\rTrain RMSE\rTest RMSE\rPredictors\r\r\r\rfit_1\r1.8884884\r1.4616608\r3\r\rfit_2\r1.0168223\r0.9117228\r7\r\rfit_3\r0.6553091\r0.6633375\r8\r\rfit_4\r0.6421909\r0.7465957\r10\r\rfit_5\r0.6120887\r0.7864181\r14\r\r\r\rTo summarize:\n\rUnderfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\rOverfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.\r\rSpecifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE. Then a model is underfitting if there exists a more complex model with lower Test RMSE.\nA number of notes on these results:\n\rThe labels of under and overfitting are relative to the best model we see, fit_3. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.\rThe train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. Here we see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later.\rOften we expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get lucky and this will not be true.\r\rA final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that predicted well, and paid no attention to a model for explaination. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613518807,"objectID":"40eeabbfe9d21a7292000fc4fe538d43","permalink":"https://ssc442kirkpatrick.netlify.app/example/07-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/07-example/","section":"example","summary":"Model Selection\rAssesing Model Accuracy\rModel Complexity\rTest-Train Split\rAdding Flexibility to Linear Models\rChoosing a Model\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\rModel Selection\rOften when we are developing a linear regression model, part of our goal is to explain a relationship.","tags":null,"title":"Linear Regression: Model Selection","type":"docs"},{"authors":null,"categories":null,"content":"\r\rOur data and goal\rSome functions\r\rBreakout #1\rBreakout #1 discussion\r\rAutomating models\rLoops\r\rBreakout #2\rBreakout #2 discussion\r\rGetting RMSE from the list\rBreakout #3 (if time)\r\r\rOur data and goal\rWe want to use the wooldridge::wage2 data on wages to generate and tune a non-parametric model of wages using a regression tree.\nWe’ve learned that our RMSE calculations have a hard time with NAs in the data. So let’s use the skim output to drop variables with NA (see n_missing). Of course, there are other things we can do (impute the NAs, or make dummies for them), but for now, it’s easiest to drop.\nwage_clean = wage2 %\u0026gt;%\rdplyr::select(-brthord, -meduc, -feduc)\rOnce cleaned, we should be able to use rpart(wage ~ ., data = wage_clean) and not have any NAs in our prediction.\nSome functions\rThese functions are taken from our previous Content and Examples:\nrmse = function(actual, predicted) {\rsqrt(mean((actual - predicted) ^ 2))\r}\rget_rmse = function(model, data, response) {\rrmse(actual = subset(data, select = response, drop = TRUE),\rpredicted = predict(model, data))\r}\r\r\rBreakout #1\rI’m going to assign you to breakout rooms to work in groups of 3-4 on live-coding. One person in the group will need to have Rstudio up, be able to share screen, and have the correct packages loaded (caret, rpart, and rpart.plot, plus skimr). Copy the rmse and get_rmse code into a blank R script (you don’t have to use RMarkdown, just use a blank R script and run from there).\nFor the first breakout, all I want you to do is the following:\n\rEstimate a default regression tree on wage_clean using the default parameters.\rUse rpart.plot to vizualize your regression tree, and talk through the interpretation with each other.\rCalculate the RMSE for your regression tree.\r\rI’ll bring you back in about 5 minutes. Remember, you can use ?wage2 to see the variable names. Make sure you know what variables are showing up in the plot and explaining wage in your model. You may find something odd at first and may need to drop more variables…\n5 minutes\nBreakout #1 discussion\rLet’s choose a group to share their plot and discuss the results.\n\r\rAutomating models\rLet’s talk about a little code shortcut that helps iterate through your model selection.\nFirst, before, we used list() to store all of our models. This is because list() can “hold” anything at all, unlike a matrix, which is only numeric, or a data.frame which needs all rows in a column to be the same data type. list() is also recursive, so each element in a list can be a list. Of lists. Of lists!\nLists are also really easy to add to iteratively. We can initiate an empty list using myList \u0026lt;- list(), then we can add things to it. Note that we use the double [ to index:\nmyList \u0026lt;- list()\rmyList[[\u0026#39;first\u0026#39;]] = \u0026#39;This is the first thing on my list\u0026#39;\rprint(myList)\r## $first\r## [1] \u0026quot;This is the first thing on my list\u0026quot;\rLists let you name the “containers” (much like you can name colums in a data.frame). Our first one is called “first”. We can add more later:\nmyList[[\u0026#39;second\u0026#39;]] = c(1,2,3)\rprint(myList)\r## $first\r## [1] \u0026quot;This is the first thing on my list\u0026quot;\r## ## $second\r## [1] 1 2 3\rAnd still more:\nmyList[[\u0026#39;third\u0026#39;]] = data.frame(a = c(1,2,3), b = c(\u0026#39;Albert\u0026#39;,\u0026#39;Alex\u0026#39;,\u0026#39;Alice\u0026#39;))\rprint(myList)\r## $first\r## [1] \u0026quot;This is the first thing on my list\u0026quot;\r## ## $second\r## [1] 1 2 3\r## ## $third\r## a b\r## 1 1 Albert\r## 2 2 Alex\r## 3 3 Alice\rNow we have a data.frame in there! We can use lapply to do something to every element in the list:\nlapply(myList, length) # the length() function with the first entry being the list element\r## $first\r## [1] 1\r## ## $second\r## [1] 3\r## ## $third\r## [1] 2\rWe get back a list of equal length but each (still-named) container is now the length of the original list’s contents.\nLoops\rR has a very useful looping function that takes the form:\nfor(i in c(\u0026#39;first\u0026#39;,\u0026#39;second\u0026#39;,\u0026#39;third\u0026#39;)){\rprint(i)\r}\r## [1] \u0026quot;first\u0026quot;\r## [1] \u0026quot;second\u0026quot;\r## [1] \u0026quot;third\u0026quot;\rHere, R is repeating the thing in the loop (print(i)) with a different value for i each time. R repeats only what is inside the curly-brackets, then when it reaches the close-curly-bracket, it goes back to the top, changes i to the next element, and repeats.\nWe can use this to train our models. First, we clear our list object myList by setting it equal to an empty list. Then, we loop over some regression tree tuning parameters. First, we have to figure out how to use the loop to set a unique name for each list container. To do this, we’ll use paste0('Tuning',i) which will result in a character string of Tuning0 when i=0, Tuning0.01 when i=0.01, etc.\nmyList \u0026lt;- list() # resets the list. Otherwise, you\u0026#39;ll just be adding to your old list!\rfor(i in c(0, 0.01, 0.02)){\rmyList[[paste0(\u0026#39;Tuning\u0026#39;,i)]] = rpart(wage ~ ., data = wage_clean, cp = i)\r}\rIf you use names(myList) you’ll see the result of our paste0 naming. If you want to see the plotted results, you can use:\nrpart.plot(myList[[\u0026#39;Tuning0.01\u0026#39;]])\r\r\rBreakout #2\rLet’s send you back to your breakout rooms. Using the loop method, generate 5 regression trees to explain wage in wage_clean. You can iterate through values of cp, the complexity parameter, or minsplit, the minimum # of points that have to be in each split.\n10 minutes\nBreakout #2 discussion\r\r\rGetting RMSE from the list\rFinally, we want to move towards getting the RMSE for each of these trees. We’ve done this before using lapply. Let’s introduce a neat coding shortcut, the anonymous function:\nmyRMSE \u0026lt;- lapply(myList, function(x){\rget_rmse(x, wage_clean, \u0026#39;wage\u0026#39;)\r} )\rprint(myRMSE)\r## $Tuning0\r## [1] 292.5856\r## ## $Tuning0.01\r## [1] 359.4592\r## ## $Tuning0.02\r## [1] 367.2748\rWell, it gets us the right answer, but whaaaaaat is going on? Curly brackets? x?\nThis is an “anonymous function”, or a function created on the fly. Here’s how it works in lapply:\n\rThe first argument is the list you want to do something to\rThe second argument would usually be the function you want to apply, like get_rmse\rHere, we’re going to ask R to temporarily create a function that takes one argument, x.\rx is going to be each list element in myList.\rThink of it as a loop:\r\rTake the first element of myList and refer to it as x. Run the function.\rThen it’ll take the second element of myList and refer to it as x and run the function.\rRepeat until all elements of x have been used.\r\rOnce the anonymous function has been applied to x, the result is passed back and saved as the new element of the list output, always in the same position from where the x was taken.\r\rSo you can think of it like this:\nx = myList[[1]]\rmyList[[1]] = get_rmse(x, wage_clean, \u0026#39;wage\u0026#39;)\rx = myList[[2]]\rmyList[[2]] = get_rmse(x, wage_clean, \u0026#39;wage\u0026#39;)\rx = myList[[3]]\rmyList[[3]] = get_rmse(x, wage_clean, \u0026#39;wage\u0026#39;)\rYou can refer to myList by name:\r- myList[['Tuning0.01']]\nOr you can refer to myList by the index:\r- myList[[2]]\nJust like you can refer to data.frame columns by name or by index.\n\rBreakout #3 (if time)\rLet’s send you to the breakout rooms one more time, and use lapply to get a list of your RMSE’s (one for each of your models). Note that we are not yet splitting into test and train (which you will need to do on your lab assignment).\nOnce you have your list, create the plot of RMSEs similar to the one we looked at in Content this week. Note: you can use unlist(myRMSE) to get a numeric vector of the RMSE’s (as long as all of the elements in myRMSE are numeric). Then, it’s a matter of plotting either with base plot or with ggplot (if you use ggplot you’ll have to tidy the RMSE by adding the index column or naming the x-axis).\nRemaining time\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641470874,"objectID":"66dae6a89dc933d1691fce47e0612205","permalink":"https://ssc442kirkpatrick.netlify.app/example/08-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/08-example/","section":"example","summary":"Our data and goal\rSome functions\r\rBreakout #1\rBreakout #1 discussion\r\rAutomating models\rLoops\r\rBreakout #2\rBreakout #2 discussion\r\rGetting RMSE from the list\rBreakout #3 (if time)\r\r\rOur data and goal\rWe want to use the wooldridge::wage2 data on wages to generate and tune a non-parametric model of wages using a regression tree.\nWe’ve learned that our RMSE calculations have a hard time with NAs in the data.","tags":null,"title":"Nonparametric Regression","type":"docs"},{"authors":null,"categories":null,"content":"\r\rReview and Clarify\rIllustration of Bias vs. Variance\r\rA quick bit of R code to help with todays example\rToday’s Example\rSimulation\rHere’s the code\r\r\r\rReview and Clarify\rBias and Variance are tricky subjects. Hopefully the illustrations from yesterday are helpful. Let’s talk through a few things based on questions some of you have asked since Content 9.\nIllustration of Bias vs. Variance\rBias is about how close you are on average to the correct answer. Variance is about how scattered your estimates would be if you repeated your experiment with new data.\n\rFigure 1: Image from MachineLearningPlus.com\r\rWe care about these things because we usually only have our one dataset (when we’re not creating simulated data, that is), but need to know something about how bias and variance tend to look when we change our model complexity.\nDeriving Bias and Variance\rFor this section, recall our model:\n\\[\ry = f(x) + \\epsilon\r\\]\nThis tells us that some of \\(y\\) can be predicted by the true \\(f(x)\\), and some is just noise \\(\\epsilon\\). In our simulation from last week, \\(f(x) = x^2\\), so \\(y = x^2 + \\epsilon\\).\n\rWe want to predict \\(y\\). We call our prediction \\(\\hat{y}\\).\n\rOur best guess for \\(f(x)\\) is \\(\\hat{f}(x)\\), where \\(\\hat{f}(x)\\) is our model. It might be from a linear regression with 1, 2, 9, 15, etc. predictors or interactions of predictors. It might be from a k-nearest-neighbors estimation with k = 4. It might be from a regression tree with cp = .1 and minsplit=2.\n\rEven when we really nail \\(\\hat{f}(x)\\) (which means \\(\\hat{f}(x) = f(x)\\)), there is still error in our prediction because of \\(\\epsilon\\)\n\r\\(y \\neq \\hat{y}\\)\r\r\rSo we think of two different measures of error:\n\\[\rEPE = E[(y - \\hat{y})^2] = \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]}_\\textrm{reducible error - MSE from imperfect model} +\r\\underbrace{\\mathbb{V}_{Y \\mid X} \\left[ Y \\mid X = x \\right]}_{\\textrm{irreducible error from }\\epsilon}\r\\]\nAnd:\n\\[\rMSE(f(x), \\hat{f}(x)) = E_{\\mathcal{D}}\\left[\\left(f(x) - \\hat{f}(x)\\right)^2\\right]\r\\]\nSome of you asked about this equation from last time that decomposed our MSE:\n\\[\r\\text{MSE}\\left(f(x), \\hat{f}(x)\\right) =\r\\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right] =\r\\underbrace{\\left(f(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}(x) \\right)} +\r\\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}(x) \\right)}\r\\]\rThis can be derived by:\n\\[\r\\begin{eqnarray*}\r\\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right] \u0026amp;=\u0026amp; \\mathbb{E}_{\\mathcal{D}} \\left[\\left(f(x) - E[\\hat{f}(x)] + E[\\hat{f}(x)] - \\hat{f}(x)\\right)^2 \\right] \\\\\r\u0026amp;=\u0026amp; \\mathbb{E}_{\\mathcal{D}} \\left[\\left(f(x) - E[\\hat{f}(x)]\\right)^2 + \\left(E[\\hat{f}(x)] - \\hat{f}(x)\\right)^2 + 2\\left(f(x) - E[\\hat{f}(x)]\\right) \\left(E[\\hat{f}(x)] - \\hat{f}(x)\\right) \\right] \\\\\r\u0026amp;=\u0026amp; \\mathbb{E}_{\\mathcal{D}} \\left[\\left(f(x) - E[\\hat{f}(x)]\\right)^2\\right] + \\mathbb{E}_{\\mathcal{D}} \\left[\\left(E[\\hat{f}(x)] - \\hat{f}(x)\\right)^2\\right] + \\mathbb{E}_{\\mathcal{D}} \\left[2\\left(f(x) - E[\\hat{f}(x)]\\right) \\left(E[\\hat{f}(x)] - \\hat{f}(x)\\right) \\right] \\\\\r\u0026amp;=\u0026amp; \\left(f(x) - E[\\hat{f}(x)]\\right)^2 + Var\\left(\\hat{f}(x)\\right) + 0\r\\end{eqnarray*}\r\\]\rLet’s talk about what’s in this equation:\n\rMSE is the Mean Squared Error between \\(f(x)\\) and \\(\\hat{f}(x)\\)\r\rIt does not have the \\(\\epsilon\\) in it\r\rIt is an expectation over all the possible \\(\\mathcal{D}\\) draws of the data we could have\r\rBecause of this \\(f(x)\\) and \\(\\mathbb{E}\\left[\\hat{f}(x)\\right]\\) can move out of the expectation. This lets us cancel that last term with the “2” in it.\r\r\rThe main takeaway is that, even given the error, \\(\\epsilon\\), we still have additional error coming from our inability to perfectly get \\(\\hat{f}(x) = f(x)\\).\n\r\r\rA quick bit of R code to help with todays example\rWe saw before the usefulness of having a list\nmyList = list()\rmyList[[\u0026#39;thisThing\u0026#39;]] = c(1,2,3)\rmyList[[\u0026#39;thisOtherThing\u0026#39;]] = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;D\u0026#39;)\rmyList\r## $thisThing\r## [1] 1 2 3\r## ## $thisOtherThing\r## [1] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;C\u0026quot; \u0026quot;D\u0026quot;\rIt worked really well for holding results from models since we could name the things in the list. But if we put it into a loop:\nfor(i in c(\u0026#39;G\u0026#39;,\u0026#39;H\u0026#39;,\u0026#39;I\u0026#39;)){\rmyList = list()\rmyList[[i]] = paste0(\u0026#39;This loop is on \u0026#39;,i)\r}\rprint(myList)\r## $I\r## [1] \u0026quot;This loop is on I\u0026quot;\rWhat happened? Every time we used the loop, it re-initiated the list, so we only get the last result!\nSo what we want is to create the list if it doesn’t exist, and add to it afterwards. We can do that with exists('myList')\nfor(i in c(\u0026#39;G\u0026#39;,\u0026#39;H\u0026#39;,\u0026#39;I\u0026#39;)){\rif(!exists(\u0026#39;myList\u0026#39;)) myList = list()\rmyList[[i]] = paste0(\u0026#39;This loop is on \u0026#39;,i)\r}\rprint(myList)\r## $I\r## [1] \u0026quot;This loop is on I\u0026quot;\r## ## $G\r## [1] \u0026quot;This loop is on G\u0026quot;\r## ## $H\r## [1] \u0026quot;This loop is on H\u0026quot;\rWe’re almost there. It turns out, we have our original \\(I\\) in there left over from the previous creation of the list. That’s why its out of order. What we want to do is start with a fresh, clean list. If we run rm(myList), the old list will no longer exist, and *our code will create a fresh one when we run it again!\nrm(myList)\rfor(i in c(\u0026#39;G\u0026#39;,\u0026#39;H\u0026#39;,\u0026#39;I\u0026#39;)){\rif(!exists(\u0026#39;myList\u0026#39;)) myList = list()\rmyList[[i]] = paste0(\u0026#39;This loop is on \u0026#39;,i)\r}\rprint(myList)\r## $G\r## [1] \u0026quot;This loop is on G\u0026quot;\r## ## $H\r## [1] \u0026quot;This loop is on H\u0026quot;\r## ## $I\r## [1] \u0026quot;This loop is on I\u0026quot;\rYou’re going to use this in your breakout rooms today. You’re going to be asked to run some code that stores a plot in a list. To reset the list that stores things, just use rm(listName) (where listName is the name of the list).\n\rToday’s Example\rOur goal today is to\n\rSee the code that produced this week’s Content\n\rWhy? Because it helps to illustrate the true sources of noise in the data\r\rSee what larger sample sizes and higher/lower irreducible error does to our Bias vs. Variance tradeoff.\n\r\rSimulation\rWe will use the exact code from Content 9, which I have reproduced here. I have removed the in-between parts with notation so we can focus on the example. I have copied all of the relevant code into one chunk down at the bottom as well\nWe’ll need the following libraries:\nlibrary(ggplot2)\rlibrary(patchwork)\rAnd here, I’ve made a little change to Content 9’s code so we can play with sample size NN and the SD of the irreducible Bayes error.\nNN = 100 #----\u0026gt; In class, we will change this to # see how our results change in response\rSD.of.Bayes.Error = .75 #-----\u0026gt; This, too, will change. # Note that both of these are used in the next chunk(s) to generate data.\rBegin Content 9 code here:\rWe will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function \\(f(x) = x^2\\).\nf = function(x) {\rx ^ 2\r}\rTo carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following R code.\nget_sim_data = function(f, sample_size = NN) {\rx = runif(n = sample_size, min = 0, max = 1)\reps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)\ry = f(x) + eps\rdata.frame(x, y)\r}\rTo completely specify the data generating process, we have made more model assumptions than simply \\(\\mathbb{E}[Y \\mid X = x] = x^2\\) and \\(\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\\). In particular,\n\rThe \\(x_i\\) in \\(\\mathcal{D}\\) are sampled from a uniform distribution over \\([0, 1]\\).\rThe \\(x_i\\) and \\(\\epsilon\\) are independent.\rThe \\(y_i\\) in \\(\\mathcal{D}\\) are sampled from the conditional normal distribution.\r\rUsing this setup, we will generate datasets, \\(\\mathcal{D}\\), with a sample size \\(NN\\) and fit four models.\n\\[\r\\begin{aligned}\r\\texttt{predict(fit0, x)} \u0026amp;= \\hat{f}_0(x) = \\hat{\\beta}_0\\\\\r\\texttt{predict(fit1, x)} \u0026amp;= \\hat{f}_1(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\\\\r\\texttt{predict(fit2, x)} \u0026amp;= \\hat{f}_2(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 \\\\\r\\texttt{predict(fit9, x)} \u0026amp;= \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9\r\\end{aligned}\r\\]\nTo get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.\nset.seed(1)\rsim_data = get_sim_data(f)\rfit_0 = lm(y ~ 1, data = sim_data)\rfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\rfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\rfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\rPlotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.\n…\nWe will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates for \\(f(x)\\) given by these four models at the point \\(x = 0.90\\). We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.\nset.seed(1)\rn_sims = 250\rn_models = 4\rx = data.frame(x = 0.90) # fixed point at which we make predictions\rpredictions = matrix(0, nrow = n_sims, ncol = n_models)\rfor (sim in 1:n_sims) {\r# simulate new, random, training data\r# this is the only random portion of the bias, var, and mse calculations\r# this allows us to calculate the expectation over D\rsim_data = get_sim_data(f, sample_size = NN)\r# fit models\rfit_0 = lm(y ~ 1, data = sim_data)\rfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\rfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\rfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\r# get predictions\rpredictions[sim, 1] = predict(fit_0, x)\rpredictions[sim, 2] = predict(fit_1, x)\rpredictions[sim, 3] = predict(fit_2, x)\rpredictions[sim, 4] = predict(fit_9, x)\r}\rCompile all of the results:\nIn your breakout room and using the code below:\nFirst Breakout: set NN = 100, the value we used in our Content 9 lecture. The value is set in one of the first code chunks. Step through the code to get your finalPlot and make sure it looks like the plot in Content 9. I changed the code to use ggplot (easier to save output), so the formatting and colors will be different - that’s OK, we want to get the same results, not copy the layout of the plot. Note that at the end of the code, a list is created that will hold all of your results. In case you need to clear this list, rm(FinalResults) will do so and the code will initate a new blank list to hold subsequent results.\nSecond Breakout: set NN to a larger number. Usually, more data means more precise predictions. Run your code again stepping through it, until you get to this plot. Note that at the end of the code provided, there is a list that aggregates your results. Repeat this with a 3rd, even larger value for NN. Don’t go much beyond 50,000 or it’ll take too long. Your FinalResults list should have 3 elements in it. Use wrap_plots(FinalResults, nrow = 1) to see all 3 side-by-side.\nThird Breakout: Finally, change the SD.of.Bayes.Error value to make it higher or lower. Remember, this is the irreducible error. Run your code again with your first, second, and third different value for sample size NN. You should have 6 plots in your FinalResults list - 3 from before, and 3 more with the new SD of Bayes Error. Use wrap_plots with the right number of rows to see a 2x3 grid of the results.\n\rUsually we think larger sample sizes and lower error lead to better overall prediction. Do we see any change in the bias vs. tradeoff relationship with lower/higher sample size NN and lower/higher SD of Bayes Error?\r\r\r\r\rHere’s the code\rI’ve merged all of the code together for you here. Copy this into a new .R script - you don’t need to use a full Markdown.\nlibrary(ggplot2)\rlibrary(patchwork)\rNN = 100 #----\u0026gt; In class, we will change this to # see how our results change in response\rSD.of.Bayes.Error = 0.75 #-----\u0026gt; This, too, will change. # Note that both of these are used in the next chunk(s) to generate data.\rf = function(x) {\rx ^ 2\r}\rget_sim_data = function(f, sample_size = NN) {\rx = runif(n = sample_size, min = 0, max = 1)\reps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)\ry = f(x) + eps\rdata.frame(x, y)\r}\rset.seed(1)\rsim_data = get_sim_data(f)\rfit_0 = lm(y ~ 1, data = sim_data)\rfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\rfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\rfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\rset.seed(42)\rplot(y ~ x, data = sim_data, col = \u0026quot;grey\u0026quot;, pch = 20,\rmain = \u0026quot;Four Polynomial Models fit to a Simulated Dataset\u0026quot;)\rgrid = seq(from = 0, to = 2, by = 0.01)\rlines(grid, f(grid), col = \u0026quot;black\u0026quot;, lwd = 3)\rlines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = \u0026quot;dodgerblue\u0026quot;, lwd = 2, lty = 2)\rlines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = \u0026quot;firebrick\u0026quot;, lwd = 2, lty = 3)\rlines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = \u0026quot;springgreen\u0026quot;, lwd = 2, lty = 4)\rlines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = \u0026quot;darkorange\u0026quot;, lwd = 2, lty = 5)\rlegend(\u0026quot;topleft\u0026quot;, c(\u0026quot;y ~ 1\u0026quot;, \u0026quot;y ~ poly(x, 1)\u0026quot;, \u0026quot;y ~ poly(x, 2)\u0026quot;, \u0026quot;y ~ poly(x, 9)\u0026quot;, \u0026quot;truth\u0026quot;), col = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;firebrick\u0026quot;, \u0026quot;springgreen\u0026quot;, \u0026quot;darkorange\u0026quot;, \u0026quot;black\u0026quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)\rset.seed(1)\rn_sims = 250\rn_models = 4\rx = data.frame(x = 0.90) # fixed point at which we make predictions\rpredictions = matrix(0, nrow = n_sims, ncol = n_models)\rfor (sim in 1:n_sims) {\r# simulate new, random, training data\r# this is the only random portion of the bias, var, and mse calculations\r# this allows us to calculate the expectation over D\rsim_data = get_sim_data(f, sample_size = NN)\r# fit models\rfit_0 = lm(y ~ 1, data = sim_data)\rfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\rfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\rfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\r# get predictions\rpredictions[sim, 1] = predict(fit_0, x)\rpredictions[sim, 2] = predict(fit_1, x)\rpredictions[sim, 3] = predict(fit_2, x)\rpredictions[sim, 4] = predict(fit_9, x)\r}\rpredictions.proc = (predictions)\rcolnames(predictions.proc) = c(\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;9\u0026quot;)\rpredictions.proc = as.data.frame(predictions.proc)\rtall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)\r## Here, you can save your ggplot output FinalPlot \u0026lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + geom_boxplot() +\rgeom_jitter(alpha = .5) +\rgeom_hline(yintercept = f(x = .90)) +\rlabs(col = \u0026#39;Model\u0026#39;, x = \u0026#39;Model\u0026#39;, y = \u0026#39;Prediction\u0026#39;, title = paste0(\u0026#39;Bias v Var - Sample Size: \u0026#39;,NN), subtitle = paste0(\u0026#39;SD of Bayes Err: \u0026#39;,SD.of.Bayes.Error)) +\rtheme_bw()\rFinalPlot\r## This is going to aggregate your results for you:\rif(!exists(\u0026#39;FinalResults\u0026#39;)) FinalResults = list()\rFinalResults[[paste0(\u0026#39;finalPlot.NN.\u0026#39;,NN,\u0026#39;.SDBayes.\u0026#39;,SD.of.Bayes.Error)]] = FinalPlot\r# # boxplot(value ~ key, data = tall_predictions, border = \u0026quot;darkgrey\u0026quot;, xlab = \u0026quot;Polynomial Degree\u0026quot;, ylab = \u0026quot;Predictions\u0026quot;,\r# main = \u0026quot;Simulated Predictions for Polynomial Models\u0026quot;)\r# grid()\r# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = \u0026quot;jitter\u0026quot;, jitter = 0.15, pch = 1, col = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;firebrick\u0026quot;, \u0026quot;springgreen\u0026quot;, \u0026quot;darkorange\u0026quot;))\r# abline(h = f(x = 0.90), lwd = 2)\rAnd to output whatever is in your list:\n## This not run automatically.\r## To plot the whole list of ggplot objects in FinalResults:\rwrap_plots(FinalResults, nrow = 2, guides = \u0026#39;collect\u0026#39;)\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641470874,"objectID":"4ca8d8f57d9585dcab132a17e9a5a6e7","permalink":"https://ssc442kirkpatrick.netlify.app/example/09-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/09-example/","section":"example","summary":"Review and Clarify\rIllustration of Bias vs. Variance\r\rA quick bit of R code to help with todays example\rToday’s Example\rSimulation\rHere’s the code\r\r\r\rReview and Clarify\rBias and Variance are tricky subjects. Hopefully the illustrations from yesterday are helpful. Let’s talk through a few things based on questions some of you have asked since Content 9.\nIllustration of Bias vs. Variance\rBias is about how close you are on average to the correct answer.","tags":null,"title":"Illustrating Bias vs. Variance","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rAccessibility\rColors\rFonts\rGraphic assets\r\rImages\rVectors\rVectors, photos, videos, and other assets\r\r\r\rAccessibility\r\rVischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\rColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)\r\r\rColors\r\rAdobe Color: Create, share, and explore rule-based and custom color palettes.\rColourLovers: Like Facebook for color palettes.\rviridis: Percetually uniform color scales.\rScientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with scico.\rColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account.\rColorgorical: Create color palettes based on fancy mathematical rules for perceptual distance.\rColorpicker for data: More fancy mathematical rules for color palettes (explanation).\riWantHue: Yet another perceptual distance-based color palette builder.\rPhotochrome: Word-based color pallettes.\rPolicyViz Design Color Tools: Large collection of useful color resources\r\r\rFonts\r\rGoogle Fonts: Huge collection of free, well-made fonts.\rThe Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).\r\r\rGraphic assets\rImages\r\rUse the Creative Commons filters on Google Images or Flickr\rUnsplash\rPexels\rPixabay\rStockSnap.io\rBurst\rfreephotos.cc\r\r\rVectors\r\rNoun Project: Thousands of free simple vector images\raiconica: 1,000+ vector icons\rVecteezy: Thousands of free vector images\r\r\rVectors, photos, videos, and other assets\r\rStockio\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"16fd04c4714e3d096bffcf19e6c524ca","permalink":"https://ssc442kirkpatrick.netlify.app/resource/design/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/design/","section":"resource","summary":"Accessibility\rColors\rFonts\rGraphic assets\r\rImages\rVectors\rVectors, photos, videos, and other assets\r\r\r\rAccessibility\r\rVischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\rColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)\r\r\rColors\r\rAdobe Color: Create, share, and explore rule-based and custom color palettes.\rColourLovers: Like Facebook for color palettes.","tags":null,"title":"Design","type":"docs"},{"authors":null,"categories":null,"content":"\rNN = 100 #----\u0026gt; In class, we will change this to # see how our results change in response\rSD.of.Bayes.Error = 0.75 #-----\u0026gt; This, too, will change. # Note that both of these are used in the next chunk(s) to generate data.\rlibrary(ggplot2)\r## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 4.0.5\rlibrary(patchwork)\rf = function(x) {\rx ^ 2\r}\rget_sim_data = function(f, sample_size = NN) {\rx = runif(n = sample_size, min = 0, max = 1)\reps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)\ry = f(x) + eps\rdata.frame(x, y)\r}\rset.seed(1)\rsim_data = get_sim_data(f)\rfit_0 = lm(y ~ 1, data = sim_data)\rfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\rfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\rfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\rset.seed(42)\rplot(y ~ x, data = sim_data, col = \u0026quot;grey\u0026quot;, pch = 20,\rmain = \u0026quot;Four Polynomial Models fit to a Simulated Dataset\u0026quot;)\rgrid = seq(from = 0, to = 2, by = 0.01)\rlines(grid, f(grid), col = \u0026quot;black\u0026quot;, lwd = 3)\rlines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = \u0026quot;dodgerblue\u0026quot;, lwd = 2, lty = 2)\rlines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = \u0026quot;firebrick\u0026quot;, lwd = 2, lty = 3)\rlines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = \u0026quot;springgreen\u0026quot;, lwd = 2, lty = 4)\rlines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = \u0026quot;darkorange\u0026quot;, lwd = 2, lty = 5)\rlegend(\u0026quot;topleft\u0026quot;, c(\u0026quot;y ~ 1\u0026quot;, \u0026quot;y ~ poly(x, 1)\u0026quot;, \u0026quot;y ~ poly(x, 2)\u0026quot;, \u0026quot;y ~ poly(x, 9)\u0026quot;, \u0026quot;truth\u0026quot;), col = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;firebrick\u0026quot;, \u0026quot;springgreen\u0026quot;, \u0026quot;darkorange\u0026quot;, \u0026quot;black\u0026quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)\rset.seed(1)\rn_sims = 250\rn_models = 4\rx = data.frame(x = 0.90) # fixed point at which we make predictions\rpredictions = matrix(0, nrow = n_sims, ncol = n_models)\rfor (sim in 1:n_sims) {\r# simulate new, random, training data\r# this is the only random portion of the bias, var, and mse calculations\r# this allows us to calculate the expectation over D\rsim_data = get_sim_data(f, sample_size = NN)\r# fit models\rfit_0 = lm(y ~ 1, data = sim_data)\rfit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)\rfit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)\rfit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)\r# get predictions\rpredictions[sim, 1] = predict(fit_0, x)\rpredictions[sim, 2] = predict(fit_1, x)\rpredictions[sim, 3] = predict(fit_2, x)\rpredictions[sim, 4] = predict(fit_9, x)\r}\rpredictions.proc = (predictions)\rcolnames(predictions.proc) = c(\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;9\u0026quot;)\rpredictions.proc = as.data.frame(predictions.proc)\rtall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)\r## Here, you can save your ggplot output FinalPlot \u0026lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + geom_boxplot() +\rgeom_jitter(alpha = .5) +\rgeom_hline(yintercept = f(x = .90)) +\rlabs(col = \u0026#39;Model\u0026#39;, x = \u0026#39;Model\u0026#39;, y = \u0026#39;Prediction\u0026#39;, title = paste0(\u0026#39;Bias v Var - Sample Size: \u0026#39;,NN), subtitle = paste0(\u0026#39;SD of Bayes Err: \u0026#39;,SD.of.Bayes.Error)) +\rtheme_bw()\rFinalPlot\r## This is going to aggregate your results for you:\rif(!exists(\u0026#39;FinalResults\u0026#39;)) FinalResults = list()\rFinalResults[[paste0(\u0026#39;finalPlot.NN.\u0026#39;,NN,\u0026#39;.SDBayes.\u0026#39;,SD.of.Bayes.Error)]] = FinalPlot\r# # boxplot(value ~ key, data = tall_predictions, border = \u0026quot;darkgrey\u0026quot;, xlab = \u0026quot;Polynomial Degree\u0026quot;, ylab = \u0026quot;Predictions\u0026quot;,\r# main = \u0026quot;Simulated Predictions for Polynomial Models\u0026quot;)\r# grid()\r# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = \u0026quot;jitter\u0026quot;, jitter = 0.15, pch = 1, col = c(\u0026quot;dodgerblue\u0026quot;, \u0026quot;firebrick\u0026quot;, \u0026quot;springgreen\u0026quot;, \u0026quot;darkorange\u0026quot;))\r# abline(h = f(x = 0.90), lwd = 2)\r## This not run automatically.\r## To plot the whole list of ggplot objects in FinalResults:\rwrap_plots(FinalResults, nrow = 2, guides = \u0026#39;collect\u0026#39;)\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641470874,"objectID":"f17acf34e4f2f9d1176c767338265931","permalink":"https://ssc442kirkpatrick.netlify.app/example/code_aggregated/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/code_aggregated/","section":"example","summary":"NN = 100 #----\u0026gt; In class, we will change this to # see how our results change in response\rSD.of.Bayes.Error = 0.75 #-----\u0026gt; This, too, will change. # Note that both of these are used in the next chunk(s) to generate data.\rlibrary(ggplot2)\r## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 4.0.5\rlibrary(patchwork)\rf = function(x) {\rx ^ 2\r}\rget_sim_data = function(f, sample_size = NN) {\rx = runif(n = sample_size, min = 0, max = 1)\reps = rnorm(n = sample_size, mean = 0, sd = SD.","tags":null,"title":"Illustrating Bias vs. Variance","type":"docs"},{"authors":null,"categories":null,"content":"\r\rBasic Markdown formatting\rMath\rTables\rFootnotes\rFront matter\rCitations\rOther references\r\r\rMarkdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting\r\r\r\r\rType…\r…or…\r…to get\r\r\r\rSome text in a paragraph.\rMore text in the next paragraph. Always\ruse empty lines between paragraphs.\r\rSome text in a paragraph.\nMore text in the next paragraph. Always\ruse empty lines between paragraphs.\n\r\r*Italic*\r_Italic_\rItalic\r\r**Bold**\r__Bold__\rBold\r\r# Heading 1\r\rHeading 1\r\r\r## Heading 2\r\rHeading 2\r\r\r### Heading 3\r\rHeading 3\r\r\r(Go up to heading level 6 with ######)\r\r\r\r[Link text](http://www.example.com)\r\rLink text\r\r![Image caption](/path/to/image.png)\r\r\r\r`Inline code` with backticks\r\rInline code with backticks\r\r\u0026gt; Blockquote\r\r\rBlockquote\n\r\r- Things in\r- an unordered\r- list\r* Things in\r* an unordered\r* list\r\rThings in\ran unordered\rlist\r\r\r1. Things in\r2. an ordered\r3. list\r1) Things in\r2) an ordered\r3) list\rThings in\ran ordered\rlist\r\r\rHorizontal line\r---\rHorizontal line\r***\rHorizontal line\n\r\r\r\r\rMath\rMarkdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n\r\r\r\rType…\r…to get\r\r\r\rBased on the DAG, the regression model for\restimating the effect of education on wages\ris $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or\r$\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$.\rBased on the DAG, the regression model for\restimating the effect of education on wages\ris \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or\r\\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).\r\r\r\rTo put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math:\r$$\rx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\r$$\rBut now we just use computers to solve for $x$.\r…to get…\n\rThe quadratic equation was an important part of high school math:\n\\[\rx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\r\\]\nBut now we just use computers to solve for \\(x\\).\n\rBecause dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n\rTables\rThere are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default\r------- ------ ---------- -------\r12 12 12 12\r123 123 123 123\r1 1 1 1\rTable: Caption goes here\r…to get…\n\rCaption goes here\r\rRight\rLeft\rCenter\rDefault\r\r\r\r12\r12\r12\r12\r\r123\r123\r123\r123\r\r1\r1\r1\r1\r\r\r\rFor pipe tables, type…\n| Right | Left | Default | Center |\r|------:|:-----|---------|:------:|\r| 12 | 12 | 12 | 12 |\r| 123 | 123 | 123 | 123 |\r| 1 | 1 | 1 | 1 |\rTable: Caption goes here\r…to get…\n\rCaption goes here\r\rRight\rLeft\rDefault\rCenter\r\r\r\r12\r12\r12\r12\r\r123\r123\r123\r123\r\r1\r1\r1\r1\r\r\r\r\rFootnotes\rThere are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags].\r[^1]: This is a note.\r[^note-on-dags]: DAGs are neat.\rAnd here\u0026#39;s more of the document.\r…to get…\n\rHere is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n\r\rThis is a note.↩︎\r\r\rDAGs are neat.↩︎\r\r\r\r\rYou can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!]\r…to get…\n\rCausal inference is neat.1\n\r\rBut it can be hard too!↩︎\r\r\r\r\r\rFront matter\rYou can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n---\rtitle: Title of your document\rdate: \u0026quot;January 13, 2020\u0026quot;\rauthor: \u0026quot;Your name\u0026quot;\r---\rYou can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n---\rtitle: \u0026quot;My cool title: a subtitle\u0026quot;\r---\rIf you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n---\rtitle: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39;\r---\r\rCitations\rOne of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n---\rtitle: Title of your document\rdate: \u0026quot;January 13, 2020\u0026quot;\rauthor: \u0026quot;Your name\u0026quot;\rbibliography: name_of_file.bib\r---\rChoose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n---\rtitle: Title of your document\rdate: \u0026quot;January 13, 2020\u0026quot;\rauthor: \u0026quot;Your name\u0026quot;\rbibliography: name_of_file.bib\rcsl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot;\r---\rSome of the most common CSLs are:\n\rChicago author-date\rChicago note-bibliography\rChicago full note-bibliography (no shortened notes or ibids)\rAPA 7th edition\rMLA 8th edition\r\rCite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n\r\r\r\rType…\r…to get…\r\r\r\rCausal inference is neat [@Rohrer:2018; @AngristPischke:2015].\rCausal inference is neat (Rohrer 2018; Angrist and Pischke 2015).\r\rCausal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1].\rCausal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).\r\rAngrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018].\rAngrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).\r\r@AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees.\rAngrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.\r\r\r\rAfter compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n\rAngrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n\r\r\rOther references\rThese websites have additional details and examples and practice tools:\n\rCommonMark’s Markdown tutorial: A quick interactive Markdown tutorial.\rMarkdown tutorial: Another interactive tutorial to practice using Markdown.\rMarkdown cheatsheet: Useful one-page reminder of Markdown syntax.\rThe Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.\r\r\r","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594409288,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"https://ssc442kirkpatrick.netlify.app/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Basic Markdown formatting\rMath\rTables\rFootnotes\rFront matter\rCitations\rOther references\r\r\rMarkdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting\r\r\r\r\rType…\r…or…\r…to get\r\r\r\rSome text in a paragraph.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rInteresting and excellent real world examples\rHow to select the appropriate chart type\rGeneral resources\rVisualization in Excel\rVisualization in Tableau\r\r\rInteresting and excellent real world examples\r\rThe Stories Behind a Line\rAustralia as 100 people: You can make something like this with d3 and the potato project.\rMarrying Later, Staying Single Longer\r\r\rHow to select the appropriate chart type\rMany people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:\n\rThe Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations.\rThe Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.).\rFrom Data to Viz: A decision tree for dozens of chart types with links to R and Python code.\rThe Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R.\rR Graph Catalog: R code for 124 ggplot graphs.\rEmery’s Essentials: Descriptions and examples of 26 different chart types.\r\r\rGeneral resources\r\rStorytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic.\rAnn K. Emery’s blog: Blog and tutorials by Ann Emery.\rEvergreen Data: Helful resources by Stephanie Evergreen.\rPolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch.\rVisualising Data: Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk.\rInfo We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field.\rFlowingData: Blog by Nathan Yau.\rInformation is Beautiful: Blog by David McCandless.\rJunk Charts: Blog by Kaiser Fung.\rWTF Visualizations: Visualizations that make you ask “wtf?”\rThe Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic.\rData Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway.\rSeeing Data: A series of research projects about perceptions and visualizations.\r\r\rVisualization in Excel\r\rHow to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel.\rAnn Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel.\r\r\rVisualization in Tableau\rBecause it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"ca403ba352e0871f06b445d2470037b3","permalink":"https://ssc442kirkpatrick.netlify.app/resource/visualization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/visualization/","section":"resource","summary":"Interesting and excellent real world examples\rHow to select the appropriate chart type\rGeneral resources\rVisualization in Excel\rVisualization in Tableau\r\r\rInteresting and excellent real world examples\r\rThe Stories Behind a Line\rAustralia as 100 people: You can make something like this with d3 and the potato project.\rMarrying Later, Staying Single Longer\r\r\rHow to select the appropriate chart type\rMany people have created many useful tools for selecting the correct chart type for a given dataset or question.","tags":null,"title":"Visualization","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rKey terms\rAdd chunks\rChunk names\rChunk options\rInline chunks\rOutput formats\r\r\rR Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms\r\rDocument: A Markdown file where you type stuff\n\rChunk: A piece of R code that is included in your document. It looks like this:\n```{r}\r# Code goes here\r```\rThere must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n\rKnit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n\r\r\rAdd chunks\rThere are three ways to insert chunks:\n\rPress ⌘⌥I on macOS or control + alt + I on Windows\n\rClick on the “Insert” button at the top of the editor window\n\rManually type all the backticks and curly braces (don’t do this)\n\r\r\rChunk names\rYou can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk}\r# Code goes here\r```\r\rChunk options\rThere are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE}\r# Code goes here\r```\rThe most common chunk options are these:\n\rfig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures\recho=FALSE: The code is not shown in the final document, but the results are\rmessage=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted\rwarning=FALSE: Any warnings that R generates are omitted\rinclude=FALSE: The chunk still runs, but the code and results are not included in the final document\r\rYou can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n\rInline chunks\rYou can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE}\ravg_mpg \u0026lt;- mean(mtcars$mpg)\r```\rThe average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon.\r… would knit into this:\n\rThe average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n\r\rOutput formats\rYou can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot;\routput:\rhtml_document: default\rpdf_document: default\rword_document: default\rYou can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n---\rtitle: \u0026quot;My document\u0026quot;\rauthor: \u0026quot;My name\u0026quot;\rdate: \u0026quot;January 13, 2020\u0026quot;\routput: html_document: toc: yes\rfig_caption: yes\rfig_height: 8\rfig_width: 10\rpdf_document: latex_engine: xelatex # More modern PDF typesetting engine\rtoc: yes\rword_document: toc: yes\rfig_caption: yes\rfig_height: 4\rfig_width: 5\r---\r\r","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"https://ssc442kirkpatrick.netlify.app/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"Key terms\rAdd chunks\rChunk names\rChunk options\rInline chunks\rOutput formats\r\r\rR Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rR style conventions\rMain style things to pay attention to for this class\r\rSpacing\rLong lines\rPipes (%\u0026gt;%) and ggplot layers (+)\rComments\r\r\r\rR style conventions\rR is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;)\rfilter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rfilter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; )\rBut you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n\rMain style things to pay attention to for this class\r\rImportant note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n\rSpacing\r\rSee the “Spacing” section in the tidyverse style guide.\n\rPut spaces after commas (like in regular English):\n# Good\rfilter(mpg, cty \u0026gt; 10)\r# Bad\rfilter(mpg , cty \u0026gt; 10)\rfilter(mpg ,cty \u0026gt; 10)\rfilter(mpg,cty \u0026gt; 10)\rPut spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good\rfilter(mpg, cty \u0026gt; 10)\r# Bad\rfilter(mpg, cty\u0026gt;10)\rfilter(mpg, cty\u0026gt; 10)\rfilter(mpg, cty \u0026gt;10)\rDon’t put spaces around parentheses that are parts of functions:\n# Good\rfilter(mpg, cty \u0026gt; 10)\r# Bad\rfilter (mpg, cty \u0026gt; 10)\rfilter ( mpg, cty \u0026gt; 10)\rfilter( mpg, cty \u0026gt; 10 )\r\rLong lines\r\rSee the “Long lines” section in the tidyverse style guide.\n\rIt’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good\rfilter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\r# Good\rfilter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\r# Good\rfilter(mpg,\rcty \u0026gt; 10,\rclass == \u0026quot;compact\u0026quot;)\r# Bad\rfilter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))\r# Good\rfilter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))\r\rPipes (%\u0026gt;%) and ggplot layers (+)\rPut each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good\rggplot(mpg, aes(x = cty, y = hwy, color = class)) +\rgeom_point() +\rgeom_smooth() +\rtheme_bw()\r# Bad\rggplot(mpg, aes(x = cty, y = hwy, color = class)) +\rgeom_point() + geom_smooth() +\rtheme_bw()\r# Super bad\rggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw()\r# Super bad and won\u0026#39;t even work\rggplot(mpg, aes(x = cty, y = hwy, color = class))\r+ geom_point()\r+ geom_smooth() + theme_bw()\rPut each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good\rmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))\r# Bad\rmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))\r# Super bad\rmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))\r# Super bad and won\u0026#39;t even work\rmpg %\u0026gt;% filter(cty \u0026gt; 10)\r%\u0026gt;% group_by(class)\r%\u0026gt;% summarize(avg_hwy = mean(hwy))\r\rComments\r\rSee the “Comments” section in the tidyverse style guide.\n\rComments should start with a comment symbol and a single space: #\n# Good\r#Bad\r#Bad\rIf the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 +\rgroup_by(class) %\u0026gt;% # Divide into class groups\rsummarize(avg_hwy = mean(hwy)) # Find the average hwy in each group\rYou can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 +\rgroup_by(class) %\u0026gt;% # Divide into class groups\rsummarize(avg_hwy = mean(hwy)) # Find the average hwy in each group\rIf the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good\r# Happy families are all alike; every unhappy family is unhappy in its own way.\r# Everything was in confusion in the Oblonskys’ house. The wife had discovered\r# that the husband was carrying on an intrigue with a French girl, who had been\r# a governess in their family, and she had announced to her husband that she\r# could not go on living in the same house with him. This position of affairs\r# had now lasted three days, and not only the husband and wife themselves, but\r# all the members of their family and household, were painfully conscious of it.\r# Bad\r# Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.\rThough, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"https://ssc442kirkpatrick.netlify.app/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions\rMain style things to pay attention to for this class\r\rSpacing\rLong lines\rPipes (%\u0026gt;%) and ggplot layers (+)\rComments\r\r\r\rR style conventions\rR is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;)\rfilter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rfilter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; )\rBut you’ll notice that only a few of those iterations (the first three) are easily readable.","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"\r\rBecause RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS\rDouble click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n\rUnzipping files on Windows\rtl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n\r","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"https://ssc442kirkpatrick.netlify.app/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"\r\rThere are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n\rData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n\rGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\n\rKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n\r360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\n\rUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n\rPolitical science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n\rFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\rThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\rErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"https://ssc442kirkpatrick.netlify.app/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n\rData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n\rGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rReadings\rGuiding Question\r\rA Brief Introduction to SSC442\rAbout Me\rAbout You\rThis Course\rMore About This Course\rAnd finally…\r\rWhat is “Data Analytics”?\rStarting point for this course\r\rOutline of the Course\rNon-Social Science Approaches to Statistical Learning\rThe Pros and Cons of Correlation\rA Case Study in Prediction\rMore Recent Examples of Prediction\rAn Aside: Nomenclature\rLearning from Data\r\rR basics\rCase study: US homicides by firearm\rThe (very) basics\rObjects\rThe workspace\rFunctions\rOther prebuilt objects\rVariable names\rSaving your workspace\rMotivating scripts\rCommenting your code\r\rData types\rData frames\rExamining an object\rThe accessor: $\rVectors: numerics, characters, and logical\rFactors\rLists\rMatrices\r\rVectors\rCreating vectors\rNames\rSequences\rSubsetting\r\rCoercion\rNot availables (NA)\r\rSorting\rsort\rorder\rmax and which.max\rrank\rBeware of recycling\r\rVector arithmetics\rRescaling a vector\rTwo vectors\r\rIndexing\rSubsetting with logicals\rLogical operators\rwhich\rmatch\r%in%\r\rRmarkdown\rLecture Video\r\r\r\rReadings\rAs noted in the syllabus, your readings will be assigned each week in this area. For this initial week, please read the course content. Read closely the following:\n\rThe syllabus, content, examples, and labs pages for this class.\rThis page. Yes, the whole thing.\r\rGuiding Question\rFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\rDo you remember anything about R?\rWhat are the different data types in R?\rHow do you index specific elements of a vector? Why might you want to do that?\r\r\r\rA Brief Introduction to SSC442\r\rI keep saying that the sexy job in the next 10 years will be statisticians. And I’m not kidding.\n\rHal Varian, Chief Economist, Google\r\rAbout Me\rMe: My primary area of expertise is environmental and energy (applied) economics.\nThis class is totally, unapologetically a work in progress. It was developed mainly by Prof. Bushong with refinements by myself.\nMaterial is a mish-mash of stuff from courses offered at Caltech, Stanford, Harvard, and Duke…so, yeah, it will be challenging. Hopefully, you’ll find it fun!\nMy research: occasionally touches the topics in the course, but mostly utilizes things in the course as tools.\n\rAbout You\rNew phone who dis? Please email me jkirk@msu.edu your\n\rname (with pronunciation guide)\n\rmajor\n\rdesired graduation year and semester\n\rinterest in this course on a 10-point scale (1: not at all interested; 10: helllllll yeah)\n\r\r\rYou must spend 5 minutes emailing me a little bit about your interests before the next class.\n\rThis Course\rThe syllabus is posted on the course website. I’ll walk through highlights now, but read it later – it’s long.\r- But eventually, please read it. It is “required.”\nSyllabus highlights:\n\rGrade is composed of weekly writings, labs, and projects (see syllabus page for exact points)\r\rWeekly writings: 19%\rParticipation: 4%\rLabs: 32%\rProjects: 45%\r\rThis structure is designed to give ~55% “for free”. Success on the projects will require real work.\rLabs consist of a practical implementation of something we’ve covered in the course (e.g., code your own Recommender System).\r\rGrading\rGrading: come to class.\nIf you complete all assignments and attend all class dates, I suspect you will do very well. Given the way the syllabus is structured, I conjecture that the following is a loose guide to grades:\n4.0 Turned in all assignments with good effort, worked hard on the projects and was proud of final product.\n3.5 Turned in all assignments with good effort, worked a bit on the projects and was indifferent to final product.\n3.0 Turned in all assignments with some effort, worked a bit on the projects and was shy about final product.\n\u0026lt; 3.0 Very little effort, or did not turn in all assignments, worked very little on the projects and was embarassed by final product.\n…of course, failing to turn in assignments can lead to a grade dramatically lower than just a 3.0.\n\r\rMore About This Course\rThere are sort of three texts for this course and sort of zero.\nThe “main text” is free and available online. The secondary text is substantially more difficult, but also free online. The third text costs about $25. Assigned readings can be found on the course website under “Content”.\nPlease please please please please: Ask questions during class via chat.\r- Most ideas will be new.\r- Sometimes (often?) the material itself will be confusing or interesting—or both!\r- Teaching online is incredibly challenging (no feedback) and chat is vital to success.\r- Note: If I find that attendance is terrible, I may have to start incorporating attendance into participation.\nReturn of the Please: If there is some topic that you really want to learn about, ask. If you are uncomfortable asking in front of the whole group, please see me during office hours.\nBecause this is a new course:\n\rSome of the lectures will be way too long or too short.\n\rSome (most?) of the lectures won’t make sense.\n\rSome of the time I’ll forget what I intended to say and awkwardly stare at you for a few moments (sorry).\n\r\rComment throughout the course, not just at the end.\nThe material will improve with time and feedback.\nI encourage measured feedback and thoughtful responses to questions. If I call on you and you don’t know immediately, don’t freak out. If you don’t know, it’s totally okay to say you don’t know.\nSUPER BIG IMPORTANT EXPLANATION OF THE COURSE\rI teach using ``math’’.\n…Don’t be afraid. The math won’t hurt you.\nI fundamentally believe that true knowledge of how we learn from data depends on a basic understanding of the underlying mathematics.\n-Good news: no black boxes.\r- You’ll actually learn stuff. (Probably. Hopefully?)\r- Also good news: level of required math is reasonably low. High-school algebra or equivalent should be fine.\n-Bad news: notation-heavy slides and reading.\n\r\rAnd finally…\rFinally: I cannot address field-specific questions in areas outside economics to any satisfying degree.\nGood news: I’m good at knowing what I don’t know and have a very small ego, which means that I’m much less likely to blow smoke up your ass than other professors.\nBad news: I can’t help with certain types of questions.\nThis course should be applicable broadly, but many of the examples will lean on my personal expertise (sorry).\nYour assignment: read syllabus the content from Week 0.\nThings to stress from syllabus:\n\rE-mail isn’t the ideal solution for technical problems\rNo appointments necessary for regularly scheduled office hours; or by appointment.\rTA office hours are great as well. Our TA has experience in this course.\rCan only reschedule exams (with good reason) if you tell me before the exam that you have a conflict.\r\rNotify me immediately if you need accommodations because of RCPD or religious convictions; If you approach me at the last minute, I may not be able to help.\r\r\rDespite my hard-assness in these intro slides: I’m here to help and I am not in the business of giving bad grades for no reason.\n\r\rWhat is “Data Analytics”?\rHow do you define “data analytics”? (Not a rhetorical question!)\n\rThis course will avoid this nomenclature. It is confusing and imprecise. But you signed up (suckers) and I owe an explanation of what this course will cover.\r\rSome “data analytics” topics we will cover:\n\rLinear regression: il classico.\rModels of classification or discrete choice.\rAnalysis of ``wide’’ data.\rDecision trees and other non-linear models.\r\rStarting point for this course\rBetter utilizing existing data can improve our predictive power whilst providing interpretable outputs for considering new policies.\nWARNING: Causation is tough and we will spend the entire course warning you to avoid making causal claims!\n\r\rOutline of the Course\rNon-Social Science Approaches to Statistical Learning\rA Brief History\nSuppose you are a researcher and you want to teach a computer to recognize images of a tree.\nNote: this is an ``easy\" problem. If you show pictures to a 3-year-old, that child will probably be able to tell you if there is a tree in the picture.\nComputer scientists spent about 20 years on this problem because they thought about the problem like nerds and tried to write down a series of rules.\nRules are difficult to form, and simply writing rules misses the key insight: the data can tell you something.\nSocial Science Approaches to Statistical Learning\rA Brief History\nSuppose you are a researcher and you want to know whether prisons reduce crime.\nfrom ``A Call for a Moratorium on Prison Building’’ (1976)\n\rBetween 1955 and 1975, fifteen states increased the collective capacity of their adult prison systems by 56% (from, on average, 63,100 to 98,649).\rFifteen other states increased capacity by less than 4% (from 49,575 to 51,440).\rIn “heavy-construction” states the crime rate increased by 167%; in “low-construction” states the crime rate increased by 145%.\r\r\r\r\rPrison Capacity\rCrime Rate\r\r\r\rHigh construction\r\\(\\uparrow\\)~56%\r\\(\\uparrow\\)~167%\r\rLow construction\r\\(\\uparrow\\)~4%\r\\(\\uparrow\\)~145%\r\r\r\r\r\rThe Pros and Cons of Correlation\rPros:\r- Nature gives you correlations for free.\r- In principle, everyone can agree on the facts.\nCons:\r- Correlations are not very helpful.\r- They show what has happened, but not why.\r- For many things, we care about why.\nWhy a Correlation Exists Between X and Y\r\\(X \\rightarrow Y\\)\rX causes Y (causality)\n\r\\(X \\leftarrow Y\\)\rY causes X (reverse causality)\n\r\\(Z \\rightarrow X\\); \\(Z \\rightarrow Y\\)\rZ causes X and Y (common cause)\n\r\\(X \\rightarrow Y\\); \\(Y \\rightarrow X\\)\rX causes Y and Y causes X (simultaneous equations)\n\r\r\rUniting Social Science and Computer Science\rWe will start in this course by examining situations where we do not care about why something has happened, but instead we care about our ability to predict its occurrence from existing data.\n(But of course keep in back of mind that if you are making policy, you must care about why something happened).\nWe will also borrow a few other ideas from CS:\r- Anything is data\r+ Satellite data\r+ Unstructured text or audio\r+ Facial expressions or vocal intonations\r- Subtle improvements on existing techniques\r- An eye towards practical implementability over ``cleanliness\"\n\r\rA Case Study in Prediction\rExample: a firm wishes to predict user behavior based on previous purchases or interactions.\nSmall margins \\(\\rightarrow\\) huge payoffs. \\(10\\% \\rightarrow\\) $1 million.\nNot obvious why this was true for Netflix; quite obvious why this is true in financial markets.\n\rMore Recent Examples of Prediction\r\rIdentify the risk factors for prostate cancer.\rClassify a tissue sample into one of several cancer classes, based on a gene expression profile.\rClassify a recorded phoneme based on a log-periodogram.\rPredict whether someone will have a heart attack on the basis of demographic, diet and clinical measurements.\rCustomize an email spam detection system.\rIdentify a hand-drawn object.\rDetermine which oscillations of stellar luminosity are likely due to exoplanets.\rEstablish the relationship between salary and demographic variables in population survey data.\r\r\rAn Aside: Nomenclature\rMachine learning arose as a subfield of Artificial Intelligence.\nStatistical learning arose as a subfield of Statistics.\nThere is much overlap; however, a few points of distinction:\n\rMachine learning has a greater emphasis on large scale applications and prediction accuracy.\n\rStatistical learning emphasizes models and their interpretability, and precision and uncertainty.\n\rBut the distinction has become more and more blurred, and there is a great deal of “cross-fertilization”.\r\r\rObviously true: machine learning has the upper hand in marketing.\n\rLearning from Data\rThe following are the basic requirements for statistical learning:\nA pattern exists.\rThis pattern is not easily expressed in a closed mathematical form.\rYou have data.\r\rALERT\nThe course content below should be considered a prerequisite for success. For those concerned about basics of R, you absolutely must read this content and attempt the coding exercises. If you struggle to follow the content, please contact the professor or TA.\n\r\r\rR basics\rIn this class, we will be using R software environment for all our analyses. You will learn R and data analysis techniques simultaneously. To follow along you will therefore need access to R. We also recommend the use of an integrated development environment (IDE), such as RStudio, to save your work.\rNote that it is common for a course or workshop to offer access to an R environment and an IDE through your web browser, as done by RStudio cloud1. If you have access to such a resource, you don’t need to install R and RStudio. However, if you intend on becoming a practicing data analyst, we highly recommend installing these tools on your computer2. This is not hard.\nBoth R and RStudio are free and available online.\nCase study: US homicides by firearm\rImagine you live in Europe (if only!) and are offered a job in a US company with many locations in every state. It is a great job, but headlines such as US Gun Homicide Rate Higher Than Other Developed Countries3 have you worried. Fox News runs a scary looking graphic, and charts like the one below only add to that concern:\nOr even worse, this version from everytown.org:\rBut then you remember that (1) this is a hypothetical exercise; (2) you’ll take literally any job at this point; and (3) Geographic diversity matters – the United States is a large and diverse country with 50 very different states (plus the District of Columbia and some lovely territories).4\n## Warning: package \u0026#39;dslabs\u0026#39; was built under R version 4.0.5\r## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please\r## use `guide = \u0026quot;none\u0026quot;` instead.\rCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial.\nThis is a relatively simple and straightforward problem in social science: you have options of where to live, and want to determine the safety of the various states. Your “research” is clearly policy-relevant: you will eventually have to live somewhere. We will begin to tackle the problem by examining data related to gun homicides in the US during 2010 using R.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Ideally, this is a refresher. However, we are aware that your preparation in previously courses varies greatly from student to student. Moreover, we want you to be aware that the usefulness of some of these early building blocks may not be immediately obvious. Later in the class you will appreciate having these skills. Mastery will be rewarded both in this class and (of course) in life.\n\rThe (very) basics\rBefore we get started with the motivating dataset, we need to cover the very basics of R.\nObjects\rSuppose a relatively math unsavvy student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). You—a savvy student—recall that the quadratic formula gives us the solutions:\n\\[\r\\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}\r\\]\nwhich of course depend on the values of \\(a\\), \\(b\\), and \\(c\\). That is, the quadratic equation represents a function with three arguments.\nOne advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define:\na \u0026lt;- 1\rb \u0026lt;- 1\rc \u0026lt;- -1\rwhich stores the values for later use. We use \u0026lt;- to assign values to the variables.\nWe can also assign values using = instead of \u0026lt;-, but we recommend against using = to avoid confusion.5\nTRY IT\nCopy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Throughout these written notes, you’ll have the most success if you continue to copy code into your own console.\n\rTo see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\na\r## [1] 1\rA more explicit way to ask R to show us the value stored in a is using print like this:\nprint(a)\r## [1] 1\rWe use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\n\rThe workspace\rAs we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\nls()\r## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;dat\u0026quot; \u0026quot;filter\u0026quot; \u0026quot;murders\u0026quot; \u0026quot;select\u0026quot;\r(Note that one of my variables listed above comes from generating the graphs above). In RStudio, the Environment tab shows the values:\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )\r## [1] 0.618034\r(-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )\r## [1] -1.618034\r\rFunctions\rOnce you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several zillion predefined functions and most of the analysis pipelines we construct make extensive use of the built-in functions. But R’s power comes from its scalability. We have access to (nearly) infinite functions via install.packages and library. As we go through the course, we will carefully note new functions we bring to each problem. For now, though, we will stick to the basics.\nNote that you’ve used a function already: you used the function sqrt to solve the quadratic equation above. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\nlog(8)\r## [1] 2.079442\rlog(a)\r## [1] 0\rYou can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\nhelp(\u0026quot;log\u0026quot;)\rFor most functions, we can also use this shorthand:\n?log\rThe help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional.6 For example, the base of the function log defaults to base = exp(1)—that is, log evaluates the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\nargs(log)\r## function (x, base = exp(1)) ## NULL\rYou can change the default values by simply assigning another object:\nlog(8, base = 2)\r## [1] 3\rNote that we have not been specifying the argument x as such:\nlog(x = 8, base = 2)\r## [1] 3\rThe above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\nlog(8,2)\r## [1] 3\rIf using the arguments’ names, then we can include them in whatever order we want:\nlog(base = 2, x = 8)\r## [1] 3\rTo specify arguments, we must use =, and cannot use \u0026lt;-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n2 ^ 3\r## [1] 8\rYou can see the arithmetic operators by typing:\nhelp(\u0026quot;+\u0026quot;)\ror\n?\u0026quot;+\u0026quot;\rand the relational operators by typing:\nhelp(\u0026quot;\u0026gt;\u0026quot;)\ror\n?\u0026quot;\u0026gt;\u0026quot;\r\rOther prebuilt objects\rThere are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\ndata()\rThis shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\nco2\rR will show you Mauna Loa atmospheric \\(CO^2\\) concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant \\(\\pi\\) and \\(\\infty\\):\npi\r## [1] 3.141593\rInf+1\r## [1] Inf\r\rVariable names\rWe have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in R. For example, don’t name one of your variables install.packages by typing something like install.packages \u0026lt;- 2. Usually, R is smart enough to prevent you from doing such nonsense, but it’s important to develop good habits.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\nsolution_1 \u0026lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a)\rsolution_2 \u0026lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a)\rFor more advice, we highly recommend studying (Hadley Wickham’s style guide)[http://adv-r.had.co.nz/Style.html].\n\rSaving your workspace\rValues remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab.\rYou can read the help pages on save, save.image, and load to learn more.\n\rMotivating scripts\rTo solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\na \u0026lt;- 3\rb \u0026lt;- 2\rc \u0026lt;- -1\r(-b + sqrt(b^2 - 4*a*c)) / (2*a)\r(-b - sqrt(b^2 - 4*a*c)) / (2*a)\rBy creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\n\rCommenting your code\rIf a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n## Code to compute solution to quadratic equation of the form ax^2 + bx + c\r## define the variables\ra \u0026lt;- 3\rb \u0026lt;- 2\rc \u0026lt;- -1\r## now compute the solution\r(-b + sqrt(b^2 - 4*a*c)) / (2*a)\r(-b - sqrt(b^2 - 4*a*c)) / (2*a)\rTRY IT\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\n\rNow use the same formula to compute the sum of the integers from 1 through 1,000.\n\rLook at the result of typing the following code into R:\n\r\rn \u0026lt;- 1000\rx \u0026lt;- seq(1, n)\rsum(x)\rBased on the result, what do you think the functions seq and sum do? You can use help.\nsum creates a list of numbers and seq adds them up.\rseq creates a list of numbers and sum adds them up.\rseq creates a random list and sum computes the sum of 1 through 1,000.\rsum always returns the same number.\r\rIn math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\n\rWhich of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\n\r\rlog(10^x)\rlog10(x^10)\rlog(exp(x))\rexp(log(x, base = 2))\r\r\r\r\rData types\rVariables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\na \u0026lt;- 2\rclass(a)\r## [1] \u0026quot;numeric\u0026quot;\rTo work efficiently in R, it is important to learn the different types of variables and what we can do with these.\nData frames\rUp to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\nlibrary(dslabs)\rdata(murders)\rTo see that this is in fact a data frame, we type:\nclass(murders)\r## [1] \u0026quot;data.frame\u0026quot;\r\rExamining an object\rThe function str is useful for finding out more about the structure of an object:\nstr(murders)\r## \u0026#39;data.frame\u0026#39;: 51 obs. of 5 variables:\r## $ state : chr \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; ...\r## $ abb : chr \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; ...\r## $ region : Factor w/ 4 levels \u0026quot;Northeast\u0026quot;,\u0026quot;South\u0026quot;,..: 2 4 4 2 4 4 1 2 2 2 ...\r## $ population: num 4779736 710231 6392017 2915918 37253956 ...\r## $ total : num 135 19 232 93 1257 ...\rThis tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\nhead(murders)\r## state abb region population total\r## 1 Alabama AL South 4779736 135\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\r## 4 Arkansas AR South 2915918 93\r## 5 California CA West 37253956 1257\r## 6 Colorado CO West 5029196 65\rIn this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let’s learn more about the components of this object.\n\rThe accessor: $\rFor our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\nmurders$population\r## [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 897934\r## [9] 601723 19687653 9920000 1360301 1567582 12830632 6483802 3046355\r## [17] 2853118 4339367 4533372 1328361 5773552 6547629 9883640 5303925\r## [25] 2967297 5988927 989415 1826341 2700551 1316470 8791894 2059179\r## [33] 19378102 9535483 672591 11536504 3751351 3831074 12702379 1052567\r## [41] 4625364 814180 6346105 25145561 2763885 625741 8001024 6724540\r## [49] 1852994 5686986 563626\rBut how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\nnames(murders)\r## [1] \u0026quot;state\u0026quot; \u0026quot;abb\u0026quot; \u0026quot;region\u0026quot; \u0026quot;population\u0026quot; \u0026quot;total\u0026quot;\rIt is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\nTip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\n\rVectors: numerics, characters, and logical\rThe object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\npop \u0026lt;- murders$population\rlength(pop)\r## [1] 51\rThis particular vector is numeric since population sizes are numbers:\nclass(pop)\r## [1] \u0026quot;numeric\u0026quot;\rIn a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\nclass(murders$state)\r## [1] \u0026quot;character\u0026quot;\rAs with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\nz \u0026lt;- 3 == 2\rz\r## [1] FALSE\rclass(z)\r## [1] \u0026quot;logical\u0026quot;\rHere the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality. Yet another reason to avoid assigning via =… it can get confusing and typos can really mess things up.\nYou can see the other relational operators by typing:\n?Comparison\rIn future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\nAdvanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\n\rFactors\rIn the murders dataset, we might expect the region to also be a character vector. However, it is not:\nclass(murders$region)\r## [1] \u0026quot;factor\u0026quot;\rIt is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\nlevels(murders$region)\r## [1] \u0026quot;Northeast\u0026quot; \u0026quot;South\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot;\rIn the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. It is also useful for computational reasons we’ll explore later.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\nregion \u0026lt;- murders$region\rvalue \u0026lt;- murders$total\rregion \u0026lt;- reorder(region, value, FUN = sum)\rlevels(region)\r## [1] \u0026quot;Northeast\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot; \u0026quot;South\u0026quot;\rThe new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\nWarning: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\n\rLists\rData frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:\nrecord\r## $name\r## [1] \u0026quot;John Doe\u0026quot;\r## ## $student_id\r## [1] 1234\r## ## $grades\r## [1] 95 82 91 97 93\r## ## $final_grade\r## [1] \u0026quot;A\u0026quot;\rclass(record)\r## [1] \u0026quot;list\u0026quot;\rAs with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list.\nrecord$student_id\r## [1] 1234\rWe can also use double square brackets ([[) like this:\nrecord[[\u0026quot;student_id\u0026quot;]]\r## [1] 1234\rYou should get used to the fact that in R there are often several ways to do the same thing. such as accessing entries.7\nYou might also encounter lists without variable names.\nrecord2\r## [[1]]\r## [1] \u0026quot;John Doe\u0026quot;\r## ## [[2]]\r## [1] 1234\rIf a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\nrecord2[[1]]\r## [1] \u0026quot;John Doe\u0026quot;\rWe won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\n\rMatrices\rMatrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this class, but much of what happens in the background when you perform a data analysis involves matrices. We describe them briefly here since some of the functions we will learn return matrices.\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\nmat \u0026lt;- matrix(1:12, 4, 3)\rmat\r## [,1] [,2] [,3]\r## [1,] 1 5 9\r## [2,] 2 6 10\r## [3,] 3 7 11\r## [4,] 4 8 12\rYou can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\nmat[2, 3]\r## [1] 10\rIf you want the entire second row, you leave the column spot empty:\nmat[2, ]\r## [1] 2 6 10\rNotice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\nmat[, 3]\r## [1] 9 10 11 12\rThis is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\nmat[, 2:3]\r## [,1] [,2]\r## [1,] 5 9\r## [2,] 6 10\r## [3,] 7 11\r## [4,] 8 12\rYou can subset both rows and columns:\nmat[1:2, 2:3]\r## [,1] [,2]\r## [1,] 5 9\r## [2,] 6 10\rWe can convert matrices into data frames using the function as.data.frame:\nas.data.frame(mat)\r## V1 V2 V3\r## 1 1 5 9\r## 2 2 6 10\r## 3 3 7 11\r## 4 4 8 12\rYou can also use single square brackets ([) to access rows and columns of a data frame:\ndata(\u0026quot;murders\u0026quot;)\rmurders[25, 1]\r## [1] \u0026quot;Mississippi\u0026quot;\rmurders[2:3, ]\r## state abb region population total\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\rTRY IT\nLoad the US murders dataset.\r\rlibrary(dslabs)\rdata(murders)\rUse the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\nThe 51 states.\rThe murder rates for all 50 states and DC.\rThe state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010.\rstr shows no relevant information.\r\rWhat are the column names used by the data frame for these five variables?\n\rUse the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\n\rNow use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\n\rWe saw that the region column stores a factor. You can corroborate this by typing:\n\r\rclass(murders$region)\rWith one line of code, use the function levels and length to determine the number of regions defined by this dataset.\nThe function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.\r\r\r\r\rVectors\rIn R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\nCreating vectors\rWe can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\ncodes \u0026lt;- c(380, 124, 818)\rcodes\r## [1] 380 124 818\rWe can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\ncountry \u0026lt;- c(\u0026quot;italy\u0026quot;, \u0026quot;canada\u0026quot;, \u0026quot;egypt\u0026quot;)\rIn R you can also use single quotes:\ncountry \u0026lt;- c(\u0026#39;italy\u0026#39;, \u0026#39;canada\u0026#39;, \u0026#39;egypt\u0026#39;)\rBut be careful not to confuse the single quote ’ with the back quote, which shares a keyboard key with ~.\nBy now you should know that if you type:\ncountry \u0026lt;- c(italy, canada, egypt)\ryou receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\n\rNames\rSometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\ncodes \u0026lt;- c(italy = 380, canada = 124, egypt = 818)\rcodes\r## italy canada egypt ## 380 124 818\rThe object codes continues to be a numeric vector:\nclass(codes)\r## [1] \u0026quot;numeric\u0026quot;\rbut with names:\nnames(codes)\r## [1] \u0026quot;italy\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;egypt\u0026quot;\rIf the use of strings without quotes looks confusing, know that you can use the quotes as well:\ncodes \u0026lt;- c(\u0026quot;italy\u0026quot; = 380, \u0026quot;canada\u0026quot; = 124, \u0026quot;egypt\u0026quot; = 818)\rcodes\r## italy canada egypt ## 380 124 818\rThere is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\nWe can also assign names using the names functions:\ncodes \u0026lt;- c(380, 124, 818)\rcountry \u0026lt;- c(\u0026quot;italy\u0026quot;,\u0026quot;canada\u0026quot;,\u0026quot;egypt\u0026quot;)\rnames(codes) \u0026lt;- country\rcodes\r## italy canada egypt ## 380 124 818\r\rSequences\rAnother useful function for creating vectors generates sequences:\nseq(1, 10)\r## [1] 1 2 3 4 5 6 7 8 9 10\rThe first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\nseq(1, 10, 2)\r## [1] 1 3 5 7 9\rIf we want consecutive integers, we can use the following shorthand:\n1:10\r## [1] 1 2 3 4 5 6 7 8 9 10\rWhen we use these functions, R produces integers, not numerics, because they are typically used to index something:\nclass(1:10)\r## [1] \u0026quot;integer\u0026quot;\rHowever, if we create a sequence including non-integers, the class changes:\nclass(seq(1, 10, 0.5))\r## [1] \u0026quot;numeric\u0026quot;\r\rSubsetting\rWe use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\ncodes[2]\r## canada ## 124\rYou can get more than one entry by using a multi-entry vector as an index:\ncodes[c(1,3)]\r## italy egypt ## 380 818\rThe sequences defined above are particularly useful if we want to access, say, the first two elements:\ncodes[1:2]\r## italy canada ## 380 124\rIf the elements have names, we can also access the entries using these names. Below are two examples.\ncodes[\u0026quot;canada\u0026quot;]\r## canada ## 124\rcodes[c(\u0026quot;egypt\u0026quot;,\u0026quot;italy\u0026quot;)]\r## egypt italy ## 818 380\r\r\rCoercion\rIn general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\nx \u0026lt;- c(1, \u0026quot;canada\u0026quot;, 3)\rBut we don’t get one, not even a warning! What happened? Look at x and its class:\nx\r## [1] \u0026quot;1\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;3\u0026quot;\rclass(x)\r## [1] \u0026quot;character\u0026quot;\rR coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \"1\" and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\nx \u0026lt;- 1:5\ry \u0026lt;- as.character(x)\ry\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5\u0026quot;\rYou can turn it back with as.numeric:\nas.numeric(y)\r## [1] 1 2 3 4 5\rThis function is actually quite useful since datasets that include numbers as character strings are common.\nNot availables (NA)\rThis “topic” seems to be wholly unappreciated and it has been our experience that students often panic when encountering an NA. This often happens when a function tries to coerce one type to another and encounters an impossible case. In such circumstances, R usually gives us a warning and turns the entry into a special value called an NA (for “not available”). For example:\nx \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;3\u0026quot;)\ras.numeric(x)\r## Warning: NAs introduced by coercion\r## [1] 1 NA 3\rR does not have any guesses for what number you want when you type b, so it does not try.\nWhile coercion is a common case leading to NAs, you’ll see them in nearly every real-world dataset. Most often, you will encounter the NAs as a stand-in for missing data. Again, this a common problem in real-world datasets and you need to be aware that it will come up.\n\r\rSorting\rNow that we have mastered some basic R knowledge (ha!), let’s try to gain some insights into the safety of different states in the context of gun murders.\nsort\rSay we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\nlibrary(dslabs)\rdata(murders)\rsort(murders$total)\r## [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32\r## [16] 36 38 53 63 65 67 84 93 93 97 97 99 111 116 118\r## [31] 120 135 142 207 219 232 246 250 286 293 310 321 351 364 376\r## [46] 413 457 517 669 805 1257\rHowever, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.\n\rorder\rThe function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:\nx \u0026lt;- c(31, 4, 15, 92, 65)\rsort(x)\r## [1] 4 15 31 65 92\rRather than sort the input vector, the function order returns the index that sorts input vector:\nindex \u0026lt;- order(x)\rx[index]\r## [1] 4 15 31 65 92\rThis is the same output as that returned by sort(x). If we look at this index, we see why it works:\nx\r## [1] 31 4 15 92 65\rorder(x)\r## [1] 2 3 1 5 4\rThe second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\nmurders$state[1:6]\r## [1] \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; \u0026quot;California\u0026quot;\r## [6] \u0026quot;Colorado\u0026quot;\rmurders$abb[1:6]\r## [1] \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;CO\u0026quot;\rThis means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\nind \u0026lt;- order(murders$total)\rmurders$abb[ind]\r## [1] \u0026quot;VT\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;WV\u0026quot; \u0026quot;NE\u0026quot;\r## [16] \u0026quot;OR\u0026quot; \u0026quot;DE\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;WA\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;DC\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;MA\u0026quot;\r## [31] \u0026quot;MS\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;IL\u0026quot; \u0026quot;GA\u0026quot;\r## [46] \u0026quot;MI\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;CA\u0026quot;\rAccording to the above, California had the most murders.\n\rmax and which.max\rIf we are only interested in the entry with the largest value, we can use max for the value:\nmax(murders$total)\r## [1] 1257\rand which.max for the index of the largest value:\ni_max \u0026lt;- which.max(murders$total)\rmurders$state[i_max]\r## [1] \u0026quot;California\u0026quot;\rFor the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\n\rrank\rAlthough not as frequently used as order and sort, the function rank is also related to order and can be useful.\rFor any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\nx \u0026lt;- c(31, 4, 15, 92, 65)\rrank(x)\r## [1] 3 1 2 5 4\rTo summarize, let’s look at the results of the three functions we have introduced:\n\r\roriginal\r\rsort\r\rorder\r\rrank\r\r\r\r\r\r31\r\r4\r\r2\r\r3\r\r\r\r4\r\r15\r\r3\r\r1\r\r\r\r15\r\r31\r\r1\r\r2\r\r\r\r92\r\r65\r\r5\r\r5\r\r\r\r65\r\r92\r\r4\r\r4\r\r\r\r\r\rBeware of recycling\rAnother common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:\nx \u0026lt;- c(1,2,3)\ry \u0026lt;- c(10, 20, 30, 40, 50, 60, 70)\rx+y\r## Warning in x + y: longer object length is not a multiple of shorter object\r## length\r## [1] 11 22 33 41 52 63 71\rWe do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\nTRY IT\nFor these exercises we will use the US murders dataset. Make sure you load it prior to starting.\nlibrary(dslabs)\rdata(\u0026quot;murders\u0026quot;)\rUse the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\n\rNow instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\n\rWe can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\n\rNow we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\n\rYou can create a data frame using the data.frame function. Here is a quick example:\n\r\rtemp \u0026lt;- c(35, 88, 42, 84, 81, 30)\rcity \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;,\r\u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;)\rcity_temps \u0026lt;- data.frame(name = city, temperature = temp)\rUse the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\nRepeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\n\rThe na_example vector represents a series of counts. You can quickly examine the object using:\n\r\rdata(\u0026quot;na_example\u0026quot;)\rstr(na_example)\r## int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...\rHowever, when we compute the average with the function mean, we obtain an NA:\nmean(na_example)\r## [1] NA\rThe is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have.\nNow compute the average again, but only for the entries that are not NA. Hint: remember the ! operator.\r\r\r\r\rVector arithmetics\rCalifornia had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\nlibrary(dslabs)\rdata(\u0026quot;murders\u0026quot;)\rmurders$state[which.max(murders$population)]\r## [1] \u0026quot;California\u0026quot;\rwith over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\nRescaling a vector\rIn R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\ninches \u0026lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\rand want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\ninches * 2.54\r## [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80\rIn the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\ninches - 69\r## [1] 0 -7 -3 1 1 4 -2 4 -2 1\r\rTwo vectors\rIf we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n\\[\r\\begin{pmatrix}\ra\\\\\rb\\\\\rc\\\\\rd\r\\end{pmatrix}\r+\r\\begin{pmatrix}\re\\\\\rf\\\\\rg\\\\\rh\r\\end{pmatrix}\r=\r\\begin{pmatrix}\ra +e\\\\\rb + f\\\\\rc + g\\\\\rd + h\r\\end{pmatrix}\r\\]\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000\rOnce we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\nmurders$abb[order(murder_rate)]\r## [1] \u0026quot;VT\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;OR\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;WA\u0026quot;\r## [16] \u0026quot;WV\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;NE\u0026quot; \u0026quot;MA\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IL\u0026quot;\r## [31] \u0026quot;OK\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;GA\u0026quot; \u0026quot;MS\u0026quot; \u0026quot;MI\u0026quot;\r## [46] \u0026quot;DE\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;DC\u0026quot;\rTRY IT\nPreviously we created this data frame:\r\rtemp \u0026lt;- c(35, 88, 42, 84, 81, 30)\rcity \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;,\r\u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;)\rcity_temps \u0026lt;- data.frame(name = city, temperature = temp)\rRemake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is \\(C = \\frac{5}{9} \\times (F - 32)\\).\nWrite code to compute the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler, we know it should be close to \\(\\pi^2/6\\).\n\rCompute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?\n\r\r\r\r\rIndexing\rIndexing is a boring name for an important tool. R provides a powerful and convenient way of referencing specific elements of vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:\nlibrary(dslabs)\rdata(\u0026quot;murders\u0026quot;)\rSubsetting with logicals\rWe have now calculated the murder rate using:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000\rImagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\nind \u0026lt;- murder_rate \u0026lt; 0.71\rIf we instead want to know if a value is less or equal, we can use:\nind \u0026lt;- murder_rate \u0026lt;= 0.71\rNote that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\nmurders$state[ind]\r## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Iowa\u0026quot; \u0026quot;New Hampshire\u0026quot; \u0026quot;North Dakota\u0026quot; ## [5] \u0026quot;Vermont\u0026quot;\rIn order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\nsum(ind)\r## [1] 5\r\rLogical operators\rSuppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with \u0026amp;. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\nTRUE \u0026amp; TRUE\r## [1] TRUE\rTRUE \u0026amp; FALSE\r## [1] FALSE\rFALSE \u0026amp; FALSE\r## [1] FALSE\rFor our example, we can form two logicals:\nwest \u0026lt;- murders$region == \u0026quot;West\u0026quot;\rsafe \u0026lt;- murder_rate \u0026lt;= 1\rand we can use the \u0026amp; to get a vector of logicals that tells us which states satisfy both conditions:\nind \u0026lt;- safe \u0026amp; west\rmurders$state[ind]\r## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Idaho\u0026quot; \u0026quot;Oregon\u0026quot; \u0026quot;Utah\u0026quot; \u0026quot;Wyoming\u0026quot;\r\rwhich\rSuppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\nind \u0026lt;- which(murders$state == \u0026quot;California\u0026quot;)\rmurder_rate[ind]\r## [1] 3.374138\r\rmatch\rIf instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function match. This function tells us which indexes of a second vector match each of the entries of a first vector:\nind \u0026lt;- match(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state)\rind\r## [1] 33 10 44\rNow we can look at the murder rates:\nmurder_rate[ind]\r## [1] 2.667960 3.398069 3.201360\r\r%in%\rIf rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\nc(\u0026quot;Boston\u0026quot;, \u0026quot;Dakota\u0026quot;, \u0026quot;Washington\u0026quot;) %in% murders$state\r## [1] FALSE FALSE TRUE\rNote that we will be using %in% often throughout the book.\nAdvanced: There is a connection between match and %in% through which. To see this, notice that the following two lines produce the same index (although in different order):\nmatch(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state)\r## [1] 33 10 44\rwhich(murders$state%in%c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;))\r## [1] 10 33 44\r\r\rRmarkdown\rIf you’re new to Rmarkdown, I have made a short video on how to use it . This video is for my EC420 course, but works for us as well.\nEXERCISES\nStart by loading the library and data.\nlibrary(dslabs)\rdata(murders)\rCompute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\n\rNow use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\n\rUse the results from the previous exercise to report the names of the states with murder rates lower than 1.\n\rNow extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator \u0026amp;.\n\rIn a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\n\rUse the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\n\rUse the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\n\rExtend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index.\n\r\r\r\rLecture Video\rVideo from lecture hosted on Mediaspace \n\r\r\rhttps://rstudio.cloud↩︎\n\rhttps://rafalab.github.io/dsbook/installing-r-rstudio.html↩︎\n\rhttp://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/↩︎\n\rI’m especially partial to Puerto Rico.↩︎\n\rThis is, without a doubt, my least favorite aspect of R. I’d even venture to call it stupid. The logic behind this pesky \u0026lt;- is a total mystery to me, but there is logic to avoiding =. But, you do you.↩︎\n\rThis equals sign is the reasons we assign values with \u0026lt;-; then when arguments of a function are assigned values, we don’t end up with multiple equals signs. But… who cares.↩︎\n\rWhether you view this as a feature or a bug is a good indicator whether you’ll enjoy working with R.↩︎\n\r\r\r","date":1641859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641480442,"objectID":"9be773bd8dbb1773f9326846c039d666","permalink":"https://ssc442kirkpatrick.netlify.app/content/00-content/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/content/00-content/","section":"content","summary":"Readings\rGuiding Question\r\rA Brief Introduction to SSC442\rAbout Me\rAbout You\rThis Course\rMore About This Course\rAnd finally…\r\rWhat is “Data Analytics”?\rStarting point for this course\r\rOutline of the Course\rNon-Social Science Approaches to Statistical Learning\rThe Pros and Cons of Correlation\rA Case Study in Prediction\rMore Recent Examples of Prediction\rAn Aside: Nomenclature\rLearning from Data\r\rR basics\rCase study: US homicides by firearm\rThe (very) basics\rObjects\rThe workspace\rFunctions\rOther prebuilt objects\rVariable names\rSaving your workspace\rMotivating scripts\rCommenting your code\r\rData types\rData frames\rExamining an object\rThe accessor: $\rVectors: numerics, characters, and logical\rFactors\rLists\rMatrices\r\rVectors\rCreating vectors\rNames\rSequences\rSubsetting\r\rCoercion\rNot availables (NA)\r\rSorting\rsort\rorder\rmax and which.","tags":null,"title":"Welcome Back to R","type":"docs"},{"authors":null,"categories":null,"content":"\r\rReadings\rGuiding Question\r\rGroup Projects\rTeams\r\rRandomness and Data Analytics\rLearning From Data\rFormalization\rThe Target Function\rWhy Estimate an Unknown Function?\rThe Parable of the Marbles\rOutside the Data\rHoeffding’s Inequality\rAn example of Hoeffding’s Inequality\r\rThe tidyverse\rTidy data\rManipulating data frames\rAdding a column with mutate\rSubsetting with filter\rSelecting columns with select\r\rThe pipe: %\u0026gt;%\rSummarizing data\rsummarize\rpull\rGroup then summarize with group_by\r\rSorting data frames\rNested sorting\rThe top \\(n\\)\r\rTibbles\rTibbles display better\rSubsets of tibbles are tibbles\rTibbles can have complex entries\rTibbles can be grouped\rCreate a tibble using tibble instead of data.frame\r\rThe dot operator\rdo\rThe purrr package\rTidyverse conditionals\rcase_when\rbetween\r\rLecture Video\r\r\r\rReadings\r\rThis page.\rChapter 1 of Introduction to Statistical Learning, available here.\rOptional: The “Tidy Your Data” tutorial on Rstudio Clould Primers\r\rGuiding Question\rFor future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n\rWhy do we want tidy data?\rWhat are the challenges associated with shaping things into a tidy format?\r\rOverview\r\r\r\r\r\r\r--\r\r\rGroup Projects\rYour final is a group project.\nYou need to start planning soon.\nTo aid in your planning, here are the required elements of your project (note: the assignment that currently exists on this site, if you find it, is old and will change a lot between now and next week).\nYou must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.\n\rYou must visualize 3 intersting features of that data.\n\rYou must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.\n\rYou must think critically about your analysis and be able to identify potential issues/\n\rYou must present your analysis as if presenting to a C-suite executive.\n\r\rTeams\rPlease form teams of 2-4 and use the survey link I will send out via email to “establish” your teams.\nIf you opt to not form a team, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with another person.\nMore on your team\r\rYou should strongly consider coordinating your work via Github\n\rYour team will earn the same scores on all projects\n\rTeams will submit only one write-up for the mini-projects and the final\n\r\rTo combat additional freeloading, we will use a reporting system. More details will follow.\n\r\r\rRandomness and Data Analytics\rAnd the fabulous importance of probabilistic inference…\nThis lecture is very “high-level,” which means it is talking about abstract concepts. It is also quite important.\nWe want to discuss why we eventually will need ot utilize tons of difficult mathematics. Why do we care so much about hypothesis tests and the like?\nMoreover, we can highlight why we want our data structured to behave nicely.\nLearning From Data\rThe following are the baisc requirements for statistical learning\nA pattern exists\n\rThis pattern is not easily expressed in a closed mathematical form\n\rYou have data\n\r\r\rFormalization\rWe think of our outcome-of-interest as a reponse or target that we wish to predict or wish to learn something about.\nWe generically refer to the response as \\(Y\\)\nOther aspects of the data are known as features, inputs, predictors, or regressors. We call one of these \\(X_i\\).\n\rThe subscript \\(i\\) indicates that we have an \\(X\\) realized for every individual in our data\r\rWe can refer to the input vector collectively as:\n\\[X = \\begin{bmatrix} x_{11} \u0026amp; x_{12} \\\\\rx_{21} \u0026amp; x_{22} \\\\\r\\vdots \u0026amp; \\vdots \\\\\rx_{N1} \u0026amp; x_{N2}\r\\end{bmatrix}\\]\nWe are seeking some unknown function that maps \\(X\\) to \\(Y\\)\nPut another way, we are seeking to explain \\(Y\\) as follows:\n\\[Y = f(X) + e\\]\n\rThe Target Function\rWe call the function \\(f: \\mathcal{X} \\rightarrow \\mathcal{Y}\\) the target function\nHow do we find the function? We don’t! We get as close as we can, though:\n\rObserve data \\((\\mathbf{x}_1, y_1), \\cdots, (\\mathbf{x}_N, y_N)\\)\n\rUse some algorithm to approximate \\(f\\)\n\rProduce final hypothesis function \\(g \\approx f\\)\n\rEvaluate how well \\(g\\) approximates \\(f\\) and iterate as needed.\n\r\r\rWhy Estimate an Unknown Function?\rWith a good estimate of \\(f\\) we can make predictions of \\(Y\\) at new points \\(X = x\\)\nWe can also understand which components of \\(X = (X_1, X_2, \\cdots, X_m)\\) are important in explaining \\(Y\\), and which are (potentially) irrelevant\n\re.g. GDP and yearsindustrialized have a big impact on emissions but hydroutilization typically does not.\r\rDepending on the complexity of \\(f\\), we may be able to meaningfully understand how each component of \\(X\\) affects \\(Y\\).\n(But we should be careful about assigning causal interpretations, more on this later)\n\rThe Parable of the Marbles\r(Courtesy of Prof. Bushong)\nImagine a bag of marbles with two types of marbles: ♣️ and ♦️.\nWe are going to pick a sample of \\(n\\) marbles (with replacement).\nWe want to learn something about \\(\\mu\\), the objective probability to pick a ♣️.\nIn addition to defining the objective probability of picking a ♣️, we have an observed fraction \\(\\eta\\), which will define as the fraction of ♣️ in the sample.\nQuestion: Can we say anything exact and for-sure about \\(\\mu\\) (outside the data) after observing \\(\\eta\\) (the data)?\n\rNo. It is possible for the sample to be all ♣️, ♣️, ♣️, ♣️, ♣️ even when the bag is is 50/50 ♦️\n\rNo matter what we draw, we can’t (based on that draw alone) eliminate the possibility of drawing a ♦️.\n\rAnd unless we assume that the only two values in the world are ♦️ and ♣️, we can’t rule out 💩1\n\r\r\rQuestion: Then why do we do things like polling (e.g. to predict the outcome of a presidential election)?\n\rThe bad case, that we draw something that has is completely misleading, is possible but not probable.\r\r\rOutside the Data\rPut another way, since \\(f\\) is unknown, it can take on any value outside the data we have, no matter how large the data.\n\rThis is called No Free Lunch\r\rYou cannot know anything for sure about \\(f\\) outside the data without making assumptions.\nIs there any hope to know anything about \\(f\\) outside the data set without making assumptions about \\(f\\)?\nYes, if we are willing to give up the “for sure”\n\rHoeffding’s Inequality\rHoeffding’s Inequality states, loosely, that \\(\\eta\\) cannot be too far from \\(\\mu\\).\n\\[\\mathbb{P}\\left[|\\eta - \\mu| \u0026gt; \\epsilon \\right] \\leq 2e^{-2\\epsilon^2n}\\]\r\\(\\eta \\approx \\mu\\) is called probably approximately correct (PAC) learning.\n\rAn example of Hoeffding’s Inequality\rExample: n = 1,000. Draw a sample and observe \\(\\eta\\)\n\r$$99% of the time, \\(\\mu - .05 \\leq \\eta \\leq \\mu+.05\\)\n\rThis is implied by setting \\(\\epsilon = 0.05\\) and using \\(n=1,000\\)\r\r99.9999996% of the time \\(\\mu - .10 \\leq \\eta \\leq \\mu + .10\\%\\)\n\r\rWhat does this mean?\nIf I repeatedly pick a sample of size 1,000, observe \\(\\eta\\) and claim that \\(\\mu \\in \\left[\\eta - .05, \\eta + .05\\right]\\) (or that the error bar is \\(\\pm 0.05\\)), I will be right 99% of the time.\nOn any particular sample you may be wrong, but not often.\nNOTE\nThis week’s content is split into two “halves”: the critical data manipulation information contained below and a more-entertaining discussion of visualization included in the Exercises.\n\r\r\rThe tidyverse\rIn the first weeks’ content, we demonstrated how to manipulate vectors by reordering and subsetting them through indexing. However, once we start more advanced analyses, the preferred unit for data storage is not the vector but the data frame. In this lecture, we learn to work directly with data frames, which greatly facilitate the organization of information. We will be using data frames for the majority of this class and you will use them for the majority of your data science life (however long that might be). We will focus on a specific data format referred to as tidy and on specific collection of packages that are particularly helpful for working with tidy data referred to as the tidyverse.\nWe can load all the tidyverse packages at once by installing and loading the tidyverse package:2\nlibrary(tidyverse)\rWe will learn how to implement the tidyverse approach throughout the book, but before delving into the details, in this chapter we introduce some of the most widely used tidyverse functionality, starting with the dplyr package for manipulating data frames and the purrr package for working with functions. Note that the tidyverse also includes a graphing package, ggplot2, which we introduce later in Chapter ?? in the Data Visualization part of the book; the readr package discussed in Chapter ??; and many others. In this chapter, we first introduce the concept of tidy data and then demonstrate how we use the tidyverse to work with data frames in this format.\nTidy data\rWe say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\nlibrary(dslabs)\r## Warning: package \u0026#39;dslabs\u0026#39; was built under R version 4.0.5\rdata(murders)\rhead(murders)\r## state abb region population total\r## 1 Alabama AL South 4779736 135\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\r## 4 Arkansas AR South 2915918 93\r## 5 California CA West 37253956 1257\r## 6 Colorado CO West 5029196 65\rEach row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\nlibrary(dslabs)\rdata(\u0026quot;gapminder\u0026quot;)\rtidy_data \u0026lt;- gapminder %\u0026gt;%\rfilter(country %in% c(\u0026quot;South Korea\u0026quot;, \u0026quot;Germany\u0026quot;) \u0026amp; !is.na(fertility)) %\u0026gt;%\rselect(country, year, fertility)\rhead(tidy_data, 6)\r## country year fertility\r## 1 Germany 1960 2.41\r## 2 South Korea 1960 6.16\r## 3 Germany 1961 2.44\r## 4 South Korea 1961 5.99\r## 5 Germany 1962 2.47\r## 6 South Korea 1962 5.79\rThis tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n## country 1960 1961 1962\r## 1 Germany 2.41 2.44 2.47\r## 2 South Korea 6.16 5.99 5.79\rThe same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\nTRY IT\nExamine the built-in dataset co2. Which of the following is true:\r\rco2 is tidy data: it has one year for each row.\rco2 is not tidy: we need at least one column with a character vector.\rco2 is not tidy: it is a matrix instead of a data frame.\rco2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.\r\rExamine the built-in dataset ChickWeight. Which of the following is true:\r\rChickWeight is not tidy: each chick has more than one row.\rChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.\rChickWeight is not tidy: we are missing the year column.\rChickWeight is tidy: it is stored in a data frame.\r\rExamine the built-in dataset BOD. Which of the following is true:\r\rBOD is not tidy: it only has six rows.\rBOD is not tidy: the first column is just an index.\rBOD is tidy: each row is an observation with two values (time and demand)\rBOD is tidy: all small datasets are tidy by definition.\r\rWhich of the following built-in datasets is tidy (you can pick more than one):\r\rBJsales\rEuStockMarkets\rDNase\rFormaldehyde\rOrange\rUCBAdmissions\r\r\r\rManipulating data frames\rThe dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\nAdding a column with mutate\rWe want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\nlibrary(dslabs)\rdata(\u0026quot;murders\u0026quot;)\rmurders \u0026lt;- mutate(murders, rate = total / population * 100000)\rNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\nhead(murders)\r## state abb region population total rate\r## 1 Alabama AL South 4779736 135 2.824424\r## 2 Alaska AK West 710231 19 2.675186\r## 3 Arizona AZ West 6392017 232 3.629527\r## 4 Arkansas AR South 2915918 93 3.189390\r## 5 California CA West 37253956 1257 3.374138\r## 6 Colorado CO West 5029196 65 1.292453\rAlthough we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n\rSubsetting with filter\rNow suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\nfilter(murders, rate \u0026lt;= 0.71)\r## state abb region population total rate\r## 1 Hawaii HI West 1360301 7 0.5145920\r## 2 Iowa IA North Central 3046355 21 0.6893484\r## 3 New Hampshire NH Northeast 1316470 5 0.3798036\r## 4 North Dakota ND North Central 672591 4 0.5947151\r## 5 Vermont VT Northeast 625741 2 0.3196211\r\rSelecting columns with select\rAlthough our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\nnew_table \u0026lt;- select(murders, state, region, rate)\rfilter(new_table, rate \u0026lt;= 0.71)\r## state region rate\r## 1 Hawaii West 0.5145920\r## 2 Iowa North Central 0.6893484\r## 3 New Hampshire Northeast 0.3798036\r## 4 North Dakota North Central 0.5947151\r## 5 Vermont Northeast 0.3196211\rIn the call to select, the first argument murders is an object, but state, region, and rate are variable names.\nTRY IT\nLoad the dplyr package and the murders dataset.\r\rlibrary(dplyr)\rlibrary(dslabs)\rdata(murders)\rYou can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\nmurders \u0026lt;- mutate(murders, population_in_millions = population / 10^6)\rWe can write population rather than murders$population. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders \u0026lt;- [your code]) so we can keep using this variable.\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\n\rWith dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n\r\rselect(murders, state, population) %\u0026gt;% head()\rUse select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:\r\rfilter(murders, state == \u0026quot;New York\u0026quot;)\rYou can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\r\rno_florida \u0026lt;- filter(murders, state != \u0026quot;Florida\u0026quot;)\rCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:\r\rfilter(murders, state %in% c(\u0026quot;New York\u0026quot;, \u0026quot;Texas\u0026quot;))\rCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\r\rfilter(murders, population \u0026lt; 5000000 \u0026amp; region == \u0026quot;Northeast\u0026quot;)\rMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank.\n\r\r\rThe pipe: %\u0026gt;%\rWith dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %\u0026gt;%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data }\r\\rightarrow \\mbox{ select }\r\\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %\u0026gt;%. The code looks like this:\nmurders %\u0026gt;% select(state, region, rate) %\u0026gt;% filter(rate \u0026lt;= 0.71)\r## state region rate\r## 1 Hawaii West 0.5145920\r## 2 Iowa North Central 0.6893484\r## 3 New Hampshire Northeast 0.3798036\r## 4 North Dakota North Central 0.5947151\r## 5 Vermont Northeast 0.3196211\rThis line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n16 %\u0026gt;% sqrt()\r## [1] 4\rWe can continue to pipe values along:\n16 %\u0026gt;% sqrt() %\u0026gt;% log2()\r## [1] 2\rThe above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n16 %\u0026gt;% sqrt() %\u0026gt;% log(base = 2)\r## [1] 2\rTherefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\nmurders %\u0026gt;% select(state, region, rate) %\u0026gt;% filter(rate \u0026lt;= 0.71)\rmurders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe.\nTRY IT\nThe pipe %\u0026gt;% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.\r\rmurders \u0026lt;- mutate(murders, rate = total / population * 100000,\rrank = rank(-rate))\rIn the solution to the previous exercise, we did the following:\nmy_states \u0026lt;- filter(murders, region %in% c(\u0026quot;Northeast\u0026quot;, \u0026quot;West\u0026quot;) \u0026amp;\rrate \u0026lt; 1)\rselect(my_states, state, rate, rank)\rThe pipe %\u0026gt;% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\nmutate(murders, rate = total / population * 100000,\rrank = rank(-rate)) %\u0026gt;%\rselect(state, rate, rank)\rNotice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %\u0026gt;%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %\u0026gt;% to do this in just one line.\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %\u0026gt;%. The code should look something like this:\r\rmy_states \u0026lt;- murders %\u0026gt;%\rmutate SOMETHING %\u0026gt;%\rfilter SOMETHING %\u0026gt;%\rselect SOMETHING\r\r\rSummarizing data\rAn important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\nsummarize\rThe summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\nlibrary(dplyr)\rlibrary(dslabs)\rdata(heights)\rThe following code computes the average and standard deviation for females:\ns \u0026lt;- heights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rsummarize(average = mean(height), standard_deviation = sd(height))\rs\r## average standard_deviation\r## 1 64.93942 3.760656\rThis takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\ns$average\r## [1] 64.93942\rs$standard_deviation\r## [1] 3.760656\rAs with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rsummarize(median = median(height), minimum = min(height),\rmaximum = max(height))\r## median minimum maximum\r## 1 64.98031 51 79\rWe can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\nheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rsummarize(range = quantile(height, c(0, 0.5, 1)))\rwe will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In Section ??, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\nmurders \u0026lt;- murders %\u0026gt;% mutate(rate = total/population*100000)\rRemember that the US murder rate is not the average of the state murder rates:\nsummarize(murders, mean(rate))\r## mean(rate)\r## 1 2.779125\rThis is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\nus_murder_rate \u0026lt;- murders %\u0026gt;%\rsummarize(rate = sum(total) / sum(population) * 100000)\rus_murder_rate\r## rate\r## 1 3.034555\rThis computation counts larger states proportionally to their size which results in a larger value.\n\rpull\rThe us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\nclass(us_murder_rate)\r## [1] \u0026quot;data.frame\u0026quot;\rsince, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\nus_murder_rate %\u0026gt;% pull(rate)\r## [1] 3.034555\rThis returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\nus_murder_rate \u0026lt;- murders %\u0026gt;%\rsummarize(rate = sum(total) / sum(population) * 100000) %\u0026gt;%\rpull(rate)\rus_murder_rate\r## [1] 3.034555\rwhich is now a numeric:\nclass(us_murder_rate)\r## [1] \u0026quot;numeric\u0026quot;\r\rGroup then summarize with group_by\rA common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\nheights %\u0026gt;% group_by(sex)\r## # A tibble: 1,050 x 2\r## # Groups: sex [2]\r## sex height\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Male 75\r## 2 Male 70\r## 3 Male 68\r## 4 Male 74\r## 5 Male 61\r## 6 Female 65\r## 7 Female 66\r## 8 Female 62\r## 9 Female 66\r## 10 Male 67\r## # ... with 1,040 more rows\rThe result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rsummarize(average = mean(height), standard_deviation = sd(height))\r## # A tibble: 2 x 3\r## sex average standard_deviation\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Female 64.9 3.76\r## 2 Male 69.3 3.61\rThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\nmurders %\u0026gt;%\rgroup_by(region) %\u0026gt;%\rsummarize(median_rate = median(rate))\r## # A tibble: 4 x 2\r## region median_rate\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Northeast 1.80\r## 2 South 3.40\r## 3 North Central 1.97\r## 4 West 1.29\r\r\rSorting data frames\rWhen examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\nmurders %\u0026gt;%\rarrange(population) %\u0026gt;%\rhead()\r## state abb region population total rate\r## 1 Wyoming WY West 563626 5 0.8871131\r## 2 District of Columbia DC South 601723 99 16.4527532\r## 3 Vermont VT Northeast 625741 2 0.3196211\r## 4 North Dakota ND North Central 672591 4 0.5947151\r## 5 Alaska AK West 710231 19 2.6751860\r## 6 South Dakota SD North Central 814180 8 0.9825837\rWith arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\nmurders %\u0026gt;%\rarrange(rate) %\u0026gt;%\rhead()\r## state abb region population total rate\r## 1 Vermont VT Northeast 625741 2 0.3196211\r## 2 New Hampshire NH Northeast 1316470 5 0.3798036\r## 3 Hawaii HI West 1360301 7 0.5145920\r## 4 North Dakota ND North Central 672591 4 0.5947151\r## 5 Iowa IA North Central 3046355 21 0.6893484\r## 6 Idaho ID West 1567582 12 0.7655102\rNote that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\nmurders %\u0026gt;%\rarrange(desc(rate))\rNested sorting\rIf we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\nmurders %\u0026gt;%\rarrange(region, rate) %\u0026gt;%\rhead()\r## state abb region population total rate\r## 1 Vermont VT Northeast 625741 2 0.3196211\r## 2 New Hampshire NH Northeast 1316470 5 0.3798036\r## 3 Maine ME Northeast 1328361 11 0.8280881\r## 4 Rhode Island RI Northeast 1052567 16 1.5200933\r## 5 Massachusetts MA Northeast 6547629 118 1.8021791\r## 6 New York NY Northeast 19378102 517 2.6679599\r\rThe top \\(n\\)\rIn the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\nmurders %\u0026gt;% top_n(5, rate)\r## state abb region population total rate\r## 1 District of Columbia DC South 601723 99 16.452753\r## 2 Louisiana LA South 4533372 351 7.742581\r## 3 Maryland MD South 5773552 293 5.074866\r## 4 Missouri MO North Central 5988927 321 5.359892\r## 5 South Carolina SC South 4625364 207 4.475323\rNote that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange.\rNote that if the third argument is left blank, top_n filters by the last column.\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\nlibrary(NHANES)\rdata(NHANES)\rThe NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\nlibrary(dslabs)\rdata(na_example)\rmean(na_example)\r## [1] NA\rsd(na_example)\r## [1] NA\rTo ignore the NAs we can use the na.rm argument:\nmean(na_example, na.rm = TRUE)\r## [1] 2.301754\rsd(na_example, na.rm = TRUE)\r## [1] 1.22338\rLet’s now explore the NHANES data.\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like \" 20-29\", with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.\r\rHint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\n\rNow report the min and max values for the same group.\n\rCompute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\n\rRepeat exercise 4 for males.\n\rWe can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\n\rFor males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure.\n\r\r\r\r\rTibbles\rTidy data must be stored in data frames. We introduced the data frame in Section ?? and have been using the murders data frame throughout the book. In Section ?? we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\nmurders %\u0026gt;% group_by(region)\r## # A tibble: 51 x 6\r## # Groups: region [4]\r## state abb region population total rate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Alabama AL South 4779736 135 2.82\r## 2 Alaska AK West 710231 19 2.68\r## 3 Arizona AZ West 6392017 232 3.63\r## 4 Arkansas AR South 2915918 93 3.19\r## 5 California CA West 37253956 1257 3.37\r## 6 Colorado CO West 5029196 65 1.29\r## 7 Connecticut CT Northeast 3574097 97 2.71\r## 8 Delaware DE South 897934 38 4.23\r## 9 District of Columbia DC South 601723 99 16.5 ## 10 Florida FL South 19687653 669 3.40\r## # ... with 41 more rows\rNotice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followd by dimensions. We can learn the class of the returned object using:\nmurders %\u0026gt;% group_by(region) %\u0026gt;% class()\r## [1] \u0026quot;grouped_df\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\rThe tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble. For example, in Chapter ?? we will see that tidyverse functions used to import data create tibbles.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\nTibbles display better\rThe print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n\rSubsets of tibbles are tibbles\rIf you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\nclass(murders[,4])\r## [1] \u0026quot;numeric\u0026quot;\ris not a data frame. With tibbles this does not happen:\nclass(as_tibble(murders)[,4])\r## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\rThis is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\nclass(as_tibble(murders)$population)\r## [1] \u0026quot;numeric\u0026quot;\rA related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\nmurders$Population\r## NULL\rreturns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\nas_tibble(murders)$Population\r## Warning: Unknown or uninitialised column: `Population`.\r## NULL\r\rTibbles can have complex entries\rWhile data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\ntibble(id = c(1, 2, 3), func = c(mean, median, sd))\r## # A tibble: 3 x 2\r## id func ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt;\r## 1 1 \u0026lt;fn\u0026gt; ## 2 2 \u0026lt;fn\u0026gt; ## 3 3 \u0026lt;fn\u0026gt;\r\rTibbles can be grouped\rThe function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n\rCreate a tibble using tibble instead of data.frame\rIt is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\ngrades \u0026lt;- tibble(names = c(\u0026quot;John\u0026quot;, \u0026quot;Juan\u0026quot;, \u0026quot;Jean\u0026quot;, \u0026quot;Yao\u0026quot;),\rexam_1 = c(95, 80, 90, 85),\rexam_2 = c(90, 85, 85, 90))\rNote that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\ngrades \u0026lt;- data.frame(names = c(\u0026quot;John\u0026quot;, \u0026quot;Juan\u0026quot;, \u0026quot;Jean\u0026quot;, \u0026quot;Yao\u0026quot;),\rexam_1 = c(95, 80, 90, 85),\rexam_2 = c(90, 85, 85, 90))\rclass(grades$names)\r## [1] \u0026quot;character\u0026quot;\rTo avoid this, we use the rather cumbersome argument stringsAsFactors:\ngrades \u0026lt;- data.frame(names = c(\u0026quot;John\u0026quot;, \u0026quot;Juan\u0026quot;, \u0026quot;Jean\u0026quot;, \u0026quot;Yao\u0026quot;),\rexam_1 = c(95, 80, 90, 85),\rexam_2 = c(90, 85, 85, 90),\rstringsAsFactors = FALSE)\rclass(grades$names)\r## [1] \u0026quot;character\u0026quot;\rTo convert a regular data frame to a tibble, you can use the as_tibble function.\nas_tibble(grades) %\u0026gt;% class()\r## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;\r\r\rThe dot operator\rOne of the advantages of using the pipe %\u0026gt;% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\ntab_1 \u0026lt;- filter(murders, region == \u0026quot;South\u0026quot;)\rtab_2 \u0026lt;- mutate(tab_1, rate = total / population * 10^5)\rrates \u0026lt;- tab_2$rate\rmedian(rates)\r## [1] 3.398069\rWe can avoid defining any new intermediate objects by instead typing:\nfilter(murders, region == \u0026quot;South\u0026quot;) %\u0026gt;%\rmutate(rate = total / population * 10^5) %\u0026gt;%\rsummarize(median = median(rate)) %\u0026gt;%\rpull(median)\r## [1] 3.398069\rWe can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\nrates \u0026lt;- filter(murders, region == \u0026quot;South\u0026quot;) %\u0026gt;%\rmutate(rate = total / population * 10^5) %\u0026gt;%\r.$rate\rmedian(rates)\r## [1] 3.398069\rIn the next section, we will see other instances in which using the . is useful.\n\rdo\rThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %\u0026gt;%, tidyverse functions consistently return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described in Section ??. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn Section ??, we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive an error: Error: expecting result of length one, got : 2.\ndata(heights)\rheights %\u0026gt;%\rfilter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;%\rsummarize(range = quantile(height, c(0, 0.5, 1)))\rWe can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame.\nmy_summary \u0026lt;- function(dat){\rx \u0026lt;- quantile(dat$height, c(0, 0.5, 1))\rtibble(min = x[1], median = x[2], max = x[3])\r}\rWe can now apply the function to the heights dataset to obtain the summaries:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rmy_summary\r## # A tibble: 1 x 3\r## min median max\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 50 68.5 82.7\rBut this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rdo(my_summary(.))\r## # A tibble: 2 x 4\r## # Groups: sex [2]\r## sex min median max\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Female 51 65.0 79 ## 2 Male 50 69 82.7\rNote that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rdo(my_summary())\rIf you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\nheights %\u0026gt;%\rgroup_by(sex) %\u0026gt;%\rdo(my_summary)\r\rThe purrr package\rIn Section ?? we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\ncompute_s_n \u0026lt;- function(n){\rx \u0026lt;- 1:n\rsum(x)\r}\rn \u0026lt;- 1:25\rs_n \u0026lt;- sapply(n, compute_s_n)\rs_n\r## [1] 1 3 6 10 15 21 28 36 45 55 66 78 91 105 120 136 153 171 190\r## [20] 210 231 253 276 300 325\rThis type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\nlibrary(purrr) # or library(tidyverse)\rn \u0026lt;- 1:25\rs_n \u0026lt;- map(n, compute_s_n)\rclass(s_n)\r## [1] \u0026quot;list\u0026quot;\rIf we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\ns_n \u0026lt;- map_dbl(n, compute_s_n)\rclass(s_n)\r## [1] \u0026quot;numeric\u0026quot;\rThis produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\ns_n \u0026lt;- map_df(n, compute_s_n)\rWe need to change the function to make this work:\ncompute_s_n \u0026lt;- function(n){\rx \u0026lt;- 1:n\rtibble(sum = sum(x))\r}\rs_n \u0026lt;- map_df(n, compute_s_n)\rhead(s_n)\r## # A tibble: 6 x 1\r## sum\r## \u0026lt;int\u0026gt;\r## 1 1\r## 2 3\r## 3 6\r## 4 10\r## 5 15\r## 6 21\rThe purrr package provides much more functionality not covered here. For more details you can consult this online resource.\n\rTidyverse conditionals\rA typical data analysis will often involve one or more conditional operations. In Section ?? we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\ncase_when\rThe case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\nx \u0026lt;- c(-2, -1, 0, 1, 2)\rcase_when(x \u0026lt; 0 ~ \u0026quot;Negative\u0026quot;,\rx \u0026gt; 0 ~ \u0026quot;Positive\u0026quot;,\rx == 0 ~ \u0026quot;Zero\u0026quot;)\r## [1] \u0026quot;Negative\u0026quot; \u0026quot;Negative\u0026quot; \u0026quot;Zero\u0026quot; \u0026quot;Positive\u0026quot; \u0026quot;Positive\u0026quot;\rA common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\nmurders %\u0026gt;%\rmutate(group = case_when(\rabb %in% c(\u0026quot;ME\u0026quot;, \u0026quot;NH\u0026quot;, \u0026quot;VT\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;RI\u0026quot;, \u0026quot;CT\u0026quot;) ~ \u0026quot;New England\u0026quot;,\rabb %in% c(\u0026quot;WA\u0026quot;, \u0026quot;OR\u0026quot;, \u0026quot;CA\u0026quot;) ~ \u0026quot;West Coast\u0026quot;,\rregion == \u0026quot;South\u0026quot; ~ \u0026quot;South\u0026quot;,\rTRUE ~ \u0026quot;Other\u0026quot;)) %\u0026gt;%\rgroup_by(group) %\u0026gt;%\rsummarize(rate = sum(total) / sum(population) * 10^5)\r## # A tibble: 4 x 2\r## group rate\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 New England 1.72\r## 2 Other 2.71\r## 3 South 3.63\r## 4 West Coast 2.90\r\rbetween\rA common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\nx \u0026gt;= a \u0026amp; x \u0026lt;= b\rHowever, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\nbetween(x, a, b)\rTRY IT\nLoad the murders dataset. Which of the following is true?\r\rmurders is in tidy format and is stored in a tibble.\rmurders is in tidy format and is stored in a data frame.\rmurders is not in tidy format and is stored in a tibble.\rmurders is not in tidy format and is stored in a data frame.\r\rUse as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\n\rUse the group_by function to convert murders into a tibble that is grouped by region.\n\rWrite tidyverse code that is equivalent to this code:\n\r\rexp(mean(log(murders$population)))\rWrite it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %\u0026gt;%.\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number.\r\r\r\r\rLecture Video\rAll videos are in the SSC442 Mediaspace channel available here \n  `{=html}\r--\r\r\r\rI discovered the emo::ji() function at 8:55am. My wife joked that I would find a way to use the poop emoji by 9:00am. It is now 8:59am. She was right.↩︎\n\rIf you have not installed this package already, you must use install.packages(\"tidyverse\") prior to the library() call you see below.↩︎\n\r\r\r","date":1641772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641419217,"objectID":"fbbae951c935dd3d35f82710943d5efd","permalink":"https://ssc442kirkpatrick.netlify.app/content/01-content/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/content/01-content/","section":"content","summary":"Readings\rGuiding Question\r\rGroup Projects\rTeams\r\rRandomness and Data Analytics\rLearning From Data\rFormalization\rThe Target Function\rWhy Estimate an Unknown Function?\rThe Parable of the Marbles\rOutside the Data\rHoeffding’s Inequality\rAn example of Hoeffding’s Inequality\r\rThe tidyverse\rTidy data\rManipulating data frames\rAdding a column with mutate\rSubsetting with filter\rSelecting columns with select\r\rThe pipe: %\u0026gt;%\rSummarizing data\rsummarize\rpull\rGroup then summarize with group_by\r\rSorting data frames\rNested sorting\rThe top \\(n\\)\r\rTibbles\rTibbles display better\rSubsets of tibbles are tibbles\rTibbles can have complex entries\rTibbles can be grouped\rCreate a tibble using tibble instead of data.","tags":null,"title":"Introduction to the tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"\r\rCourse Times and Zoom links\rLecture\rOffice hours\rTeaching Assistant\rSlack\r\rWhat is This Course and Can / Should You Take It?\rWhat This Course is Not\rSuccess in this Course\rCourse materials\rR and RStudio\rOnline help\r\rEvaluations and Grades\rClass Participation\rAcademic honesty\rGrading\r\rResources\rAccommodations\rCOVID-19\rMandated Reporting\rAcknowledgements\rMiscellanea\rUsing Office Hours\rContacting Me\rLetters of Recommendation / References\r\r\r\rCourse Times and Zoom links\rPlease bookmark lecture and office hour Zoom links. As per MSU, we will start online on Tuesday Jan 11th at our regular time (6:00-7:20pm). When we return to in-person classes, we will meet in Berkey 122.\nLecture\rTuesday/Thursday 6:00-7:20pm\nLecture Zoom: https://msu.zoom.us/j/97488486753 (Passcode: SSC442)\n\rOffice hours\rWednesdays 3:00pm - 4:00pm and by appointment\nOffice Hours Zoom (same as lecture): https://msu.zoom.us/j/97488486753 (Passcode: SSC442)\n\rTeaching Assistant\rXueshi Wang - wangxu36@msu.edu\nTA Office Hours: Monday 8:00am - 9:30am (online only; no in-person office hours)\nTA Office Hours Zoom: https://msu.zoom.us/j/3382548927 (Passcode: officehour)\n\rSlack\rWe will use Slack as a forum for asking questions about the course, including course policies and, primarily, help questions with R. Students are encouraged to help answer each others’ questions, and to use the forum as a first-step for seeking help. I, and our TA, will monitor slack and answer questions regularly. You can join our Slack channel with this link\n\r\rWhat is This Course and Can / Should You Take It?\rInnovations in statistical learning have created many engineering breakthroughs. From real time voice recognition to automatic categorization (and in some cases production) of news stories, machine learning is transforming the way we live our lives. These techniques are, at their heart, novel ways to work with data, and therefore they should have implications for social science. This course explores the intersection of statistical learning (or machine learning) and social science and aims to answer two primary questions about these new techniques:\nHow does statistical learning work and what kinds of statistical guarantees can be made about the performance of statistical-learning algorithms?\n\rHow can statistical learning be used to answer questions that interest social science researchers, such as testing theories or improving social policy?\n\r\rIn order to address these questions, we will cover so-called “standard” techniques such as supervised and unsupervised learning, statistical learning theory and nonparametric and Bayesian approaches. If it were up to me, this course would be titled “Statistical Learning for Social Scientists”—I believe this provides a more appropriate guide to the content of this course. And while this class will cover these novel statistical methodologies in some detail, it is not a substitute for the appropriate class in Computer Science or Statistics. Nor is this a class that teaches specific skills for the job market. Rather, this class will teach you to think about data analytics broadly. We will spend a great deal of time learning how to interpret the output of statistical learning algorithms and approaches, and will also spend a great deal of time on better understanding the basic ideas in statistical learning. This, of course, comes at some cost in terms of time spent on learning computational and/or programming skills.\nEnrollment for credit in this course is simply not suitable for those unprepared in or uninterested in elementary statistical theory no matter the intensity of interest in machine learning or “Big Data”. Really.\nYou will be required to understand elementary mathematics in this course and should have at least some exposure to statistical theory. The class is front-loaded technically: early lectures are more mathematically oriented, while later lectures are more applied.\nThe topics covered in this course are listed later in this document. I will assign readings sparingly from Introduction to Statistical Learning, henceforth referred to as ISL. This text is available for free online and, for those who like physical books, can be purchased for about $25. Importantly, the lectures deviate a fair bit from the reading, and thus you will rely on your course notes much more than you might in other classes.\nIf—after you have read this document and preferably after attending the first lecture—you have any questions about whether this course is appropriate for you, please come talk to me.\n\rWhat This Course is Not\rThe focus of this course is conceptual. The goal is to create a working understanding of when and how tools from computer science and statistics can be profitably applied to problems in social science. Though students will be required to apply some of these techniques themselves, this course is not…\n…a replacement for EC420, EC422, or a course in causal inference.\nAs social scientists, we are most often concerned with causal inference in order to analyze and write policies. Statistical learning and the other methods we will discuss in this course are generally not well-suited to these problems, and while I’ll give a short overview of standard methods, this is only to build intuitions. Ultimately, this course has a different focus and you should still pursue standard methodological insights from your home departments.\n…a course on the computational aspects of the underlying methods.\nThere are many important innovations that have made machine learning techniques computationally feasible. We will not discuss these, as there are computer science courses better equipped to cover them. When appropriate, we will discuss whether something is computable, and we will even give rough approximations of the amount of time required (e.g. P vs NP). But we will not discuss how optimizers work or best practices in programming.\n…a primer on the nitty-gritty of how to use these tools or a way to pad your resume.\nThe mechanics of implementation, whether it be programming languages or learning to use APIs, will not be covered in any satisfying level of depth. Students will be expected to learn most of the programming skills on their own. Specifically, while there will be some material to remind you of basic R commands, this is not a good course for people who are simply looking to learn the mechanics of programming. This course is designed to get you to use both traditional analytics and, eventually, machine learning tools. We will do some review of basic programming, and you will have the opportunity to explore topics that interest you through a final project, but ultimately this is a course that largely focuses on the theoretical and practical aspects of statistical learning as applied to social science and not a class on programming.\nPerhaps most importantly, this course is an attempt to push undergraduate education toward the frontiers in social science. Accordingly, please allow some messiness. Some topics may be underdeveloped for a given person’s passions, but given the wide variety of technical skills and overall interests, this is a near certainty. Both the challenge and opportunity of this area comes from the fact that there is no fully developed, wholly unifying framework. Our collective struggle—me from teaching, you from learning—will ultimately bear fruit.\n\rSuccess in this Course\rI promise, you are equipped to succeed in this course.\nLearning R can be difficult at first. Like learning a new language—Spanish, French, or Chinese—it takes dedication and perseverance. Hadley Wickham (the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like) ggplot2—made this wise observation:\n\rIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\rEven experienced programmers (like me) find themselves bashing their heads against seemingly intractable errors.1 If you’re finding yourself bashing your head against a wall and not making progress, try the following. First, take a break. Sometimes you just need space to see an error. Next, talk to classmates. Finally, if you genuinely cannot see the solution, e-mail the TA. But, honestly, it’s probably just a typo.\n\n\rCourse materials\rThe course website can be found at https://ssc442Kirkpatrick.netlify.app (but you know that. You’re on it right now.)\nAll of the readings and software in this class are free. There are free online version of all the texts including Introduction to Statistical Learning (2nd Ed) and R / RStudio are free. (Don’t pay for RStudio.) We will reference outside readings and there exist paper versions of some “books” but you won’t need to buy anything2\nR and RStudio\rYou will do all of your analysis with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R. We recommend this for those who may be switching between computers and are trying to get some work done. That said, while RStudio.cloud is convenient, it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis and graphics. You also can’t use your own custom fonts with RStudio.cloud.3 And, generally speaking, you should have (from the prerequisite course) sufficient experience to make your R work. If not, over the course of the semester, you’ll probably want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud. If you plan on making a career out of data science, you should consider this a necessary step.\nYou can find instructions for installing R, RStudio, and all the tidyverse packages here. And you may find some other goodies.\n\rOnline help\rData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Likewise, whenever using a specific package, try searching for that package name instead of the letter “r” (e.g. “ggplot scatterplot”). Good, concise searches are generally more effective.\nHelp with Using R: There are some excellent additional tutorials on R available through Rstudio Clould Primers.\n\r\rEvaluations and Grades\rYour grade in this course will be based on attendance/participation, labs, weekly writings, and a final project.\nThe general breakdown will be approximately 55% for labs, participation, and weekly writings, and 45% for projects (see below for specific details). The primary focus of the course is a final project; this requires two “mini-projects” to ensure you’re making satisfactory progress. Assignment of numeric grades will follow the standard, where ties (e.g., 91.5%) are rounded to favor the student. Evaluations (read: grades) are designed not to deter anyone from taking this course who might otherwise be interested, but will be taken seriously.\nWeekly writings are intended to be an easy way to get some points. Labs will be short homework assignments that require you to do something practical using a basic statistical language. Support will be provided for the R language only. You must have access to computing resources and the ability to program basic statistical analyses. As mentioned above, this course will not teach you how to program or how to write code in a specific language. If you are unprepared to do implement basic statistical coding, please take (or retake) PLS202. I highly encourage seeking coding advice from those who instruct computer science courses – it’s their job and they are better at it than I am. I’ll try to provide a good service, but I’m really not an expert in computer science.\nMore in-depth descriptions for all the assignments are on the assignments page. As the course progresses, the assignments themselves will be posted within that page.\nTo Recap:\n\r\r\rAssignment\rPoints\rPercent\r\r\r\rClass Participation\r20\r4%\r\rWeekly Writings (14 x 8), drop\rtwo lowest\r96\r19%\r\rLabs (12 x 15), drop one\rlowest\r165\r32%\r\rMini project 1\r50\r10%\r\rMini project 2\r50\r10%\r\rFinal project\r130\r25%\r\rTotal\r511\r-\r\r\r\r\r\r\r\rGrade\rRange\rGrade\rRange\r\r\r\r4.0\r92-100%\r2.0\r72-76%\r\r3.5\r87-91%\r1.5\r67-72%\r\r3.0\r82-87%\r1.0\r62-67%\r\r2.5\r77-81%\r0.0\rbad-66%\r\r\r\r\rClass Participation\rParticipation can take many forms in an online synchronous course. Most preferred is active participation during class using the Zoom chat and hand-raising with camera and mic on. However, I know that myriad reasons may preclude you from this participation. The bare minimum can best be described as “showing your presence and having some engagement” – questions typed in chat and questions posed after class count towards participation. To encourage some form of participation, I will often pose questions to the class. I am not above bribery - your response to these extra credit questions will earn extra credit points, up to 5, for participation. Thus, you can easily pad your score by (1) meeting the minimum participation requirements such that I know you are present, and (2) earning extra credit by responding (through chat or mic, with or without video) to in-class extra credit prompts. I will clearly state which questions are extra credit. Wrong answers get the same credit as right answers. We are here to learn. If you knew everything already, you wouldn’t be in the class.\n\rAcademic honesty\rViolation of MSU’s Spartan Code of Honor will result in a grade of 0.0 in the course. Moreover, I am required by MSU policy to report suspected cases of academic dishonesty for possible disciplinary action.4\n\rGrading\rAll grades are considered final. Any request for a re-grade beyond simple point-tallying mistakes will require that the entire assignment be re-graded. Any points previously awarded may be changed in either direction in the re-grade.\n\r\rResources\rMental health concerns or stressful events may lead to diminished academic performance or reduce a student’s ability to participate in daily activities. Services are available to assist you with addressing these and other concerns you may be experiencing. You can learn more about the broad range of confidential mental health services available on campus via the Counseling \u0026amp; Psychiatric Services (CAPS) website at www.caps.msu.edu.\n\rAccommodations\rIf you need a special accommodation for a disability, religious observance, or have any other concerns about your ability to perform well in this course, please contact me immediately so that we can discuss the issue and make appropriate arrangements. MSU has a specific policy for religious observance available here.\nMichigan State University is committed to providing equal opportunity for participation in all programs, services and activities. Requests for accommodations by persons with disabilities may be made by contacting the Resource Center for Persons with Disabilities at 517-884-RCPD or on the web at rcpd.msu.edu. Once your eligibility for an accommodation has been determined, you will be issued a verified individual services accommodation (“VISA”) form. Please present this form to me at the start of the term and/or two weeks prior to the accommodation date (test, project, etc). Requests received after this date will be honored whenever possible.\n\rCOVID-19\rThings are hard right now. You most likely know people who have lost their jobs, have tested positive for COVID-19, have been hospitalized, or perhaps have even died. You all have increased (or possibly decreased) work responsibilities and increased family care responsibilities. You might be caring for extra people right now, and you are likely facing uncertain job prospects.\nI’m fully committed to making sure that you learn everything you were hoping to learn from this class! I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Under pandemic conditions, that flexibility and leniency is intensified.\nIf you feel like you’re behind or not understanding everything, do not suffer in silence! Please contact me. I’m available at e-mail.\n\rMandated Reporting\rEssays, journals, and other materials submitted for this class are generally considered confidential pursuant to the University’s student record policies. However, students should be aware that University employees, including instructors, may not be able to maintain confidentiality when it conflicts with their responsibility to report certain issues to protect the health and safety of MSU community members and others. As the instructor, I must report the following information to other University offices (including the Department of Police and Public Safety) if you share it with me:\r• Suspected child abuse/neglect, even if this maltreatment happened when you were a child;\r• Allegations of sexual assault, relationship violence, stalking, or sexual harassment; and\r• Credible threats of harm to oneself or to others.\rThese reports may trigger contact from a campus official who will want to talk with you about the incident that you have shared. In almost all cases, it will be your decision whether you wish to speak with that individual. If you would like to talk about these events in a more confidential setting, you are encouraged to make an appointment with the MSU Counseling and Psychiatric Services.\n\rAcknowledgements\rThis syllabus and course structure was developed by Prof. Ben Bushong with iterative refinements by myself. All credit goes to Prof. Bushong. All errors are my own.\n\rMiscellanea\rAll class material will be posted on https://ssc442kirkpatrick.netlify.app. D2L will be used sparingly for submission of weekly writings and assignments and distribution of grades.\nUsing Office Hours\rPlease use my office hours (Passcode: SSC442) Wednesdays 3-4pm and by appointment. It would be remarkable if you didn’t need some assistance with the material, and I am here to help. One of the benefits of open office hours is to accommodate many students at once; if fellow students are “in my office”, please join in and feel very free to show up in groups. (This bit of the syllabus obviously less critical in the COVID Era). Office hours will move around a little bit throughout the semester to attempt to meet the needs of all students.\nIn addition to drop-in office hours, I always have sign-up office hours for advising and other purposes. They are online, linked from my web page. As a general rule, please first seek course-related help from the drop-in office hours. However, if my scheduled office hours are always infeasible for you, let me know, and then I may encourage you to make appointments with me. I ask that you schedule your studying so that you are prepared to ask questions during office hours – office hours are not a lecture and if you’re not prepared with questions we will end up awkwardly staring at each other for an hour until you leave.\nSome gentle requests regarding office hours and on contacting me. First, my office hours end sharply at the end, so don’t arrive 10 minutes before the scheduled end and expect a full session. Please arrive early if you have lengthy questions, or if you don’t want to risk not having time due to others’ questions. You are free to ask me some stuff by e-mail, (e.g. a typo or something on a handout), but please know e-mail sucks for answering many types of questions. “How do I do this lab?” or “What’s up with R?” are short questions with long answers. Come to office hours.\n\rContacting Me\rEmail is a blessing and a curse. Instant communication is wonderful, but often email is the wrong medium to have a productive conversation about course material. Moreover, I get a lot of emails. This means that I am frequently triaging emails into two piles: “my house is burning down” and “everything else”. Your email is unlikely to make the former pile. So… asking questions about course material is always best done in-class or in office hours. Students always roll their eyes when professors say things like that, but it’s true that if you have a question, it’s very likely someone else has the same question.\nThat said, email is still useful. If you’re going to use it, you should at least use if effectively. There’s a running joke in academia that professors only read an email until they find a question. They then respond to that question and ignore the rest of the email. I won’t do this, but I do think it is helpful to assume that the person on the receiving end of an email will operate this way. By keeping this in mind, you will write a much more concise and easy to understand email.\nSome general tips:\n\rAlways include [SSC442] in your subject line (brackets included).\rUse a short but informative subject line. For example: [SSC442] Final Project Grading\rUse your University-supplied email for University business. This helps me know who you are.\rOne topic, one email. If you have multiple things to discuss, and you anticipate followup replies, it is best to split them into two emails so that the threads do not get cluttered.\rAsk direct questions. If you’re asking multiple questions in one email, use a bulleted list.\rDon’t ask questions that are answered by reading the syllabus! This drives me nuts.\rI’ve also found that students are overly polite in emails. I suppose it may be intimidating to email a professor, and you should try to match the style that the professor prefers, but I view email for a course as a casual form of communication. Said another way: get to the point. Students often send an entire paragraph introducing themselves, but if you use your University email address, and add the course name in the subject, I will already know who you are. Here’s an example of a perfectly reasonable email:\r\r\rSubject: [SSC442] Lab, Question 2, Typo\nHi Prof. Kirkpatrick,\nThere seems to be a typo in the Lab on Question 2. The problem says to use a column of data that doesn’t seem to exist. Can you correct this or which should we use?\nThanks,\rStudent McStudentFace\n\r\rLetters of Recommendation / References\rIf you are applying for further study or another pursuit that requires letters of recommendation and you’d like me to recommend you, I will be glad to write a letter on your behalf if your final grade is a 4.0. Grades below a 4.0 may be handled on a case-by-case basis. In addition, you should have held at least three substantial conversations with me about the course material or other academic subjects over the course of the semester.\n\r\r\rBy the end of the course, you will realize that 1) I make many many many errors; 2) that I frequently cannot remember a command or the correct syntax; and 3) that none of this matters too much in the big picture because I know the broad approaches I’m trying to take and I know how to Google stuff. Learn from my idiocy.↩︎\n\rIf you’ve got money to burn, you can buy me a burrito.↩︎\n\rThis bothers me way more than it should.↩︎\n\rSo just don’t cheat or plagiarize. This is an easy problem to avoid.↩︎\n\r\r\r","date":1641427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641480442,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"https://ssc442kirkpatrick.netlify.app/syllabus/","publishdate":"2022-01-06T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Course Times and Zoom links\rLecture\rOffice hours\rTeaching Assistant\rSlack\r\rWhat is This Course and Can / Should You Take It?\rWhat This Course is Not\rSuccess in this Course\rCourse materials\rR and RStudio\rOnline help\r\rEvaluations and Grades\rClass Participation\rAcademic honesty\rGrading\r\rResources\rAccommodations\rCOVID-19\rMandated Reporting\rAcknowledgements\rMiscellanea\rUsing Office Hours\rContacting Me\rLetters of Recommendation / References\r\r\r\rCourse Times and Zoom links\rPlease bookmark lecture and office hour Zoom links.","tags":null,"title":"Syllabus","type":"page"},{"authors":null,"categories":null,"content":"\r\rRequirements\rTeams\rSuggested outline\rIntroduction\rTheory and Background\rData and Analyses\rConclusion\r\r\r\rRequirements\rData analytics is inherently a hands-on endeavor. Accordingly, the final project for this class is hands-on. As per the overview page, the final project has the following elements:\nFor your final project in this class, you will analyze existing data in some area of interest to you.1 Aggregating data from multiple sources is encouraged, but is not required.\r\rYou must visualize (at least) three interesting features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.[^4]\n\rYou must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.\n\rYou will submit three things via D2L:\n\r\r\rA PDF of your report (see the outline below for details of what this needs to contain) rendered from your R Markdown. You might want to write the prose-heavy sections in a word processor like Word or Google Docs and copy/paste the text into your R Markdown document, since RStudio doesn’t have a nice spell checker or grammar checker. This should have no visible R code, warnings, or messages in it. To do this, you must set echo = FALSE in the code chunk options knitr::opts_chunk$set(echo = FALSE, ...) at the beginning of your document template before you knit.\n\rThe same PDF as above, but with all the R code in it (set echo = TRUE at the beginning of your document and reknit the file). Please label files in an obvious way.\n\rA CSV file of your data; or a link to the data online if your code pulls from the internet. This must be a separate file titled “data.csv” or “data.txt” as applicable.\n\r\rThis project is due by 11:59 PM on Tuesday, April 27th, 2021. No late work will be accepted. For real. MSU has grading deadlines and I’ve given you every second that can be spared.\nThere is no final exam. This project is your final exam.\nThe project will not be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:\nTechnical skills: Was the project easy? Does it showcase mastery of data analysis? (20%)\n\rVisual design: Was the information smartly conveyed and usable? Was it beautiful? (25%)\n\rAnalytic design: Was the analysis appropriate? Was it sensible, given the dataset? (20%)\n\rStory: Did we learn something? (25%)\n\rFollowing instructions: Did you surpress R code as asked? Did you submit a separate datafile and label it correctly? (10%)\n\r\rIf you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.\n\rTeams\r\rMy team sucks; how can I punish them for their lack of effort?\n\rOn this front, we will be more supportive. While you have to put up with your team regardless of their quality, you can indicate that your team members are not carrying their fair share by issuing a strike. This processs works as follows:\r1. A team member systematically fails to exert effort on collaborative projects (for example, by not showing up for meetings or not communicating, or by simply leeching off others without contributing.)\r2. Your frustration reaches a boiling point. You decide this has to stop. You decide to issue a strike\r3. You send an email with the following information:\r- Subject line: [SSC442] Strike against [Last name of Recipient]\r- Body: You do not need to provide detailed reasoning. However, you must discuss the actions (plural) you took to remedy the situation before sending the strike email.\nA strike is a serious matter, and will reduce that team member’s grade on joint work by 10%. If any team-member gets strikes from all other members of his or her team, their grade will be reduced by 50%.\nStrikes are anonymous so that you do not need to fear social retaliation. However, they are not anonymous to allow you to issue them without thoughtful consideration. Perhaps the other person has a serious issue that is preventing them from completing work (e.g., a relative passing away). Please be thoughtful in using this remedy and consider it a last resort.\nDo I really need to create a team GitHub repository? I don't like GitHub / programming/ work. --\r\rI’m on a smaller-than-normal team. Does this mean that I have to do more work?\n\rYour instructors are able to count and are aware the teams are imbalanced. Evaluations of final projects will take this into account. While your final product should reflect the best ability of your team, we do not anticipate that the uneven teams will lead to substantively different outputs.\n\rSuggested outline\rYou must write and present your analysis as if presenting to a C-suite executive. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.2\r- Concretely, this requires a written memo, which describes the data, analyses, and results. This must be clear and easy to understand for a non-expert in your field. Figures and tables do not apply to the page limit.\nBelow is a very loose guide to the sort of content that we expect for the final project. Word limits are suggestions only. Note your final report will be approximately\nIntroduction\rDescribe the motivation for this analysis. Briefly describe the dataset, and explain why the analysis you’re undertaking matters for society. (Or matters for some decision-making. You should not feel constrained to asking only “big questions.” The best projects will be narrow-scope but well-defined.) (≈300 words)\n\rTheory and Background\rProvide in-depth background about the data of interest and about your analytics question. (≈300 words)\n“Theory”\rProvide some theoretical guidance to the functional relationship you hope to explore. If you’re interested on how, say, height affects scoring in the NBA, write down a proposed function that might map height to scoring. Describe how you might look for this unknown relationship in the data.(≈300 words)\n\rHypotheses\rMake predictions. Declare what you think will happen. (Note, this may carry over from second project.) (≈250 words)\n\r\rData and Analyses\rData\rGiven your motivations, limits on feasibility, and hypotheses, describe the data you use. (≈100 words)\n\rAnalyses\rGenerate the analyses relevant to your hypotheses and interests. Here you must include three figures and must describe what they contain in simple, easy to digest language. Why did you visualize these elements? Your analyses also must include brief discussion.\n(As many words as you need to fully describe your analysis and results)\n\r\rConclusion\rWhat caveats should we consider? Do you believe this is a truly causal relationship? Why does any of this matter to the decision-maker? (≈75 words)\n\r\r\rNote that existing is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged. But you cannot stand with a clipboard outside a store and count visitors (for instance).↩︎\n\rThis exercise provides you with an opportunity to identify your marketable skills and to practice them. I encourage those who will be looking for jobs soon to take this exercise seriously.↩︎\n\r\r\r","date":1617494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617538026,"objectID":"8d16837a0c729f9c31150a71deaf1f1e","permalink":"https://ssc442kirkpatrick.netlify.app/assignment/final-project/","publishdate":"2021-04-04T00:00:00Z","relpermalink":"/assignment/final-project/","section":"assignment","summary":"Requirements\rTeams\rSuggested outline\rIntroduction\rTheory and Background\rData and Analyses\rConclusion\r\r\r\rRequirements\rData analytics is inherently a hands-on endeavor. Accordingly, the final project for this class is hands-on. As per the overview page, the final project has the following elements:\nFor your final project in this class, you will analyze existing data in some area of interest to you.1 Aggregating data from multiple sources is encouraged, but is not required.","tags":null,"title":"Final project","type":"docs"},{"authors":null,"categories":null,"content":"\r\rPart 1: Instructions\rPart 2: Hypotheses\rEvaluation\r\rData cleaning code\rData to possibly use in your plot\rCountry totals over time\rCumulative country totals over time\rContinent totals over time\rCumulative continent totals over time\rWhat about other data?\r\r\r\rAs with Project 1, each member of the group should submit an identical copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top. Upload the components (see below) to the Project 2 assignment on D2L by 11:59pm on April 2, 2021.\n\rThe United States has resettled more than 600,000 refugees from 60 different countries since 2006.\nIn this project, you will use R, ggplot and some form of graphics editor to explore where these refugees have come from.\nPart 1: Instructions\rHere’s what you need to do:\n\rDownload the Department of Homeland Security’s annual count of people granted refugee status between 2006-2015:\n DHS refugees, 2006-2015\nSave this somewhere on your computer (you might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text). This data was originally uploaded by the Department of Homeland Security to Kaggle, and is provided with a public domain license.\n\rClean the data using the code we’ve given you below. As always, this code is presented without guarantee. You may need to deal with a few issues, depending on your computer’s setup.\n\rSummarize the data somehow. There is data for 60 countries over 10 years, so you’ll probably need to aggregate or reshape the data somehow (unless you do a 60-country sparkline). I’ve included some examples down below.\n\rCreate an appropriate time-based visualization based on the data. I’ve shown a few different ways to summarize the data so that it’s plottable down below. Don’t just calculate overall averages or totals per country—the visualization needs to deal with change over time, and it needs to illustrate something non-obvious and insightful in the data. Do as much polishing and refining in R—make adjustments to the colors, scales, labels, grid lines, and even fonts, etc. You may have more than one visualization, but only one is required. If you have more than one, they must be visually consistent (same appearance, coordinated colors, etc.)\n\rRefine and polish the saved image, adding annotations, changing colors, and otherwise enhancing it.\n\rDesign and create an infographic “poster”. Your poster should look like a polished image that you might see on a newspaper website like the NYT. Your infographic “poster” should include an eye-catching title, your plot, the caption describing the plot, and 2-4 short paragraphs succinctly describing the insights you are sharing about the data. You can (and should consider) integrating other images like national flags or arrows to convey some semantic meaning. *You may do the layout of the infographic “poster” in any software you choose - Publisher (do people still use that?), Adobe Illustrator, etc. Again, the idea is to have a polished plot with an interesting insight from the data, a polished layout to make it attractive, and a polished 2-4 paragraphs that sets up the plot and elaborates on your insight.\n\rUpload the following outputs to D2L:\n\rYour code (.Rmd) that generates the graphic.\rYour final poster, saved as a PDF.\r\r\rFor this assignment, we are less concerned with the code (that’s why we gave most of it to you), and more concerned with the design. Choose good colors based on palettes. Choose good, clean fonts. Use the heck out of theme(). Add informative design elements. Make it look beautiful. Refer to the design resources here.\nPlease seek out help when you need it! You know enough R (and have enough examples of code from class and your readings) to be able to do this. You can do this, and you’ll feel like a budding dataviz witch/wizard when you’re done.\n\rPart 2: Hypotheses\rFor this part of the assignment, you need to provide five hypotheses about the relationship between variables in a dataset. You can (and should) consider making hypotheses about the dataset that you plan to use for your final project. However, this is not a requirement. All that is required is that you provide five hypotheses about some data. Your write-up should have an enumerated list of questions (e.g., “1. Are there more murders in states that have high unemployment.”). You will receive 2 points for each hypothesis.\nEvaluation\rI will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:\nPart 1\nTechnical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (15 points)\n\rProfessionalism of visuals: Does the visualizations look like something you might see on TV or in the newspaper? (15 points)\n\rPoster clarity: Does your poster clearly convey some point? (10 points)\n\r\rPart 2\nEach hypothesis is worth 2 points. (This is intended to be some free points for all; 10 points)\n\r\rData cleaning code\rThe data isn’t perfectly clean and tidy, but it’s real world data, so this is normal. Because the emphasis for this assignment is on design, not code, we’ve provided code to help you clean up the data.\nThese are the main issues with the data:\n\rThere are non-numeric values in the data, like -, X, and D. The data isn’t very well documented; we’re assuming - indicates a missing value, but we’re not sure what X and D mean, so for this assignment, we’ll just assume they’re also missing.\n\rThe data generally includes rows for dozens of countries, but there are also rows for some continents, “unknown,” “other,” and a total row. Because Africa is not a country, and neither are the other continents, we want to exclude all non-countries.\n\rMaintaining consistent country names across different datasets is literally the woooooooorst. Countries have different formal official names and datasets are never consistent in how they use those names.1 It’s such a tricky problem that social scientists have spent their careers just figuring out how to properly name and code countries. Really.2 There are international standards for country codes, though, like ISO 3166-1 alpha 3 (my favorite), also known as ISO3. It’s not perfect—it omits microstates (some Polynesian countries) and gray area states (Palestine, Kosovo)—but it’s an international standard, so it has that going for it.\n\rTo ensure that country names are consistent in this data, we use the countrycode package (install it if you don’t have it), which is amazing. The countrycode() function will take a country name in a given coding scheme and convert it to a different coding scheme using this syntax:\n countrycode(variable, \u0026quot;current-coding-scheme\u0026quot;, \u0026quot;new-coding-scheme\u0026quot;)\rIt also does a farily good job at guessing and parsing inconsistent country names (e.g. it will recognize “Congo, Democratic Republic”, even though it should technically be “Democratic Republic of the Congo”). Here, we use countrycode() to convert the inconsistent country names into ISO3 codes. We then create a cleaner version of the origin_country column by converting the ISO3 codes back into country names. Note that the function chokes on North Korea initially, since it’s included as “Korea, North”—we use the custom_match argument to help the function out.\n\rThe data isn’t tidy—there are individual columns for each year. gather() takes every column and changes it to a row. We exclude the country, region, continent, and ISO3 code from the change-into-rows transformation with -origin_country, -iso3, -origin_region, -origin_continent.\n\rCurrently, the year is being treated as a number, but it’s helpful to also treat it as an actual date. We create a new variable named year_date that converts the raw number (e.g. 2009) into a date. The date needs to have at least a month, day, and year, so we actually convert it to January 1, 2009 with ymd(paste0(year, \"-01-01\")).\n\r\rlibrary(tidyverse) # For ggplot, dplyr, and friends\rlibrary(countrycode) # For dealing with country names, abbreviations, and codes\rlibrary(lubridate) # For dealing with dates\rlibrary(WDI)\rrefugees_raw \u0026lt;- read_csv(\u0026quot;data/refugee_status.csv\u0026quot;, na = c(\u0026quot;-\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;D\u0026quot;))\rnon_countries \u0026lt;- c(\u0026quot;Africa\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Europe\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;Oceania\u0026quot;,\r\u0026quot;South America\u0026quot;, \u0026quot;Unknown\u0026quot;, \u0026quot;Other\u0026quot;, \u0026quot;Total\u0026quot;)\rrefugees_clean \u0026lt;- refugees_raw %\u0026gt;%\r# Make this column name easier to work with\rrename(origin_country = `Continent/Country of Nationality`) %\u0026gt;%\r# Get rid of non-countries\rfilter(!(origin_country %in% non_countries)) %\u0026gt;%\r# Convert country names to ISO3 codes\rmutate(iso3 = countrycode(origin_country, \u0026quot;country.name\u0026quot;, \u0026quot;iso3c\u0026quot;,\rcustom_match = c(\u0026quot;Korea, North\u0026quot; = \u0026quot;PRK\u0026quot;))) %\u0026gt;%\r# Convert ISO3 codes to country names, regions, and continents\rmutate(origin_country = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;country.name\u0026quot;),\rorigin_region = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;region\u0026quot;),\rorigin_continent = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;continent\u0026quot;)) %\u0026gt;%\r# Make this data tidy\rgather(year, number, -origin_country, -iso3, -origin_region, -origin_continent) %\u0026gt;%\r# Make sure the year column is numeric + make an actual date column for years\rmutate(year = as.numeric(year),\ryear_date = ymd(paste0(year, \u0026quot;-01-01\u0026quot;)))\r\rData to possibly use in your plot\rHere are some possible summaries of the data you might use…\nCountry totals over time\rThis is just the refugees_clean data frame I gave you. You’ll want to filter it and select specific countries, though—you won’t really be able to plot 60 countries all at once unless you use sparklines.\n## # A tibble: 6 x 7\r## origin_country iso3 origin_region origin_continent year number year_date ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; ## 1 Afghanistan AFG South Asia Asia 2006 651 2006-01-01\r## 2 Angola AGO Sub-Saharan Afr~ Africa 2006 13 2006-01-01\r## 3 Armenia ARM Europe \u0026amp; Centra~ Asia 2006 87 2006-01-01\r## 4 Azerbaijan AZE Europe \u0026amp; Centra~ Asia 2006 77 2006-01-01\r## 5 Belarus BLR Europe \u0026amp; Centra~ Europe 2006 350 2006-01-01\r## 6 Bhutan BTN South Asia Asia 2006 3 2006-01-01\r\rCumulative country totals over time\rNote the cumsum() function—it calculates the cumulative sum of a column.\nrefugees_countries_cumulative \u0026lt;- refugees_clean %\u0026gt;%\rarrange(year_date) %\u0026gt;%\rgroup_by(origin_country) %\u0026gt;%\rmutate(cumulative_total = cumsum(number))\r## # A tibble: 6 x 7\r## # Groups: origin_country [1]\r## origin_country iso3 origin_continent year number year_date cumulative_total\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan AFG Asia 2006 651 2006-01-01 651\r## 2 Afghanistan AFG Asia 2007 441 2007-01-01 1092\r## 3 Afghanistan AFG Asia 2008 576 2008-01-01 1668\r## 4 Afghanistan AFG Asia 2009 349 2009-01-01 2017\r## 5 Afghanistan AFG Asia 2010 515 2010-01-01 2532\r## 6 Afghanistan AFG Asia 2011 428 2011-01-01 2960\r\rContinent totals over time\rNote the na.rm = TRUE argument in sum(). This makes R ignore any missing data when calculating the total. Without it, if R finds a missing value in the column, it will mark the final sum as NA too, which we don’t want.\nrefugees_continents \u0026lt;- refugees_clean %\u0026gt;%\rgroup_by(origin_continent, year_date) %\u0026gt;%\rsummarize(total = sum(number, na.rm = TRUE))\r## `summarise()` has grouped output by \u0026#39;origin_continent\u0026#39;. You can override using the `.groups` argument.\r## # A tibble: 6 x 3\r## # Groups: origin_continent [1]\r## origin_continent year_date total\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 2006-01-01 18116\r## 2 Africa 2007-01-01 17473\r## 3 Africa 2008-01-01 8931\r## 4 Africa 2009-01-01 9664\r## 5 Africa 2010-01-01 13303\r## 6 Africa 2011-01-01 7677\r\rCumulative continent totals over time\rNote that there are two group_by() functions here. First we get the total number of refugees per continent per year, then we group by continent only to get the cumulative sum of refugees across continents.\nrefugees_continents_cumulative \u0026lt;- refugees_clean %\u0026gt;%\rgroup_by(origin_continent, year_date) %\u0026gt;%\rsummarize(total = sum(number, na.rm = TRUE)) %\u0026gt;%\rarrange(year_date) %\u0026gt;%\rgroup_by(origin_continent) %\u0026gt;%\rmutate(cumulative_total = cumsum(total))\r## `summarise()` has grouped output by \u0026#39;origin_continent\u0026#39;. You can override using the `.groups` argument.\r## # A tibble: 6 x 4\r## # Groups: origin_continent [1]\r## origin_continent year_date total cumulative_total\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 2006-01-01 18116 18116\r## 2 Africa 2007-01-01 17473 35589\r## 3 Africa 2008-01-01 8931 44520\r## 4 Africa 2009-01-01 9664 54184\r## 5 Africa 2010-01-01 13303 67487\r## 6 Africa 2011-01-01 7677 75164\r\rWhat about other data?\rIn your prerequisite course, you learned how to use the merge function (and possibly its Tidyverse cousin, left_join). Since our refugee data has a standardized country code, we can find other datasets that might have useful information.\nThe WDI package acts as an interface to the World Bank Development Inidcators dataset, which has a lot of information on the countries in our data. To merge WDI data to our refugee data, we’ll need to make a little change to our cleaning code - WDI needs the iso2 country code, which is a unique 2-letter code instead of the iso3 3-letter code we have. Here’s the cleaning data:\nrefugees_clean \u0026lt;- refugees_raw %\u0026gt;%\rrename(origin_country = `Continent/Country of Nationality`) %\u0026gt;%\rdplyr::filter(!(origin_country %in% non_countries)) %\u0026gt;%\rmutate(iso2 = countrycode(origin_country, \u0026quot;country.name\u0026quot;, \u0026quot;iso2c\u0026quot;,\rcustom_match = c(\u0026quot;Korea, North\u0026quot; = \u0026quot;KP\u0026quot;))) %\u0026gt;%\rmutate(origin_country = countrycode(iso2, \u0026quot;iso2c\u0026quot;, \u0026quot;country.name\u0026quot;),\rorigin_region = countrycode(iso2, \u0026quot;iso2c\u0026quot;, \u0026quot;region\u0026quot;),\rorigin_continent = countrycode(iso2, \u0026quot;iso2c\u0026quot;, \u0026quot;continent\u0026quot;)) %\u0026gt;%\rgather(year, number, -origin_country, -iso2, -origin_region, -origin_continent) %\u0026gt;%\rmutate(year = as.numeric(year),\ryear_date = ymd(paste0(year, \u0026quot;-01-01\u0026quot;)),\riso2c = iso2)\rThe only difference here is that we now have iso2 instead of iso3. If you use your iso3 field in your code, you’ll have to create both.\nNow, the WDI function:\nlibrary(WDI)\rmyData = WDI(country = refugees_clean$iso2, indicator = \u0026#39;SP.POP.TOTL\u0026#39;, start = 2006, end = 2015) %\u0026gt;%\rdplyr::select(-country)\rThe function needs four arguments (see ?WDI after installing the WDI package for more). The first has to be list of iso2 codes, which we have…in the form of the iso2 column. So we pass that column as the first argument, and WDI will give us data for every country in that column.\nThe second argument, indicator, tells WDI what data you want. The WDI data dictionary is here. Use the drop-down under “Select Database” and choose just the World Development Indicators option to simplify. Then, use the search box to search for intersting indicators. The website shows the “code” of each of the indicators. That’s the code you use. For instance, SP.POP.TOTL is the total population by country, so you can generate per-capita measures of refugees if you want. Finally, WDI needs the start and end years. Our data is 2006 to 2015, so that makes the most sense.\nNow, once you have myData retrieved from WDI, take a look at it. We need to see which columns to merge on - that is, which columns should R match to put the data together? We want to merge on 'iso2c' and on 'year' because WDI gives us a tidy data frame by country-year.\nrefugees_clean_merged = left_join(refugees_clean, myData, by = c(\u0026#39;iso2c\u0026#39;,\u0026#39;year\u0026#39;)) \rWe didn’t have an iso2c column before, just an iso2 column, so you might notice that the new cleaning code I gave you added iso2c = iso2 at the end using mutate. That way, the column names that we’re using to merge will match.\nYou can search the WDI data dictionary and find the “Code” for the indicator you’d like to use, then just merge it into your data. Look at your data closely before you merge, and make sure you know what you’re merging in. Use the slack if you get stuck. Happy data hunting!\nhead(refugees_clean_merged %\u0026gt;% dplyr::select(origin_country, number, iso2, year, SP.POP.TOTL))\r## # A tibble: 6 x 5\r## origin_country number iso2 year SP.POP.TOTL\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan 651 AF 2006 26433049\r## 2 Angola 13 AO 2006 20149901\r## 3 Armenia 87 AM 2006 2958307\r## 4 Azerbaijan 77 AZ 2006 8484550\r## 5 Belarus 350 BY 2006 9604924\r## 6 Bhutan 3 BT 2006 657410\r\r\r\rFor instance, “North Korea”, “Korea, North”, “DPRK”, “Korea, Democratic People’s Republic of”, and “Democratic People’s Republic of Korea”, and “Korea (DPRK)” are all perfectly normal versions of the country’s name and you’ll find them all in the wild.↩︎\n\rSee Gleditsch, Kristian S. \u0026amp; Michael D. Ward. 1999. “Interstate System Membership: A Revised List of the Independent States since 1816.” International Interactions 25: 393-413; or the “ICOW Historical State Names Data Set”.↩︎\n\r\r\r","date":1615248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616800857,"objectID":"5b66ec5f9db457ac4a522cdf36fd69ce","permalink":"https://ssc442kirkpatrick.netlify.app/assignment/project2/","publishdate":"2021-03-09T00:00:00Z","relpermalink":"/assignment/project2/","section":"assignment","summary":"Part 1: Instructions\rPart 2: Hypotheses\rEvaluation\r\rData cleaning code\rData to possibly use in your plot\rCountry totals over time\rCumulative country totals over time\rContinent totals over time\rCumulative continent totals over time\rWhat about other data?\r\r\r\rAs with Project 1, each member of the group should submit an identical copy of the project to D2L (for ease of evaluation and to ensure communication across the group).","tags":null,"title":"Project 2","type":"docs"},{"authors":null,"categories":null,"content":"\r\rRequired Reading\rGuiding Questions\rSlides\r\rIntroduction to Linear Regression\rCase study: is height hereditary?\rThe correlation coefficient\rSample correlation is a random variable\rCorrelation is not always a useful summary\r\rConditional expectations\rThe regression line\rRegression improves precision\rBivariate normal distribution (advanced)\rVariance explained\rWarning: there are two regression lines\r\r\r\r\rRequired Reading\r\rThis page.\r\rGuiding Questions\r\rComing soon.\r\r\rSlides\rIntroduction\r\r\r\r\r\r\rToday’s lecture will ask you to touch real data during the lecture. Please download the following dataset and load it into R.\n\r Ames.csv\r\rThis dataset is from houses in Ames, Iowa. (Thrilling!) We will use this dataset during the lecture to illustrate some of the points discussed below.\n\r\rIntroduction to Linear Regression\rUp to this point, this class has focused mainly on single variables. However, in data analytics applications, it is very common to be interested in the relationship between two or more variables. For instance, in the coming days we will use a data-driven approach that examines the relationship between player statistics and success to guide the building of a baseball team with a limited budget. Before delving into this more complex example, we introduce necessary concepts needed to understand regression using a simpler illustration. We actually use the dataset from which regression was born.\nThe example is from genetics. Francis Galton1 studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity. While doing this, he developed the concepts of correlation and regression, as well as a connection to pairs of data that follow a normal distribution. Of course, at the time this data was collected our knowledge of genetics was quite limited compared to what we know today. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height? The technique he developed to answer this question, regression, can also be applied to our baseball question. Regression can be applied in many other circumstances as well.\nHistorical note: Galton made important contributions to statistics and genetics, but he was also one of the first proponents of eugenics, a scientifically flawed philosophical movement favored by many biologists of Galton’s time but with horrific historical consequences. These consequences still reverberate to this day, and form the basis for much of the Western world’s racist policies. You can read more about it here: https://pged.org/history-eugenics-and-genetics/.\nCase study: is height hereditary?\rWe have access to Galton’s family height data through the HistData package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:\nlibrary(tidyverse)\rlibrary(HistData)\rdata(\u0026quot;GaltonFamilies\u0026quot;)\rset.seed(1983)\rgalton_heights \u0026lt;- GaltonFamilies %\u0026gt;%\rfilter(gender == \u0026quot;male\u0026quot;) %\u0026gt;%\rgroup_by(family) %\u0026gt;%\rsample_n(1) %\u0026gt;%\rungroup() %\u0026gt;%\rselect(father, childHeight) %\u0026gt;%\rrename(son = childHeight)\rIn the exercises, we will look at other relationships including mothers and daughters.\nSuppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:\ngalton_heights %\u0026gt;%\rsummarize(mean(father), sd(father), mean(son), sd(son))\r## # A tibble: 1 x 4\r## `mean(father)` `sd(father)` `mean(son)` `sd(son)`\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 69.1 2.55 69.2 2.71\rHowever, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.\ngalton_heights %\u0026gt;% ggplot(aes(father, son)) +\rgeom_point(alpha = 0.5)\rWe will learn that the correlation coefficient is an informative summary of how two variables move together and then see how this can be used to predict one variable using the other.\n\rThe correlation coefficient\rThe correlation coefficient is defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as the average of the product of the standardized values:\n\\[\r\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\r\\]\rwith \\(\\mu_x, \\mu_y\\) the averages of \\(x_1,\\dots, x_n\\) and \\(y_1, \\dots, y_n\\), respectively, and \\(\\sigma_x, \\sigma_y\\) the standard deviations. The Greek letter \\(\\rho\\) is commonly used in statistics books to denote the correlation. The Greek letter for \\(r\\), \\(\\rho\\), because it is the first letter of regression. Soon we learn about the connection between correlation and regression. We can represent the formula above with R code using:\nrho \u0026lt;- mean(scale(x) * scale(y))\rTo understand why this equation does in fact summarize how two variables move together, consider the \\(i\\)-th entry of \\(x\\) is \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\) SDs away from the average. Similarly, the \\(y_i\\) that is paired with \\(x_i\\), is \\(\\left( \\frac{y_1-\\mu_y}{\\sigma_y} \\right)\\) SDs away from the average \\(y\\). If \\(x\\) and \\(y\\) are unrelated, the product \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\\) will be positive ( \\(+ \\times +\\) and \\(- \\times -\\) ) as often as negative (\\(+ \\times -\\) and \\(- \\times +\\)) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products ( \\(+ \\times +\\) and \\(- \\times -\\)) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation.\nThe correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:\n\\[\r\\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)^2 =\r\\frac{1}{\\sigma_x^2} \\frac{1}{n} \\sum_{i=1}^n \\left( x_i-\\mu_x \\right)^2 =\r\\frac{1}{\\sigma_x^2} \\sigma^2_x =\r1\r\\]\nA similar derivation, but with \\(x\\) and its exact opposite, proves the correlation has to be bigger or equal to -1.\nFor other pairs, the correlation is in between -1 and 1. The correlation between father and son’s heights is about 0.5:\ngalton_heights %\u0026gt;% summarize(r = cor(father, son)) %\u0026gt;% pull(r)\r## [1] 0.4334102\rTo see what data looks like for different values of \\(\\rho\\), here are six examples of pairs with correlations ranging from -0.9 to 0.99:\nSample correlation is a random variable\rBefore we continue connecting correlation to regression, let’s remind ourselves about random variability.\nIn most data science applications, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest but rather for a random sample. As with the average and standard deviation, the sample correlation is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.\nBy way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:\nR \u0026lt;- sample_n(galton_heights, 25, replace = TRUE) %\u0026gt;%\rsummarize(r = cor(father, son)) %\u0026gt;% pull(r)\rR is a random variable. We can run a Monte Carlo simulation to see its distribution:\nB \u0026lt;- 1000\rN \u0026lt;- 25\rR \u0026lt;- replicate(B, {\rsample_n(galton_heights, N, replace = TRUE) %\u0026gt;%\rsummarize(r=cor(father, son)) %\u0026gt;%\rpull(r)\r})\rqplot(R, geom = \u0026quot;histogram\u0026quot;, binwidth = 0.05, color = I(\u0026quot;black\u0026quot;))\rWe see that the expected value of R is the population correlation:\nmean(R)\r## [1] 0.4307393\rand that it has a relatively high standard error relative to the range of values R can take:\nsd(R)\r## [1] 0.1609393\rSo, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nAlso, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough \\(N\\), the distribution of R is approximately normal with expected value \\(\\rho\\). The standard deviation, which is somewhat complex to derive, is \\(\\sqrt{\\frac{1-r^2}{N-2}}\\).\nIn our example, \\(N=25\\) does not seem to be large enough to make the approximation a good one:\nggplot(aes(sample=R), data = data.frame(R)) +\rstat_qq() +\rgeom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))\rIf you increase \\(N\\), you will see the distribution converging to normal.\n\rCorrelation is not always a useful summary\rCorrelation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:\n## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rCorrelation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction.\n\r\rConditional expectations\rSuppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?\nIt turns out that if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.\nIn general, we call this approach conditioning. The general idea is that we stratify a population into groups and compute summaries in each group. To provide a mathematical description of conditioning, consider we have a population of pairs of values \\((x_1,y_1),\\dots,(x_n,y_n)\\), for example all father and son heights in England. In the previous week’s content, we learned that if you take a random pair \\((X,Y)\\), the expected value and best predictor of \\(Y\\) is \\(\\mbox{E}(Y) = \\mu_y\\), the population average \\(1/n\\sum_{i=1}^n y_i\\). However, we are no longer interested in the general population, instead we are interested in only the subset of a population with a specific \\(x_i\\) value, 72 inches in our example. This subset of the population, is also a population and thus the same principles and properties we have learned apply. The \\(y_i\\) in the subpopulation have a distribution, referred to as the conditional distribution, and this distribution has an expected value referred to as the conditional expectation. In our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches. The statistical notation for the conditional expectation is\n\\[\r\\mbox{E}(Y \\mid X = x)\r\\]\nwith \\(x\\) representing the fixed value that defines that subset, for example 72 inches. Similarly, we denote the standard deviation of the strata with\n\\[\r\\mbox{SD}(Y \\mid X = x) = \\sqrt{\\mbox{Var}(Y \\mid X = x)}\r\\]\nBecause the conditional expectation \\(E(Y\\mid X=x)\\) is the best predictor for the random variable \\(Y\\) for an individual in the strata defined by \\(X=x\\), many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.\nIn the example we have been considering, we are interested in computing the average son height conditioned on the father being 72 inches tall. We want to estimate \\(E(Y|X=72)\\) using the sample collected by Galton. We previously learned that the sample average is the preferred approach to estimating the population average. However, a challenge when using this approach to estimating conditional expectations is that for continuous data we don’t have many data points matching exactly one value in our sample. For example, we have only:\nsum(galton_heights$father == 72)\r## [1] 8\rfathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:\nsum(galton_heights$father == 72.5)\r## [1] 1\rA practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of \\(x\\). In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:\nconditional_avg \u0026lt;- galton_heights %\u0026gt;%\rfilter(round(father) == 72) %\u0026gt;%\rsummarize(avg = mean(son)) %\u0026gt;%\rpull(avg)\rconditional_avg\r## [1] 70.5\rNote that a 72-inch father is taller than average – specifically, 72 - 69.1/2.5 =\r1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son. The sons of 72-inch fathers have regressed some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later lecture, this is not a coincidence.\nIf we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:\ngalton_heights %\u0026gt;% mutate(father_strata = factor(round(father))) %\u0026gt;%\rggplot(aes(father_strata, son)) +\rgeom_boxplot() +\rgeom_point()\rNot surprisingly, the centers of the groups are increasing with height.\rFurthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:\n## `summarise()` ungrouping output (override with `.groups` argument)\rThe fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the regression line, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line.\n\rThe regression line\rIf we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(\\mu_X\\), \\(Y\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\). The formula for the regression is therefore:\n\\[\r\\left( \\frac{Y-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right)\r\\]\nWe can rewrite it like this:\n\\[\rY = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y\r\\]\nIf there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use \\(x\\) at all for the prediction and simply predict the average \\(\\mu_Y\\). For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.\nNote that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, \\(x\\), is to the average of the \\(x\\)s. This is why we call it regression: the son regresses to the average height. In fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature. To add regression lines to plots, we will need the above formula in the form:\n\\[\ry= b + mx \\mbox{ with slope } m = \\rho \\frac{\\sigma_y}{\\sigma_x} \\mbox{ and intercept } b=\\mu_y - m \\mu_x\r\\]\nHere we add the regression line to the original data:\nmu_x \u0026lt;- mean(galton_heights$father)\rmu_y \u0026lt;- mean(galton_heights$son)\rs_x \u0026lt;- sd(galton_heights$father)\rs_y \u0026lt;- sd(galton_heights$son)\rr \u0026lt;- cor(galton_heights$father, galton_heights$son)\rgalton_heights %\u0026gt;%\rggplot(aes(father, son)) +\rgeom_point(alpha = 0.5) +\rgeom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x)\rThe regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). You can make same plot, but using standard units like this:\ngalton_heights %\u0026gt;%\rggplot(aes(scale(father), scale(son))) +\rgeom_point(alpha = 0.5) +\rgeom_abline(intercept = 0, slope = r)\rRegression improves precision\rLet’s compare the two approaches to prediction that we have presented:\nRound fathers’ heights to closest inch, stratify, and then take the average.\rCompute the regression line and use it to predict.\r\rWe use a Monte Carlo simulation sampling \\(N=50\\) families:\nB \u0026lt;- 1000\rN \u0026lt;- 50\rset.seed(1983)\rconditional_avg \u0026lt;- replicate(B, {\rdat \u0026lt;- sample_n(galton_heights, N)\rdat %\u0026gt;% filter(round(father) == 72) %\u0026gt;%\rsummarize(avg = mean(son)) %\u0026gt;%\rpull(avg)\r})\rregression_prediction \u0026lt;- replicate(B, {\rdat \u0026lt;- sample_n(galton_heights, N)\rmu_x \u0026lt;- mean(dat$father)\rmu_y \u0026lt;- mean(dat$son)\rs_x \u0026lt;- sd(dat$father)\rs_y \u0026lt;- sd(dat$son)\rr \u0026lt;- cor(dat$father, dat$son)\rmu_y + r*(72 - mu_x)/s_x*s_y\r})\rAlthough the expected value of these two random variables is about the same:\nmean(conditional_avg, na.rm = TRUE)\r## [1] 70.49368\rmean(regression_prediction)\r## [1] 70.50941\rThe standard error for the regression prediction is substantially smaller:\nsd(conditional_avg, na.rm = TRUE)\r## [1] 0.9635814\rsd(regression_prediction)\r## [1] 0.4520833\rThe regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use na.rm=TRUE. The regression always uses all the data.\nSo why not always use the regression for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The justification, which we include in the next section, is somewhat more advanced than the rest of this reading.\n\rBivariate normal distribution (advanced)\rCorrelation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.\nThe main way we motivate the use of correlation involves what is called the bivariate normal distribution.\nWhen a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. These ovals can be thin (high correlation) or circle-shaped (no correlation).\n--\rA more technical way to define the bivariate normal distribution is the following: if \\(X\\) is a normally distributed random variable, \\(Y\\) is also a normally distributed random variable, and the conditional distribution of \\(Y\\) for any \\(X=x\\) is approximately normal, then the pair is approximately bivariate normal.\nIf we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:\ngalton_heights %\u0026gt;%\rmutate(z_father = round((father - mean(father)) / sd(father))) %\u0026gt;%\rfilter(z_father %in% -2:2) %\u0026gt;%\rggplot() +\rstat_qq(aes(sample = son)) +\rfacet_wrap( ~ z_father)\rNow we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is:\n\\[\r\\mbox{E}(Y | X=x) = \\mu_Y + \\rho \\frac{X-\\mu_X}{\\sigma_X}\\sigma_Y\r\\]\nThis is the regression line, with slope \\[\\rho \\frac{\\sigma_Y}{\\sigma_X}\\] and intercept \\(\\mu_y - m\\mu_X\\). It is equivalent to the regression equation we showed earlier which can be written like this:\n\\[\r\\frac{\\mbox{E}(Y \\mid X=x) - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X}\r\\]\nThis implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.\nIn summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line.\n\rVariance explained\rThe bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is:\n\\[\r\\mbox{SD}(Y \\mid X=x ) = \\sigma_Y \\sqrt{1-\\rho^2}\r\\]\nTo see why this is intuitive, notice that without conditioning, \\(\\mbox{SD}(Y) = \\sigma_Y\\), we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.\nSpecifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.\nThe statement “\\(X\\) explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance.\nBut it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.\n\rWarning: there are two regression lines\rWe computed a regression line to predict the son’s height from father’s height. We used these calculations:\nmu_x \u0026lt;- mean(galton_heights$father)\rmu_y \u0026lt;- mean(galton_heights$son)\rs_x \u0026lt;- sd(galton_heights$father)\rs_y \u0026lt;- sd(galton_heights$son)\rr \u0026lt;- cor(galton_heights$father, galton_heights$son)\rm_1 \u0026lt;- r * s_y / s_x\rb_1 \u0026lt;- mu_y - m_1*mu_x\rwhich gives us the function \\(\\mbox{E}(Y\\mid X=x) =\\) 37.3 + 0.46 \\(x\\).\nWhat if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function:\r\\(x = \\{ \\mbox{E}(Y\\mid X=x) -\\) 37.3 \\(\\} /\\) 0.5.\nWe need to compute \\(\\mbox{E}(X \\mid Y=y)\\). Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:\nm_2 \u0026lt;- r * s_x / s_y\rb_2 \u0026lt;- mu_x - m_2 * mu_y\rSo we get \\(\\mbox{E}(X \\mid Y=y) =\\) 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average.\nHere is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:\ngalton_heights %\u0026gt;%\rggplot(aes(father, son)) +\rgeom_point(alpha = 0.5) +\rgeom_abline(intercept = b_1, slope = m_1, col = \u0026quot;blue\u0026quot;) +\rgeom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = \u0026quot;red\u0026quot;)\rTRY IT\nLoad the GaltonFamilies data from the HistData. The children in each family are listed by gender and then by height. Create a dataset called galton_heights by picking a male and female at random.\n\rMake a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.\n\rCompute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.\n\r\r\r\r\r\r\rhttps://en.wikipedia.org/wiki/Francis_Galton↩︎\n\r\r\r","date":1614038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613523213,"objectID":"744a9e81e6eae570052294b144f8d401","permalink":"https://ssc442kirkpatrick.netlify.app/content/05-content/","publishdate":"2021-02-23T00:00:00Z","relpermalink":"/content/05-content/","section":"content","summary":"Required Reading\rGuiding Questions\rSlides\r\rIntroduction to Linear Regression\rCase study: is height hereditary?\rThe correlation coefficient\rSample correlation is a random variable\rCorrelation is not always a useful summary\r\rConditional expectations\rThe regression line\rRegression improves precision\rBivariate normal distribution (advanced)\rVariance explained\rWarning: there are two regression lines\r\r\r\r\rRequired Reading\r\rThis page.\r\rGuiding Questions\r\rComing soon.\r\r\rSlides\rIntroduction\r\r\r\r\r\r\rToday’s lecture will ask you to touch real data during the lecture.","tags":null,"title":"Introduction to Regression","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rReadings\rGuiding Questions\rData visualization principles\rEncoding data using visual cues\rAvoid pseudo-three-dimensional plots\rAvoid too many significant digits\rKnow your audience\rKnow when to include 0\rDo not distort quantities\rOrder categories by a meaningful value\rShow the data\rFaceting\rUse common axes with facets\rAlign plots vertically to see horizontal changes and horizontally to see vertical changes\rFacet grids\rVisual cues to be compared should be adjacent, continued\rUse color\r\rThink of the color blind\rUsing a discrete color palette\rUsing a continuous color palette\r\rgridExtra and grid.arrange\r\r\r\rReadings\r\rThis page.\r\r\rGuiding Questions\r\rWhy do we create visualizations? What types of data are best suited for visuals?\rHow do we best visualize the variability in our data?\rWhat makes a visual compelling?\rWhat are the worst visuals? Which of these are most frequently used? Why?\r\rAs with last week’s content, the technical aspects of this lecture will be explored in greater detail in the Thursday practical lecture. Today, we will focus on some principles.\n\r“The greatest value of a picture is when it forces us to notice what we never expected to see.” – John Tukey\n\r\rData visualization principles\rWe have already provided some rules to follow as we created plots for our examples. Here, we aim to provide some general principles we can use as a guide for effective data visualization. Much of this section is based on a talk by Karl Broman1 titled “Creating Effective Figures and Tables”2 and includes some of the figures which were made with code that Karl makes available on his GitHub repository3, as well as class notes from Peter Aldhous’ Introduction to Data Visualization course4. Following Karl’s approach, we show some examples of plot styles we should avoid, explain how to improve them, and use these as motivation for a list of principles. We compare and contrast plots that follow these principles to those that don’t.\nThe principles are mostly based on research related to how humans detect patterns and make visual comparisons. The preferred approaches are those that best fit the way our brains process visual information. When deciding on a visualization approach, it is also important to keep our goal in mind. We may be comparing a viewable number of quantities, describing distributions for categories or numeric values, comparing the data from two groups, or describing the relationship between two variables. As a final note, we want to emphasize that for a data scientist it is important to adapt and optimize graphs to the audience. For example, an exploratory plot made for ourselves will be different than a chart intended to communicate a finding to a general audience.\nAs with the discussion above, we will be using these libraries—note the addition of gridExtra:\nlibrary(tidyverse)\rlibrary(dslabs)\rlibrary(gridExtra)\rEncoding data using visual cues\rWe start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue.\nTo illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:\n## Warning: package \u0026#39;ggthemes\u0026#39; was built under R version 4.0.4\rHere we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. The donut chart is an example of a plot that uses only area:\nTo see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In fact, the pie R function help file states that:\n\rPie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.\n\rIn this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy:\n\r\rBrowser\r\r2000\r\r2015\r\r\r\r\r\rOpera\r\r3\r\r2\r\r\r\rSafari\r\r21\r\r22\r\r\r\rFirefox\r\r23\r\r21\r\r\r\rChrome\r\r26\r\r29\r\r\r\rIE\r\r28\r\r27\r\r\r\r\rThe preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.\nNotice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.\nIf for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:\nIn general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once.\n\rAvoid pseudo-three-dimensional plots\rThe figure below, taken from the scientific literature5,\rshows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.\n(Image courtesy of Karl Broman)\nHumans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:\n##First read data\rurl \u0026lt;- \u0026quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig8dat.csv\u0026quot;\rdat \u0026lt;- read.csv(url)\r##Now make alternative plot\rdat %\u0026gt;% gather(drug, survival, -log.dose) %\u0026gt;%\rmutate(drug = gsub(\u0026quot;Drug.\u0026quot;,\u0026quot;\u0026quot;,drug)) %\u0026gt;%\rggplot(aes(log.dose, survival, color = drug)) +\rgeom_line()\rNotice how much easier it is to determine the survival values.\nPseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:\n(Images courtesy of Karl Broman)\n\rAvoid too many significant digits\rBy default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:\n\r\rstate\r\ryear\r\rMeasles\r\rPertussis\r\rPolio\r\r\r\r\r\rCalifornia\r\r1940\r\r37.8826320\r\r18.3397861\r\r0.8266512\r\r\r\rCalifornia\r\r1950\r\r13.9124205\r\r4.7467350\r\r1.9742639\r\r\r\rCalifornia\r\r1960\r\r14.1386471\r\rNA\r\r0.2640419\r\r\r\rCalifornia\r\r1970\r\r0.9767889\r\rNA\r\rNA\r\r\r\rCalifornia\r\r1980\r\r0.3743467\r\r0.0515466\r\rNA\r\r\r\r\rWe are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:\n\r\rstate\r\ryear\r\rMeasles\r\rPertussis\r\rPolio\r\r\r\r\r\rCalifornia\r\r1940\r\r37.9\r\r18.3\r\r0.8\r\r\r\rCalifornia\r\r1950\r\r13.9\r\r4.7\r\r2.0\r\r\r\rCalifornia\r\r1960\r\r14.1\r\rNA\r\r0.3\r\r\r\rCalifornia\r\r1970\r\r1.0\r\rNA\r\rNA\r\r\r\rCalifornia\r\r1980\r\r0.4\r\r0.1\r\rNA\r\r\r\r\rUseful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3).\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:\n\r\rstate\r\rdisease\r\r1940\r\r1950\r\r1960\r\r1970\r\r1980\r\r\r\r\r\rCalifornia\r\rMeasles\r\r37.9\r\r13.9\r\r14.1\r\r1\r\r0.4\r\r\r\rCalifornia\r\rPertussis\r\r18.3\r\r4.7\r\rNA\r\rNA\r\r0.1\r\r\r\rCalifornia\r\rPolio\r\r0.8\r\r2.0\r\r0.3\r\rNA\r\rNA\r\r\r\r\r\rKnow your audience\rGraphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot.\nAs a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest.\n\rKnow when to include 0\rWhen using barplots, it is misinformative not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example used by Peter Aldhous in this lecture: http://paldhous.github.io/ucb/2016/dataviz/week2.html.\n(Source: Fox News, via Media Matters6.)\nFrom the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:\nHere is another example, described in detail in a Flowing Data blog post:\n(Source: Fox News, via Flowing Data7.)\nThis plot makes a 13% increase look like a five fold change. Here is the appropriate plot:\nFinally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change:\n(Source:\rVenezolana de Televisión via Pakistan Today8 and Diego Mariano.)\nHere is the appropriate plot:\nWhen using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:\nNote that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability.\n\rDo not distort quantities\rDuring President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations:\n(Source: The 2011 State of the Union Address9)\nJudging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:\ngdp \u0026lt;- c(14.6, 5.7, 5.3, 3.3, 2.5)\rgdp_data \u0026lt;- data.frame(Country = rep(c(\u0026quot;United States\u0026quot;, \u0026quot;China\u0026quot;, \u0026quot;Japan\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;France\u0026quot;),2),\ry = factor(rep(c(\u0026quot;Radius\u0026quot;,\u0026quot;Area\u0026quot;),each=5), levels = c(\u0026quot;Radius\u0026quot;, \u0026quot;Area\u0026quot;)),\rGDP= c(gdp^2/min(gdp^2), gdp/min(gdp))) %\u0026gt;%\rmutate(Country = reorder(Country, GDP))\rgdp_data %\u0026gt;%\rggplot(aes(Country, y, size = GDP)) +\rgeom_point(show.legend = FALSE, color = \u0026quot;blue\u0026quot;) +\rscale_size(range = c(2,25)) +\rcoord_flip() + ylab(\u0026quot;\u0026quot;) + xlab(\u0026quot;\u0026quot;) # identical to labs(y = \u0026quot;\u0026quot;, x = \u0026quot;\u0026quot;)\rNot surprisingly, ggplot2 defaults to using area rather than\rradius. Of course, in this case, we really should not be using area at all since we can use position and length:\ngdp_data %\u0026gt;%\rfilter(y == \u0026quot;Area\u0026quot;) %\u0026gt;%\rggplot(aes(Country, GDP)) +\rgeom_bar(stat = \u0026quot;identity\u0026quot;, width = 0.5) +\rlabs(y = \u0026quot;GDP in trillions of US dollars\u0026quot;)\r\rOrder categories by a meaningful value\rWhen one of the axes is used to show categories, as is done in barplots, the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.\nWe previously learned how to use the reorder function, which helps us achieve this goal.\rTo appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate:\ndata(murders)\rp1 \u0026lt;- murders %\u0026gt;% mutate(murder_rate = total / population * 100000) %\u0026gt;%\rggplot(aes(x = state, y = murder_rate)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;) +\rcoord_flip() +\rtheme(axis.text.y = element_text(size = 8)) +\rxlab(\u0026quot;\u0026quot;)\rp2 \u0026lt;- murders %\u0026gt;% mutate(murder_rate = total / population * 100000) %\u0026gt;%\rmutate(state = reorder(state, murder_rate)) %\u0026gt;% # here\u0026#39;s the magic!\rggplot(aes(x = state, y = murder_rate)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;) +\rcoord_flip() +\rtheme(axis.text.y = element_text(size = 8)) +\rxlab(\u0026quot;\u0026quot;)\rgrid.arrange(p1, p2, ncol = 2) # we\u0026#39;ll cover this later\rWe can make the second plot like this:\ndata(murders)\rmurders %\u0026gt;% mutate(murder_rate = total / population * 100000) %\u0026gt;%\rmutate(state = reorder(state, murder_rate)) %\u0026gt;%\rggplot(aes(state, murder_rate)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;) +\rcoord_flip() +\rtheme(axis.text.y = element_text(size = 6)) +\rxlab(\u0026quot;\u0026quot;)\rThe reorder function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other:\nThe first orders the regions alphabetically, while the second orders them by the group’s median.\n\rShow the data\rWe have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups.\nTo motivate our first principle, “show the data”, we go back to our artificial example of describing heights to a person who is unaware of some basic facts about the population of interest (and is otherwise unsophisticated). This time let’s assume that this person is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors.10 The plot looks like this:\nThe average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.\nThis brings us to our first principle: show the data. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points:\nheights %\u0026gt;%\rggplot(aes(sex, height)) +\rgeom_point()\rFor example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.\nThe first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:\nheights %\u0026gt;%\rggplot(aes(sex, height)) +\rgeom_jitter(width = 0.1, alpha = 0.2)\rNow we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer.\n\rFaceting\rLooking at the previous plot, it’s easy to tell that males tend to be taller than females. Before, we showed how we can plot two distributions over each other using an aesthetic mapping. Something like this:\nheights %\u0026gt;%\rggplot(aes(x = height, fill = sex)) +\rgeom_histogram(alpha = .5, show.legend = TRUE) +\rlabs(fill = \u0026#39;Sex\u0026#39;)\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\rSometimes, putting the plots on top of each other, even with a well-chosen alpha, does not clearly communicate the differences in the distribution. When we want to compare side-by-side, we will often use facets. Facets are a bit like supercharged aesthetic mapping because they let us separate plots based on categorical variables, but instead of putting them together, we can have side-by-side plots.\nTwo functions in ggplot give facets: facet_wrap and facet_grid. We’ll use facet_grid as this is a little more powerful.\nFacets are added as an additional layer like this: + facet_grid(. ~ sex). Inside the function, we have a “formula” that is written without quotes (which is unusual for R). Since facet_grid takes a “formula”, all we have to do to facet is decide how we want to lay out our plots. If we want each of the faceting groups to lie along the vertical axis, we put the variable on which we want to facet before the “~”, and after the “~” we simply put a period. If we want the groups to lie along the horizontal axis, we put the variable after the “~” and the period before. In the example, we’ll separate the histogram by drawing them side by side along the horizontal axis.\nheights %\u0026gt;%\rggplot(aes(x = height)) +\rgeom_histogram(binwidth = 1, color=\u0026quot;black\u0026quot;) +\rfacet_grid(.~sex)\rThis would be the result if we took the females, plotted the histogram, then took the males, made another histogram, and then put them side by side. But we do it in one command by adding +facet_grid(...)\nUse common axes with facets\rSince we have plots side-by-side, they can have different scales along the x-axis (or along the y-axis if we were stacking with sex ~ .). We want to be careful here - if we don’t have matching scales on these axes, then it’ll be really hard to visually see differences in the distribution.\nAs an example of what not to do, and to show that we can use the scales argument in facet_grid, we can allow the x-axis to freely scale between the plots. This makes it hard to tell that males are, on average, taller because the average male height, despite being larger than the average female height (70 vs. 65 or so) falls in the same location within the plot box. Note that 80 is the extreme edge for the left plot, but not in the right plot.\nheights %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram(binwidth = 1, color=\u0026quot;black\u0026quot;) +\rfacet_grid(. ~ sex, scales = \u0026quot;free_x\u0026quot;)\r\rAlign plots vertically to see horizontal changes and horizontally to see vertical changes\rIn these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:\nheights %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram(binwidth = 1, color=\u0026quot;black\u0026quot;) +\rfacet_grid(. ~ sex)\rp2 \u0026lt;- heights %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram(binwidth = 1, color=\u0026quot;black\u0026quot;) +\rfacet_grid(sex~.)\rp2\rThis plot makes it much easier to notice that men’s heights are, on average, higher.\nThe sample size of females is smaller than of males – that is, we have more males in the data. Try table(heights$sex) to see this. It’s also clear from the above plot because the height of the bars on the y-axis (count) are smaller for females. If we are interested in the distribution within our sample, this is useful. If we’re interested in the distribution of females vs. the distribution of males, we might want to re-scale the y-axis.\np2 \u0026lt;- heights %\u0026gt;%\rggplot(aes(height)) +\rgeom_histogram(binwidth = 1, color=\u0026quot;black\u0026quot;) +\rfacet_grid(sex~., scales = \u0026#39;free_y\u0026#39;)\rp2\rWe still have count on the y-axis, so we didn’t switch to density (though it would look the same). Instead, we rescaled the y-axis, which gives us a different perspective but still contains the count information.\nIf we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points:\np3=heights %\u0026gt;%\rggplot(aes(sex, height)) +\rgeom_boxplot(coef=3) +\rgeom_jitter(width = 0.1, alpha = 0.2) +\rylab(\u0026quot;Height in inches\u0026quot;)\rp3\rNow contrast and compare these three plots, based on exactly the same data:\nNotice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions.\n\rFacet grids\rAs the name implies, facet_grid can make more than just side-by-plots. If we specify variables on boths sides of the “~”, we get a grid of plots.\ngapminder::gapminder %\u0026gt;%\rfilter(year %in% c(1952,1972, 1992, 2002)) %\u0026gt;%\rfilter(continent != \u0026#39;Oceania\u0026#39;) %\u0026gt;%\rggplot(aes(x = lifeExp)) + geom_density() +\rfacet_grid(continent ~ year)\rThis makes it easy to read the life expectancy distribution over time (left-to-right) and across continents (up-and-down). It makes it easy to see that Africa has spread it’s life expectancy distribution (some improved, some didn’t), while Europe has become more clustered at the top end over time. Faceting in a grid is very helpful when you have a time dimension.\n\rVisual cues to be compared should be adjacent, continued\rFor each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.\nNote that there are two gapminder datasets, one in dslabs and one in the gapminder package. The dslabs version has more data, so I will switch to that here by using dslabs::gapminder as our data.\ndslabs::gapminder %\u0026gt;%\rfilter(year %in% c(1970, 2010) \u0026amp; !is.na(gdp)) %\u0026gt;%\rmutate(dollars_per_day = gdp/population/365) %\u0026gt;%\rmutate(labels = paste(year, continent)) %\u0026gt;% # creating text labels\rggplot(aes(x = labels, y = dollars_per_day)) +\rgeom_boxplot() +\rtheme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +\rscale_y_continuous(trans = \u0026quot;log2\u0026quot;) +\rylab(\u0026quot;Income in dollars per day\u0026quot;)\rThe default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:\ndslabs::gapminder %\u0026gt;%\rfilter(year %in% c(1970, 2010) \u0026amp; !is.na(gdp)) %\u0026gt;%\rmutate(dollars_per_day = gdp/population/365) %\u0026gt;%\rmutate(labels = paste(continent, year)) %\u0026gt;%\rggplot(aes(labels, dollars_per_day)) +\rgeom_boxplot() +\rtheme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25)) +\rscale_y_continuous(trans = \u0026quot;log2\u0026quot;) +\rylab(\u0026quot;Income in dollars per day\u0026quot;)\r\rUse color\rThe comparison becomes even easier to make if we use color to denote the two things we want to compare. Now we do not have to make the labels column and can just use continent on the x-axis:\n\r\rThink of the color blind\rAbout 10% of the population is color blind. Unfortunately, the default colors used in ggplot2 are not optimal for this group. However, ggplot2 does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette:\ncolor_blind_friendly_cols \u0026lt;-\rc(\u0026quot;#999999\u0026quot;, \u0026quot;#E69F00\u0026quot;, \u0026quot;#56B4E9\u0026quot;, \u0026quot;#009E73\u0026quot;,\r\u0026quot;#F0E442\u0026quot;, \u0026quot;#0072B2\u0026quot;, \u0026quot;#D55E00\u0026quot;, \u0026quot;#CC79A7\u0026quot;)\rHere are the colors\rFrom Seafood Prices Reveal Impacts of a Major Ecological Disturbance:\nThere are several resources that can help you select colors, for example this one: http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/.\nUsing a discrete color palette\rIf you’re simply trying to differentiate between groups by using color, there are many ways of changing your color palette in ggplot. Most use scale_fill_discrete or scale_color_discrete (depending on the aesthetic for which you’re setting the color).\nThe easiest way of getting good-looking (e.g. non-default) colors is the scale_fill_viridis_d function, which “inherits” (takes the place of and has the properties of) scale_fill_discrete. Viridis has four color palettes and each is designed to be used to maximize the differentiation between colors.\nWe will subset our dslabs::gapminder dataset to five different years and take a look at what Viridis colors can do across those five:\ngp = dslabs::gapminder %\u0026gt;% filter(year == 1990 | year == 1995 | year==2000 | year == 2005 | year==2010 ) %\u0026gt;%\rggplot(aes(x = continent, y = gdp/population, fill = as.factor(year))) + coord_flip()\rgp + geom_boxplot() + labs(title = \u0026#39;Default\u0026#39;)\rThe default uses five different colors plucked seemingly at random. They are actually drawn from a palette of default ggplot colors.\nLet’s try Viridis\ngp = dslabs::gapminder %\u0026gt;% filter(year == 1990 | year == 1995 | year==2000 | year == 2005 | year==2010 ) %\u0026gt;%\rggplot(aes(x = continent, y = gdp/population, fill = as.factor(year))) + coord_flip() + labs(fill = \u0026#39;Year\u0026#39;)\rviridis_a = gp + geom_boxplot() + labs(title = \u0026#39;Viridis A\u0026#39;) + scale_fill_viridis_d(option = \u0026#39;A\u0026#39;)\rviridis_b = gp + geom_boxplot() + labs(title = \u0026#39;Viridis B\u0026#39;) + scale_fill_viridis_d(option = \u0026#39;B\u0026#39;)\rviridis_c = gp + geom_boxplot() + labs(title = \u0026#39;Viridis C\u0026#39;) + scale_fill_viridis_d(option = \u0026#39;C\u0026#39;)\rviridis_d = gp + geom_boxplot() + labs(title = \u0026#39;Viridis D\u0026#39;) + scale_fill_viridis_d(option = \u0026#39;D\u0026#39;)\rgrid.arrange(viridis_a, viridis_b, viridis_c, viridis_d)\rViridis uses a better palette of colors that, though distinct, have some cohesiveness to them.\nWe can also use a custom palette, like the colorblind palette from before. If the palette has more entries than we have (N) distinct categories, R reverts to the default.\ngp = dslabs::gapminder %\u0026gt;% filter(year == 1990 | year == 1995 | year==2000 | year == 2005 | year==2010 ) %\u0026gt;%\rggplot(aes(x = continent, y = gdp/population, fill = as.factor(year))) + coord_flip() + labs(fill = \u0026#39;Year\u0026#39;) custom_a = gp + geom_boxplot() + labs(title = \u0026#39;Viridis A\u0026#39;) + scale_fill_discrete(type = color_blind_friendly_cols)\rcustom_b = gp + geom_boxplot() + labs(title = \u0026#39;Viridis A\u0026#39;) + scale_fill_discrete(type = color_blind_friendly_cols[1:3])\rgrid.arrange(custom_a, custom_b)\rIn the lower plot, we only give it a length-3 vector of colors, and it needs 5, so it returns to default.\n\rUsing a continuous color palette\rWe may often want to use the color to indicate a numeric value instead of simply using it to delineate groupings. When this is the case, the fill or color aesthetic is set to a continuous value. For instance, if one were to plot election results by precinct, we may represent precincts with heavy Republican support as dark red, swing districts as purple or white, and Democratic districts as blue. The intensity of red/blue indicates how heavily slanted votes in that precinct were in the election. This is known as a color ramp.\nLets plot one country’s GDP by year, but have the color indicate the life expectancy:\ndslabs::gapminder %\u0026gt;%\rfilter(country==\u0026#39;Romania\u0026#39; \u0026amp; year\u0026gt;1980) %\u0026gt;%\rggplot(aes(x = year, y = gdp/population, color = life_expectancy)) + scale_fill_continuous() + geom_point(size = 5) +\rlabs(x = \u0026#39;Year\u0026#39;, y = \u0026#39;GDP Per Capita\u0026#39;, fill = \u0026#39;Life Expectancy\u0026#39;)\rWe can see that GDP per capita went up, then down in 1989 (fall of the Soviet Union), then up after that. The color ramp tells us that life expectancy reached 75 years near the end, and it certainly improved in the post-2000 era.\nWe can set some of the points on the ramp manually - here, the ramp starts at dark blue and ends at light blue, but what if we wanted to start at red, and at blue, and cross white in the middle? Easy! We use scale_color_gradient2 and specify the colors for low, mid, and high, and specify the midpoint at 72.5 years.\ndslabs::gapminder %\u0026gt;%\rfilter(country==\u0026#39;Romania\u0026#39; \u0026amp; year\u0026gt;1980) %\u0026gt;%\rggplot(aes(x = year, y = gdp/population, color = life_expectancy)) + scale_color_gradient2(low = \u0026#39;red\u0026#39;, mid = \u0026#39;white\u0026#39;, high = \u0026#39;blue\u0026#39;, midpoint = 72.5) + geom_point(size = 5) +\rlabs(x = \u0026#39;Year\u0026#39;, y = \u0026#39;GDP Per Capita\u0026#39;, fill = \u0026#39;Life Expectancy\u0026#39;)\rThe midpoint specification is extra useful when there is a threshold (like 50% of the vote) that indicates a different qualitative outcome.\nThe gradient2 method does not always work with the colorblind discrete palette - the colors interpolated may be in the range in which colorblindness tends to be a problem:\ndslabs::gapminder %\u0026gt;%\rfilter(country==\u0026#39;Romania\u0026#39; \u0026amp; year\u0026gt;1980) %\u0026gt;%\rggplot(aes(x = year, y = gdp/population, color = life_expectancy)) + scale_color_gradient2(low = color_blind_friendly_cols[3], mid = color_blind_friendly_cols[4], high = color_blind_friendly_cols[5], midpoint = 72.5) + geom_point(size = 5) +\rlabs(x = \u0026#39;Year\u0026#39;, y = \u0026#39;GDP Per Capita\u0026#39;, fill = \u0026#39;Life Expectancy\u0026#39;)\r\r\rgridExtra and grid.arrange\rThe gridExtra package has been used a few times in this lesson to combine plots using the grid.arrange function. The use is pretty intuitive - you save your plots as objects plot1 \u0026lt;- ggplot(data, aes(x = var1)) and plot2 \u0026lt;- ggplot(data, aes(x = var2)), and then use grid.arrange(plot1, plot2) to combine. The function will align as best it can, and there are more advanced grob-based functions that can adjust and align axes between plots, but we won’t get into them. If we want to set the layout, we can specify nrow and ncol to set the rows and columns.\nThe very-useful patchwork package is quickly replacing grid.arrange and provides more flexibility.\n\r\r\rhttp://kbroman.org/↩︎\n\rhttps://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf↩︎\n\rhttps://github.com/kbroman/Talk_Graphs↩︎\n\rhttp://paldhous.github.io/ucb/2016/dataviz/index.html↩︎\n\rhttps://projecteuclid.org/download/pdf_1/euclid.ss/1177010488↩︎\n\rhttp://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507↩︎\n\rhttp://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/↩︎\n\rhttps://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote↩︎\n\rhttps://www.youtube.com/watch?v=kl2g40GoRxg↩︎\n\rIf you’re unfamiliar, standard errors are defined later in the course—do not confuse them with the standard deviation of the data.↩︎\n\r\r\r","date":1612828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641414964,"objectID":"267a7446c3b4fe31bb41b58ba3b49b11","permalink":"https://ssc442kirkpatrick.netlify.app/content/03-content/","publishdate":"2021-01-25T00:00:00Z","relpermalink":"/content/03-content/","section":"content","summary":"Readings\rGuiding Questions\rData visualization principles\rEncoding data using visual cues\rAvoid pseudo-three-dimensional plots\rAvoid too many significant digits\rKnow your audience\rKnow when to include 0\rDo not distort quantities\rOrder categories by a meaningful value\rShow the data\rFaceting\rUse common axes with facets\rAlign plots vertically to see horizontal changes and horizontally to see vertical changes\rFacet grids\rVisual cues to be compared should be adjacent, continued\rUse color\r\rThink of the color blind\rUsing a discrete color palette\rUsing a continuous color palette\r\rgridExtra and grid.","tags":null,"title":"Visualizations in Practice","type":"docs"},{"authors":null,"categories":null,"content":"\r\rPart 1: Rats, rats, rats.\rInstructions\rStarter code\r\rPart 2: Data Hunting\rEvaluations\r\r\rEach member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.\nTurn in your copies by 11:59pm on March 4th (I have extended everyone’s deadline by 1 week due to slow rollout of groups).\n\rPart 1: Rats, rats, rats.\rNew York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.\n  NYC keeps incredibly detailed data regarding animal sightings, including rats, and it makes this data publicly available.\nFor this first project, you will use R and ggplot2 to tell an interesting story hidden in the data. You must create a story by looking carefully at the data.\nInstructions\rHere’s what you need to do:\nDownload New York City’s database of rat sightings since 2010:\n\r Rat_sightings.csv\r\rSummarize the data somehow. The raw data has more than 100,000 rows, which means you’ll need to aggregate the data (filter(), group_by(), and summarize() will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2016.\n\rCreate an appropriate visualization based on the data you summarized.\n\rWrite a memo explaining your process. We are specifically looking for a discussion of the following:\n\rWhat story are you telling with your new graphic?\rHow have you applied reasonable standards in visual storytelling?\rWhat policy implication is there (if any)?\r\rUpload the following outputs to D2L:\n\rA PDF file of your memo with your final code and graphic embedded in it.1 This means you’ll need to do all your coding in an R Markdown file and embed your code in chunks. Note that Part 2 of this project should be included in this PDF (see below).\rA standalone PDF version of your graphic. Use ggsave(plot_name, filename = \"output/blah.pdf\", width = XX, height = XX)\r\r\r\rStarter code\rI’ve provided some starter code below. A couple comments about it:\n\rBy default, read_csv() treats cells that are empty or “NA” as missing values. This rat dataset uses “N/A” to mark missing values, so we need to add that as a possible marker of missingness (hence na = c(\"\", \"NA\", \"N/A\"))\rTo make life easier, I’ve renamed some of the key variables you might work with. You can rename others if you want.\rI’ve also created a few date-related variables (sighting_year, sighting_month, sighting_day, and sighting_weekday). You don’t have to use them, but they’re there if you need them. The functions that create these, like year() and wday() are part of the lubridate library.\rThe date/time variables are formatted like 04/03/2017 12:00:00 AM, which R is not able to automatically parse as a date when reading the CSV file. You can use the mdy_hms() function in the lubridate library to parse dates that are structured as “month-day-year-hour-minute”. There are also a bunch of other iterations of this function, like ymd(), dmy(), etc., for other date formats.\rThere’s one row with an unspecified borough, so I filter that out.\r\rlibrary(tidyverse)\rlibrary(lubridate)\rrats_raw \u0026lt;- read_csv(\u0026quot;data/Rat_Sightings.csv\u0026quot;, na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;N/A\u0026quot;))\r# If you get an error that says \u0026quot;All formats failed to parse. No formats\r# found\u0026quot;, it\u0026#39;s because the mdy_hms function couldn\u0026#39;t parse the date. The date\r# variable *should* be in this format: \u0026quot;04/03/2017 12:00:00 AM\u0026quot;, but in some\r# rare instances, it might load without the seconds as \u0026quot;04/03/2017 12:00 AM\u0026quot;.\r# If there are no seconds, use mdy_hm() instead of mdy_hms().\rrats_clean \u0026lt;- rats_raw %\u0026gt;%\rrename(created_date = `Created Date`,\rlocation_type = `Location Type`,\rborough = Borough) %\u0026gt;%\rmutate(created_date = mdy_hms(created_date)) %\u0026gt;%\rmutate(sighting_year = year(created_date),\rsighting_month = month(created_date),\rsighting_day = day(created_date),\rsighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %\u0026gt;%\rfilter(borough != \u0026quot;Unspecified\u0026quot;)\rYou’ll summarize the data with functions from dplyr, including stuff like count(), arrange(), filter(), group_by(), summarize(), and mutate(). Here are some examples of ways to summarize the data:\n# See the count of rat sightings by weekday\rrats_clean %\u0026gt;%\rcount(sighting_weekday)\r# Assign a summarized data frame to an object to use it in a plot\rrats_by_weekday \u0026lt;- rats_clean %\u0026gt;%\rcount(sighting_weekday, sighting_year)\rggplot(rats_by_weekday, aes(x = fct_rev(sighting_weekday), y = n)) +\rgeom_col() +\rcoord_flip() +\rfacet_wrap(~ sighting_year)\r# See the count of rat sightings by weekday and borough\rrats_clean %\u0026gt;%\rcount(sighting_weekday, borough, sighting_year)\r# An alternative to count() is to specify the groups with group_by() and then\r# be explicit about how you\u0026#39;re summarizing the groups, such as calculating the\r# mean, standard deviation, or number of observations (we do that here with\r# `n()`).\rrats_clean %\u0026gt;%\rgroup_by(sighting_weekday, borough) %\u0026gt;%\rsummarize(n = n())\r\r\rPart 2: Data Hunting\rFor the second part of the project, your task is simple. Your group must identify three different data sources2 for potential use in your final project. You are not bound to this decision.\nFor each, you must write a single paragraph about what about this data interests you. Add this to the memo from Part 1.\n\rEvaluations\rI will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:\nPart 1\nTechnical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (10 points)\n\rAppropriateness of visuals: Do the visualizations tell a clear story? Have we learned something? (10 points)\n\rStorytelling: Does your memo clearly convey what you’re doing and why? (9 points)\n\r\rPart 2\nEach piece of data (and description) is worth 7 points. (21 points total)\n\r\rYou can approach this in a couple different ways—you can write the memo and then include the full figure and code at the end, similar to this blog post, or you can write the memo in an incremental way, describing the different steps of creating the figure, ultimately arriving at a clean final figure, like this blog post.↩︎\n\rThe three different sources need not be different websites or from different organizations. For example, three different tables from the US Census would be sufficient↩︎\n\r\r\r","date":1611187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613518807,"objectID":"1ee2ebbd4938399d5199cb912763ad52","permalink":"https://ssc442kirkpatrick.netlify.app/assignment/project1/","publishdate":"2021-01-21T00:00:00Z","relpermalink":"/assignment/project1/","section":"assignment","summary":"Part 1: Rats, rats, rats.\rInstructions\rStarter code\r\rPart 2: Data Hunting\rEvaluations\r\r\rEach member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.\nTurn in your copies by 11:59pm on March 4th (I have extended everyone’s deadline by 1 week due to slow rollout of groups).","tags":null,"title":"Project 1","type":"docs"},{"authors":null,"categories":null,"content":"\rBelow is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon. Accordingly, you should visit this page frequently throughout the term.\nAs mentioned in the syllabus, the course is structured by topics; each week introduces a new topic. Moreover, every week is divided into three important sections that you should engage with.\nOverview\rThe class is structured with three distinct bits. First, the Tuesday lecture will give an overview of the topic for the week. Next, the Thursday lecture will have a short, practical lecture and an activity which is designed to give you hands-on experience and a greater understanding of the broader material. Finally, you will complete weekly writings (short) and labs (also short; requiring coding in R). Out of class, you will complete readings and complete assignments.\n\rContent (): This page contains the readings for the topic. These pages should be read completely. Lectures are not an exact replication of the written content; on the contrary, the lectures are intended to keep you focused on the high-level ideas, while the readings are broader and more comprehensive. Accordingly, lectures are shorter than the (often quite lengthy) written content.\n\rExamples (): This page the material that we will discuss in Thursday classes. In addition to teaching specific content, there are many more R code examples. These are intended as a useful reference to various functions that you will need when working on (nearly) weekly labs and your group project.\n\rAssignments (): This page contains the instructions for the weekly lab (1–3 brief tasks) and for the two mini projects + final project. Labs are due by 11:59 PM (Eastern) on the Monday after they’re posted.\n\r\r\rOffice Hours (TA) Monday 8-9:30am: msu.zoom.us/j/3382548927 Passcode: officehour\rThe teaching assistant for this course (Xueshi Wang; wangxu36@msu.edu) will host office each week to help promote additional understanding. I highly encourage you to utilize this resource, especially if you struggle with basic R programming.\ntl;dr: You should follow this general process (in order) each week:\n\rDo everything on the content () page before Tuesday\rCome to the lecture on Tuesday.\rWhile “in class” on Thursday, work through the example () page\rComplete the lab () and the weekly writing (assigned in class) before the next Tuesday.\rAs needed, attend the lab hours hosted by the TA.\r\r\r\r\r\rProgramming Foundations\rContent\rExample\rAssignment\r\r\rWeek 0 (Jan 11/13)\r(Re-) Introduction to R\r\r\r\r\r\rWeek 1 (Jan 18/20)\rProgramming Basics, the tidyverse, and Visualization\r\r\r\r\r\rWeek 2 (Jan 25/27)\rVisualization II\r\r\r\r\r\r\r\r\rWeek 3 (Feb 1/3)\rVisualization III\r\r\r\r\r\r\r\r\r\rData Analysis Foundations\rContent\rExample\rAssignment\r\r\rWeek 4 (Feb 8/10)\rProbability and Statistics in R\r\r\r\r\r\r\r\r\rWeek 5 (Feb 15/17)\rLinear Regression I\r\r\r\r\r\r\r\r\rWeek 6 (Feb 22/24)\rLinear Regression II\r\r\r\r\r\r\r\r\rFeb 25th\r Project 1 Due\r\r\r\r\r\r\r\r\rWeek 7 (March 1/3)\rLinear Regression III\r\r\r\r\r\r\r\r\r\rApplications of Data Analysis\rContent\rExample\rAssignment\r\r\rWeek 8 (March 15/17)\rNonlinear Regression\r\r\r\r\r\r\r\r\rWeek 9 (March 22/24)\rBias vs Variance\r\r\r\r\r\r\r\r\rWeek 10 (March 29/31)\rClassification\r\r\r\r\r\r\r\r\rApril 1st\r Project 2 Due\r\r\r\r\r\r\r\r\rWeek 11 (Apr 5/7)\rWrangling Data\r\r\r\r\r\r\r\r\r\rFurther Extensions\rContent\rExample\rAssignment\r\r\rWeek 12 (Apr 12/14)\rText as Data\r\r\r\r\r\r\r\r\rWeek 13 (Apr 19/21)\rGeospatial in R\r\r\r\r\r\r\r\r\r\rConclusions\rContent\rExample\rAssignment\r\r\rWeek 14 (Apr 26/28)\rWild Card\r\r\r\r\r\r\r\r\rApril 29th, 11:59pm\r Final Project Due\r\r\r\r\r\r\r\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641473121,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"https://ssc442kirkpatrick.netlify.app/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"Below is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon. Accordingly, you should visit this page frequently throughout the term.\nAs mentioned in the syllabus, the course is structured by topics; each week introduces a new topic. Moreover, every week is divided into three important sections that you should engage with.","tags":null,"title":"Schedule","type":"page"}]