[{"authors":["Ben"],"categories":null,"content":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron. Prior to coming to Michigan State University, I worked with the U.S. Army to help soldiers become more psychologically resilient.\nI hold a Ph.D. in Social Science (Economics) from the California Institute of Technology (Caltech), and a B.S. in Economics from the University of Oregon.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592849563,"objectID":"bf008f22d9b0754cde4f6972811c28b7","permalink":"https://datavizm20.classes.andrewheiss.com/authors/ben/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ben/","section":"authors","summary":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron.","tags":null,"title":"Ben Bushong","type":"authors"},{"authors":["Justin"],"categories":null,"content":"I am an Assistant Professor at Michigan State University who has not yet filled out this description.\nI hold a Ph.D. in Environmental Economics from Duke University, an M.E.M. in Environmental Policy and Economics from Duke University, and a B.S. in Environmental Policy Analysis and City Planning from the University of California at Davis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1607443236,"objectID":"8422260f0f3251af15c00666a8df9838","permalink":"https://datavizm20.classes.andrewheiss.com/authors/justin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/justin/","section":"authors","summary":"I am an Assistant Professor at Michigan State University who has not yet filled out this description.\nI hold a Ph.D. in Environmental Economics from Duke University, an M.E.M. in Environmental Policy and Economics from Duke University, and a B.S. in Environmental Policy Analysis and City Planning from the University of California at Davis.","tags":null,"title":"Justin Kirkpatrick","type":"authors"},{"authors":null,"categories":null,"content":"\rIn these (modestly) secret pages, I’ve included some resources for those who read the syllabus closely.\nIf you’re stuck with anything or want help with, say, using markdown, you’ll find some basic guidance here. Additionally, there are links throughout to outside resources.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1594409288,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"https://datavizm20.classes.andrewheiss.com/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"In these (modestly) secret pages, I’ve included some resources for those who read the syllabus closely.\nIf you’re stuck with anything or want help with, say, using markdown, you’ll find some basic guidance here. Additionally, there are links throughout to outside resources.","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":"\rEach week has a set of required readings that you should complete before coming to the (online) Tuesday lecture. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due).\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts. These questions can serve as helpful starting places for your thinking; they are not representative of the totality of the content and are not intended to be limiting. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read.\nEvery weekly session also has some collection of YouTube videos (recordings of the lecture) that are associated with each week. Again, these lectures are inherently different from the written content, and you should consider them as high-level overviews of the written content. I am not replicating the written text as-is and (especially if you’re struggling) you should engage with lectures and written materials in equal measure. The lecture slides are HTML files made with the R package xaringan. For each of the weekly pages, you will see buttons for opening the presentation in a new tab.1.\n View all slides in new window\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).\n\rI aspire to include a link for downloading a PDF of the slides in case you want to print them or store them on your computer. However, this seems… ambitious. As of right now, I have not finished this.↩︎\n\r\r\r","date":1598918400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1598465283,"objectID":"1413f960ec974b9863bc45d887efa8bd","permalink":"https://datavizm20.classes.andrewheiss.com/content/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Each week has a set of required readings that you should complete before coming to the (online) Tuesday lecture. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due).\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts.","tags":null,"title":"Readings, lectures, and videos","type":"docs"},{"authors":null,"categories":null,"content":"\r\rWeekly Writings\rWeekly Writing Template\r\rLabs\rProjects\rFinal project\r\r\rThis course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nEngage with the readings and lecture materials\rRegularly use R (aka engage daily or almost every day in some way)\r\rEach type of assignment in this class helps with one of these strategies.\nWeekly Writings\rTo encourage you to actively engage with the course content, you will write a ≈150 word memorandum about the reading or lecture each week. That’s fairly short: there are ≈250 words on a typical double-spaced page. You must complete a total of twelve of these; there are more than eleven weeks in the course, so you have some flexibility.1 Your actual prompt will be assigned in class, so you must login each day to ensure you get these assignments. To keep you on your toes, we will vary whether these are assigned on Tuesdays or Thursdays.\nYou can do a lot of different things with this memo: discuss something you learned from the course content, write about the best or worst data visualization you saw recently, connect the course content to your own work, etc. These reflections let you explore and answer some of the key questions of this course, including:\n\rWhen is a link correlational vs causal? How can we still make useful statements about non-causal things?\rWhy do we visualize data?\rWhat makes a great data analysis? What makes a bad analysis?\rHow do you choose which kind of analysis method to use?\rWhat is the role of the data structure in choosing an analysis? Can we be flexible?\r\rThe course content for each day will also include a set of questions specific to that topic. You do not have to answer all (or any) of these questions. That would be impossible. They exist to guide your thinking and to make complex reading more digestible. The specific topic for each week will be assigned in class. (We can’t emphasize this enough.)\nThe TA will grade these mini-exercises using a very simple system:\n\r✔+: (11.5 points (115%) in gradebook) Work shows phenomenal thought and engagement with the course content. We will not assign these often.\r✔: (10 points (100%) in gradebook) Work is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance.\r✔−: (5 points (50%) in gradebook) Work is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.\r\r(There is an implicit 0 above for work that is not turned in on-time). Notice that this is essentially a pass/fail or completion-based system. We’re not grading your writing ability; we’re not counting the exact number of words you’re writing; and we’re not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. We are looking for thoughtful engagement. Read the material, engage with the work and you’ll get a ✓.\nWeekly Writing Template\rYou will turn these reflections in via D2L. You will write them using R Markdown and this weekly writing template (right-click to Save Link As…) . You must knit your work to a PDF document (this will be what you turn in). D2L will have eleven weekly writing assignments available. Upload your first weekly writing assignment to number 1, your second (regardless of which week you are writing on) to number 2, etc.\n\r\rLabs\rEach week of the course has fully annotated examples of code that teach and demonstrate how to do specific tasks in R. However, without practicing these principles and making graphics on your own, you won’t remember what you learn.\nPlease do not do labs ahead of time. I am updating the assignments as the semester proceeds, and you may do an entire assignment that is completely changed.\nFor example, to practice working with ggplot2 and making data-based graphics, you will complete a brief set of exercises over a few class sessions. These exercises will have 1–3 short tasks that are directly related to the topic for the week. You need to show that you made a good faith effort to work each question. There will also be a final question which requires significantly more thought and work. This will be where you get to show some creativity and stretch your abilities. Overall, labs will be graded the same check system:\n\r✔+: (17.5 points (115%) in gradebook) Exercises are complete. Every task was attempted and answered, and most answers are correct. Knitted document is clean and easy to follow. Work on the final problem shows creativity or is otherwise exceptional. We will not assign these often.\r✔: (15 points (100%) in gradebook) Exercises are complete and most answers are correct. This is the expected level of performance.\r✔−: (7.5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. We will hopefully not assign these often, but subpar work can expect a ✔−.\r\rNote that this is also essentially a pass/fail system. As noted in the syllabus, we are not grading your coding ability. We are not checking each line of code to make sure it produces some exact final figure, and we do not expect perfection. Also note that a ✓ does not require 100% success. You will sometimes get stuck with weird errors that you can’t solve, or the demands of pandemic living might occasionally become overwhelming. We are looking for good faith effort. Try hard, engage with the task, and you’ll get a ✓.\nYou may work together on the labs, but you must turn in your own answers. You will turn these labs in via D2L. You will write them using R Markdown and must knit your work to a PDF document.\n\rProjects\rTo give you practice with the data and design principles you’ll learn in this class, you will complete two projects en route to the overarching final project of the course. Both these mini projects and the final project must be completed in groups.\nThe two (mini) projects are checkpoints to ensure you’re working on your project seriously. They will be graded using a check system:\n\r✔+: (55 points (≈115%) in gradebook) Project is phenomenally well-designed and uses advanced R techniques. The project uncovers an important story that is not readily apparent from just looking at the raw data. I will not assign these often.\r✔: (50 points (100%) in gradebook) Project is fine, follows most design principles, answers a question from the data, and uses R correctly. This is the expected level of performance.\r✔−: (25 points (50%) in gradebook) Project is missing large components, is poorly designed, does not answer a relevant question, and/or uses R incorrectly. This indicates that you need to improve next time. I will hopefully not assign these often.\r\rBecause these mini projects give you practice for the final project, we will provide you with substantial feedback on your design and code.\n\rFinal project\rAt the end of the course, you will demonstrate your skills by completing a final project. Complete details for the final project (including past examples of excellent projects) are here. In brief, the final project has the following elements:\nYou must find existing data to analyze.2 Aggregating data from multiple sources is encouraged, but is not required.\r\rYou must visualize (at least) three interesting features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.3\r\rYou must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.4\r\rYou must write your analysis as if presenting to a C-suite executive. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.\r\rThere is no final exam. This project is your final exam.\nThe project will not be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:\nTechnical skills: Was the project easy? Does it showcase mastery of data analysis?\rVisual design: Was the information smartly conveyed and usable? Was it beautiful?\rAnalytic design: Was the analysis appropriate? Was it sensible, given the dataset?\rStory: Did we learn something?\r\rIf you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.\n\r\rNote that sometimes the writing takes the form of a very short coding assignment.↩︎\n\rNote that existing is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged.↩︎\n\rPie charts of any kind will result in a 25% grade deduction.↩︎\n\rThis is an extremely dumb idea for a number of reasons. Moreover, it’s worth mentioning that sports data, while rich, can be overwhelming due to its sheer magnitude and the variety of approaches that can be applied. Use with caution.↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1609330935,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/","section":"assignment","summary":"Weekly Writings\rWeekly Writing Template\r\rLabs\rProjects\rFinal project\r\r\rThis course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nEngage with the readings and lecture materials\rRegularly use R (aka engage daily or almost every day in some way)\r\rEach type of assignment in this class helps with one of these strategies.","tags":null,"title":"Assignments and Evaluations","type":"docs"},{"authors":null,"categories":null,"content":"\rThis section contains the content covered in Thursday lectures and some annotated R code that you can use as a reference for creating your own work. The intention is that in the Content section, you will sequentially build up your understanding of R and data analytics; here, you can see how all the pieces work together.\nVisit this section after you have finished the readings in the Content section and any supplemental lecture videos.\nMany of the examples also contain videos of me live-coding so you can see what it looks like to work with R in real time.1 Hopefully, you’ll find it useful to watch the practice of coding. You’ll also notice me make all sorts of errors. This is normal. If you’re finding yourself making lots of errors and generally struggling to get your code to run, start by working with the final code and reverse-engineer the solution you want.\n\rI might edit these videos for length and because I like to talk to myself when I’m programming. Also, too much swearing.↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1598465283,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"https://datavizm20.classes.andrewheiss.com/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"This section contains the content covered in Thursday lectures and some annotated R code that you can use as a reference for creating your own work. The intention is that in the Content section, you will sequentially build up your understanding of R and data analytics; here, you can see how all the pieces work together.\nVisit this section after you have finished the readings in the Content section and any supplemental lecture videos.","tags":null,"title":"Practical Content","type":"docs"},{"authors":null,"categories":null,"content":"\r\rInstall R\rInstall RStudio\rInstall tidyverse\rInstall tinytex\r\r\rAs mentioned in the syllabus, you will do all of your work in this class with the open source programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—–R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nHopefully you’re well-versed in dealing with these things, but if you’re lost, here’s how you install the required software for the course.\nInstall R\rFirst you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n\rClick on “Download R for XXX”, where XXX is either Mac or Windows:\n\rIf you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.\n\rIf you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n\r\rDouble click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n\rIf you use macOS, download and install XQuartz. You do not need to do this on Windows.\n\r\r\rInstall RStudio\rNext, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n\rThe website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n\rDouble click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n\r\rDouble click on RStudio to run it (check your applications folder or start menu).\n\rInstall tidyverse\rR packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including the ever-present ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel. Hopefully you’ve experienced installing packages before now; if not, consider this a crash course!\n\rInstall tinytex\rWhen you knit to PDF, R uses a special scientific typesetting program named LaTeX.2\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB. To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console.\rRun tinytex::install_tinytex() in the console.\rWait for a bit while R downloads and installs everything you need.\rThe end! You should now be able to knit to PDF.\r\r\r\rIt’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n\rPronounced “lay-tek” for those who are correct; or “lah-tex” to those who love goofy nerdy pronunciation. Technically speaking, the x is the “ch” sound in “Bach”, but most people just say it as “k”. While either saying “lay” or “lah” is correct, “layteks” is frowned upon because it clearly shows you’re not cool.↩︎\n\r\r\r","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594409288,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"https://datavizm20.classes.andrewheiss.com/resource/install/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"Install R\rInstall RStudio\rInstall tidyverse\rInstall tinytex\r\r\rAs mentioned in the syllabus, you will do all of your work in this class with the open source programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—–R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"\r\rProgramming basics\rConditional expressions\rDefining functions\rNamespaces\rFor-loops\rVectorization and functionals\r\r\r\rNOTE:\nAs you read through this assignment, practice with each of the examples (copy-paste them into an empty R script and run them). At the bottom of this page you will find the questions that comprise the assignment. These questions apply and expand on the topics and R functions in the assignment.\nRight-click to download the homework .Rmd template . Please save the template into the labs folder in the SSC442 folder on your local hard drive. If you don’t have a nice file structure setup for the course, please do so now. It will save you headaches in the future.\n\rProgramming basics\rWe teach R because it greatly facilitates data analysis, the main topic of this book. By coding in R, we can efficiently perform exploratory data analysis, build data analysis pipelines, and prepare data visualization to communicate results. However, R is not just a data analysis environment but a programming language. Advanced R programmers can develop complex packages and even improve R itself, but we do not cover advanced programming in this book. Nonetheless, in this section, we introduce three key programming concepts: conditional expressions, for-loops, and functions. These are not just key building blocks for advanced programming, but are sometimes useful during data analysis. We also note that there are several functions that are widely used to program in R but that we will not cover in this book. These include split, cut, do.call, and Reduce, as well as the data.table package. These are worth learning if you plan to become an expert R programmer.\nConditional expressions\rConditional expressions are one of the basic features of programming. They are used for what is called flow control. The most common conditional expression is the if-else statement. In R, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally, and you will need them once you start writing your own functions and packages.\nHere is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of a unless a is 0:\na \u0026lt;- 0\rif(a!=0){\rprint(1/a)\r} else{\rprint(\u0026quot;No reciprocal for 0.\u0026quot;)\r}\r## [1] \u0026quot;No reciprocal for 0.\u0026quot;\rLet’s look at one more example using the US murders data frame:\nlibrary(dslabs)\rdata(murders)\rmurder_rate \u0026lt;- murders$total / murders$population*100000\rHere is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The if statement protects us from the case in which no state satisfies the condition.\nind \u0026lt;- which.min(murder_rate)\rif(murder_rate[ind] \u0026lt; 0.5){\rprint(murders$state[ind])\r} else{\rprint(\u0026quot;No state has murder rate that low\u0026quot;)\r}\r## [1] \u0026quot;Vermont\u0026quot;\rIf we try it again with a rate of 0.25, we get a different answer:\nif(murder_rate[ind] \u0026lt; 0.25){\rprint(murders$state[ind])\r} else{\rprint(\u0026quot;No state has a murder rate that low.\u0026quot;)\r}\r## [1] \u0026quot;No state has a murder rate that low.\u0026quot;\rA related function that is very useful is ifelse. This function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. Here is an example:\na \u0026lt;- 0\rifelse(a \u0026gt; 0, 1/a, NA)\r## [1] NA\rThe function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is TRUE, or elements from the vector provided in the third argument, if the entry is FALSE.\na \u0026lt;- c(0, 1, 2, -4, 5)\rresult \u0026lt;- ifelse(a \u0026gt; 0, 1/a, NA)\rThis table helps us see what happened:\r\r\r\ra\r\ris_a_positive\r\ranswer1\r\ranswer2\r\rresult\r\r\r\r\r\r0\r\rFALSE\r\rInf\r\rNA\r\rNA\r\r\r\r1\r\rTRUE\r\r1.00\r\rNA\r\r1.0\r\r\r\r2\r\rTRUE\r\r0.50\r\rNA\r\r0.5\r\r\r\r-4\r\rFALSE\r\r-0.25\r\rNA\r\rNA\r\r\r\r5\r\rTRUE\r\r0.20\r\rNA\r\r0.2\r\r\r\r\rHere is an example of how this function can be readily used to replace all the missing values in a vector with zeros:\ndata(na_example)\rno_nas \u0026lt;- ifelse(is.na(na_example), 0, na_example)\rsum(is.na(no_nas))\r## [1] 0\rTwo other useful functions are any and all. The any function takes a vector of logicals and returns TRUE if any of the entries is TRUE. The all function takes a vector of logicals and returns TRUE if all of the entries are TRUE. Here is an example:\nz \u0026lt;- c(TRUE, TRUE, FALSE)\rany(z)\r## [1] TRUE\rall(z)\r## [1] FALSE\r\rDefining functions\rAs you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector x using the sum and length functions: sum(x)/length(x). Because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the mean function and it is included in base R. However, you will encounter situations in which the function does not already exist, so R permits you to write your own. A simple version of a function that computes the average can be defined like this:\navg \u0026lt;- function(x){\rs \u0026lt;- sum(x)\rn \u0026lt;- length(x)\rs/n\r}\rNow avg is a function that computes the mean:\nx \u0026lt;- 1:100\ridentical(mean(x), avg(x))\r## [1] TRUE\rNotice that variables defined inside a function are not saved in the workspace. So while we use s and n when we call avg, the values are created and changed only during the call. Here is an illustrative example:\ns \u0026lt;- 3\ravg(1:10)\r## [1] 5.5\rs\r## [1] 3\rNote how s is still 3 after we call avg.\nIn general, functions are objects, so we assign them to variable names with \u0026lt;-. The function function tells R you are about to define a function. The general form of a function definition looks like this:\nmy_function \u0026lt;- function(VARIABLE_NAME){\rperform operations on VARIABLE_NAME and calculate VALUE\rVALUE\r}\rThe functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this:\navg \u0026lt;- function(x, arithmetic = TRUE){\rn \u0026lt;- length(x)\rifelse(arithmetic, sum(x)/n, prod(x)^(1/n))\r}\rWe will learn more about how to create functions through experience as we face more complex tasks.\n\rNamespaces\rOnce you start becoming more of an R expert user, you will likely need to load several add-on packages for some of your analysis. Once you start doing this, it is likely that two packages use the same name for two different functions. And often these functions do completely different things. In fact, you have already encountered this because both dplyr and the R-base stats package define a filter function. There are five other examples in dplyr. We know this because when we first load dplyr we see the following message:\nThe following objects are masked from ‘package:stats’:\rfilter, lag\rThe following objects are masked from ‘package:base’:\rintersect, setdiff, setequal, union\rSo what does R do when we type filter? Does it use the dplyr function or the stats function? From our previous work we know it uses the dplyr one. But what if we want to use the stats version?\nThese functions live in different namespaces. R will follow a certain order when searching for a function in these namespaces. You can see the order by typing:\nsearch()\rThe first entry in this list is the global environment which includes all the objects you define.\nSo what if we want to use the stats filter instead of the dplyr filter but dplyr appears first in the search list? You can force the use of a specific namespace by using double colons (::) like this:\nstats::filter\rIf we want to be absolutely sure that we use the dplyr filter, we can use\ndplyr::filter\rAlso note that if we want to use a function in a package without loading the entire package, we can use the double colon as well.\nFor more on this more advanced topic we recommend the R packages book1.\n\rFor-loops\rIf we had to write this section in a single sentence, it would be: Don’t use for-loops. Looping is intuitive, but R is designed to provide more computationally efficient solutions. For-loops should be considered a quick-and-dirty way to get an answer. But, hey, you live your own life. Below we provide a brief overview to for-looping.\nThe formula for the sum of the series \\(1+2+\\dots+n\\) is \\(n(n+1)/2\\). What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the \\(S_n\\):\ncompute_s_n \u0026lt;- function(n){\rx \u0026lt;- 1:n\rsum(x)\r}\rHow can we compute \\(S_n\\) for various values of \\(n\\), say \\(n=1,\\dots,25\\)? Do we write 25 lines of code calling compute_s_n? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over, and the only thing that is changing is the value of \\(n\\). For-loops let us define the range that our variable takes (in our example \\(n=1,\\dots,10\\)), then change the value and evaluate expression as you loop.\nPerhaps the simplest example of a for-loop is this useless piece of code:\nfor(i in 1:5){\rprint(i)\r}\r## [1] 1\r## [1] 2\r## [1] 3\r## [1] 4\r## [1] 5\rHere is the for-loop we would write for our \\(S_n\\) example:\nm \u0026lt;- 25\rs_n \u0026lt;- vector(length = m) # create an empty vector\rfor(n in 1:m){\rs_n[n] \u0026lt;- compute_s_n(n)\r}\rIn each iteration \\(n=1\\), \\(n=2\\), etc…, we compute \\(S_n\\) and store it in the \\(n\\)th entry of s_n.\nNow we can create a plot to search for a pattern:\nn \u0026lt;- 1:m\rplot(n, s_n)\rIf you noticed that it appears to be a quadratic, you are on the right track because the formula is \\(n(n+1)/2\\).\r--\n\rVectorization and functionals\rAlthough for-loops are an important concept to understand, in R we rarely use them. As you learn more R, you will realize that vectorization is preferred over for-loops since it results in shorter and clearer code. (It’s also vastly more efficient computationally, which can matter as your data grows.) A vectorized function is a function that will apply the same operation on each of the vectors.\nx \u0026lt;- 1:10\rsqrt(x)\r## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\r## [9] 3.000000 3.162278\ry \u0026lt;- 1:10\rx*y\r## [1] 1 4 9 16 25 36 49 64 81 100\rTo make this calculation, there is no need for for-loops. However, not all functions work this way. For instance, the function we just wrote, compute_s_n, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of n:\nn \u0026lt;- 1:25\rcompute_s_n(n)\rFunctionals are functions that help us apply the same function to each entry in a vector, matrix, data frame, or list. Here we cover the functional that operates on numeric, logical, and character vectors: sapply.\nThe function sapply permits us to perform element-wise operations on any function. Here is how it works:\nx \u0026lt;- 1:10\rsapply(x, sqrt)\r## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\r## [9] 3.000000 3.162278\rEach element of x is passed on to the function sqrt and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original x. This implies that the for-loop above can be written as follows:\nn \u0026lt;- 1:25\rs_n \u0026lt;- sapply(n, compute_s_n)\rOther functionals are apply, lapply, tapply, mapply, vapply, and replicate. We mostly use sapply, apply, and replicate in this book, but we recommend familiarizing yourselves with the others as they can be very useful.\nEXERCISES\nWhat will this conditional expression return?\r\rx \u0026lt;- c(1,2,-3,4)\rif(all(x\u0026gt;0)){\rprint(\u0026quot;All Postives\u0026quot;)\r} else{\rprint(\u0026quot;Not all positives\u0026quot;)\r}\rWhich of the following expressions is always FALSE when at least one entry of a logical vector x is TRUE?\r\rall(x)\rany(x)\rany(!x)\rall(!x)\r\rThe function nchar tells you how many characters long a character vector is. Write a line of code that assigns to the object new_names the state abbreviation when the state name is longer than 8 characters.\n\rCreate a function sum_n that for any given value, say \\(n\\), computes the sum of the integers from 1 to n (inclusive). Use the function to determine the sum of integers from 1 to 5,000.\n\rCreate a function altman_plot that takes two arguments, x and y, and plots the difference against the sum.\n\rAfter running the code below, what is the value of x?\n\r\rx \u0026lt;- 3\rmy_func \u0026lt;- function(y){\rx \u0026lt;- 5\ry+5\r}\rWrite a function compute_s_n that for any given \\(n\\) computes the sum \\(S_n = 1^2 + 2^2 + 3^2 + \\dots n^2\\). Report the value of the sum when \\(n=10\\).\n\rDefine an empty numerical vector s_n of size 25 using s_n \u0026lt;- vector(\"numeric\", 25) and store in the results of \\(S_1, S_2, \\dots S_{25}\\) using a for-loop.\n\rRepeat exercise 8, but this time use sapply.\n\rRepeat exercise 8, but this time use map_dbl.\n\rPlot \\(S_n\\) versus \\(n\\). Use points defined by \\(n=1,\\dots,25\\).\n\rConfirm that the formula for this sum is \\(S_n= n(n+1)(2n+1)/6\\).\n\r\r\r\r\r\rhttp://r-pkgs.had.co.nz/namespace.html↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609269967,"objectID":"5e86e029830987df59b0fed9d67636a4","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/00-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/00-assignment/","section":"assignment","summary":"Programming basics\rConditional expressions\rDefining functions\rNamespaces\rFor-loops\rVectorization and functionals\r\r\r\rNOTE:\nAs you read through this assignment, practice with each of the examples (copy-paste them into an empty R script and run them). At the bottom of this page you will find the questions that comprise the assignment. These questions apply and expand on the topics and R functions in the assignment.\nRight-click to download the homework .","tags":null,"title":"Programming Basics in R","type":"docs"},{"authors":null,"categories":null,"content":"\r\rToday’s example will come from the “Content” tab.\nWe may use last week’s dataset (it is covered in Lab 9). You can find it below.\n\r bank.csv\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604954267,"objectID":"0d7091da7131dcaeb0a7a2758ca2db8e","permalink":"https://datavizm20.classes.andrewheiss.com/example/10-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/10-example/","section":"example","summary":"\r\rToday’s example will come from the “Content” tab.\nWe may use last week’s dataset (it is covered in Lab 9). You can find it below.\n\r bank.csv\r\r\r","tags":null,"title":"Illustrating Classification","type":"docs"},{"authors":null,"categories":null,"content":"\r\rIntroduction to Examples\rGetting started with R and RStudio\rThe R console\rScripts\rRStudio\rThe panes\rKey bindings\rRunning commands while editing scripts\r\rInstalling R packages\r\r\r\rIntroduction to Examples\rExamples in this class are designed to be presented in-class. Accordingly, the notes here are not comprehensive. Instead, they are intended to guide students through\nI’m also aware that my writing is dry and lifeless. If you’re reading this online without the advantage of seeing it in person, don’t worry—I’ll be “funnier” in class.1\nGetting started with R and RStudio\rR is not a programming language like C or Java. It was not created by software engineers for software development. Instead, it was developed by statisticians as an interactive environment for data analysis. You can read the full history in the paper A Brief History of S2. The interactivity is an indispensable feature in data science because, as you will soon learn, the ability to quickly explore data is a necessity for success in this field. However, like in other programming languages, you can save your work as scripts that can be easily executed at any moment. These scripts serve as a record of the analysis you performed, a key feature that facilitates reproducible work. If you are an expert programmer, you should not expect R to follow the conventions you are used—assuming this will leave you disappointed. If you are patient, you will come to appreciate the unequal power of R when it comes to data analysis and data visualization.\nOther attractive features of R are:\nR is free and open source3.\rIt runs on all major platforms: Windows, Mac OS, UNIX/Linux.\rScripts and data objects can be shared seamlessly across platforms.\rThere is a large, growing, and active community of R users and, as a result, there are numerous resources for learning and asking questions4 5 6.\rIt is easy for others to contribute add-ons which enables developers to share software implementations of new data science methodologies. The latest methods and tools are developed in R for a wide variety of disciplines and since social science is so broad, R is one of the few tools that spans the varied social sciences.\r\r\rThe R console\rInteractive data analysis usually occurs on the R console that executes commands as you type them. There are several ways to gain access to an R console. One way is to simply start R on your computer. The console looks something like this:\nAs a quick example, try using the console to calculate a 15% tip on a meal that cost $19.71:7\n0.15 * 19.71 \r## [1] 2.9565\rNote that in this course (at least, on most browsers), grey boxes are used to show R code typed into the R console. The symbol ## is used to denote what the R console outputs.\n\rScripts\rOne of the great advantages of R over point-and-click analysis software is that you can save your work as scripts. You can edit and save these scripts using a text editor. The material in this course was developed using the interactive integrated development environment (IDE) RStudio8. RStudio includes an editor with many R specific features, a console to execute your code, and other useful panes, including one to show figures.\nMost web-based R consoles also provide a pane to edit scripts, but not all permit you to save the scripts for later use. On the upper-right part of this webpage you’ll see a little button with the R logo. You can access a web-based console there.\n\rRStudio\rRStudio will be our launching pad for data science projects. It not only provides an editor for us to create and edit our scripts but also provides many other useful tools. In this section, we go over some of the basics.\nThe panes\rWhen you start RStudio for the first time, you will see three panes. The left pane shows the R console. On the right, the top pane includes tabs such as Environment and History, while the bottom pane shows five tabs: File, Plots, Packages, Help, and Viewer (these tabs may change in new versions). You can click on each tab to move across the different features.\nTo start a new script, you can click on File, then New File, then R Script.\nThis starts a new pane on the left and it is here where you can start writing your script.\n\rKey bindings\rMany tasks we perform with the mouse can be achieved with a combination of key strokes instead. These keyboard versions for performing tasks are referred to as key bindings. For example, we just showed how to use the mouse to start a new script, but you can also use a key binding: Ctrl+Shift+N on Windows and command+shift+N on the Mac.\nAlthough in this tutorial we often show how to use the mouse, we highly recommend that you memorize key bindings for the operations you use most. RStudio provides a useful cheat sheet with the most widely used commands. You might want to keep this handy so you can look up key-bindings when you find yourself performing repetitive point-and-clicking.\n\rRunning commands while editing scripts\rThere are many editors specifically made for coding. These are useful because color and indentation are automatically added to make code more readable. RStudio is one of these editors, and it was specifically developed for R. One of the main advantages provided by RStudio over other editors is that we can test our code easily as we edit our scripts. Below we show an example.\nLet’s start by opening a new script as we did before. A next step is to give the script a name. We can do this through the editor by saving the current new unnamed script. To do this, click on the save icon or use the key binding Ctrl+S on Windows and command+S on the Mac.\nWhen you ask for the document to be saved for the first time, RStudio will prompt you for a name. A good convention is to use a descriptive name, with lower case letters, no spaces, only hyphens to separate words, and then followed by the suffix .R. We will call this script my-first-script.R.\nNow we are ready to start editing our first script. The first lines of code in an R script are dedicated to loading the libraries we will use. Another useful RStudio feature is that once we type library() it starts auto-completing with libraries that we have installed. Note what happens when we type library(ti):\nAnother feature you may have noticed is that when you type library( the second parenthesis is automatically added. This will help you avoid one of the most common errors in coding: forgetting to close a parenthesis.\nNow we can continue to write code. As an example, we will make a graph showing murder totals versus population totals by state. Once you are done writing the code needed to make this plot, you can try it out by executing the code. To do this, click on the Run button on the upper right side of the editing pane. You can also use the key binding: Ctrl+Shift+Enter on Windows or command+shift+return on the Mac.\nOnce you run the code, you will see it appear in the R console and, in this case, the generated plot appears in the plots console. Note that the plot console has a useful interface that permits you to click back and forward across different plots, zoom in to the plot, or save the plots as files.\nTo run one line at a time instead of the entire script, you can use Control-Enter on Windows and command-return on the Mac.\nSETUP TIP\nChange the option Save workspace to .RData on exit to Never and uncheck the Restore .RData into workspace at start. By default, when you exit R saves all the objects you have created into a file called .RData. This is done so that when you restart the session in the same folder, it will load these objects. I find that this causes confusion especially when sharing code with colleagues or peers.\n\r\r\rInstalling R packages\rThe functionality provided by a fresh install of R is only a small fraction of what is possible. In fact, we refer to what you get after your first install as base R. The extra functionality comes from add-ons available from developers. There are currently hundreds of these available from CRAN and many others shared via other repositories such as GitHub. However, because not everybody needs all available functionality, R instead makes different components available via packages. R makes it very easy to install packages from within R. For example, to install the dslabs package, which we use to share datasets and code related to this book, you would type:\ninstall.packages(\u0026quot;dslabs\u0026quot;)\rIn RStudio, you can navigate to the Tools tab and select install packages. We can then load the package into our R sessions using the library function:\nlibrary(dslabs)\rAs you go through this book, you will see that we load packages without installing them. This is because once you install a package, it remains installed and only needs to be loaded with library. The package remains loaded until we quit the R session. If you try to load a package and get an error, it probably means you need to\rinstall it first.\nWe can install more than one package at once by feeding a character vector to this function:\ninstall.packages(c(\u0026quot;tidyverse\u0026quot;, \u0026quot;dslabs\u0026quot;))\rOne advantage of using RStudio is that it auto-completes package names once you start typing, which is helpful when you do not remember the exact spelling of the package. Once you select your package, we recommend selecting all the defaults. Note that installing tidyverse actually installs several packages. This commonly occurs when a package has dependencies, or uses functions from other packages. When you load a package using library, you also load its dependencies.\nOnce packages are installed, you can load them into R and you do not need to install them again, unless you install a fresh version of R. Remember packages are installed in R not RStudio.\nIt is helpful to keep a list of all the packages you need for your work in a script because if you need to perform a fresh install of R, you can re-install all your packages by simply running a script.\nYou can see all the packages you have installed using the following function:\ninstalled.packages()\rAs we move through this course, we will constantly be adding to our toolbox of packages. Accordingly, you will need to keep track to ensure you have the requisite package for any given lecture.\n\r\r\rComments from previous classes indicate that I am not, in fact, funny.↩︎\n\rhttps://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf↩︎\n\rhttps://opensource.org/history↩︎\n\rhttps://stats.stackexchange.com/questions/138/free-resources-for-learning-r↩︎\n\rhttps://www.r-project.org/help.html↩︎\n\rhttps://stackoverflow.com/documentation/r/topics↩︎\n\rBut probably tip more than 15%. Times are tough, man.↩︎\n\rhttps://www.rstudio.com/↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594409288,"objectID":"bbf45ee74dc37731d7fd26186d3a77a6","permalink":"https://datavizm20.classes.andrewheiss.com/example/00-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/00-example/","section":"example","summary":"Introduction to Examples\rGetting started with R and RStudio\rThe R console\rScripts\rRStudio\rThe panes\rKey bindings\rRunning commands while editing scripts\r\rInstalling R packages\r\r\r\rIntroduction to Examples\rExamples in this class are designed to be presented in-class. Accordingly, the notes here are not comprehensive. Instead, they are intended to guide students through\nI’m also aware that my writing is dry and lifeless. If you’re reading this online without the advantage of seeing it in person, don’t worry—I’ll be “funnier” in class.","tags":null,"title":"Working with R and RStudio","type":"docs"},{"authors":null,"categories":null,"content":"\r\rBackstory and Set Up\r\r\rYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, November 9.\n\rBackstory and Set Up\rYou work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.) They’ve given you a dataset on defaults (encoded as the variable y). You’re going to try to predict this.\nThis is some new data. The snippet below loads it.\nbank \u0026lt;- read.table(\u0026quot;https://msudataanalytics.github.io/SSC442/Labs/data/bank.csv\u0026quot;,\rheader = TRUE,\rsep = \u0026quot;,\u0026quot;)\rThere’s not going to be a whole lot of wind-up here. You should be well-versed in doing these sorts of things by now (if not, look back at the previous lab for sample code).\nEXERCISE 1\nSplit the data into an 80/20 train vs. test split. Make sure you explicitly set the seed for replicability, but do not share your seed with others in the class. (We may compare some results across people.)\n\rRun a series of logistic regressions with between 1 and 4 predictors.\n\rCreate eight total confusion matrices: four by applying your models to the training data, and four by applying your models to the test data. Briefly discuss your findings.\n\r\r\rFor those who did not already complete the weekly writing, your assignment is to complete #1 - #5 of the second “Try It” section in the content tab for this week. (The first question begins “1. Install and load the Lahman library.”)\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609269967,"objectID":"4efa69e572576f44efce024275312c9f","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/11-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/11-assignment/","section":"assignment","summary":"Backstory and Set Up\r\r\rYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, November 9.\n\rBackstory and Set Up\rYou work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.","tags":null,"title":"Applied Logistic Regression","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rUsing ggplot2\r\rHow to use ggplot2 – the too-fast and wholly unclear recipe\r\rMappings Link Data to Things You See\rThe Recipe\r\rMapping Aesthetics vs Setting them\r\r\r\rNOTE\nYou must turn in a PDF document of your R markdown code. Submit this to D2L by 11:59 PM on Monday.\n\rOur primary tool for data visualization in the course will be ggplot. Technically, we’re using ggplot2; the o.g. version lacked some of the modern features of its big brother. ggplot2 implements the grammar of graphics, a coherent and relatively straightforward system for describing and building graphs. With ggplot2, you can do more faster by learning one system and applying it in many places. Other languages provide more specific tools, but require you to learn a different tool for each application. In this class, we’ll dig into a single package for our visuals.\nUsing ggplot2\rIn order to get our hands dirty, we will first have to load ggplot2. To do this, and to access the datasets, help pages, and functions that we will use in this assignment, we will load the so-called tidyverse by running this code:\nlibrary(tidyverse)\rIf you run this code and get an error message “there is no package called ‘tidyverse’”, you’ll need to first install it, then run library() once again. To install packages in R, we utilize the simple function install.packages(). In this case, we would write:\ninstall.packages(\u0026quot;tidyverse\u0026quot;)\rlibrary(tidyverse)\rOnce we’re up and running, we’re ready to dive into some basic exercises. ggplot2 works by specifying the connections between the variables in the data and the colors, points, and shapes you see on the screen. These logical connections are called aesthetic mappings or simply aesthetics.\nHow to use ggplot2 – the too-fast and wholly unclear recipe\r\rdata =: Define what your data is. For instance, below we’ll use the mpg data frame found in ggplot2 (by using ggplot2::mpg). As a reminder, a data frame is a rectangular collection of variables (in the columns) and observations (in the rows). This structure of data is often called a “table” but we’ll try to use terms slightly more precisely. The mpg data frame contains observations collected by the US Environmental Protection Agency on 38 different models of car.\n\rmapping = aes(...): How to map the variables in the data to aesthetics\n\rAxes, size of points, intensities of colors, which colors, shape of points, lines/points\r\rThen say what type of plot you want:\n\rboxplot, scatterplot, histogram, …\rthese are called ‘geoms’ in ggplot’s grammar, such as geom_point() giving scatter plots\r\r\rlibrary(ggplot2)\r... + geom_point() # Produces scatterplots\r... + geom_bar() # Bar plots\r.... + geom_boxplot() # boxplots\r... #\rYou link these steps by literally adding them together with + as we’ll see.\nTry it: What other types of plots are there? Try to find several more geom_ functions.\n\r\rMappings Link Data to Things You See\rlibrary(gapminder)\rlibrary(ggplot2)\rgapminder\r## # A tibble: 1,704 x 6\r## country continent year lifeExp pop gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan Asia 1952 28.8 8425333 779.\r## 2 Afghanistan Asia 1957 30.3 9240934 821.\r## 3 Afghanistan Asia 1962 32.0 10267083 853.\r## 4 Afghanistan Asia 1967 34.0 11537966 836.\r## 5 Afghanistan Asia 1972 36.1 13079460 740.\r## 6 Afghanistan Asia 1977 38.4 14880372 786.\r## 7 Afghanistan Asia 1982 39.9 12881816 978.\r## 8 Afghanistan Asia 1987 40.8 13867957 852.\r## 9 Afghanistan Asia 1992 41.7 16317921 649.\r## 10 Afghanistan Asia 1997 41.8 22227415 635.\r## # … with 1,694 more rows\rp \u0026lt;- ggplot(data = gapminder,\rmapping = aes(x = gdpPercap, y = lifeExp))\rp + geom_point()\rAbove we’ve loaded a different dataset and have started to explore a particular relationship. Before putting in this code yourself, try to intuit what might be going on.\nAny ideas?\nHere’s a breakdown of everything that happens after the p\u0026lt;- ggplot() call:\n\rdata = gapminder tells ggplot to use gapminder dataset, so if variable names are mentioned, they should be looked up in gapminder\rmapping = aes(...) shows that the mapping is a function call. There is a deeper logic to this that I will disucss below, but it’s easiest to simply accept that this is how you write it. Put another way, the mapping = aes(...) argument links variables to things you will see on the plot.\raes(x = gdpPercap, y = lifeExp) maps the GDP data onto x, which is a known aesthetic (the x-coordinate) and life expectancy data onto x\r\rx and y are predefined names that are used by ggplot and friends\r\r\rExercise 1:\nLet’s return to the mpg data. Among the variables in mpg are:\n\rdispl, a car’s engine size, in litres.\n\rhwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance.\n\r\rGenerate a scatterplot between these two variables. Does it capture the intuitive relationship you expected? What happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\rIt turns out there’s a reason for doing all of this:\n\r“The greatest value of a picture is when it forces us to notice what we never expected to see.”\" — John Tukey\n\rIn the plot you made above, one group of points seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars?\nLet’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular).\nYou can add a third variable, like class, to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its aesthetic properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe aesthetic properties. Thus, we are interested in exploring class as a level.\nYou can convey information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. To map an aesthetic to a variable, associate the name of the aesthetic to the name of the variable inside aes(). ggplot2 will automatically assign a unique level of the aesthetic (here a unique color) to each unique value of the variable, a process known as scaling. ggplot2 will also add a legend that explains which levels correspond to which values.\nExercise 2:\nUsing your previous scatterplot of displ and hwy, map the colors of your points to the class variable to reveal the class of each car. What conclusions can we make?\n\rLet’s explore our previously saved p in greater detail. As with Exercise 1, we’ll add a layer. This says how some data gets turned into concrete visual aspects.\np + geom_point()\rp + geom_smooth()\rNote: Both of the above geom’s use the same mapping, where the x-axis represents gdpPercap and the y-axis represents lifeExp. You can find this yourself with some ease. But the first one maps the data to individual points, the other one maps it to a smooth line with error ranges.\nWe get a message that tells us that geom_smooth() is using the method = ‘gam’, so presumably we can use other methods. Let’s see if we can figure out which other methods there are.\n?geom_smooth\rp + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ...)\rp + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ..., color = \u0026quot;red\u0026quot;)\rYou may start to see why ggplot2’s way of breaking up tasks is quite powerful: the geometric objects can all reuse the same mapping of data to aesthetics, yet the results are quite different. And if we want later geoms to use different mappings, then we can override them – but it isn’t necessary.\nConsider the output we’ve explored thus far. One potential issue lurking in the data is that most of it is bunched to the left. If we instead used a logarithmic scale, we should be able to spread the data out better.\np + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) + scale_x_log10()\rTry it: Describe what the scale_x_log10() does. Why is it a more evenly distributed cloud of points now? (2-3 sentences.)\nNice. We’re starting to get somewhere. But, you might notice that the x-axis now has scientific notation. Let’s change that.\nlibrary(scales)\rp + geom_point() +\rgeom_smooth(method = \u0026quot;lm\u0026quot;) +\rscale_x_log10(labels = scales::dollar)\rp + geom_point() +\rgeom_smooth(method = \u0026quot;lm\u0026quot;) +\rscale_x_log10(labels = scales::...)\rTry it: What does the dollar() call do? How can you find other ways of relabeling the scales when using scale_x_log10()?\n?dollar()\r\rThe Recipe\rTell the ggplot() function what our data is.\rTell ggplot() what relationships we want to see. For convenience we will put the results of the first two steps in an object called p.\rTell ggplot how we want to see the relationships in our data.\rLayer on geoms as needed, by adding them on the p object one at a time.\rUse some additional functions to adjust scales, labels, tickmarks, titles.\r\r\re.g. scale_, labs(), and guides() functions\r\rAs you start to run more R code, you’re likely to run into problems. Don’t worry — it happens to everyone. I have been writing code in numerous languages for years, and every day I still write code that doesn’t work. Sadly, R is particularly persnickity, and its error messages are often opaque.\nStart by carefully comparing the code that you’re running to the code in these notes. R is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every \" is paired with another \". Sometimes you’ll run the code and nothing happens. Check the left-hand of your console: if it’s a +, it means that R doesn’t think you’ve typed a complete expression and it’s waiting for you to finish it. In this case, it’s usually easy to start from scratch again by pressing ESCAPE to abort processing the current command.\nOne common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start.\nMapping Aesthetics vs Setting them\rp \u0026lt;- ggplot(data = gapminder,\rmapping = aes(x = gdpPercap, y = lifeExp, color = \u0026#39;yellow\u0026#39;))\rp + geom_point() + scale_x_log10()\rThis is interesting (or annoying): the points are not yellow. How can we tell ggplot to draw yellow points?\np \u0026lt;- ggplot(data = gapminder,\rmapping = aes(x = gdpPercap, y = lifeExp, ...))\rp + geom_point(...) + scale_x_log10()\rTry it: describe in your words what is going on.\rOne way to avoid such mistakes is to read arguments inside aes(\u0026lt;property\u0026gt; = \u0026lt;variable\u0026gt;)as the property  in the graph is determined by the data in .\nTry it: Write the above sentence for the original call aes(x = gdpPercap, y = lifeExp, color = 'yellow').\nAesthetics convey information about a variable in the dataset, whereas setting the color of all points to yellow conveys no information about the dataset - it changes the appearance of the plot in a way that is independent of the underlying data.\nRemember: color = 'yellow' and aes(color = 'yellow') are very different, and the second makes usually no sense, as 'yellow' is treated as data.\np \u0026lt;- ggplot(data = gapminder,\rmapping = aes(x = gdpPercap, y = lifeExp))\rp + geom_point() + geom_smooth(color = \u0026quot;orange\u0026quot;, se = FALSE, size = 8, method = \u0026quot;lm\u0026quot;) + scale_x_log10()\rTry it: Write down what all those arguments in geom_smooth(...) do.\np + geom_point(alpha = 0.3) +\rgeom_smooth(method = \u0026quot;gam\u0026quot;) +\rscale_x_log10(labels = scales::dollar) +\rlabs(x = \u0026quot;GDP Per Capita\u0026quot;, y = \u0026quot;Life Expectancy in Years\u0026quot;,\rtitle = \u0026quot;Economic Growth and Life Expectancy\u0026quot;,\rsubtitle = \u0026quot;Data Points are country-years\u0026quot;,\rcaption = \u0026quot;Source: Gapminder\u0026quot;)\rColoring by continent:\nlibrary(scales)\rp \u0026lt;- ggplot(data = gapminder,\rmapping = aes(x = gdpPercap, y = lifeExp, color = continent, fill = continent))\rp + geom_point()\rp + geom_point() + scale_x_log10(labels = dollar)\rp + geom_point() + scale_x_log10(labels = dollar) + geom_smooth()\rTry it: What does fill = continent do? What do you think about the match of colors between lines and error bands?\np \u0026lt;- ggplot(data = gapminder,\rmapping = aes(x = gdpPercap, y = lifeExp))\rp + geom_point(mapping = aes(color = continent)) + geom_smooth() + scale_x_log10()\rTry it: Notice how the above code leads to a single smooth line, not one per continent. Why?\nTry it: What is bad about the following example, assuming the graph is the one we want? Think about why you should set aesthetics at the top level rather than at the individual geometry level if that’s your intent.\np \u0026lt;- ggplot(data = gapminder,\rmapping = aes(x = gdpPercap, y = lifeExp))\rp + geom_point(mapping = aes(color = continent)) +\rgeom_smooth(mapping = aes(color = continent, fill = continent)) +\rscale_x_log10() +\rgeom_smooth(mapping = aes(color = continent), method = \u0026quot;gam\u0026quot;)\rExercise 3:\nGenerate two new plots with data = gapminder (note: you’ll need to install the package by the same name if you have not already). Label the axes and the header with clear, easy to understand language. In a few sentences, describe what you’ve visualized and why.\nNote that this is your first foray into ggplot2; accordingly, you should ry to make sure that you do not bite off more than you can chew. We will improve and refine our abilities as we progress through the semester.\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605796661,"objectID":"94cae82c16c517ab19420570e5d8c2ad","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/01-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/01-assignment/","section":"assignment","summary":"Using ggplot2\r\rHow to use ggplot2 – the too-fast and wholly unclear recipe\r\rMappings Link Data to Things You See\rThe Recipe\r\rMapping Aesthetics vs Setting them\r\r\r\rNOTE\nYou must turn in a PDF document of your R markdown code. Submit this to D2L by 11:59 PM on Monday.\n\rOur primary tool for data visualization in the course will be ggplot.","tags":null,"title":"Basics of ggplot","type":"docs"},{"authors":null,"categories":null,"content":"\r\rIntroduction to data visualization\rCode\rVideo\r\r\r\rIntroduction to data visualization\rLooking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:\nlibrary(dslabs)\rdata(murders)\rhead(murders)\r## state abb region population total\r## 1 Alabama AL South 4779736 135\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\r## 4 Arkansas AR South 2915918 93\r## 5 California CA West 37253956 1257\r## 6 Colorado CO West 5029196 65\rWhat do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t learn anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:\nlibrary(tidyverse)\rlibrary(ggthemes)\rlibrary(ggrepel)\rr \u0026lt;- murders %\u0026gt;%\rsummarize(pop=sum(population), tot=sum(total)) %\u0026gt;%\rmutate(rate = tot/pop*10^6) %\u0026gt;% pull(rate)\rmurders %\u0026gt;% ggplot(aes(x = population/10^6, y = total, label = abb)) +\rgeom_abline(intercept = log10(r), lty=2, col=\u0026quot;darkgrey\u0026quot;) +\rgeom_point(aes(color=region), size = 3) +\rgeom_text_repel() +\rscale_x_log10() +\rscale_y_log10() +\rxlab(\u0026quot;Populations in millions (log scale)\u0026quot;) +\rylab(\u0026quot;Total number of murders (log scale)\u0026quot;) +\rggtitle(\u0026quot;US Gun Murders in 2010\u0026quot;) +\rscale_color_discrete(name=\u0026quot;Region\u0026quot;) +\rtheme_economist_white()\rWe are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.\nThe growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing data journalism and including effective infographics as part of their reporting.\nA particularly salient example—given the current state of the world—is a Wall Street Journal article1 showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.\n(Source: Wall Street Journal)\nAnother striking example comes from a New York Times chart2, which summarizes scores from the NYC Regents Exams. As described in\rthe article3, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:\n(Source: New York Times via Amanda Cox)\nThe most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.\nThis is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call exploratory data analysis (EDA). John W. Tukey4, considered the father of EDA, once said,\n\r\r“The greatest value of a picture is when it forces us to notice what we never expected to see.”\n\r\rMany widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.\nData visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty5 and The Best Stats You’ve Ever Seen6, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.\nIt is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.\nToday, we will discuss the basics of data visualization and exploratory data analysis. We will use the ggplot2 package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.\nOf course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:\n\rER Tufte (1983) The visual display of quantitative information.\rGraphics Press.\rER Tufte (1990) Envisioning information. Graphics Press.\rER Tufte (1997) Visual explanations. Graphics Press.\rWS Cleveland (1993) Visualizing data. Hobart Press.\rWS Cleveland (1994) The elements of graphing data. CRC Press.\rA Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach:\rTurning tables into graphs. The American Statistician 56:121-130.\rNB Robbins (2004) Creating more effective graphs. Wiley.\rA Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders.\rN Yau (2013) Data points: Visualization that means something. Wiley.\r\rWe also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:\n\rhttps://shiny.rstudio.com/\rhttps://d3js.org/\r\rCode\rSome of the code from today’s class will be available below after the class.\n\rVideo\rVideo from today’s class will be available below after the class.\n\r\r\rhttp://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e↩︎\n\rhttp://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif↩︎\n\rhttps://www.nytimes.com/2011/02/19/nyregion/19schools.html↩︎\n\rhttps://en.wikipedia.org/wiki/John_Tukey↩︎\n\rhttps://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩︎\n\rhttps://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599753603,"objectID":"059bb398e999a9d10b388c3df2b5644f","permalink":"https://datavizm20.classes.andrewheiss.com/example/01-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/01-example/","section":"example","summary":"Introduction to data visualization\rCode\rVideo\r\r\r\rIntroduction to data visualization\rLooking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:\nlibrary(dslabs)\rdata(murders)\rhead(murders)\r## state abb region population total\r## 1 Alabama AL South 4779736 135\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\r## 4 Arkansas AR South 2915918 93\r## 5 California CA West 37253956 1257\r## 6 Colorado CO West 5029196 65\rWhat do you learn from staring at this table?","tags":null,"title":"Introduction to Visualization","type":"docs"},{"authors":null,"categories":null,"content":"\r\rPreliminaries\rBackground\rR Markdown\r\rTurning everything in\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, September 21.\n\rPreliminaries\rAs always, we will first have to load ggplot2. To do this, we will load the tidyverse by running this code:\nlibrary(tidyverse)\r\rBackground\rThe New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided an interactive map here where you can see the different projects. There’s also a link there to download the complete dataset.\nFor this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).\nAs you hopefully figured out by now, you’ll be doing all your R work in R Markdown. You can use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud), but this is optional. If you decide to do so, either create a new project for this exercise only, or make a project for all your work in this class.\nYou’ll need to download one CSV file and put it somewhere on your computer or upload it to RStudio.cloud—preferably in a folder named data in your project folder. You can download the data from the DOB’s map, or use this link to get it directly:\n\r EssentialConstruction.csv\r\rTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and use it to begin your lab this week. Note: skip this step at your own peril.\n\r 02-lab.Rmd\r\rR Markdown\r(We learned after the first assignment the following.) Many of you have not worked with R Markdown before. That’s okay—we’ll teach you. Importantly, there are resources here to help.\nWriting regular text with R Markdown follows the rules of Markdown. You can make lists; different-size headers, etc. This should be relatively straightfoward; consult the resouces for more information.\nYou’ll also need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\nExercise 1: Essential pandemic construction\nMake the following plots and briefly explain what they show. Note that the included .Rmd file above provides some intial guidance.\nShow the count or proportion of approved projects by borough using a bar chart.\n\rShow the count or proportion of approved projects by category using a lollipop chart\n\rShow the proportion of approved projects by borough and category simultaneously using a heatmap\n\r\rYou don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing fill colors with scale_fill_manual() or with palettes.\nBonus\nOverlay the data from Part 1 above onto a map of NYC. For double bonus, color the boroughs.\n\r\r\rTurning everything in\rWhen you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. If you haven’t already install tinytex) to ensure that works. Upload the PDF file to D2L.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600175601,"objectID":"b6a0ce80cabafe6d7d9f272294abfa85","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/02-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/02-assignment/","section":"assignment","summary":"Preliminaries\rBackground\rR Markdown\r\rTurning everything in\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, September 21.\n\rPreliminaries\rAs always, we will first have to load ggplot2. To do this, we will load the tidyverse by running this code:\nlibrary(tidyverse)\r\rBackground\rThe New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order.","tags":null,"title":"Applying ggplot2 to Real Data","type":"docs"},{"authors":null,"categories":null,"content":"\r\rggplot2\rThe components of a graph\rggplot objects\rGeometries\rAesthetic mappings\rLayers\rTinkering with arguments\r\rGlobal versus local aesthetic mappings\rScales\rLabels and titles\rCategories as colors\rAnnotation, shapes, and adjustments\rAdd-on packages\rPutting it all together\rQuick plots with qplot\rGrids of plots\r\r\r\rggplot2\rExploratory data visualization is perhaps the greatest strength of R. One can quickly go from idea to data to plot with a unique balance of flexibility and ease. For example, Excel may be easier than R for some plots, but it is nowhere near as flexible. D3.js may be more flexible and powerful than R, but it takes much longer to generate a plot. One of the reasons we use R is its incredible flexibility and ease.\nThroughout this course, we will be creating plots using the ggplot21 package.\nlibrary(dplyr)\rlibrary(ggplot2)\rMany other approaches are available for creating plots in R. In fact, the plotting capabilities that come with a basic installation of R are already quite powerful. There are also other packages for creating graphics such as grid and lattice. We chose to use ggplot2 in this course because it breaks plots into components in a way that permits beginners to create relatively complex and aesthetically pleasing plots using syntax that is intuitive and comparatively easy to remember.\nOne reason ggplot2 is generally more intuitive for beginners is that it uses a so-called “grammar of graphics”2, the letters gg in ggplot2. This is analogous to the way learning grammar can help a beginner construct hundreds of different sentences by learning just a handful of verbs, nouns and adjectives without having to memorize each specific sentence. Similarly, by learning a handful of ggplot2 building blocks and its grammar, you will be able to create hundreds of different plots.\nAnother reason ggplot2 is easy for beginners is that its default behavior is carefully chosen to satisfy the great majority of cases and is visually pleasing. As a result, it is possible to create informative and elegant graphs with relatively simple and readable code.\nOne limitation is that ggplot2 is designed to work exclusively with data tables in tidy format (where rows are observations and columns are variables). However, a substantial percentage of datasets that beginners work with are in, or can be converted into, this format. An advantage of this approach is that, assuming that our data is tidy, ggplot2 simplifies plotting code and the learning of grammar for a variety of plots. You should review the previous content about tidy data if you are feeling lost.\nTo use ggplot2 you will have to learn several functions and arguments. These are hard to memorize, so we highly recommend you have the ggplot2 cheat sheet handy. You can get a copy here: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf or simply perform an internet search for “ggplot2 cheat sheet”.\nThe components of a graph\rWe will construct a graph that summarizes the US murders dataset that looks like this:\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main three components to note are:\n\rData: The US murders data table is being summarized. We refer to this as the data component.\rGeometry: The plot above is a scatterplot. This is referred to as the\rgeometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, and boxplot. We will learn more about these in the Data Visualization part of the book.\rAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\r\rWe also note that:\n\rThe points are labeled with the state abbreviations.\rThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\rThere are labels, a title, a legend, and we use the style of The Economist magazine.\r\rWe will now construct the plot piece by piece.\nWe start by loading the dataset:\nlibrary(dslabs)\rdata(murders)\r\rggplot objects\rThe first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:\nggplot(data = murders)\rWe can also pipe the data in as the first argument. So this line of code is equivalent to the one above:\nmurders %\u0026gt;% ggplot()\rIt renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.\nWhat has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:\np \u0026lt;- ggplot(data = murders)\rclass(p)\r## [1] \u0026quot;gg\u0026quot; \u0026quot;ggplot\u0026quot;\rTo render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\nprint(p)\rp\r\rGeometries\rIn ggplot2 we create graphs by adding layers. Layers can define geometries, compute summary statistics, define what scales to use, or even change styles.\rTo add layers, we use the symbol +. In general, a line of code will look like this:\n\rDATA %\u0026gt;% ggplot() + LAYER 1 + LAYER 2 + … + LAYER N\n\rUsually, the first added layer defines the geometry. We want to make a scatterplot. What geometry do we use?\nTaking a quick look at the cheat sheet, we see that the function used to create plots with this geometry is geom_point.\n(Image courtesy of RStudio3. CC-BY-4.0 license4.)\nGeometry function names follow the pattern: geom_X where X is the name of some specific geometry. Some examples include geom_point, geom_bar, and geom_histogram. You’ve already seen a few of these.\nFor geom_point to run properly we need to provide data and a mapping. We have already connected the object p with the murders data table, and if we add the layer geom_point it defaults to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file geom_point help file:\n\u0026gt; Aesthetics\r\u0026gt;\r\u0026gt; geom_point understands the following aesthetics (required aesthetics are in bold):\r\u0026gt;\r\u0026gt; x\r\u0026gt;\r\u0026gt; y\r\u0026gt;\r\u0026gt; alpha\r\u0026gt;\r\u0026gt; colour\rand—although it does not show in bold above—we see that at least two arguments are required: x and y.\n\rAesthetic mappings\rAesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of total murders versus population in millions:\nmurders %\u0026gt;% ggplot() +\rgeom_point(aes(x = population/10^6, y = total))\rWe can drop the x = and y = if we wanted to since these are the first and second expected arguments, as seen in the help page.\nInstead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p \u0026lt;- ggplot(data = murders):\np + geom_point(aes(population/10^6, total))\rThe scale and labels are defined by default when adding this layer. Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error.\n\rLayers\rA second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file, we learn that we supply the mapping between point and label through the label argument of aes. So the code looks like this:\np + geom_point(aes(population/10^6, total)) +\rgeom_text(aes(population/10^6, total, label = abb))\rWe have successfully added a second layer to the plot.\nAs an example of the unique behavior of aes mentioned above, note that this call:\np_test \u0026lt;- p + geom_text(aes(population/10^6, total, label = abb))\ris fine, whereas this call:\np_test \u0026lt;- p + geom_text(aes(population/10^6, total), label = abb)\rwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable.\nTinkering with arguments\rEach geometry function has many arguments other than aes and data. They tend to be specific to the function. For example, in the plot we wish to make, the points are larger than the default size. In the help file we see that size is an aesthetic and we can change it like this:\np + geom_point(aes(population/10^6, total), size = 3) +\rgeom_text(aes(population/10^6, total, label = abb))\rsize is not a mapping: whereas mappings use data from specific observations and need to be inside aes(), operations we want to affect all the points the same way do not need to be included inside aes.\nNow because the points are larger it is hard to see the labels. If we read the help file for geom_text, we see the nudge_x argument, which moves the text slightly to the right or to the left:\np + geom_point(aes(population/10^6, total), size = 3) +\rgeom_text(aes(population/10^6, total, label = abb), nudge_x = 1.5)\rThis is preferred as it makes it easier to read the text. There are alternatives, though, and we will pepper in examples with better labels as we move forward.\n\r\rGlobal versus local aesthetic mappings\rIn the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings:\nargs(ggplot)\r## function (data = NULL, mapping = aes(), ..., environment = parent.frame()) ## NULL\rIf we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. We redefine p:\np \u0026lt;- murders %\u0026gt;% ggplot(aes(population/10^6, total, label = abb))\rand then we can simply write the following code to produce the previous plot:\np + geom_point(size = 3) +\rgeom_text(nudge_x = 1.5)\rWe keep the size and nudge_x arguments in geom_point and geom_text, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in aes then they would apply to both plots. Also note that the geom_point function does not need a label argument and therefore ignores that aesthetic.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\np + geom_point(size = 3) +\rgeom_text(aes(x = 10, y = 800, label = \u0026quot;Hello there!\u0026quot;))\rClearly, the second call to geom_text does not use population and total.\n\rScales\rFirst, our desired scales are in log-scale. This is not the default, so this change needs to be added through a scales layer. A quick look at the cheat sheet reveals the scale_x_continuous function lets us control the behavior of scales. We use them like this:\np + geom_point(size = 3) +\rgeom_text(nudge_x = 0.05) +\rscale_x_continuous(trans = \u0026quot;log10\u0026quot;) +\rscale_y_continuous(trans = \u0026quot;log10\u0026quot;)\rBecause we are in the log-scale now, the nudge must be made smaller.\nThis particular transformation is so common that ggplot2 provides the specialized functions scale_x_log10 and scale_y_log10, which we can use to rewrite the code like this:\np + geom_point(size = 3) +\rgeom_text(nudge_x = 0.05) +\rscale_x_log10() +\rscale_y_log10()\r\rLabels and titles\rSimilarly, the cheat sheet quickly reveals that to change labels and add a title, we use the following functions:\np + geom_point(size = 3) +\rgeom_text(nudge_x = 0.05) +\rscale_x_log10() +\rscale_y_log10() +\rxlab(\u0026quot;Populations in millions (log scale)\u0026quot;) +\rylab(\u0026quot;Total number of murders (log scale)\u0026quot;) +\rggtitle(\u0026quot;US Gun Murders in 2010\u0026quot;)\rWe are almost there! All we have left to do is add color, a legend, and optional changes to the style.\n\rCategories as colors\rWe can change the color of the points using the col argument in the geom_point function. To facilitate demonstration of new features, we will redefine p to be everything except the points layer:\np \u0026lt;- murders %\u0026gt;% ggplot(aes(population/10^6, total, label = abb)) +\rgeom_text(nudge_x = 0.05) +\rscale_x_log10() +\rscale_y_log10() +\rxlab(\u0026quot;Populations in millions (log scale)\u0026quot;) +\rylab(\u0026quot;Total number of murders (log scale)\u0026quot;) +\rggtitle(\u0026quot;US Gun Murders in 2010\u0026quot;)\rand then test out what happens by adding different calls to geom_point. We can make all the points blue by adding the color argument:\np + geom_point(size = 3, color =\u0026quot;blue\u0026quot;)\rThis, of course, is not what we want. We want to assign color depending on the geographical region. A nice default behavior of ggplot2 is that if we assign a categorical variable to color, it automatically assigns a different color to each category and also adds a legend.\nSince the choice of color is determined by a feature of each observation, this is an aesthetic mapping. To map each point to a color, we need to use aes. We use the following code:\np + geom_point(aes(col=region), size = 3)\rThe x and y mappings are inherited from those already defined in p, so we do not redefine them. We also move aes to the first argument since that is where mappings are expected in this function call.\nHere we see yet another useful default behavior: ggplot2 automatically adds a legend that maps color to region. To avoid adding this legend we set the geom_point argument show.legend = FALSE.\n\rAnnotation, shapes, and adjustments\rWe often want to add shapes or annotation to figures that are not derived directly from the aesthetic mapping; examples include labels, boxes, shaded areas, and lines.\nHere we want to add a line that represents the average murder rate for the entire country. Once we determine the per million rate to be \\(r\\), this line is defined by the formula: \\(y = r x\\), with \\(y\\) and \\(x\\) our axes: total murders and population in millions, respectively. In the log-scale this line turns into: \\(\\log(y) = \\log(r) + \\log(x)\\). So in our plot it’s a line with slope 1 and intercept \\(\\log(r)\\). To compute this value, we use our dplyr skills:\nr \u0026lt;- murders %\u0026gt;%\rsummarize(rate = sum(total) / sum(population) * 10^6) %\u0026gt;%\rpull(rate)\rTo add a line we use the geom_abline function. ggplot2 uses ab in the name to remind us we are supplying the intercept (a) and slope (b). The default line has slope 1 and intercept 0 so we only have to define the intercept:\np + geom_point(aes(col=region), size = 3) +\rgeom_abline(intercept = log10(r))\rHere geom_abline does not use any information from the data object.\nWe can change the line type and color of the lines using arguments. Also, we draw it first so it doesn’t go over our points.\np \u0026lt;- p + geom_abline(intercept = log10(r), lty = 2, color = \u0026quot;darkgrey\u0026quot;) +\rgeom_point(aes(col=region), size = 3)\rNote that we have redefined p and used this new p below and in the next section.\nThe default plots created by ggplot2 are already very useful. However, we frequently need to make minor tweaks to the default behavior. Although it is not always obvious how to make these even with the cheat sheet, ggplot2 is very flexible.\nFor example, we can make changes to the legend via the scale_color_discrete function. In our plot the word region is capitalized and we can change it like this:\np \u0026lt;- p + scale_color_discrete(name = \u0026quot;Region\u0026quot;)\r\rAdd-on packages\rThe power of ggplot2 is augmented further due to the availability of add-on packages.\rThe remaining changes needed to put the finishing touches on our plot require the ggthemes and ggrepel packages.\nThe style of a ggplot2 graph can be changed using the theme functions. Several themes are included as part of the ggplot2 package. In fact, for most of the plots in this book, we use a function in the dslabs package that automatically sets a default theme:\nds_theme_set()\rMany other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this:\nlibrary(ggthemes)\rp + theme_economist()\rYou can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead.\nThe final difference has to do with the position of the labels. In our plot, some of the labels fall on top of each other. The add-on package ggrepel includes a geometry that adds labels while ensuring that they don’t fall on top of each other. We simply change geom_text with geom_text_repel.\n\rPutting it all together\rNow that we are done testing, we can write one piece of code that produces our desired plot from scratch.\nlibrary(ggthemes)\rlibrary(ggrepel)\rr \u0026lt;- murders %\u0026gt;%\rsummarize(rate = sum(total) / sum(population) * 10^6) %\u0026gt;%\rpull(rate)\rmurders %\u0026gt;% ggplot(aes(population/10^6, total, label = abb)) +\rgeom_abline(intercept = log10(r), lty = 2, color = \u0026quot;darkgrey\u0026quot;) +\rgeom_point(aes(col=region), size = 3) +\rgeom_text_repel() +\rscale_x_log10() +\rscale_y_log10() +\rxlab(\u0026quot;Populations in millions (log scale)\u0026quot;) +\rylab(\u0026quot;Total number of murders (log scale)\u0026quot;) +\rggtitle(\u0026quot;US Gun Murders in 2010\u0026quot;) +\rscale_color_discrete(name = \u0026quot;Region\u0026quot;) +\rtheme_economist_white()\r\rQuick plots with qplot\rWe have learned the powerful approach to generating visualization with ggplot. However, there are instances in which all we want is to make a quick plot of, for example, a histogram of the values in a vector, a scatterplot of the values in two vectors, or a boxplot using categorical and numeric vectors. We demonstrated how to generate these plots with hist, plot, and boxplot. However, if we want to keep consistent with the ggplot style, we can use the function qplot.\nIf we have values in two vectors, say:\ndata(murders)\rx \u0026lt;- log10(murders$population)\ry \u0026lt;- murders$total\rand we want to make a scatterplot with ggplot, we would have to type something like:\ndata.frame(x = x, y = y) %\u0026gt;%\rggplot(aes(x, y)) +\rgeom_point()\rThis seems like too much code for such a simple plot.\rThe qplot function sacrifices the flexibility provided by the ggplot approach, but allows us to generate a plot quickly.\nqplot(x, y)\rAlthough we won’t discuss qtplot in much detail, you should feel free to use it in the early stages of your data exploration. Once you’re settled on a final design, then move to ggplot.\n\rGrids of plots\rThere are often reasons to graph plots next to each other. The gridExtra package permits us to do that:\nlibrary(gridExtra)\rp1 \u0026lt;- qplot(x)\rp2 \u0026lt;- qplot(x,y)\rgrid.arrange(p1, p2, ncol = 2)\rTRY IT\nStart by loading the dplyr and ggplot2 library as well as the murders and heights data.\nlibrary(dplyr)\rlibrary(ggplot2)\rlibrary(dslabs)\rdata(heights)\rdata(murders)\rWith ggplot2 plots can be saved as objects. For example we can associate a dataset with a plot object like this\r\rp \u0026lt;- ggplot(data = murders)\rBecause data is the first argument we don’t need to spell it out\np \u0026lt;- ggplot(murders)\rand we can also use the pipe:\np \u0026lt;- murders %\u0026gt;% ggplot()\rWhat is class of the object p?\nRemember that to print an object you can use the command print or simply type the object.\rPrint the object p defined in exercise one and describe what you see.\r\rNothing happens.\rA blank slate plot.\rA scatterplot.\rA histogram.\r\rUsing the pipe %\u0026gt;%, create an object p but this time associated with the heights dataset instead of the murders dataset.\n\rWhat is the class of the object p you have just created?\n\rNow we are going to add a layer and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the murders data frame to remind yourself what are the names for these two variables and select the correct answer. Hint: Look at ?murders.\n\r\rstate and abb.\rtotal_murders and population_size.\rtotal and population.\rmurders and size.\r\rTo create the scatterplot we add a layer with geom_point. The aesthetic mappings require us to define the x-axis and y-axis variables, respectively. So the code looks like this:\r\rmurders %\u0026gt;% ggplot(aes(x = , y = )) +\rgeom_point()\rexcept we have to define the two variables x and y. Fill this out with the correct variable names.\nNote that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this:\r\rmurders %\u0026gt;% ggplot(aes(population, total)) +\rgeom_point()\rRemake the plot but now with total in the x-axis and population in the y-axis.\nIf instead of points we want to add text, we can use the geom_text() or geom_label() geometries. The following code\r\rmurders %\u0026gt;% ggplot(aes(population, total)) + geom_label()\rwill give us the error message: Error: geom_label requires the following missing aesthetics: label\nWhy is this?\nWe need to map a character to each point through the label argument in aes.\rWe need to let geom_label know what character to use in the plot.\rThe geom_label geometry does not require x-axis and y-axis values.\rgeom_label is not a ggplot2 command.\r\rRewrite the code above to use abbreviation as the label through aes\n\rChange the color of the labels to blue. How will we do this?\n\r\rAdding a column called blue to murders.\rBecause each label needs a different color we map the colors through aes.\rUse the color argument in ggplot.\rBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\r\rRewrite the code above to make the labels blue.\n\rNow suppose we want to use color to represent the different regions. In this case which of the following is most appropriate:\n\r\rAdding a column called color to murders with the color we want to use.\rBecause each label needs a different color we map the colors through the color argument of aes .\rUse the color argument in ggplot.\rBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\r\rRewrite the code above to make the labels’ color be determined by the state’s region.\n\rNow we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by defining an object p holding the plot we have made up to now\n\r\rp \u0026lt;- murders %\u0026gt;%\rggplot(aes(population, total, label = abb, color = region)) +\rgeom_label()\rTo change the y-axis to a log scale we learned about the scale_x_log10() function. Add this layer to the object p to change the scale and render the plot.\nRepeat the previous exercise but now change both axes to be in the log scale.\n\rNow edit the code above to add the title “Gun murder data” to the plot. Hint: use the ggtitle function.\n\r\r\r\r\r\rhttps://ggplot2.tidyverse.org/↩︎\n\rhttp://www.springer.com/us/book/9780387245447↩︎\n\rhttps://github.com/rstudio/cheatsheets↩︎\n\rhttps://github.com/rstudio/cheatsheets/blob/master/LICENSE↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600351227,"objectID":"ce879375c9ab42490f4d0d112e48c07c","permalink":"https://datavizm20.classes.andrewheiss.com/example/02-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/02-example/","section":"example","summary":"ggplot2\rThe components of a graph\rggplot objects\rGeometries\rAesthetic mappings\rLayers\rTinkering with arguments\r\rGlobal versus local aesthetic mappings\rScales\rLabels and titles\rCategories as colors\rAnnotation, shapes, and adjustments\rAdd-on packages\rPutting it all together\rQuick plots with qplot\rGrids of plots\r\r\r\rggplot2\rExploratory data visualization is perhaps the greatest strength of R. One can quickly go from idea to data to plot with a unique balance of flexibility and ease.","tags":null,"title":"ggplot2: Everything you ever wanted to know","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rGetting started\r\rBonus Exercise\rTurning everything in\rPostscript: how we got this unemployment data\r\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, September 28.\n\rGetting started\rFor this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, we describe how we built this dataset down below).\n\r unemployment.csv\r\rTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n\r 03-lab.Rmd\r\rIn the end, to help you master file organization, we suggest that the structure of your project directory should look something like this:\nyour-project-name\\\r03-lab.Rmd\ryour-project-name.Rproj\rdata\\\runemployment.csv\rThe example for today’s session will be incredibly helpful for this exercise. Reference it.\nFor this week, you need to start making your plots look nice. Label axes. Label the plot. Experiment with themes. Experiment with adding a labs() layer or changing colors. Or, if you’re super brave, try modifying a theme and its elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\nEXERCISE 1\nUse data from the US Bureau of Labor Statistics (BLS) to show the trends in employment rate for all 50 states between 2006 and 2016. What stories does this plot tell? Which states struggled to recover from the 2008–09 recession?\nSome hints/tips:\n\rYou won’t need to filter out any missing rows because the data here is complete—there are no state-year combinations with missing unemployment data.\n\rYou’ll be plotting 51 facets. You can filter out DC if you want to have a better grid (like 5 × 10), or you can try using facet_geo() from the geofacet package to lay out the plots like a map of the US (try this!).\n\rPlot the date column along the x-axis, not the year column. If you plot by year, you’ll get weird looking lines (try it for fun?), since these observations are monthly. If you really want to plot by year only, you’ll need to create a different data frame where you group by year and state and calculate the average unemployment rate for each year/state combination (i.e. group_by(year, state) %\u0026gt;% summarize(avg_unemployment = mean(unemployment)))\n\rTry mapping other aesthetics onto the graph too. You’ll notice there are columns for region and division—play with those as colors, for instance.\n\rThis plot might be big, so make sure you adjust fig.width and fig.height in the chunk options so that it’s visible when you knit it. You might also want to used ggsave() to save it with extra large dimensions.\n\r\rEXERCISE 2\nUse data from the BLS to create a slopegraph that compares the unemployment rate in January 2006 with the unemployment rate in January 2009, either for all 50 states at once (good luck with that!) or for a specific region or division. Make sure the plot doesn’t look too busy or crowded in the end.\nWhat story does this plot tell? Which states in the US (or in the specific region you selected) were the most/least affected the Great Recession?\nSome hints/tips:\n\rYou should use filter() to only select rows where the year is 2006 or 2009 (i.e. filter(year %in% c(2006, 2009)) and to select rows where the month is January (filter(month == 1) or filter(month_name == \"January\"))\n\rIn order for the year to be plotted as separate categories on the x-axis, it needs to be a factor, so use mutate(year = factor(year)) to convert it.\n\rTo make ggplot draw lines between the 2006 and 2009 categories, you need to include group = state in the aesthetics.\n\r\r\rBonus Exercise\rThis is entirely optional but might be fun. Then again, it might not be fun. I don’t know.\nFor extra fun times, if you feel like it, create a bump chart showing something from the unemployment data (perhaps the top 10 states or bottom 10 states in unemployment?) Adapt the code in the example for today’s session.\nIf you do this, plotting 51 lines is going to be a huge mess. But filtering the data is also a bad idea, because states could drop in and out of the top/bottom 10 over time, and we don’t want to get rid of them. Instead, you can zoom in on a specific range of data in your plot with coord_cartesian(ylim = c(1, 10)), for instance.\n\rTurning everything in\rWhen you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. If you haven’t already install tinytex) to ensure that works. Upload the PDF file to D2L.\n\rPostscript: how we got this unemployment data\rFor the curious, here’s the code we used to download the unemployment data from the BLS.\nAnd to pull the curtain back and show how much googling is involved in data visualization (and data analysis and programming in general), here was my process for getting this data:\nWe thought “We want to have students show variation in something domestic over time” and then we googled “us data by state”. Nothing really came up (since it was an exceedingly vague search in the first place), but some results mentioned unemployment rates, so we figured that could be cool.\rWe googled “unemployment statistics by state over time” and found that the BLS keeps statistics on this. We clicked on the “Data Tools” link in their main navigation bar, clicked on “Unemployment”, and then clicked on the “Multi-screen data search” button for the Local Area Unemployment Statistics (LAUS).\rWe walked through the multiple screens and got excited that we’d be able to download all unemployment stats for all states for a ton of years, but then the final page had links to 51 individual Excel files, which was dumb.\rSo we went back to Google and searched for “download bls data r” and found a few different packages people have written to do this. The first one we clicked on was blscrapeR at GitHub, and it looked like it had been updated recently, so we went with it.\rWe followed the examples in the blscrapeR package and downloaded data for every state.\r\rAnother day in the life of doing modern data science. This is an example of something you will be able to do by the end of this class. we had no idea people had written R packages to access BLS data, but there are (at least) 3 packages out there. After a few minutes of tinkering, we got it working and it is relatively straightforward.\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605796661,"objectID":"af0b365ca488e9ad2d9f06d6c238b02e","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/03-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/03-assignment/","section":"assignment","summary":"Getting started\r\rBonus Exercise\rTurning everything in\rPostscript: how we got this unemployment data\r\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, September 28.\n\rGetting started\rFor this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, we describe how we built this dataset down below).","tags":null,"title":"Visualizing Large(ish) Data","type":"docs"},{"authors":null,"categories":null,"content":"\r\rPreliminaries\rComplete code\rLoad and clean data\rSmall multiples\rSparklines\rSlopegraphs\rBump charts\r\r\r\rToday’s example will continue (and conclude) some of the discussion from Tuesday. The code may be useful for future work.\nPreliminaries\rFor today’s example, we’re going to use cross-national data. But instead of using the typical gapminder dataset as with the Tuuesday lecture, we’re going to collect data directly from the World Bank’s Open Data portal\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As”). However, it may be instructive for your group projects to explore the collection process. It’s not particularly hard!\n\r wdi_raw.csv\r\r\rComplete code\rLoad and clean data\rFirst, we load the libraries we’ll be using. Note: there are some new packages below. You will almost surely need to add these. Moreover, these will almost surely throw an error unless you use the dependencies = TRUE argument when installing.\nlibrary(tidyverse) # For ggplot, dplyr, and friends\rlibrary(WDI) # For getting data from the World Bank\rlibrary(geofacet) # For map-shaped facets\rlibrary(scales) # For helpful scale functions like dollar()\rlibrary(ggrepel) # For non-overlapping labels\rThe World Bank has a ton of country-level data at data.worldbank.org. We can use a package named WDI (world development indicators) to access their servers and download the data directly into R.\nTo do this, we need to find the special World Bank codes for specific variables we want to get. These codes come from the URLs of the World Bank’s website. For instance, if you search for “access to electricity” at the World Bank’s website, you’ll find this page. If you look at the end of the URL, you’ll see a cryptic code: EG.ELC.ACCS.ZS. That’s the World Bank’s ID code for the “Access to electricity (% of population)” indicator.\nWe can feed a list of ID codes to the WDI() function to download data for those specific indicators. We want data from 1995-2015, so we set the start and end years accordingly. The extra=TRUE argument means that it’ll also include other helpful details like region, aid status, etc. Without it, it would only download the indicators we listed.\nindicators \u0026lt;- c(\u0026quot;SP.DYN.LE00.IN\u0026quot;, # Life expectancy\r\u0026quot;EG.ELC.ACCS.ZS\u0026quot;, # Access to electricity\r\u0026quot;EN.ATM.CO2E.PC\u0026quot;, # CO2 emissions\r\u0026quot;NY.GDP.PCAP.KD\u0026quot;) # GDP per capita\rwdi_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE,\rstart = 1995, end = 2015)\rhead(wdi_raw)\rDownloading data from the World Bank every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). It’s good practice to save this raw data as a CSV file and then work with that.\nwrite_csv(wdi_raw, \u0026quot;data/wdi_raw.csv\u0026quot;)\rSince we care about reproducibility, we still want to include the code we used to get data from the World Bank, we just don’t want it to actually run. You can include chunks but not run them by setting eval=FALSE in the chunk options. In this little example, we show the code for downloading the data, but we don’t evaluate the chunk. We then include a chunk that loads the data from a CSV file with read_csv(), but we don’t include it (include=FALSE). That way, in the knitted file we see the WDI() code, but in reality it’s loading the data from CSV. Super tricky.\nI first download data from the World Bank:\r```{r get-wdi-data, eval=FALSE}\rwdi_raw \u0026lt;- WDI(...)\rwrite_csv(wdi_raw, \u0026quot;data/wdi_raw.csv\u0026quot;)\r```\r```{r load-wdi-data-real, include=FALSE}\rwdi_raw \u0026lt;- read_csv(\u0026quot;data/wdi_raw.csv\u0026quot;)\r```\rThen we clean up the data a little, filtering out rows that aren’t actually countries and renaming the ugly World Bank code columns to actual words:\nwdi_clean \u0026lt;- wdi_raw %\u0026gt;%\rfilter(region != \u0026quot;Aggregates\u0026quot;) %\u0026gt;%\rselect(iso2c, country, year,\rlife_expectancy = SP.DYN.LE00.IN,\raccess_to_electricity = EG.ELC.ACCS.ZS,\rco2_emissions = EN.ATM.CO2E.PC,\rgdp_per_cap = NY.GDP.PCAP.KD,\rregion, income)\rhead(wdi_clean)\r## # A tibble: 6 x 9\r## iso2c country year life_expectancy access_to_electricity co2_emissions gdp_per_cap region income ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AD Andorra 2015 NA 100 NA 41768. Europe \u0026amp; Central Asia High income\r## 2 AD Andorra 2004 NA 100 7.36 47033. Europe \u0026amp; Central Asia High income\r## 3 AD Andorra 2001 NA 100 7.79 41421. Europe \u0026amp; Central Asia High income\r## 4 AD Andorra 2002 NA 100 7.59 42396. Europe \u0026amp; Central Asia High income\r## 5 AD Andorra 2014 NA 100 5.83 40790. Europe \u0026amp; Central Asia High income\r## 6 AD Andorra 1995 NA 100 6.66 32918. Europe \u0026amp; Central Asia High income\r\rSmall multiples\rFirst we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country.\nlife_expectancy_small \u0026lt;- wdi_clean %\u0026gt;%\rfilter(country %in% c(\u0026quot;Argentina\u0026quot;, \u0026quot;Bolivia\u0026quot;, \u0026quot;Brazil\u0026quot;,\r\u0026quot;Belize\u0026quot;, \u0026quot;Canada\u0026quot;, \u0026quot;Chile\u0026quot;))\rggplot(data = life_expectancy_small,\rmapping = aes(x = year, y = life_expectancy)) +\rgeom_line(size = 1) +\rfacet_wrap(vars(country))\rSmall multiples! That’s all we need to do.\nWe can do some fancier things, though. We can make this plot hyper minimalist:\nggplot(data = life_expectancy_small,\rmapping = aes(x = year, y = life_expectancy)) +\rgeom_line(size = 1) +\rfacet_wrap(vars(country), scales = \u0026quot;free_y\u0026quot;) +\rtheme_void() +\rtheme(strip.text = element_text(face = \u0026quot;bold\u0026quot;))\rWe can do a whole part of a continent (poor Iraq and Syria 😭)\nlife_expectancy_mena \u0026lt;- wdi_clean %\u0026gt;%\rfilter(region == \u0026quot;Middle East \u0026amp; North Africa\u0026quot;)\rggplot(data = life_expectancy_mena,\rmapping = aes(x = year, y = life_expectancy)) +\rgeom_line(size = 1) +\rfacet_wrap(vars(country), scales = \u0026quot;free_y\u0026quot;, nrow = 3) +\rtheme_void() +\rtheme(strip.text = element_text(face = \u0026quot;bold\u0026quot;))\rWe can use the geofacet package to arrange these facets by geography:\nlife_expectancy_eu \u0026lt;- wdi_clean %\u0026gt;%\rfilter(region == \u0026quot;Europe \u0026amp; Central Asia\u0026quot;)\rggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) +\rgeom_line(size = 1) +\rfacet_geo(vars(country), grid = \u0026quot;eu_grid1\u0026quot;, scales = \u0026quot;free_y\u0026quot;) +\rlabs(x = NULL, y = NULL, title = \u0026quot;Life expectancy from 1995–2015\u0026quot;,\rcaption = \u0026quot;Source: The World Bank (SP.DYN.LE00.IN)\u0026quot;) +\rtheme_minimal() +\rtheme(strip.text = element_text(face = \u0026quot;bold\u0026quot;),\rplot.title = element_text(face = \u0026quot;bold\u0026quot;),\raxis.text.x = element_text(angle = 45, hjust = 1))\rNeat!\n\rSparklines\rSparklines are just line charts (or bar charts) that are really really small.\nindia_co2 \u0026lt;- wdi_clean %\u0026gt;%\rfilter(country == \u0026quot;India\u0026quot;)\rplot_india \u0026lt;- ggplot(india_co2, aes(x = year, y = co2_emissions)) +\rgeom_line() +\rtheme_void()\rplot_india\rggsave(\u0026quot;india_co2.pdf\u0026quot;, plot_india, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;)\rggsave(\u0026quot;india_co2.png\u0026quot;, plot_india, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;)\rchina_co2 \u0026lt;- wdi_clean %\u0026gt;%\rfilter(country == \u0026quot;China\u0026quot;)\rplot_china \u0026lt;- ggplot(china_co2, aes(x = year, y = co2_emissions)) +\rgeom_line() +\rtheme_void()\rplot_china\rggsave(\u0026quot;china_co2.pdf\u0026quot;, plot_china, width = 1, heighlt = 0.15, units = \u0026quot;in\u0026quot;)\rggsave(\u0026quot;china_co2.png\u0026quot;, plot_china, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;)\rYou can then use those saved tiny plots in your text.\n\rBoth India and China have seen increased CO2 emissions over the past 20 years.\n\r\rSlopegraphs\rWe can make a slopegraph to show changes in GDP per capita between two time periods. We need to first filter our WDI to include only the start and end years (here 1995 and 2015). Then, to make sure that we’re using complete data, we’ll get rid of any country that has missing data for either 1995 or 2015. The group_by(...) %\u0026gt;% filter(...) %\u0026gt;% ungroup() pipeline does this, with the !any(is.na(gdp_per_cap)) test keeping any rows where any of the gdp_per_cap values are not missing for the whole country.\nWe then add a couple special columns for labels. The paste0() function concatenates strings and variables together, so that paste0(\"2 + 2 = \", 2 + 2) would show “2 + 2 = 4”. Here we make labels that say either “Country name: $GDP” or “$GDP” depending on the year.\ngdp_south_asia \u0026lt;- wdi_clean %\u0026gt;%\rfilter(region == \u0026quot;South Asia\u0026quot;) %\u0026gt;%\rfilter(year %in% c(1995, 2015)) %\u0026gt;%\r# Look at each country individually\rgroup_by(country) %\u0026gt;%\r# Remove the country if any of its gdp_per_cap values are missing\rfilter(!any(is.na(gdp_per_cap))) %\u0026gt;%\rungroup() %\u0026gt;%\r# Make year a factor\rmutate(year = factor(year)) %\u0026gt;%\r# Make some nice label columns\r# If the year is 1995, format it like \u0026quot;Country name: $GDP\u0026quot;. If the year is\r# 2015, format it like \u0026quot;$GDP\u0026quot;\rmutate(label_first = ifelse(year == 1995, paste0(country, \u0026quot;: \u0026quot;, dollar(round(gdp_per_cap))), NA),\rlabel_last = ifelse(year == 2015, dollar(round(gdp_per_cap, 0)), NA))\rWith the data filtered like this, we can plot it by mapping year to the x-axis, GDP per capita to the y-axis, and coloring by country. To make the lines go across the two categorical labels in the x-axis (since we made year a factor/category), we need to also specify the group aesthetic.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\rgeom_line(size = 1.5)\rCool! We’re getting closer. We can definitely see different slopes, but with 7 different colors, it’s hard to see exactly which country is which. Instead, we can directly label each of these lines with geom_text():\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\rgeom_line(size = 1.5) +\rgeom_text(aes(label = country)) +\rguides(color = FALSE)\rThat gets us a little closer, but the country labels are hard to see, and we could include more information, like the actual values. Remember those label_first and label_last columns we made? Let’s use those instead:\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\rgeom_line(size = 1.5) +\rgeom_text(aes(label = label_first)) +\rgeom_text(aes(label = label_last)) +\rguides(color = FALSE)\rNow we have dollar amounts and country names, but the labels are still overlapping and really hard to read. To fix this, we can make the labels repel away from each other and randomly position in a way that makes them not overlap. The ggrepel package lets us do this with geom_text_repel()\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\rgeom_line(size = 1.5) +\rgeom_text_repel(aes(label = label_first)) +\rgeom_text_repel(aes(label = label_last)) +\rguides(color = FALSE)\rNow none of the labels are on top of each other, but the labels are still on top of the lines. Also, some of the labels moved inward and outward along the x-axis, but they don’t need to do that—they just need to shift up and down. We can force the labels to only move up and down by setting the direction = \"y\" argument, and we can move all the labels to the left or right with the nudge_x argument. The seed argument makes sure that the random label placement is the same every time we run this. It can be whatever number you want—it just has to be a number.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\rgeom_line(size = 1.5) +\rgeom_text_repel(aes(label = label_first), direction = \u0026quot;y\u0026quot;, nudge_x = -1, seed = 1234) +\rgeom_text_repel(aes(label = label_last), direction = \u0026quot;y\u0026quot;, nudge_x = 1, seed = 1234) +\rguides(color = FALSE)\rThat’s it! Let’s take the theme off completely, change the colors a little, and it should be perfect.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +\rgeom_line(size = 1.5) +\rgeom_text_repel(aes(label = label_first), direction = \u0026quot;y\u0026quot;, nudge_x = -1, seed = 1234) +\rgeom_text_repel(aes(label = label_last), direction = \u0026quot;y\u0026quot;, nudge_x = 1, seed = 1234) +\rguides(color = FALSE) +\rscale_color_viridis_d(option = \u0026quot;magma\u0026quot;, end = 0.9) +\rtheme_void()\r\rBump charts\rFinally, we can make a bump chart that shows changes in rankings over time. We’ll look at CO2 emissions in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the rank() function to rank countries by the co2_emissions column.\nsa_co2 \u0026lt;- wdi_clean %\u0026gt;%\rfilter(region == \u0026quot;South Asia\u0026quot;) %\u0026gt;%\rfilter(year \u0026gt;= 2004, year \u0026lt; 2015) %\u0026gt;%\rgroup_by(year) %\u0026gt;%\rmutate(rank = rank(co2_emissions))\rWe then plot this with points and lines, reversing the y-axis so 1 is at the top:\nggplot(sa_co2, aes(x = year, y = rank, color = country)) +\rgeom_line() +\rgeom_point() +\rscale_y_reverse(breaks = 1:8)\rAfghanistan and Nepal switched around for the number 1 spot, while India dropped from 4 to 6, switching places with Pakistan.\nAs with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use geom_text() again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the data argument in geom_text() though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner.\nggplot(sa_co2, aes(x = year, y = rank, color = country)) +\rgeom_line(size = 2) +\rgeom_point(size = 4) +\rgeom_text(data = filter(sa_co2, year == 2004),\raes(label = iso2c, x = 2003.25),\rfontface = \u0026quot;bold\u0026quot;) +\rgeom_text(data = filter(sa_co2, year == 2014),\raes(label = iso2c, x = 2014.75),\rfontface = \u0026quot;bold\u0026quot;) +\rguides(color = FALSE) +\rscale_y_reverse(breaks = 1:8) +\rscale_x_continuous(breaks = 2004:2014) +\rscale_color_viridis_d(option = \u0026quot;magma\u0026quot;, begin = 0.2, end = 0.9) +\rlabs(x = NULL, y = \u0026quot;Rank\u0026quot;) +\rtheme_minimal() +\rtheme(panel.grid.major.y = element_blank(),\rpanel.grid.minor.y = element_blank(),\rpanel.grid.minor.x = element_blank())\rIf you want to be super fancy, you can use flags instead of country codes, but that’s a little more complicated (you need to install the ggflags package. See here for an example.\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601143971,"objectID":"73664027ae41c739f0c70a62c901a4c5","permalink":"https://datavizm20.classes.andrewheiss.com/example/03-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/03-example/","section":"example","summary":"Preliminaries\rComplete code\rLoad and clean data\rSmall multiples\rSparklines\rSlopegraphs\rBump charts\r\r\r\rToday’s example will continue (and conclude) some of the discussion from Tuesday. The code may be useful for future work.\nPreliminaries\rFor today’s example, we’re going to use cross-national data. But instead of using the typical gapminder dataset as with the Tuuesday lecture, we’re going to collect data directly from the World Bank’s Open Data portal","tags":null,"title":"Visualizations ","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\r\rStatistical models\r\rPoll aggregators\r\rPoll data\rPollster bias\r\rData-driven models\r\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 5.\n\rFor this exercise you will need to ensure that you’ve carefully read this week’s content and example. We will build on both. The exercises (which you will turn in as this week’s lab) are at the bottom. Note that this week’s lab is much more theoretical than any other week in this class. This is to ensure that you have the foundations necessary to build rich statistical models and apply them to real-world data.\nStatistical models\r\r“All models are wrong, but some are useful.” –George E. P. Box\n\rThe day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. Four years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer. Political commentator Joe Scarborough said during his show1:\n\rAnybody that thinks that this race is anything but a toss-up right now is such an ideologue … they’re jokes.\n\rTo which Nate Silver responded via Twitter:\n\rIf you think it’s a toss-up, let’s bet. If Obama wins, you donate $1,000 to the American Red Cross. If Romney wins, I do. Deal?\n\rIn 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. In contrast, most other forecasters were almost certain she would win. She lost. But 71% is still more than 50%, so was Mr. Silver wrong? And what does probability mean in this context anyway? Are dice being tossed somewhere?\nIn this lab we will demonstrate how poll aggregators, such as FiveThirtyEight, collected and combined data reported by different experts to produce improved predictions. We will introduce ideas behind the statistical models, also known as probability models, that were used by poll aggregators to improve election forecasts beyond the power of individual polls. First, we’ll motivate the models, building on the statistical inference concepts we learned in this week’s content and example. We start with relatively simple models, realizing that the actual data science exercise of forecasting elections involves rather complex ones. We will introduce such modeks towards the end of this section of the course.\nPoll aggregators\rA few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.\nlibrary(tidyverse)\rlibrary(dslabs)\rd \u0026lt;- 0.039\rNs \u0026lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)\rp \u0026lt;- (d + 1) / 2\rpolls \u0026lt;- map_df(Ns, function(N) {\rx \u0026lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))\rx_hat \u0026lt;- mean(x)\rse_hat \u0026lt;- sqrt(x_hat * (1 - x_hat) / N)\rlist(estimate = 2 * x_hat - 1,\rlow = 2*(x_hat - 1.96*se_hat) - 1,\rhigh = 2*(x_hat + 1.96*se_hat) - 1,\rsample_size = N)\r}) %\u0026gt;% mutate(poll = seq_along(Ns))\rHere is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:\nNot surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.\nPoll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.\nAlthough as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:\nsum(polls$sample_size)\r## [1] 11269\rparticipants. Basically, we construct an estimate of the spread, let’s call it \\(d\\), with a weighted average in the following way:\nd_hat \u0026lt;- polls %\u0026gt;%\rsummarize(avg = sum(estimate*sample_size) / sum(sample_size)) %\u0026gt;%\rpull(avg)\rOnce we have an estimate of \\(d\\), we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.0184545.\nThus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.\nOf course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits.\nSince the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. The day before the election the New York Times reported2 the following probabilities for Hillary Clinton winning the presidency:\n\r\r\rNYT\r\r538\r\rHuffPost\r\rPW\r\rPEC\r\rDK\r\rCook\r\rRoth\r\r\r\r\r\rWin Prob\r\r85%\r\r71%\r\r98%\r\r89%\r\r\u0026gt;99%\r\r92%\r\rLean Dem\r\rLean Dem\r\r\r\r\rFor example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton3.\rBy understanding statistical models and how these forecasters use them, we will start to understand how this happened.\nAlthough not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. FiveThirtyEight predicted a 3.6% advantage for Clinton4, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her an 81.4% chance. Their prediction was summarized with a chart like this:\nThe colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model.\rWe introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. To understand the “81.4% chance” statement we need to describe Bayesian statistics, which we do in Sections ?? and ??.\nPoll data\rWe use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of the dslabs package:\ndata(polls_us_election_2016)\rThe table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those:\npolls \u0026lt;- polls_us_election_2016 %\u0026gt;%\rfilter(state == \u0026quot;U.S.\u0026quot; \u0026amp; enddate \u0026gt;= \u0026quot;2016-10-31\u0026quot; \u0026amp;\r(grade %in% c(\u0026quot;A+\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;A-\u0026quot;,\u0026quot;B+\u0026quot;) | is.na(grade)))\rWe add a spread estimate:\npolls \u0026lt;- polls %\u0026gt;%\rmutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\rFor this example, we will assume that there are only two parties and call \\(p\\) the proportion voting for Clinton and \\(1-p\\) the proportion voting for Trump. We are interested in the spread \\(2p-1\\). Let’s call the spread \\(d\\) (for difference).\nWe have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread \\(d\\) and the standard error is \\(2\\sqrt{p (1 - p) / N}\\). Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:\nd_hat \u0026lt;- polls %\u0026gt;%\rsummarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %\u0026gt;%\rpull(d_hat)\rand the standard error is:\np_hat \u0026lt;- (d_hat+1)/2\rmoe \u0026lt;- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))\rmoe\r## [1] 0.006623178\rSo we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?\nA histogram of the reported spreads shows a problem:\npolls %\u0026gt;%\rggplot(aes(spread)) +\rgeom_histogram(color=\u0026quot;black\u0026quot;, binwidth = .01)\rThe data does not appear to be normally distributed and the standard error appears to be larger than 0.0066232. The theory is not quite working here.\n\rPollster bias\rNotice that various pollsters are involved and some are taking several polls a week:\npolls %\u0026gt;% group_by(pollster) %\u0026gt;% summarize(n())\r## `summarise()` ungrouping output (override with `.groups` argument)\r## # A tibble: 15 x 2\r## pollster `n()`\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 ABC News/Washington Post 7\r## 2 Angus Reid Global 1\r## 3 CBS News/New York Times 2\r## 4 Fox News/Anderson Robbins Research/Shaw \u0026amp; Company Research 2\r## 5 IBD/TIPP 8\r## 6 Insights West 1\r## 7 Ipsos 6\r## 8 Marist College 1\r## 9 Monmouth University 1\r## 10 Morning Consult 1\r## 11 NBC News/Wall Street Journal 1\r## 12 RKM Research and Communications, Inc. 1\r## 13 Selzer \u0026amp; Company 1\r## 14 The Times-Picayune/Lucid 8\r## 15 USC Dornsife/LA Times 8\rLet’s visualize the data for the pollsters that are regularly polling:\nThis plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll:\npolls %\u0026gt;% group_by(pollster) %\u0026gt;%\rfilter(n() \u0026gt;= 6) %\u0026gt;%\rsummarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))\r## `summarise()` ungrouping output (override with `.groups` argument)\r## # A tibble: 5 x 2\r## pollster se\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 ABC News/Washington Post 0.0265\r## 2 IBD/TIPP 0.0333\r## 3 Ipsos 0.0225\r## 4 The Times-Picayune/Lucid 0.0196\r## 5 USC Dornsife/LA Times 0.0183\ris between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences across the polls. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We also call them pollster bias.\nIn the following section, rather than use the urn model theory, we are instead going to develop a data-driven model.\n\r\rData-driven models\rFor each pollster, let’s collect their last reported result before the election:\none_poll_per_pollster \u0026lt;- polls %\u0026gt;% group_by(pollster) %\u0026gt;%\rfilter(enddate == max(enddate)) %\u0026gt;%\rungroup()\rHere is a histogram of the data for these 15 pollsters:\nqplot(spread, data = one_poll_per_pollster, binwidth = 0.01)\rIn the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly.\nThe new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We assume that the expected value of our urn is the actual spread \\(d=2p-1\\).\nBecause instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer \\(\\sqrt{p(1-p)}\\). Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol \\(\\sigma\\) is used to represent this parameter.\nIn summary, we have two unknown parameters: the expected value \\(d\\) and the standard deviation \\(\\sigma\\).\nOur task is to estimate \\(d\\). Because we model the observed values \\(X_1,\\dots X_N\\) as a random sample from the urn, the CLT might still work in this situation because it is an average of independent random variables. For a large enough sample size \\(N\\), the probability distribution of the sample average \\(\\bar{X}\\) is approximately normal with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\). If we are willing to consider \\(N=15\\) large enough, we can use this to construct confidence intervals.\nA problem is that we don’t know \\(\\sigma\\). But theory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as\r\\(s = \\sqrt{ \\sum_{i=1}^N (X_i - \\bar{X})^2 / (N-1)}\\).\nUnlike for the population standard deviation definition, we now divide by \\(N-1\\). This makes \\(s\\) a better estimate of \\(\\sigma\\). There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here.\nThe sd function in R computes the sample standard deviation:\nsd(one_poll_per_pollster$spread)\r## [1] 0.02419369\rWe are now ready to form a new confidence interval based on our new data-driven model:\nresults \u0026lt;- one_poll_per_pollster %\u0026gt;%\rsummarize(avg = mean(spread),\rse = sd(spread) / sqrt(length(spread))) %\u0026gt;%\rmutate(start = avg - 1.96 * se,\rend = avg + 1.96 * se)\rround(results * 100, 1)\r## avg se start end\r## 1 2.9 0.6 1.7 4.1\rOur confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.\nEXERCISES\nNote that using dollar signs $ $ to enclose some text is how you make the fancy math you see below. If you installed tinytex or some other Latex distribution in order to render your PDFs, you should be equipped to insert mathematics directly into your .Rmd file.\nIn this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.\r\rdata(polls_us_election_2016)\rpolls \u0026lt;- polls_us_election_2016 %\u0026gt;%\rfilter(pollster %in% c(\u0026quot;Rasmussen Reports/Pulse Opinion Research\u0026quot;,\r\u0026quot;The Times-Picayune/Lucid\u0026quot;) \u0026amp;\renddate \u0026gt;= \u0026quot;2016-10-15\u0026quot; \u0026amp;\rstate == \u0026quot;U.S.\u0026quot;) %\u0026gt;%\rmutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\rWe want to answer the question: is there a poll bias? First, make a plot showing the spreads for each poll.\nThe data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.\r\rThe urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(d\\).\nWe will model the observed data \\(Y_{i,j}\\) in the following way:\n\\[\rY_{i,j} = d + b_i + \\varepsilon_{i,j}\r\\]\nwith \\(i=1,2\\) indexing the two pollsters, \\(b_i\\) the bias for pollster \\(i\\) and \\(\\varepsilon_ij\\) poll to poll chance variability. We assume the \\(\\varepsilon\\) are independent from each other, have expected value \\(0\\) and standard deviation \\(\\sigma_i\\) regardless of \\(j\\).\nWhich of the following best represents our question?\nIs \\(\\varepsilon_{i,j}\\) = 0?\rHow close are the \\(Y_{i,j}\\) to \\(d\\)?\rIs \\(b_1 \\neq b_2\\)?\rAre \\(b_1 = 0\\) and \\(b_2 = 0\\) ?\r\rSuppose we define \\(\\bar{Y}_1\\) as the average of poll results from the first poll, \\(Y_{1,1},\\dots,Y_{1,N_1}\\) with \\(N_1\\) the number of polls conducted by the first pollster:\r\rpolls %\u0026gt;%\rfilter(pollster==\u0026quot;Rasmussen Reports/Pulse Opinion Research\u0026quot;) %\u0026gt;%\rsummarize(N_1 = n())\rWhat is the expected value of \\(\\bar{Y}_1\\)?\nWhat is the standard error of \\(\\bar{Y}_1\\)? (It may be helpful to compute the expected value and standard error of \\(\\bar{Y}_2\\) as well.)\n\rSuppose we define \\(\\bar{Y}_2\\) as the average of poll results from the first poll, \\(Y_{2,1},\\dots,Y_{2,N_2}\\) with \\(N_2\\) the number of polls conducted by the first pollster. What is the expected value \\(\\bar{Y}_2\\)?\n\rWhat does the CLT tell us about the distribution of \\(\\bar{Y}_2 - \\bar{Y}_1\\)?\n\r\rNothing because this is not the average of a sample.\rBecause the \\(Y_{ij}\\) are approximately normal, so are the averages.\rNote that \\(\\bar{Y}_2\\) and \\(\\bar{Y}_1\\) are sample averages, so if we assume \\(N_2\\) and \\(N_1\\) are large enough, each is approximately normal. The difference of normals is also normal.\rThe data are not 0 or 1, so CLT does not apply.\r\rConstruct a random variable that has expected value \\(b_2 - b_1\\), the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on \\(\\sigma_1\\) and \\(\\sigma_2\\) (the variances of the \\(Y\\) above), but we can plug the sample standard deviations. Compute those now.\r\rThe statistic formed by dividing our estimate of \\(b_2-b_1\\) by its estimated standard error:\n\\[\r\\frac{\\bar{Y}_2 - \\bar{Y}_1}{\\sqrt{s_2^2/N_2 + s_1^2/N_1}}\r\\]\nis called the t-statistic. Now you should be able to answer the question: is \\(b_2 - b_1\\) different from 0?\nNotice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can actually construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect?\nFor this exercise, create a new table:\npolls \u0026lt;- polls_us_election_2016 %\u0026gt;%\rfilter(enddate \u0026gt;= \u0026quot;2016-10-15\u0026quot; \u0026amp;\rstate == \u0026quot;U.S.\u0026quot;) %\u0026gt;%\rgroup_by(pollster) %\u0026gt;%\rfilter(n() \u0026gt;= 5) %\u0026gt;%\rmutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %\u0026gt;%\rungroup()\rCompute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.\n\r\r\r\rhttps://www.youtube.com/watch?v=TbKkjm-gheY↩︎\n\rhttps://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html↩︎\n\rhttps://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/↩︎\n\rhttps://projects.fivethirtyeight.com/2016-election-forecast/↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605796661,"objectID":"e52acfab5d487b6c8267ead23da8e20e","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/04-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/04-assignment/","section":"assignment","summary":"Statistical models\r\rPoll aggregators\r\rPoll data\rPollster bias\r\rData-driven models\r\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 5.\n\rFor this exercise you will need to ensure that you’ve carefully read this week’s content and example. We will build on both. The exercises (which you will turn in as this week’s lab) are at the bottom.","tags":null,"title":"Statistical Models","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rPart 1: Statistical Inference and Polls\rPolls\rThe sampling model for polls\r\rPopulations, samples, parameters, and estimates\rThe sample average\rParameters\rPolling versus forecasting\rProperties of our estimate: expected value and standard error\r\rCentral Limit Theorem\rA Monte Carlo simulation\rThe spread\rBias: why not run a very large poll?\r\r\rPart 2: (Supplemental) Additional Visualization Techniques\rCode\rLoad and clean data\rHistograms\rDensity plots\rBox, violin, and rain cloud plots\r\r\r\r\rProbabilistic thinking is central in the human experience. How we describe that thinking is mixed, but most of the time we use (rather imprecise) language. With only a few moments of searching, one can find thousands of articles that use probabilistic words to describe events. Here are some examples:\n\r“‘Highly unlikely’ State of the Union will happen amid shutdown” – The Hill\n\r\r“Tiger Woods makes Masters 15th and most improbable major” – Fox\n\r\r“Trump predicts ‘very good chance’ of China trade deal” – CNN\n\rYet people don’t have a good sense of what these things mean. Uncertainty is key to data analytics: if we were certain of things, A study in the 1960s explored the perception of probabilistic words like these among NATO officers. A more modern replication of this found the following basic pattern:\nA deep, basic fact about humans is that we struggle to understand probabilities. But visualizing things can help. The graphic above shows the uncertainty about uncertainty (very meta). We can convey all manner of uncertainty with clever graphics. Today’s practical example works through some of the myriad of ways to visualize variable data. We’ll cover some territory that we’ve already hit, in hopes of locking in some key concepts.\nPart 1: Statistical Inference and Polls\rIn this Example we will describe, in some detail, how poll aggregators such as FiveThirtyEight use data to predict election outcomes. To understand how they do this, we first need to learn the basics of Statistical Inference, the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.\nPolls\rOpinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as inference and it is the main topic of this chapter.\nPerhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.\nElections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.\nAlthough typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.\nReal Clear Politics1 is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election2:\n\r\rPoll\r\rDate\r\rSample\r\rMoE\r\rClinton\r\rTrump\r\rSpread\r\r\r\r\r\rFinal Results\r\r–\r\r–\r\r–\r\r48.2\r\r46.1\r\rClinton +2.1\r\r\r\rRCP Average\r\r11/1 - 11/7\r\r–\r\r–\r\r46.8\r\r43.6\r\rClinton +3.2\r\r\r\rBloomberg\r\r11/4 - 11/6\r\r799 LV\r\r3.5\r\r46.0\r\r43.0\r\rClinton +3\r\r\r\rIBD\r\r11/4 - 11/7\r\r1107 LV\r\r3.1\r\r43.0\r\r42.0\r\rClinton +1\r\r\r\rEconomist\r\r11/4 - 11/7\r\r3669 LV\r\r–\r\r49.0\r\r45.0\r\rClinton +4\r\r\r\rLA Times\r\r11/1 - 11/7\r\r2935 LV\r\r4.5\r\r44.0\r\r47.0\r\rTrump +3\r\r\r\rABC\r\r11/3 - 11/6\r\r2220 LV\r\r2.5\r\r49.0\r\r46.0\r\rClinton +3\r\r\r\rFOX News\r\r11/3 - 11/6\r\r1295 LV\r\r2.5\r\r48.0\r\r44.0\r\rClinton +4\r\r\r\rMonmouth\r\r11/3 - 11/6\r\r748 LV\r\r3.6\r\r50.0\r\r44.0\r\rClinton +6\r\r\r\rNBC News\r\r11/3 - 11/5\r\r1282 LV\r\r2.7\r\r48.0\r\r43.0\r\rClinton +5\r\r\r\rCBS News\r\r11/2 - 11/6\r\r1426 LV\r\r3.0\r\r47.0\r\r43.0\r\rClinton +4\r\r\r\rReuters\r\r11/2 - 11/6\r\r2196 LV\r\r2.3\r\r44.0\r\r39.0\r\rClinton +5\r\r\r\r\rAlthough in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we will go into some detail on this later.\nLet’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different spread: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled MoE which stands for margin of error.\nIn this example, we will show how the probability concepts we learned in the previous content can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define estimates and margins of errors, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.\nWe start by connecting probability theory to the task of using polls to learn about a population.\nThe sampling model for polls\rTo help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this hypothetical urn.\nBefore making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you $0.10 per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.\nThe dslabs package includes a function that shows a random draw from this urn:\nlibrary(tidyverse)\rlibrary(dslabs)\rtake_poll(25)\rThink about how you would construct your interval based on the data shown above.\nWe have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.\n\r\rPopulations, samples, parameters, and estimates\rWe want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\).\nIn statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter \\(p\\) using the observed data in the sample.\nCan we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that \\(p\\) \u0026gt; .9 or \\(p\\) \u0026lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?\nWe want to construct an estimate of \\(p\\) using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.\nNote that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.\nThe sample average\rConducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate, we can easily report an estimate for the spread \\(2p-1\\), but for simplicity we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion \\(p\\).\nWe start by defining the random variable \\(X\\) as: \\(X=1\\) if we pick a blue bead at random and \\(X=0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads and dividing this count by the total \\(N\\) is equivalent to computing a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant \\(1/N\\):\n\\[\\bar{X} = 1/N \\times \\sum_{i=1}^N X_i\\]\nFor simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads.\nHere we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\).\n\rParameters\rJust like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. \\(p\\) is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is \\(p\\), we are going to estimate this parameter.\nThe ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.\n\rPolling versus forecasting\rBefore we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment and not for election day. The \\(p\\) for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.\n\rProperties of our estimate: expected value and standard error\rTo understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws so the rules we covered in the probability chapter apply.\nUsing what we have learned, the expected value of the sum \\(N\\bar{X}\\) is \\(N \\times\\) the average of the urn, \\(p\\). So dividing by the non-random constant \\(N\\) gives us that the expected value of the average \\(\\bar{X}\\) is \\(p\\). We can write it using our mathematical notation:\n\\[\r\\mbox{E}(\\bar{X}) = p\r\\]\nWe can also use what we learned to figure out the standard error: the standard error of the sum is \\(\\sqrt{N} \\times\\) the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is \\((1-0) \\sqrt{p (1-p)}\\) = \\(\\sqrt{p (1-p)}\\). Because we are dividing the sum by \\(N\\), we arrive at the following formula for the standard error of the average:\n\\[\r\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\r\\]\nThis result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\) and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough poll, our estimate converges to \\(p\\).\nIf we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?\nOne problem is that we do not know \\(p\\), so we can’t compute the standard error. However, for illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\):\nFrom the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is:\nsqrt(p*(1-p))/sqrt(1000)\r## [1] 0.01580823\ror 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the \\(p\\).\n\r\rCentral Limit Theorem\rThe Central Limit Theorem (CLT) tells us that the distribution function for a sum of draws is approximately normal. You also may recall that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of \\(\\bar{X}\\) is approximately normal.\nIn summary, we have that \\(\\bar{X}\\) has an approximately normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\nNow how does this help us? Suppose we want to know what is the probability that we are within 1% from \\(p\\). We are basically asking what is\n\\[\r\\mbox{Pr}(| \\bar{X} - p| \\leq .01)\r\\]\rwhich is the same as:\n\\[\r\\mbox{Pr}(\\bar{X}\\leq p + .01) - \\mbox{Pr}(\\bar{X} \\leq p - .01)\r\\]\nCan we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it \\(Z\\), on the left. Since \\(p\\) is the expected value and \\(\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\\) is the standard error we get:\n\\[\r\\mbox{Pr}\\left(Z \\leq \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right) -\r\\mbox{Pr}\\left(Z \\leq - \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right)\r\\]\nOne problem we have is that since we don’t know \\(p\\), we don’t know \\(\\mbox{SE}(\\bar{X})\\). But it turns out that the CLT still works if we estimate the standard error by using \\(\\bar{X}\\) in place of \\(p\\). We say that we plug-in the estimate. Our estimate of the standard error is therefore:\n\\[\r\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N}\r\\]\rIn statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and \\(N\\).\nNow we continue with our calculation, but dividing by \\(\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N})\\) instead. In our first sample we had 12 blue and 13 red so \\(\\bar{X} = 0.48\\) and our estimate of standard error is:\nx_hat \u0026lt;- 0.48\rse \u0026lt;- sqrt(x_hat*(1-x_hat)/25)\rse\r## [1] 0.09991997\rAnd now we can answer the question of the probability of being close to \\(p\\). The answer is:\npnorm(0.01/se) - pnorm(-0.01/se)\r## [1] 0.07971926\rTherefore, there is a small chance that we will be close. A poll of only \\(N=25\\) people is not really very useful, at least not for a close election.\nEarlier we mentioned the margin of error. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:\n1.96*se\r## [1] 0.1958431\rWhy do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from \\(p\\), we get:\n\\[\r\\mbox{Pr}\\left(Z \\leq \\, 1.96\\,\\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right) -\r\\mbox{Pr}\\left(Z \\leq - 1.96\\, \\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right)\r\\]\rwhich is:\n\\[\r\\mbox{Pr}\\left(Z \\leq 1.96 \\right) -\r\\mbox{Pr}\\left(Z \\leq - 1.96\\right)\r\\]\nwhich we know is about 95%:\npnorm(1.96)-pnorm(-1.96)\r## [1] 0.9500042\rHence, there is a 95% probability that \\(\\bar{X}\\) will be within \\(1.96\\times \\hat{SE}(\\bar{X})\\), in our case within about 0.2, of \\(p\\). Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.\nIn summary, the CLT tells us that our poll based on a sample size of \\(25\\) is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.\nFrom the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a \\(\\bar{X}\\)=0.48 with a sample size of 2,000, our standard error \\(\\hat{\\mbox{SE}}(\\bar{X})\\) would have been 0.0111714. So our result is an estimate of 48% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.\nA Monte Carlo simulation\rSuppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:\nB \u0026lt;- 10000\rN \u0026lt;- 1000\rx_hat \u0026lt;- replicate(B, {\rx \u0026lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\rmean(x)\r})\rThe problem is, of course, we don’t know p. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.\nOne thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll:\np \u0026lt;- 0.45\rN \u0026lt;- 1000\rx \u0026lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\rx_hat \u0026lt;- mean(x)\rIn this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:\nB \u0026lt;- 10000\rx_hat \u0026lt;- replicate(B, {\rx \u0026lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))\rmean(x)\r})\rTo review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45 and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.0157321. The simulation confirms this:\nmean(x_hat)\r## [1] 0.4500761\rsd(x_hat)\r## [1] 0.01579523\rA histogram and qq-plot confirm that the normal approximation is accurate as well:\nOf course, in real life we would never be able to run such an experiment because we don’t know \\(p\\). But we could run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N.\n\rThe spread\rThe competition is to predict the spread, not the proportion \\(p\\). However, because we are assuming there are only two parties, we know that the spread is \\(p - (1-p) = 2p - 1\\). As a result, everything we have done can easily be adapted to an estimate of \\(2p - 1\\). Once we have our estimate \\(\\bar{X}\\) and \\(\\hat{\\mbox{SE}}(\\bar{X})\\), we estimate the spread with \\(2\\bar{X} - 1\\) and, since we are multiplying by 2, the standard error is \\(2\\hat{\\mbox{SE}}(\\bar{X})\\). Note that subtracting 1 does not add any variability so it does not affect the standard error.\nFor our 25 item sample above, our estimate \\(p\\) is .48 with margin of error .20 and our estimate of the spread is 0.04 with margin of error .40. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for \\(p\\), we have it for the spread \\(2p-1\\).\n\rBias: why not run a very large poll?\rFor realistic values of \\(p\\), say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:\nOne reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is \\(p\\). We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this shortly.\n\r\r\rPart 2: (Supplemental) Additional Visualization Techniques\rFor this second part of the example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n\r atl-weather-2019.csv\r\rCode\rLoad and clean data\rFirst, we load the libraries we’ll be using:\nlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(ggridges)\rlibrary(gghalves)\rThen we load the data with read_csv(). Here we assume that the CSV file lives in a subfolder in my project named data. Naturally, you’ll need to point this to wherever you stashed the data.\nweather_atl_raw \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;)\rWe’ll add a couple columns that we can use for faceting and filling using the month() and wday() functions from lubridate for extracting parts of the date:\nweather_atl \u0026lt;- weather_atl_raw %\u0026gt;%\rmutate(Month = month(time, label = TRUE, abbr = FALSE),\rDay = wday(time, label = TRUE, abbr = FALSE))\rNow we’re ready to go!\n\rHistograms\rWe can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:\nggplot(weather_atl, aes(x = windSpeed)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;)\rThis is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the boundary argument for that. We also add scale_x_continuous() to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:\nggplot(weather_atl, aes(x = windSpeed)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) +\rscale_x_continuous(breaks = seq(0, 12, by = 1))\rWe can show the distribution of wind speed by month if we map the Month column we made onto the fill aesthetic:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) +\rscale_x_continuous(breaks = seq(0, 12, by = 1))\rThis is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\rgeom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) +\rscale_x_continuous(breaks = seq(0, 12, by = 1)) +\rguides(fill = FALSE) +\rfacet_wrap(vars(Month))\rNeat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.\n\rDensity plots\rThe code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the geom layer:\nggplot(weather_atl, aes(x = windSpeed)) +\rgeom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;)\rIf we want, we can mess with some of the calculus options like the kernel and bandwidth:\nggplot(weather_atl, aes(x = windSpeed)) +\rgeom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;,\rbw = 0.1, kernel = \u0026quot;epanechnikov\u0026quot;)\rWe can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\rgeom_density(alpha = 0.5)\rEven with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) +\rgeom_density(alpha = 0.5) +\rguides(fill = FALSE) +\rfacet_wrap(vars(Month))\rOr we can stack the density plots behind each other with ggridges. For that to work, we also need to map Month to the y-axis. We can reverse the y-axis so that January is at the top if we use the fct_rev() function:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\rgeom_density_ridges() +\rguides(fill = FALSE)\rWe can add some extra information to geom_density_ridges() with some other arguments like quantile_lines. We can use the quantiles argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +\rgeom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\rguides(fill = FALSE)\rNow that we have good working code, we can easily substitute in other variables by changing the x mapping:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) +\rgeom_density_ridges(quantile_lines = TRUE, quantiles = 2) +\rguides(fill = FALSE)\rWe can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use geom_density_ridges_gradient(), and we need to change the fill mapping to the strange looking ..x.., which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, fill = temperatureHigh doesn’t work 🤷:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) +\rgeom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\rscale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) +\rlabs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;)\rAnd finally, we can get extra fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: temperatureLow and temperatureHigh. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like linetype), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using pivot_longer() from tidyr, which was already loaded with library(tidyverse). In the RStudio primers, you did this same thing with gather()—pivot_longer() is the newer version of gather():\nweather_atl_long \u0026lt;- weather_atl %\u0026gt;%\rpivot_longer(cols = c(temperatureLow, temperatureHigh),\rnames_to = \u0026quot;temp_type\u0026quot;,\rvalues_to = \u0026quot;temp\u0026quot;) %\u0026gt;%\r# Clean up the new temp_type column so that \u0026quot;temperatureHigh\u0026quot; becomes \u0026quot;High\u0026quot;, etc.\rmutate(temp_type = recode(temp_type,\rtemperatureHigh = \u0026quot;High\u0026quot;,\rtemperatureLow = \u0026quot;Low\u0026quot;)) %\u0026gt;%\r# This is optional—just select a handful of columns\rselect(time, temp_type, temp, Month)\r# Show the first few rows\rhead(weather_atl_long)\r## # A tibble: 6 x 4\r## time temp_type temp Month ## \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; ## 1 2019-01-01 05:00:00 Low 50.6 January\r## 2 2019-01-01 05:00:00 High 63.9 January\r## 3 2019-01-02 05:00:00 Low 49.0 January\r## 4 2019-01-02 05:00:00 High 57.4 January\r## 5 2019-01-03 05:00:00 Low 53.1 January\r## 6 2019-01-03 05:00:00 High 55.3 January\rNow we have a column for the temperature (temp) and a column indicating if it is high or low (temp_type). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the linetype aesthetic to show high/low in the border of the plots:\nggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month),\rfill = ..x.., linetype = temp_type)) +\rgeom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +\rscale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) +\rlabs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;)\rWe can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.\n\rBox, violin, and rain cloud plots\rFinally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the Day variable we made indicating weekday:\nggplot(weather_atl,\raes(y = windSpeed, fill = Day)) +\rgeom_boxplot()\rWe can switch this to a violin plot by just changing the geom layer and mapping Day to the x-axis:\nggplot(weather_atl,\raes(y = windSpeed, x = Day, fill = Day)) +\rgeom_violin()\rWith violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:\nggplot(weather_atl,\raes(y = windSpeed, x = Day, fill = Day)) +\rgeom_violin() +\rgeom_point(size = 0.5, position = position_jitter(width = 0.1)) +\rguides(fill = FALSE)\rWe can also add larger points for the daily averages. We’ll use a special layer for this: stat_summary(). It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here \"mean\") and then plotting that result:\nggplot(weather_atl,\raes(y = windSpeed, x = Day, fill = Day)) +\rgeom_violin() +\rstat_summary(geom = \u0026quot;point\u0026quot;, fun = \u0026quot;mean\u0026quot;, size = 5, color = \u0026quot;white\u0026quot;) +\rgeom_point(size = 0.5, position = position_jitter(width = 0.1)) +\rguides(fill = FALSE)\rWe can also show the mean and confidence interval at the same time by changing the summary function:\nggplot(weather_atl,\raes(y = windSpeed, x = Day, fill = Day)) +\rgeom_violin() +\rstat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, size = 1, color = \u0026quot;white\u0026quot;) +\rgeom_point(size = 0.5, position = position_jitter(width = 0.1)) +\rguides(fill = FALSE)\rOverlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use the gghalves package, we can use special halved versions of some of these geoms like so:\nggplot(weather_atl,\raes(x = fct_rev(Day), y = temperatureHigh)) +\rgeom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) +\rgeom_half_boxplot(aes(fill = Day), side = \u0026quot;r\u0026quot;) +\rguides(color = FALSE, fill = FALSE)\rNote the side argument for specifying which half of the column the geom goes. We can also use geom_half_violin():\nggplot(weather_atl,\raes(x = fct_rev(Day), y = temperatureHigh)) +\rgeom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) +\rgeom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) +\rguides(color = FALSE, fill = FALSE)\rIf we flip the plot, we can make a rain cloud plot:\nggplot(weather_atl,\raes(x = fct_rev(Day), y = temperatureHigh)) +\rgeom_half_boxplot(aes(fill = Day), side = \u0026quot;l\u0026quot;, width = 0.5, nudge = 0.1) +\rgeom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) +\rgeom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) +\rguides(color = FALSE, fill = FALSE) +\rcoord_flip()\r\r\r\r\rhttp://www.realclearpolitics.com↩︎\n\rhttp://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601477330,"objectID":"c977a82d5f9c9c6bf14f43f56de0b41e","permalink":"https://datavizm20.classes.andrewheiss.com/example/04-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/04-example/","section":"example","summary":"Part 1: Statistical Inference and Polls\rPolls\rThe sampling model for polls\r\rPopulations, samples, parameters, and estimates\rThe sample average\rParameters\rPolling versus forecasting\rProperties of our estimate: expected value and standard error\r\rCentral Limit Theorem\rA Monte Carlo simulation\rThe spread\rBias: why not run a very large poll?\r\r\rPart 2: (Supplemental) Additional Visualization Techniques\rCode\rLoad and clean data\rHistograms\rDensity plots\rBox, violin, and rain cloud plots\r\r\r\r\rProbabilistic thinking is central in the human experience.","tags":null,"title":"Visualizing Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rBackstory and Set Up\r\rData Exploration and Processing\r\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 12.\n\rThis week’s lab will (hopefully) not repeat the disaster that was last week’s lab.\nBackstory and Set Up\rYou have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nFirst, let’s load the data.\nameslist \u0026lt;- read.table(\u0026quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv\u0026quot;,\rheader = TRUE,\rsep = \u0026quot;,\u0026quot;)\rBefore we proceed, let’s note a few things about the (simple) code above. First, we have specified header = TRUE because—you guessed it—the original dataset has headers. Although simple, this is an incredibly important step because it allows R to do some smart R things. Specifically, once the headers are in, the variables are formatted as int and factor where appropriate. It is absolutely vital that we format the data correctly; otherwise, many R commands will whine at us.\nTry it: Run the above, but instead specifying header = FALSE. What data type are the various columns? Now try ommitting the line altogether. What is the default behavior of the read.table function?1\nData Exploration and Processing\rWe are not going to tell you anything about this data. This is intended to replicate a real-world experience that you will all encounter in the (possibly near) future: someone hands you data and you’re expected to make sense of it. Fortunately for us, this data is (somewhat) self-contained. We’ll first check the variable names to try to divine some information. Recall, we have a handy little function for that:\nnames(ameslist)\rNote that, when doing data exploration, we will sometimes choose to not save our output. This is a judgement call; here we’ve chosen to merely inspect the variables rather than diving in.\nInspection yields some obvious truths. For example:\n\r\rVariable\rExplanation\rType\r\r\r\rID\rUnique identifier for each row\rint\r\rLotArea\rSize of lot (units unknown)\rint\r\rSalePrice\rSale price of house ($)\rint\r\r\r\r…but we face some not-so-obvious things as well. For example:\n\r\rVariable\rExplanation\rType\r\r\r\rLotShape\r? Something about the lot\rfactor\r\rMSSubClass\r? No clue at all\rint\r\rCondition1\r? Seems like street info\rfactor\r\r\r\rIt will be difficult to learn anything about the data that is of type int without outside documentation. However, we can learn something more about the factor-type variables. In order to understand these a little better, we need to review some of the values that each take on.\nTry it: Go through the variables in the dataset and make a note about your interpretation for each. Many will be obvious, but some require additional thought.\nWe now turn to another central issue—and one that explains our nomenclature choice thus far: the data object is of type list. To verify this for yourself, check:\ntypeof(ameslist)\rThis isn’t ideal—for some visualization packages, for instance, we need data frames and not lists. We’ll make a mental note of this as something to potentially clean up if we desire.\nAlthough there are some variables that would be difficult to clean, there are a few that we can address with relative ease. Consider, for instance, the variable GarageType. This might not be that important, but, remember, the weather in Ames, IA is pretty crummy—a detached garage might be a dealbreaker for some would-be homebuyers. Let’s inspect the values:\n\u0026gt; unique(ameslist$GarageType)\r[1] Attchd Detchd BuiltIn CarPort \u0026lt;NA\u0026gt; Basment 2Types\rWith this, we could make an informed decision and create a new variable. Let’s create OutdoorGarage to indicate, say, homes that have any type of garage that requires the homeowner to walk outdoors after parking their car. (For those who aren’t familiar with different garage types, a car port is not insulated and is therefore considered outdoors. A detached garage presumably requires that the person walks outside after parking. The three other types are inside the main structure, and 2Types we can assume includes at least one attached garage of some sort). This is going to require a bit more coding and we will have to think through each step carefully.\nFirst, let’s create a new object that has indicator variables (that is, a variable whose values are either zero or one) for each of the GarageType values. As with everything in R, there’s a handy function to do this for us:\nGarageTemp = model.matrix( ~ GarageType - 1, data=ameslist )\rWe now have two separate objects living in our computer’s memory: ameslist and GarageTemp—so named to indicate that it is a temporary object.2 We now need to stitch it back onto our original data; we’ll use a simple concatenation and write over our old list with the new one:\nameslist \u0026lt;- cbind(ameslist, GarageTemp)\r\u0026gt; Error in data.frame(..., check.names = FALSE) :\rarguments imply differing number of rows: 1460, 1379\rHuh. What’s going on?\nTry it: Figure out what’s going on above. Fix this code so that you have a working version.\nNow that we’ve got that working (ha!) we can generate a new variable for our outdoor garage. We’ll use a somewhat gross version below because it is verbose; that said, this can be easily accomplished using logical indexing for those who like that approach.\nameslist$GarageOutside \u0026lt;- ifelse(ameslist$GarageTypeDetchd == 1 | ameslist$GarageTypeCarPort == 1, 1, 0)\runique(ameslist$GarageOutside)\r[1] 0 1 NA\rThis seems to have worked. The command above ifelse() does what it says: if some condition is met (here, either of two variables equals one) then it returns a one; else it returns a zero. Such functions are very handy, though as mentioned above, there are other ways of doing this. Also note, that while fixed the issue with NA above, we’ve got new issues: we definitely don’t want NA outputted from this operation. Accordingly, we’re going to need to deal with it somehow.\nTry it: Utilizing a similar approach to what you did above, fix this so that the only outputs are zero and one.\nGenerally speaking, this is a persistent issue, and you will spend an extraordinary amount of time dealing with missing data or data that does not encode a variable exactly as you want it. This is expecially true if you deal with real-world data: you will need to learn how to handle NAs. There are a number of fixes (as always, Google is your friend) and anything that works is good. But you should spend some time thinking about this and learning at least one approach.\nEXERCISES\nPrune the data to all of the variables that are type = int about which you have some reasonable intuition for what they mean. This must include the variable SalePrice. Save this new dataset as Ames. Produce documentation for this object in the form of a .txt file. This must describe each of the preserved variables, the values it can take (e.g., can it be negative?) and your interpretation of the variable.\n\rProduce a scatterplot matrix which includes 12 of the variables that are type = int in the data set. Choose those that you believe are likely to be correlated with SalePrice.3\n\rCompute a matrix of correlations between these variables using the function cor(). Does this match your prior beliefs? Briefly discuss the correlation between the miscellaneous variables and SalePrice.\n\rProduce a scatterplot between SalePrice and GrLivArea. Run a linear model using lm() to explore the relationship. Finally, use the abline() function to plot the relationship that you’ve found in the simple linear regression.\n\rWhat is the largest outlier that is above the regression line? Produce the other information about this house.\r\r\r(Bonus) Create a visualization that shows the rise of air conditioning over time in homes in Ames.\n\r\r\r\rOf course, you could find out the defaults of the function by simply using the handy ? command. Don’t forget about this tool!↩︎\n\rIt’s not exactly true that these objects are in memory. They are… sort of. But how R handles memory is complicated and silly and blah blah who cares. It’s basically in memory.↩︎\n\rIf you are not familiar with this type of visualization, consult the book (Introduction to Statistical Learning), Chapters 2 and 3. Google it; it’s free.↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605796661,"objectID":"7337e9261c3514039025dedd39df2948","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/05-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/05-assignment/","section":"assignment","summary":"Backstory and Set Up\r\rData Exploration and Processing\r\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 12.\n\rThis week’s lab will (hopefully) not repeat the disaster that was last week’s lab.\nBackstory and Set Up\rYou have been recently hired to Zillow’s Zestimate product team as a junior analyst.","tags":null,"title":"Correlations and Simple Models","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rPreliminaries\rCode\r\rLoad and clean data\rLegal dual y-axes\rCombining plots\rScatterplot matrices\rCorrelograms\rSimple regression\rCoefficient plots\rMarginal effects plots\r\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again (in addition to the Atlanta weather data). The Ames data is linked below:\n\r Ames.csv\r\r\rPreliminaries\rFor this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n\r atl-weather-2019.csv\r\r\rCode\rLoad and clean data\rFirst, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends\rlibrary(patchwork) # For combining ggplot plots\rlibrary(GGally) # For scatterplot matrices\rlibrary(broom) # For converting model objects to data frames\rThen we load the data with read_csv(). Here we assume that the CSV file lives in a subfolder named data:\nweather_atl \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;)\r\rLegal dual y-axes\rIt is fine (and often helpful) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.\nTo do this, you need to add an argument (sec.axis) to scale_y_continuous() to tell it to use a second axis. This sec.axis argument takes a sec_axis() function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a ~, which indicates that it’s a function, and it needs to use . to stand in for the original value in the original axis.\nSince the equation for converting Fahrenheit to Celsius is this…\n\\[\r\\text{C} = (32 - \\text{F}) \\times -\\frac{5}{9}\r\\]\n…we can specify this with code like so (where . stands for the Fahrenheit value):\n~ (32 - .) * -5 / 9\rHere’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\rgeom_line() +\rscale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\rname = \u0026quot;Celsius\u0026quot;)) +\rlabs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) +\rtheme_minimal()\rFor fun, we could also convert it to Kelvin, which uses this formula:\n\\[\r\\text{K} = (\\text{F} - 32) \\times \\frac{5}{9} + 273.15\r\\]\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\rgeom_line() +\rscale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15,\rname = \u0026quot;Kelvin\u0026quot;)) +\rlabs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) +\rtheme_minimal()\r\rCombining plots\rA good alternative to using two y-axes is to use two plots instead. The patchwork package makes this really easy to do with R. There are other similar packages that do this, like cowplot and gridExtra, but I’ve found that patchwork is the easiest to use and it actually aligns the different plot elements like axis lines and legends (yay alignment in CRAP!). The documentation for patchwork is really great and full of examples—you should check it out to see all the things you can do with it!\nTo use patchwork, we need to (1) save our plots as objects and (2) add them together with +.\nFor instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:\n# Temperature in Atlanta\rtemp_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +\rgeom_line() +\rgeom_smooth() +\rscale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,\rname = \u0026quot;Celsius\u0026quot;)) +\rlabs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) +\rtheme_minimal()\rtemp_plot\r# Humidity in Atlanta\rhumidity_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = humidity)) +\rgeom_line() +\rgeom_smooth() +\rlabs(x = NULL, y = \u0026quot;Humidity\u0026quot;) +\rtheme_minimal()\rhumidity_plot\rRight now, these are two separate plots, but we can combine them with + if we load patchwork:\nlibrary(patchwork)\rtemp_plot + humidity_plot\rBy default, patchwork will put these side-by-side, but we can change that with the plot_layout() function:\ntemp_plot + humidity_plot +\rplot_layout(ncol = 1)\rWe can also play with other arguments in plot_layout(). If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:\ntemp_plot + humidity_plot +\rplot_layout(ncol = 1, heights = c(0.7, 0.3))\r\rScatterplot matrices\rWe can visualize the correlations between pairs of variables with the ggpairs() function in the GGally package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into ggpairs() to see all the correlation information:\nlibrary(GGally)\rweather_correlations \u0026lt;- weather_atl %\u0026gt;%\rselect(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability)\rggpairs(weather_correlations)\rIt looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind spped and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).\nEven though ggpairs() doesn’t use the standard ggplot(...) + geom_whatever() syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:\nggpairs(weather_correlations) +\rlabs(title = \u0026quot;Correlations!\u0026quot;) +\rtheme_dark()\rTRY IT\nMake a ggpairs plot for some of the Ames data.\n\r\rCorrelograms\rScatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In the readings for today, Claus Wilke showed a type of plot called a correlogram which is more appropriate for publication.\nThese are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.\nFirst we need to build a correlation matrix of the main variables we care about. Ordinarily the cor() function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into cor() though, it’ll calculate the correlation between each pair of columns\n# Create a correlation matrix\rthings_to_correlate \u0026lt;- weather_atl %\u0026gt;%\rselect(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %\u0026gt;%\rcor()\rthings_to_correlate\r## temperatureHigh temperatureLow humidity windSpeed precipProbability\r## temperatureHigh 1.00 0.920 -0.030 -0.377 -0.124\r## temperatureLow 0.92 1.000 0.112 -0.450 -0.026\r## humidity -0.03 0.112 1.000 0.011 0.722\r## windSpeed -0.38 -0.450 0.011 1.000 0.196\r## precipProbability -0.12 -0.026 0.722 0.196 1.000\rThe two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to NA):\n# Get rid of the lower triangle\rthings_to_correlate[lower.tri(things_to_correlate)] \u0026lt;- NA\rthings_to_correlate\r## temperatureHigh temperatureLow humidity windSpeed precipProbability\r## temperatureHigh 1 0.92 -0.03 -0.377 -0.124\r## temperatureLow NA 1.00 0.11 -0.450 -0.026\r## humidity NA NA 1.00 0.011 0.722\r## windSpeed NA NA NA 1.000 0.196\r## precipProbability NA NA NA NA 1.000\rFinally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the things_to_correlate matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named measure1, and take all the correlation numbers and put them in a column named cor In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)\nthings_to_correlate_long \u0026lt;- things_to_correlate %\u0026gt;%\r# Convert from a matrix to a data frame\ras.data.frame() %\u0026gt;%\r# Matrixes have column names that don\u0026#39;t get converted to columns when using\r# as.data.frame(), so this adds those names as a column\rrownames_to_column(\u0026quot;measure2\u0026quot;) %\u0026gt;%\r# Make this long. Take all the columns except measure2 and put their names in\r# a column named measure1 and their values in a column named cor\rpivot_longer(cols = -measure2,\rnames_to = \u0026quot;measure1\u0026quot;,\rvalues_to = \u0026quot;cor\u0026quot;) %\u0026gt;%\r# Make a new column with the rounded version of the correlation value\rmutate(nice_cor = round(cor, 2)) %\u0026gt;%\r# Remove rows where the two measures are the same (like the correlation\r# between humidity and humidity)\rfilter(measure2 != measure1) %\u0026gt;%\r# Get rid of the empty triangle\rfilter(!is.na(cor)) %\u0026gt;%\r# Put these categories in order\rmutate(measure1 = fct_inorder(measure1),\rmeasure2 = fct_inorder(measure2))\rthings_to_correlate_long\r## # A tibble: 10 x 4\r## measure2 measure1 cor nice_cor\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 temperatureHigh temperatureLow 0.920 0.92\r## 2 temperatureHigh humidity -0.0301 -0.03\r## 3 temperatureHigh windSpeed -0.377 -0.38\r## 4 temperatureHigh precipProbability -0.124 -0.12\r## 5 temperatureLow humidity 0.112 0.11\r## 6 temperatureLow windSpeed -0.450 -0.45\r## 7 temperatureLow precipProbability -0.0255 -0.03\r## 8 humidity windSpeed 0.0108 0.01\r## 9 humidity precipProbability 0.722 0.72\r## 10 windSpeed precipProbability 0.196 0.2\rPhew. With the data all tidied like that, we can make a correlogram with a heatmap. We have manipulated the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.\nggplot(things_to_correlate_long,\raes(x = measure2, y = measure1, fill = cor)) +\rgeom_tile() +\rgeom_text(aes(label = nice_cor)) +\rscale_fill_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;,\rlimits = c(-1, 1)) +\rlabs(x = NULL, y = NULL) +\rcoord_equal() +\rtheme_minimal() +\rtheme(panel.grid = element_blank())\rInstead of using a heatmap, we can also use points, which encode the correlation information both as color and as size. To do that, we just need to switch geom_tile() to geom_point() and set the size = cor mapping:\nggplot(things_to_correlate_long,\raes(x = measure2, y = measure1, color = cor)) +\r# Size by the absolute value so that -0.7 and 0.7 are the same size\rgeom_point(aes(size = abs(cor))) +\rscale_color_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;,\rlimits = c(-1, 1)) +\rscale_size_area(max_size = 15, limits = c(-1, 1), guide = FALSE) +\rlabs(x = NULL, y = NULL) +\rcoord_equal() +\rtheme_minimal() +\rtheme(panel.grid = element_blank())\r\rSimple regression\rWe finally get to this week’s content. Although correlation is nice, we eventually may want to visualize regression. The lecture has shown us some very intuitive, straightforward ways to think about regression (aka, a line). Simple regression is easy to visualize, since you’re only working with an \\(X\\) and a \\(Y\\). For instance, what’s the relationship between humidity and high temperatures during the summer?\nFirst, let’s filter the data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.\nweather_atl_summer \u0026lt;- weather_atl %\u0026gt;%\rfilter(time \u0026gt;= \u0026quot;2019-05-01\u0026quot;, time \u0026lt;= \u0026quot;2019-09-30\u0026quot;) %\u0026gt;%\rmutate(humidity_scaled = humidity * 100,\rmoonPhase_scaled = moonPhase * 100,\rprecipProbability_scaled = precipProbability * 100,\rcloudCover_scaled = cloudCover * 100)\rThen we can build a simple regression model:\nmodel_simple \u0026lt;- lm(temperatureHigh ~ humidity_scaled,\rdata = weather_atl_summer)\rtidy(model_simple, conf.int = TRUE)\r## # A tibble: 2 x 7\r## term estimate std.error statistic p.value conf.low conf.high\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 104. 2.35 44.3 1.88e-88 99.5 109. ## 2 humidity_scaled -0.241 0.0358 -6.74 3.21e-10 -0.312 -0.170\rWe can interpret these coefficients like so:\n\rThe intercept shows that the average temperature when there’s 0% humidity is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line.\rThe coefficient for humidity_scaled shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.\r\rVisualizing this model is simple, since there are only two variables:\nggplot(weather_atl_summer,\raes(x = humidity_scaled, y = temperatureHigh)) +\rgeom_point() +\rgeom_smooth(method = \u0026quot;lm\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rAnd indeed, as humidity increases, temperatures decrease.\n\rCoefficient plots\rBut if we use multiple variables in the model (and we will do this a lot going forward), it gets really hard to visualize the results since we’re working with multiple dimensions. Instead, we can use coefficient plots to see the individual coefficients in the model.\nFirst, let’s build a more complex model:\nmodel_complex \u0026lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled +\rprecipProbability_scaled + windSpeed + pressure + cloudCover_scaled,\rdata = weather_atl_summer)\rtidy(model_complex, conf.int = TRUE)\r## # A tibble: 7 x 7\r## term estimate std.error statistic p.value conf.low conf.high\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 262. 125. 2.09 0.0380 14.8 510. ## 2 humidity_scaled -0.111 0.0757 -1.47 0.143 -0.261 0.0381\r## 3 moonPhase_scaled 0.0116 0.0126 0.917 0.360 -0.0134 0.0366\r## 4 precipProbability_scaled 0.0356 0.0203 1.75 0.0820 -0.00458 0.0758\r## 5 windSpeed -1.78 0.414 -4.29 0.0000326 -2.59 -0.958 ## 6 pressure -0.157 0.122 -1.28 0.203 -0.398 0.0854\r## 7 cloudCover_scaled -0.0952 0.0304 -3.14 0.00207 -0.155 -0.0352\rWe can interpret these coefficients like so:\n\rHolding everything else constant, a 1% increase in humidity is associated with a 0.11° decrease in the high temperature, on average, but the effect is not statistically significant\rHolding everything else constant, a 1% increase in moon visibility is associated with a 0.01° increase in the high temperature, on average, and the effect is not statistically significant\rHolding everything else constant, a 1% increase in the probability of precipitation is associated with a 0.04° increase in the high temperature, on average, and the effect is not statistically significant\rHolding everything else constant, a 1 mph increase in the wind speed is associated with a 1.8° decrease in the high temperature, on average, and the effect is statistically significant\rHolding everything else constant, a 1 unit increase in barometric pressure is associated with a 0.15° decrease in the high temperature, on average, and the effect is not statistically significant\rHolding everything else constant, a 1% increase in cloud cover is associated with a 0.01° decrease in the high temperature, on average, and the effect is statistically significant\rThe intercept is pretty useless. It shows that the predicted temperature will be 262° when humidity is 0%, the moon is invisible, there’s no chance of precipitation, no wind, no barometric pressure, and no cloud cover. Yikes.\r\rTo plot all these things at once, we’ll store the results of tidy(model_complex) as a data frame, remove the useless intercept, and plot it using geom_pointrange():\nmodel_tidied \u0026lt;- tidy(model_complex, conf.int = TRUE) %\u0026gt;%\rfilter(term != \u0026quot;(Intercept)\u0026quot;)\rggplot(model_tidied,\raes(x = estimate, y = term)) +\rgeom_vline(xintercept = 0, color = \u0026quot;red\u0026quot;, linetype = \u0026quot;dotted\u0026quot;) +\rgeom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +\rlabs(x = \u0026quot;Coefficient estimate\u0026quot;, y = NULL) +\rtheme_minimal()\rNeat! Now we can see how big these different coefficients are and how close they are to zero. Wind speed has a big significant effect on temperature. The others are all very close to zero.\n\rMarginal effects plots\rInstead of just looking at the coefficients, we can also see the effect of moving different variables up and down like sliders and switches. Remember that regression coefficients allow us to build actual mathematical formulas that predict the value of Y. The coefficients from model_compex yield the following big hairy ugly equation:\n\\[\r\\begin{aligned}\r\\hat{\\text{High temperature}} =\u0026amp; 262 - 0.11 \\times \\text{humidity_scaled } \\\\\r\u0026amp; + 0.01 \\times \\text{moonPhase_scaled } + 0.04 \\times \\text{precipProbability_scaled } \\\\\r\u0026amp; - 1.78 \\times \\text{windSpeed} - 0.16 \\times \\text{pressure} - 0.095 \\times \\text{cloudCover_scaled}\r\\end{aligned}\r\\]\nIf we plug in values for each of the explanatory variables, we can get the predicted value of high temperature, or \\(\\hat{y}\\).\nThe augment() function in the broom library allows us to take a data frame of explanatory variable values, plug them into the model equation, and get predictions out. For example, let’s set each of the variables to some arbitrary values (50% for humidity, moon phase, chance of rain, and cloud cover; 1000 for pressure, and 1 MPH for wind speed).\nnewdata_example \u0026lt;- tibble(humidity_scaled = 50, moonPhase_scaled = 50,\rprecipProbability_scaled = 50, windSpeed = 1,\rpressure = 1000, cloudCover_scaled = 50)\rnewdata_example\r## # A tibble: 1 x 6\r## humidity_scaled moonPhase_scaled precipProbability_scaled windSpeed pressure cloudCover_scaled\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 50 50 50 1 1000 50\rWe can plug these values into the model with augment():\n# I use select() here because augment() returns columns for all the explanatory\r# variables, and the .fitted column with the predicted value is on the far right\r# and gets cut off\raugment(model_complex, newdata = newdata_example, se_fit=TRUE) %\u0026gt;%\rselect(.fitted, .se.fit)\rGiven these weather conditions, the predicted high temperature is 96.2°. Now you’re an armchair meteorologist!\nWe can follow the same pattern to show how the predicted temperature changes as specific variables change across a whole range. Here, we create a data frame of possible wind speeds and keep all the other explanatory variables at their means:\nnewdata \u0026lt;- tibble(windSpeed = seq(0, 8, 0.5),\rpressure = mean(weather_atl_summer$pressure),\rprecipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),\rmoonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),\rhumidity_scaled = mean(weather_atl_summer$humidity_scaled),\rcloudCover_scaled = mean(weather_atl_summer$cloudCover_scaled))\rnewdata\r## # A tibble: 17 x 6\r## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 1016. 40.2 50.7 64.8 29.5\r## 2 0.5 1016. 40.2 50.7 64.8 29.5\r## 3 1 1016. 40.2 50.7 64.8 29.5\r## 4 1.5 1016. 40.2 50.7 64.8 29.5\r## 5 2 1016. 40.2 50.7 64.8 29.5\r## 6 2.5 1016. 40.2 50.7 64.8 29.5\r## 7 3 1016. 40.2 50.7 64.8 29.5\r## 8 3.5 1016. 40.2 50.7 64.8 29.5\r## 9 4 1016. 40.2 50.7 64.8 29.5\r## 10 4.5 1016. 40.2 50.7 64.8 29.5\r## 11 5 1016. 40.2 50.7 64.8 29.5\r## 12 5.5 1016. 40.2 50.7 64.8 29.5\r## 13 6 1016. 40.2 50.7 64.8 29.5\r## 14 6.5 1016. 40.2 50.7 64.8 29.5\r## 15 7 1016. 40.2 50.7 64.8 29.5\r## 16 7.5 1016. 40.2 50.7 64.8 29.5\r## 17 8 1016. 40.2 50.7 64.8 29.5\rIf we feed this big data frame into augment(), we can get the predicted high temperature for each row. We can also use the .se.fit column to calculate the 95% confidence interval for each predicted value. We take the standard error, multiply it by -1.96 and 1.96 (or the quantile of the normal distribution at 2.5% and 97.5%), and add that value to the estimate.\npredicted_values \u0026lt;- augment(model_complex, newdata = newdata, se_fit=TRUE) %\u0026gt;%\rmutate(conf.low = .fitted + (-1.96 * .se.fit),\rconf.high = .fitted + (1.96 * .se.fit))\rpredicted_values %\u0026gt;%\rselect(windSpeed, .fitted, .se.fit, conf.low, conf.high) %\u0026gt;%\rhead()\r## # A tibble: 6 x 5\r## windSpeed .fitted .se.fit conf.low conf.high\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 95.3 1.63 92.2 98.5\r## 2 0.5 94.5 1.42 91.7 97.2\r## 3 1 93.6 1.22 91.2 96.0\r## 4 1.5 92.7 1.03 90.7 94.7\r## 5 2 91.8 0.836 90.1 93.4\r## 6 2.5 90.9 0.653 89.6 92.2\rCool! Just looking at the data in the table, we can see that predicted temperature drops as windspeed increases. We can plot this to visualize the effect:\nggplot(predicted_values, aes(x = windSpeed, y = .fitted)) +\rgeom_ribbon(aes(ymin = conf.low, ymax = conf.high),\rfill = \u0026quot;#BF3984\u0026quot;, alpha = 0.5) +\rgeom_line(size = 1, color = \u0026quot;#BF3984\u0026quot;) +\rlabs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) +\rtheme_minimal()\rWe just manipulated one of the model coefficients and held everything else at its mean. We can manipulate multiple variables too and encode them all on the graph. For instance, what is the effect of windspeed and cloud cover on the temperature?\nWe’ll follow the same process, but vary both windSpeed and cloudCover_scaled. Instead of using tibble(), we use exapnd_grid(), which creates every combination of the variables we specify. Instead of varying cloud cover by every possible value (like from 0 to 100), we’ll choose four possible cloud cover types: 0%, 33%, 66%, and 100%. Everything else will be at its mean.\nnewdata_fancy \u0026lt;- expand_grid(windSpeed = seq(0, 8, 0.5),\rpressure = mean(weather_atl_summer$pressure),\rprecipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),\rmoonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),\rhumidity_scaled = mean(weather_atl_summer$humidity_scaled),\rcloudCover_scaled = c(0, 33, 66, 100))\rnewdata_fancy\r## # A tibble: 68 x 6\r## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 1016. 40.2 50.7 64.8 0\r## 2 0 1016. 40.2 50.7 64.8 33\r## 3 0 1016. 40.2 50.7 64.8 66\r## 4 0 1016. 40.2 50.7 64.8 100\r## 5 0.5 1016. 40.2 50.7 64.8 0\r## 6 0.5 1016. 40.2 50.7 64.8 33\r## 7 0.5 1016. 40.2 50.7 64.8 66\r## 8 0.5 1016. 40.2 50.7 64.8 100\r## 9 1 1016. 40.2 50.7 64.8 0\r## 10 1 1016. 40.2 50.7 64.8 33\r## # … with 58 more rows\rNotice now that windSpeed repeats four times (0, 0, 0, 0, 0.5, 0.5, etc.), since there are four possible cloudCover_scaled values (0, 33, 66, 100).\nWe can plot this now, just like before, with wind speed on the x-axis, the predicted temperature on the y-axis, and colored and faceted by cloud cover:\npredicted_values_fancy \u0026lt;- augment(model_complex, newdata = newdata_fancy, se_fit=TRUE) %\u0026gt;%\rmutate(conf.low = .fitted + (-1.96 * .se.fit),\rconf.high = .fitted + (1.96 * .se.fit)) %\u0026gt;%\r# Make cloud cover a categorical variable\rmutate(cloudCover_scaled = factor(cloudCover_scaled))\rggplot(predicted_values_fancy, aes(x = windSpeed, y = .fitted)) +\rgeom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled),\ralpha = 0.5) +\rgeom_line(aes(color = cloudCover_scaled), size = 1) +\rlabs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) +\rtheme_minimal() +\rguides(fill = FALSE, color = FALSE) +\rfacet_wrap(vars(cloudCover_scaled), nrow = 1)\rNice. Temperatures go down slightly as cloud cover increases. If we wanted to improve the model, we’d add an interaction term between cloud cover and windspeed so that each line would have a different slope in addition to a different intercept, but that’s beyond the scope of this class.\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601930279,"objectID":"ac11eb4b300887ac6e4fc5c1906670ba","permalink":"https://datavizm20.classes.andrewheiss.com/example/05-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/05-example/","section":"example","summary":"Preliminaries\rCode\r\rLoad and clean data\rLegal dual y-axes\rCombining plots\rScatterplot matrices\rCorrelograms\rSimple regression\rCoefficient plots\rMarginal effects plots\r\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again (in addition to the Atlanta weather data). The Ames data is linked below:\n\r Ames.csv\r\r\rPreliminaries\rFor this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.","tags":null,"title":"Introduction to Regression","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rBackstory and Set Up\rBuilding a Model\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 19.\n\rThis week’s lab will extend last week’s lab. The introduction is a direct repeat.\nBackstory and Set Up\rYou have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nFirst, let’s load the data.\nameslist \u0026lt;- read.table(\u0026quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv\u0026quot;,\rheader = TRUE,\rsep = \u0026quot;,\u0026quot;)\r\rBuilding a Model\rWe’re now ready to start playing with a model. We will start by using the lm() function to fit a simple linear regression\rmodel, with SalePrice as the response and lstat as the predictor.\nRecall that the basic lm() syntax is lm(y∼x,data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept. Let’s quickly run this with two variables:\nlm.fit = lm(SalePrice ~ GrLivArea)\rThis yields:\rError in eval(expr, envir, enclos) : Object \"SalePrice\" not found\nThis command causes an error because R does not know where to find the variables. We can fix this by attaching the data:\nattach(Ames)\rlm.fit = lm(SalePrice ~ GrLivArea)\r# Alternatively...\rlm.fit = lm(SalePrice ~ GrLivArea, data=Ames)\rThe next line tells R that the variables are in the object known as Ames. If you haven’t created this object yet (as in the previous lab) you’ll get an error at this stage. But once we attach Ames, the first line works fine because R now recognizes the variables. Alternatively, we could specify this within the lm() call using data = Ames. We’ve presented this way because it may be new to you; choose whichever you find most reasonable.\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us p-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the entire model.1\nUtilizing these functions hels us see some interesting results. Note that we built (nearly) the simplest possible model:\n\\[\\text{SalePrice} = \\beta_0 + \\beta_1*(\\text{GrLivArea}) + \\epsilon.\\]\nBut even on its own, this model is instructive. It suggest that an increase in overall living area of 1 ft \\(^2\\) is correlated with an expected increase in sales price of $107. (Note that we cannot make causal claims!)\nSaving the model as we did above is useful because we can explore other pieces of information it stores. Specifically, we can use the names() function in order to find out what else is stored in lm.fit. Although we can extract these quan- tities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them. We can also use a handy tool like plot() applied directly to lm.fit to see some interesting data that is automatically stored by the model.\nTry it: Use plot() to explore the model above. Do you suspect that some outliers have a large influence on the data? We will explore this point specifically in the future.\nWe can now go crazy adding variables to our model. It’s as simple as appending them to the previous code—though you should be careful executing this, as it will overwrite your previous output:\nlm.fit = lm(SalePrice ~ GrLivArea + LotArea)\rTry it: Does controlling for LotArea change the qualitative conclusions from the previous regression? What about the quantitative results? Does the direction of the change in the quantitative results make sense to you?\nEXERCISES\nUse the lm() function in a simple linear regression (e.g., with only one predictor) with SalePrice as the response to determine the value of a garage.\n\rUse the lm() function to perform a multiple linear regression with SalePrice as the response and all other variables from your Ames data as the predictors. Use the summary() function to print the results. Comment on the output. For instance:\n\rIs there a relationship between the predictors and the response?\rWhich predictors appear to have a statistically significant relationship to the response? (Hint: look for stars)\rWhat does the coefficient for the year variable suggest?\r\rUse the : symbols to fit a linear regression model with one well-chosen interaction effects. Why did you do this?\n\rTry a few (e.g., two) different transformations of the variables, such as \\(ln(x)\\), \\(x^2\\), \\(\\sqrt x\\). Do any of these make sense to include in a model of SalePrice? Comment on your findings.\n\r\r(Bonus; very very challenging) How might we build a model to estimate the elasticity of demand from this dataset?\n\r\r\rWhen we use the simple regression model with a single input, the \\(F\\)-stat includes the intercept term. Otherwise, it does not. See Lecture 5 for more detail.↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605796661,"objectID":"e1ed0bbd55230066a650c8b68228a273","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/06-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/06-assignment/","section":"assignment","summary":"Backstory and Set Up\rBuilding a Model\r\r\rNOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 19.\n\rThis week’s lab will extend last week’s lab. The introduction is a direct repeat.\nBackstory and Set Up\rYou have been recently hired to Zillow’s Zestimate product team as a junior analyst.","tags":null,"title":"Model Building","type":"docs"},{"authors":null,"categories":null,"content":"\r\rPreliminaries\rDummy Variables\rInteractions\rFactor Variables\rFactors with More Than Two Levels\r\rParameterization\rBuilding Larger Models\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\rPreliminaries\rSo far in each of our analyses, we have only used numeric variables as predictors. We have also only used additive models, meaning the effect any predictor had on the response was not dependent on the other predictors. In this chapter, we will remove both of these restrictions. We will fit models with categorical predictors, and use models that allow predictors to interact. The mathematics of multiple regression will remain largely unchanging, however, we will pay close attention to interpretation, as well as some difference in R usage.\n\rDummy Variables\rFor this example and discussion, we will briefly use the built in dataset mtcars before returning to our favorite autompg dataset. During the in-class lecture / example, I will also use much more interesting datasets. The reason to use these easy, straightforward datasets is that they make visualization of the entire dataset trivially easy. Accordingly, the mtcars dataset is small, so we’ll quickly take a look at the entire dataset.\nmtcars\r## mpg cyl disp hp drat wt qsec vs am gear carb\r## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4\r## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4\r## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1\r## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1\r## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2\r## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1\r## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4\r## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2\r## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2\r## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4\r## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4\r## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3\r## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3\r## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3\r## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4\r## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4\r## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4\r## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1\r## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2\r## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1\r## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1\r## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2\r## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2\r## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4\r## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2\r## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1\r## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2\r## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2\r## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4\r## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6\r## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8\r## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2\rWe will be interested in three of the variables: mpg, hp, and am.\n\rmpg: fuel efficiency, in miles per gallon.\rhp: horsepower, in foot-pounds per second.\ram: transmission. Automatic or manual.\r\rAs we often do, we will start by plotting the data. We are interested in mpg as the response variable, and hp as a predictor.\nplot(mpg ~ hp, data = mtcars, cex = 2)\rSince we are also interested in the transmission type, we could also label the points accordingly.\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Automatic\u0026quot;, \u0026quot;Manual\u0026quot;), col = c(1, 2), pch = c(1, 2))\rWe now fit the SLR model\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\epsilon,\r\\]\nwhere \\(Y\\) is mpg and \\(x_1\\) is hp. For notational brevity, we drop the index \\(i\\) for observations.\nmpg_hp_slr = lm(mpg ~ hp, data = mtcars)\rWe then re-plot the data and add the fitted line to the plot.\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\rabline(mpg_hp_slr, lwd = 3, col = \u0026quot;grey\u0026quot;)\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Automatic\u0026quot;, \u0026quot;Manual\u0026quot;), col = c(1, 2), pch = c(1, 2))\rWe should notice a pattern here. The red, manual observations largely fall above the line, while the black, automatic observations are mostly below the line. This means our model underestimates the fuel efficiency of manual transmissions, and overestimates the fuel efficiency of automatic transmissions. To correct for this, we will add a predictor to our model, namely, am as \\(x_2\\).\nOur new model is\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\r\\]\nwhere \\(x_1\\) and \\(Y\\) remain the same, but now\n\\[\rx_2 =\r\\begin{cases}\r1 \u0026amp; \\text{manual transmission} \\\\\r0 \u0026amp; \\text{automatic transmission}\r\\end{cases}.\r\\]\nIn this case, we call \\(x_2\\) a dummy variable. A dummy variable (also called an “indicator” variable) is somewhat unfortunately named, as it is in no way “dumb”. In fact, it is actually somewhat clever. A dummy variable is a numerical variable that is used in a regression analysis to “code” for a binary categorical variable. Let’s see how this works.\nFirst, note that am is already a dummy variable, since it uses the values 0 and 1 to represent automatic and manual transmissions. Often, a variable like am would store the character values auto and man and we would either have to convert these to 0 and 1, or, as we will see later, R will take care of creating dummy variables for us.\nSo, to fit the above model, we do so like any other multiple regression model we have seen before.\nmpg_hp_add = lm(mpg ~ hp + am, data = mtcars)\rBriefly checking the output, we see that R has estimated the three \\(\\beta\\) parameters.\nmpg_hp_add\r## ## Call:\r## lm(formula = mpg ~ hp + am, data = mtcars)\r## ## Coefficients:\r## (Intercept) hp am ## 26.58491 -0.05889 5.27709\rSince \\(x_2\\) can only take values 0 and 1, we can effectively write two different models, one for manual and one for automatic transmissions.\nFor automatic transmissions, that is \\(x_2 = 0\\), we have,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\epsilon.\r\\]\nThen for manual transmissions, that is \\(x_2 = 1\\), we have,\n\\[\rY = (\\beta_0 + \\beta_2) + \\beta_1 x_1 + \\epsilon.\r\\]\nNotice that these models share the same slope, \\(\\beta_1\\), but have different intercepts, differing by \\(\\beta_2\\). So the change in mpg is the same for both models, but on average mpg differs by \\(\\beta_2\\) between the two transmission types.\nWe’ll now calculate the estimated slope and intercept of these two models so that we can add them to a plot. Note that:\n\r\\(\\hat{\\beta}_0\\) = coef(mpg_hp_add)[1] = 26.5849137\r\\(\\hat{\\beta}_1\\) = coef(mpg_hp_add)[2] = -0.0588878\r\\(\\hat{\\beta}_2\\) = coef(mpg_hp_add)[3] = 5.2770853\r\rWe can then combine these to calculate the estimated slope and intercepts.\nint_auto = coef(mpg_hp_add)[1]\rint_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3]\rslope_auto = coef(mpg_hp_add)[2]\rslope_manu = coef(mpg_hp_add)[2]\rRe-plotting the data, we use these slopes and intercepts to add the “two” fitted models to the plot.\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)\rabline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # add line for auto\rabline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # add line for manual\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Automatic\u0026quot;, \u0026quot;Manual\u0026quot;), col = c(1, 2), pch = c(1, 2))\rWe notice right away that the points are no longer systematically incorrect. The red, manual observations vary about the red line in no particular pattern without underestimating the observations as before. The black, automatic points vary about the black line, also without an obvious pattern.\nThey say a picture is worth a thousand words, but as a statistician, sometimes a picture is worth an entire analysis. The above picture makes it plainly obvious that \\(\\beta_2\\) is significant, but let’s verify mathematically. Essentially we would like to test:\n\\[\rH_0: \\beta_2 = 0 \\quad \\text{vs} \\quad H_1: \\beta_2 \\neq 0.\r\\]\nThis is nothing new. Again, the math is the same as the multiple regression analyses we have seen before. We could perform either a \\(t\\) or \\(F\\) test here. The only difference is a slight change in interpretation. We could think of this as testing a model with a single line (\\(H_0\\)) against a model that allows two lines (\\(H_1\\)).\nTo obtain the test statistic and p-value for the \\(t\\)-test, we would use\nsummary(mpg_hp_add)$coefficients[\u0026quot;am\u0026quot;,]\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## 5.277085e+00 1.079541e+00 4.888270e+00 3.460318e-05\rTo do the same for the \\(F\\) test, we would use\nanova(mpg_hp_slr, mpg_hp_add)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ hp\r## Model 2: mpg ~ hp + am\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 30 447.67 ## 2 29 245.44 1 202.24 23.895 3.46e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rNotice that these are indeed testing the same thing, as the p-values are exactly equal. (And the \\(F\\) test statistic is the \\(t\\) test statistic squared.)\nRecapping some interpretations:\n\r\\(\\hat{\\beta}_0 = 26.5849137\\) is the estimated average mpg for a car with an automatic transmission and 0 hp.\n\r\\(\\hat{\\beta}_0 + \\hat{\\beta}_2 = 31.8619991\\) is the estimated average mpg for a car with a manual transmission and 0 hp.\n\r\\(\\hat{\\beta}_2 = 5.2770853\\) is the estimated difference in average mpg for cars with manual transmissions as compared to those with automatic transmission, for any hp.\n\r\\(\\hat{\\beta}_1 = -0.0588878\\) is the estimated change in average mpg for an increase in one hp, for either transmission types.\n\r\rWe should take special notice of those last two. In the model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\r\\]\nwe see \\(\\beta_1\\) is the average change in \\(Y\\) for an increase in \\(x_1\\), no matter the value of \\(x_2\\). Also, \\(\\beta_2\\) is always the difference in the average of \\(Y\\) for any value of \\(x_1\\). These are two restrictions we won’t always want, so we need a way to specify a more flexible model.\nHere we restricted ourselves to a single numerical predictor \\(x_1\\) and one dummy variable \\(x_2\\). However, the concept of a dummy variable can be used with larger multiple regression models. We only use a single numerical predictor here for ease of visualization since we can think of the “two lines” interpretation. But in general, we can think of a dummy variable as creating “two models,” one for each category of a binary categorical variable.\n\rInteractions\rTo remove the “same slope” restriction, we will now discuss interaction. To illustrate this concept, we will return to the autompg dataset we created in the last chapter, with a few more modifications.\n# read data frame from the web\rautompg = read.table(\r\u0026quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\u0026quot;,\rquote = \u0026quot;\\\u0026quot;\u0026quot;,\rcomment.char = \u0026quot;\u0026quot;,\rstringsAsFactors = FALSE)\r# give the dataframe headers\rcolnames(autompg) = c(\u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;acc\u0026quot;, \u0026quot;year\u0026quot;, \u0026quot;origin\u0026quot;, \u0026quot;name\u0026quot;)\r# remove missing data, which is stored as \u0026quot;?\u0026quot;\rautompg = subset(autompg, autompg$hp != \u0026quot;?\u0026quot;)\r# remove the plymouth reliant, as it causes some issues\rautompg = subset(autompg, autompg$name != \u0026quot;plymouth reliant\u0026quot;)\r# give the dataset row names, based on the engine, year and name\rrownames(autompg) = paste(autompg$cyl, \u0026quot;cylinder\u0026quot;, autompg$year, autompg$name)\r# remove the variable for name\rautompg = subset(autompg, select = c(\u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;acc\u0026quot;, \u0026quot;year\u0026quot;, \u0026quot;origin\u0026quot;))\r# change horsepower from character to numeric\rautompg$hp = as.numeric(autompg$hp)\r# create a dummary variable for foreign vs domestic cars. domestic = 1.\rautompg$domestic = as.numeric(autompg$origin == 1)\r# remove 3 and 5 cylinder cars (which are very rare.)\rautompg = autompg[autompg$cyl != 5,]\rautompg = autompg[autompg$cyl != 3,]\r# the following line would verify the remaining cylinder possibilities are 4, 6, 8\r#unique(autompg$cyl)\r# change cyl to a factor variable\rautompg$cyl = as.factor(autompg$cyl)\rstr(autompg)\r## \u0026#39;data.frame\u0026#39;: 383 obs. of 9 variables:\r## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ...\r## $ cyl : Factor w/ 3 levels \u0026quot;4\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;8\u0026quot;: 3 3 3 3 3 3 3 3 3 3 ...\r## $ disp : num 307 350 318 304 302 429 454 440 455 390 ...\r## $ hp : num 130 165 150 150 140 198 220 215 225 190 ...\r## $ wt : num 3504 3693 3436 3433 3449 ...\r## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\r## $ year : int 70 70 70 70 70 70 70 70 70 70 ...\r## $ origin : int 1 1 1 1 1 1 1 1 1 1 ...\r## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ...\rWe’ve removed cars with 3 and 5 cylinders , as well as created a new variable domestic which indicates whether or not a car was built in the United States. Removing the 3 and 5 cylinders is simply for ease of demonstration later in the chapter and would not be done in practice. The new variable domestic takes the value 1 if the car was built in the United States, and 0 otherwise, which we will refer to as “foreign.” (We are arbitrarily using the United States as the reference point here.) We have also made cyl and origin into factor variables, which we will discuss later.\nWe’ll now be concerned with three variables: mpg, disp, and domestic. We will use mpg as the response. We can fit a model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x_1\\) is disp, the displacement in cubic inches,\r\\(x_2\\) is domestic as described above, which is a dummy variable.\r\r\\[\rx_2 =\r\\begin{cases}\r1 \u0026amp; \\text{Domestic} \\\\\r0 \u0026amp; \\text{Foreign}\r\\end{cases}\r\\]\nWe will fit this model, extract the slope and intercept for the “two lines,” plot the data and add the lines.\nmpg_disp_add = lm(mpg ~ disp + domestic, data = autompg)\rint_for = coef(mpg_disp_add)[1]\rint_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3]\rslope_for = coef(mpg_disp_add)[2]\rslope_dom = coef(mpg_disp_add)[2]\rplot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)\rabline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # add line for foreign cars\rabline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # add line for domestic cars\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Foreign\u0026quot;, \u0026quot;Domestic\u0026quot;), pch = c(1, 2), col = c(1, 2))\rThis is a model that allows for two parallel lines, meaning the mpg can be different on average between foreign and domestic cars of the same engine displacement, but the change in average mpg for an increase in displacement is the same for both. We can see this model isn’t doing very well here. The red line fits the red points fairly well, but the black line isn’t doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes.\nConsider the following model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\r\\]\nwhere \\(x_1\\), \\(x_2\\), and \\(Y\\) are the same as before, but we have added a new interaction term \\(x_1 x_2\\) which multiplies \\(x_1\\) and \\(x_2\\), so we also have an additional \\(\\beta\\) parameter \\(\\beta_3\\).\nThis model essentially creates two slopes and two intercepts, \\(\\beta_2\\) being the difference in intercepts and \\(\\beta_3\\) being the difference in slopes. To see this, we will break down the model into the two “sub-models” for foreign and domestic cars.\nFor foreign cars, that is \\(x_2 = 0\\), we have\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\epsilon.\r\\]\nFor domestic cars, that is \\(x_2 = 1\\), we have\n\\[\rY = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) x_1 + \\epsilon.\r\\]\nThese two models have both different slopes and intercepts.\n\r\\(\\beta_0\\) is the average mpg for a foreign car with 0 disp.\r\\(\\beta_1\\) is the change in average mpg for an increase of one disp, for foreign cars.\r\\(\\beta_0 + \\beta_2\\) is the average mpg for a domestic car with 0 disp.\r\\(\\beta_1 + \\beta_3\\) is the change in average mpg for an increase of one disp, for domestic cars.\r\rHow do we fit this model in R? There are a number of ways.\nOne method would be to simply create a new variable, then fit a model like any other.\nautompg$x3 = autompg$disp * autompg$domestic # THIS CODE NOT RUN!\rdo_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # THIS CODE NOT RUN!\rYou should only do this as a last resort. We greatly prefer not to have to modify our data simply to fit a model. Instead, we can tell R we would like to use the existing data with an interaction term, which it will create automatically when we use the : operator.\nmpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg)\rAn alternative method, which will fit the exact same model as above would be to use the * operator. This method automatically creates the interaction term, as well as any “lower order terms,” which in this case are the first order terms for disp and domestic\nmpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg)\rWe can quickly verify that these are doing the same thing.\ncoef(mpg_disp_int)\r## (Intercept) disp domestic disp:domestic ## 46.0548423 -0.1569239 -12.5754714 0.1025184\rcoef(mpg_disp_int2)\r## (Intercept) disp domestic disp:domestic ## 46.0548423 -0.1569239 -12.5754714 0.1025184\rWe see that both the variables, and their coefficient estimates are indeed the same for both models.\nsummary(mpg_disp_int)\r## ## Call:\r## lm(formula = mpg ~ disp + domestic + disp:domestic, data = autompg)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -10.8332 -2.8956 -0.8332 2.2828 18.7749 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 46.05484 1.80582 25.504 \u0026lt; 2e-16 ***\r## disp -0.15692 0.01668 -9.407 \u0026lt; 2e-16 ***\r## domestic -12.57547 1.95644 -6.428 3.90e-10 ***\r## disp:domestic 0.10252 0.01692 6.060 3.29e-09 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 4.308 on 379 degrees of freedom\r## Multiple R-squared: 0.7011, Adjusted R-squared: 0.6987 ## F-statistic: 296.3 on 3 and 379 DF, p-value: \u0026lt; 2.2e-16\rWe see that using summary() gives the usual output for a multiple regression model. We pay close attention to the row for disp:domestic which tests,\n\\[\rH_0: \\beta_3 = 0.\r\\]\nIn this case, testing for \\(\\beta_3 = 0\\) is testing for two lines with parallel slopes versus two lines with possibly different slopes. The disp:domestic line in the summary() output uses a \\(t\\)-test to perform the test.\nWe could also use an ANOVA \\(F\\)-test. The additive model, without interaction is our null model, and the interaction model is the alternative.\nanova(mpg_disp_add, mpg_disp_int)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp + domestic\r## Model 2: mpg ~ disp + domestic + disp:domestic\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 380 7714.0 ## 2 379 7032.6 1 681.36 36.719 3.294e-09 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rAgain we see this test has the same p-value as the \\(t\\)-test. Also the p-value is extremely low, so between the two, we choose the interaction model.\nint_for = coef(mpg_disp_int)[1]\rint_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3]\rslope_for = coef(mpg_disp_int)[2]\rslope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4]\rHere we again calculate the slope and intercepts for the two lines for use in plotting.\nplot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)\rabline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # line for foreign cars\rabline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # line for domestic cars\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;Foreign\u0026quot;, \u0026quot;Domestic\u0026quot;), pch = c(1, 2), col = c(1, 2))\rWe see that these lines fit the data much better, which matches the result of our tests.\nSo far we have only seen interaction between a categorical variable (domestic) and a numerical variable (disp). While this is easy to visualize, since it allows for different slopes for two lines, it is not the only type of interaction we can use in a model. We can also consider interactions between two numerical variables.\nConsider the model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x_1\\) is disp, the displacement in cubic inches,\r\\(x_2\\) is hp, the horsepower, in foot-pounds per second.\r\rHow does mpg change based on disp in this model? We can rearrange some terms to see how.\n\\[\rY = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon\r\\]\nSo, for a one unit increase in \\(x_1\\) (disp), the mean of \\(Y\\) (mpg) increases \\(\\beta_1 + \\beta_3 x_2\\), which is a different value depending on the value of \\(x_2\\) (hp)!\nSince we’re now working in three dimensions, this model can’t be easily justified via visualizations like the previous example. Instead, we will have to rely on a test.\nmpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg)\rmpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg)\rsummary(mpg_disp_int_hp)\r## ## Call:\r## lm(formula = mpg ~ disp * hp, data = autompg)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -10.7849 -2.3104 -0.5699 2.1453 17.9211 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.241e+01 1.523e+00 34.42 \u0026lt;2e-16 ***\r## disp -1.002e-01 6.638e-03 -15.09 \u0026lt;2e-16 ***\r## hp -2.198e-01 1.987e-02 -11.06 \u0026lt;2e-16 ***\r## disp:hp 5.658e-04 5.165e-05 10.96 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 3.896 on 379 degrees of freedom\r## Multiple R-squared: 0.7554, Adjusted R-squared: 0.7535 ## F-statistic: 390.2 on 3 and 379 DF, p-value: \u0026lt; 2.2e-16\rUsing summary() we focus on the row for disp:hp which tests,\n\\[\rH_0: \\beta_3 = 0.\r\\]\nAgain, we see a very low p-value so we reject the null (additive model) in favor of the interaction model. Again, there is an equivalent \\(F\\)-test.\nanova(mpg_disp_add_hp, mpg_disp_int_hp)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp + hp\r## Model 2: mpg ~ disp * hp\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 380 7576.6 ## 2 379 5754.2 1 1822.3 120.03 \u0026lt; 2.2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rWe can take a closer look at the coefficients of our fitted interaction model.\ncoef(mpg_disp_int_hp)\r## (Intercept) disp hp disp:hp ## 52.4081997848 -0.1001737655 -0.2198199720 0.0005658269\r\r\\(\\hat{\\beta}_0 = 52.4081998\\) is the estimated average mpg for a car with 0 disp and 0 hp.\r\\(\\hat{\\beta}_1 = -0.1001738\\) is the estimated change in average mpg for an increase in 1 disp, for a car with 0 hp.\r\\(\\hat{\\beta}_2 = -0.21982\\) is the estimated change in average mpg for an increase in 1 hp, for a car with 0 disp.\r\\(\\hat{\\beta}_3 = 5.658269\\times 10^{-4}\\) is an estimate of the modification to the change in average mpg for an increase in disp, for a car of a certain hp (or vice versa).\r\rThat last coefficient needs further explanation. Recall the rearrangement we made earlier\n\\[\rY = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon.\r\\]\nSo, our estimate for \\(\\beta_1 + \\beta_3 x_2\\), is \\(\\hat{\\beta}_1 + \\hat{\\beta}_3 x_2\\), which in this case is\n\\[\r-0.1001738 + 5.658269\\times 10^{-4} x_2.\r\\]\nThis says that, for an increase of one disp we see an estimated change in average mpg of \\(-0.1001738 + 5.658269\\times 10^{-4} x_2\\). So how disp and mpg are related, depends on the hp of the car.\nSo for a car with 50 hp, the estimated change in average mpg for an increase of one disp is\n\\[\r-0.1001738 + 5.658269\\times 10^{-4} \\cdot 50 = -0.0718824\r\\]\nAnd for a car with 350 hp, the estimated change in average mpg for an increase of one disp is\n\\[\r-0.1001738 + 5.658269\\times 10^{-4} \\cdot 350 = 0.0978657\r\\]\nNotice the sign changed!\n\rFactor Variables\rSo far in this chapter, we have limited our use of categorical variables to binary categorical variables. Specifically, we have limited ourselves to dummy variables which take a value of 0 or 1 and represent a categorical variable numerically.\nWe will now discuss factor variables, which is a special way that R deals with categorical variables. With factor variables, a human user can simply think about the categories of a variable, and R will take care of the necessary dummy variables without any 0/1 assignment being done by the user.\nis.factor(autompg$domestic)\r## [1] FALSE\rEarlier when we used the domestic variable, it was not a factor variable. It was simply a numerical variable that only took two possible values, 1 for domestic, and 0 for foreign. Let’s create a new variable origin that stores the same information, but in a different way.\nautompg$origin[autompg$domestic == 1] = \u0026quot;domestic\u0026quot;\rautompg$origin[autompg$domestic == 0] = \u0026quot;foreign\u0026quot;\rhead(autompg$origin)\r## [1] \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot;\rNow the origin variable stores \"domestic\" for domestic cars and \"foreign\" for foreign cars.\nis.factor(autompg$origin)\r## [1] FALSE\rHowever, this is simply a vector of character values. A vector of car models is a character variable in R. A vector of Vehicle Identification Numbers (VINs) is a character variable as well. But those don’t represent a short list of levels that might influence a response variable. We will want to coerce this origin variable to be something more: a factor variable.\nautompg$origin = as.factor(autompg$origin)\rNow when we check the structure of the autompg dataset, we see that origin is a factor variable.\nstr(autompg)\r## \u0026#39;data.frame\u0026#39;: 383 obs. of 9 variables:\r## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ...\r## $ cyl : Factor w/ 3 levels \u0026quot;4\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;8\u0026quot;: 3 3 3 3 3 3 3 3 3 3 ...\r## $ disp : num 307 350 318 304 302 429 454 440 455 390 ...\r## $ hp : num 130 165 150 150 140 198 220 215 225 190 ...\r## $ wt : num 3504 3693 3436 3433 3449 ...\r## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...\r## $ year : int 70 70 70 70 70 70 70 70 70 70 ...\r## $ origin : Factor w/ 2 levels \u0026quot;domestic\u0026quot;,\u0026quot;foreign\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ...\r## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ...\rFactor variables have levels which are the possible values (categories) that the variable may take, in this case foreign or domestic.\nlevels(autompg$origin)\r## [1] \u0026quot;domestic\u0026quot; \u0026quot;foreign\u0026quot;\rRecall that previously we have fit the model\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x_1\\) is disp, the displacement in cubic inches,\r\\(x_2\\) is domestic a dummy variable where 1 indicates a domestic car.\r\r(mod_dummy = lm(mpg ~ disp * domestic, data = autompg))\r## ## Call:\r## lm(formula = mpg ~ disp * domestic, data = autompg)\r## ## Coefficients:\r## (Intercept) disp domestic disp:domestic ## 46.0548 -0.1569 -12.5755 0.1025\rSo here we see that\n\\[\r\\hat{\\beta}_0 + \\hat{\\beta}_2 = 46.0548423 + -12.5754714 = 33.4793709\r\\]\nis the estimated average mpg for a domestic car with 0 disp.\nNow let’s try to do the same, but using our new factor variable.\n(mod_factor = lm(mpg ~ disp * origin, data = autompg))\r## ## Call:\r## lm(formula = mpg ~ disp * origin, data = autompg)\r## ## Coefficients:\r## (Intercept) disp originforeign disp:originforeign ## 33.47937 -0.05441 12.57547 -0.10252\rIt seems that it doesn’t produce the same results. Right away we notice that the intercept is different, as is the the coefficient in front of disp. We also notice that the remaining two coefficients are of the same magnitude as their respective counterparts using the domestic variable, but with a different sign. Why is this happening?\nIt turns out, that by using a factor variable, R is automatically creating a dummy variable for us. However, it is not the dummy variable that we had originally used ourselves.\nR is fitting the model\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x_1\\) is disp, the displacement in cubic inches,\r\\(x_2\\) is a dummy variable created by R. It uses 1 to represent a foreign car.\r\rSo now,\n\\[\r\\hat{\\beta}_0 = 33.4793709\r\\]\nis the estimated average mpg for a domestic car with 0 disp, which is indeed the same as before.\nWhen R created \\(x_2\\), the dummy variable, it used domestic cars as the reference level, that is the default value of the factor variable. So when the dummy variable is 0, the model represents this reference level, which is domestic. (R makes this choice because domestic comes before foreign alphabetically.)\nSo the two models have different estimated coefficients, but due to the different model representations, they are actually the same model.\nFactors with More Than Two Levels\rLet’s now consider a factor variable with more than two levels. In this dataset, cyl is an example.\nis.factor(autompg$cyl)\r## [1] TRUE\rlevels(autompg$cyl)\r## [1] \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot;\rHere the cyl variable has three possible levels: 4, 6, and 8. You may wonder, why not simply use cyl as a numerical variable? You certainly could.\nHowever, that would force the difference in average mpg between 4 and 6 cylinders to be the same as the difference in average mpg between 6 and 8 cylinders. That usually make senses for a continuous variable, but not for a discrete variable with so few possible values. In the case of this variable, there is no such thing as a 7-cylinder engine or a 6.23-cylinder engine in personal vehicles. For these reasons, we will simply consider cyl to be categorical. This is a decision that will commonly need to be made with ordinal variables. Often, with a large number of categories, the decision to treat them as numerical variables is appropriate because, otherwise, a large number of dummy variables are then needed to represent these variables.\nLet’s define three dummy variables related to the cyl factor variable.\n\\[\rv_1 =\r\\begin{cases}\r1 \u0026amp; \\text{4 cylinder} \\\\\r0 \u0026amp; \\text{not 4 cylinder}\r\\end{cases}\r\\]\n\\[\rv_2 =\r\\begin{cases}\r1 \u0026amp; \\text{6 cylinder} \\\\\r0 \u0026amp; \\text{not 6 cylinder}\r\\end{cases}\r\\]\n\\[\rv_3 =\r\\begin{cases}\r1 \u0026amp; \\text{8 cylinder} \\\\\r0 \u0026amp; \\text{not 8 cylinder}\r\\end{cases}\r\\]\nNow, let’s fit an additive model in R, using mpg as the response, and disp and cyl as predictors. This should be a model that uses “three regression lines” to model mpg, one for each of the possible cyl levels. They will all have the same slope (since it is an additive model), but each will have its own intercept.\n(mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg))\r## ## Call:\r## lm(formula = mpg ~ disp + cyl, data = autompg)\r## ## Coefficients:\r## (Intercept) disp cyl6 cyl8 ## 34.99929 -0.05217 -3.63325 -2.03603\rThe question is, what is the model that R has fit here? It has chosen to use the model\n\\[\rY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon,\r\\]\nwhere\n\r\\(Y\\) is mpg, the fuel efficiency in miles per gallon,\r\\(x\\) is disp, the displacement in cubic inches,\r\\(v_2\\) and \\(v_3\\) are the dummy variables define above.\r\rWhy doesn’t R use \\(v_1\\)? Essentially because it doesn’t need to. To create three lines, it only needs two dummy variables since it is using a reference level, which in this case is a 4 cylinder car. The three “sub models” are then:\n\r4 Cylinder: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\)\r6 Cylinder: \\(Y = (\\beta_0 + \\beta_2) + \\beta_1 x + \\epsilon\\)\r8 Cylinder: \\(Y = (\\beta_0 + \\beta_3) + \\beta_1 x + \\epsilon\\)\r\rNotice that they all have the same slope. However, using the two dummy variables, we achieve the three intercepts.\n\r\\(\\beta_0\\) is the average mpg for a 4 cylinder car with 0 disp.\r\\(\\beta_0 + \\beta_2\\) is the average mpg for a 6 cylinder car with 0 disp.\r\\(\\beta_0 + \\beta_3\\) is the average mpg for a 8 cylinder car with 0 disp.\r\rSo because 4 cylinder is the reference level, \\(\\beta_0\\) is specific to 4 cylinders, but \\(\\beta_2\\) and \\(\\beta_3\\) are used to represent quantities relative to 4 cylinders.\nAs we have done before, we can extract these intercepts and slopes for the three lines, and plot them accordingly.\nint_4cyl = coef(mpg_disp_add_cyl)[1]\rint_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3]\rint_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4]\rslope_all_cyl = coef(mpg_disp_add_cyl)[2]\rplot_colors = c(\u0026quot;Darkorange\u0026quot;, \u0026quot;Darkgrey\u0026quot;, \u0026quot;Dodgerblue\u0026quot;)\rplot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))\rabline(int_4cyl, slope_all_cyl, col = plot_colors[1], lty = 1, lwd = 2)\rabline(int_6cyl, slope_all_cyl, col = plot_colors[2], lty = 2, lwd = 2)\rabline(int_8cyl, slope_all_cyl, col = plot_colors[3], lty = 3, lwd = 2)\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;4 Cylinder\u0026quot;, \u0026quot;6 Cylinder\u0026quot;, \u0026quot;8 Cylinder\u0026quot;),\rcol = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))\rOn this plot, we have\n\r4 Cylinder: orange dots, solid orange line.\r6 Cylinder: grey dots, dashed grey line.\r8 Cylinder: blue dots, dotted blue line.\r\rThe odd result here is that we’re estimating that 8 cylinder cars have better fuel efficiency than 6 cylinder cars at any displacement! The dotted blue line is always above the dashed grey line. That doesn’t seem right. Maybe for very large displacement engines that could be true, but that seems wrong for medium to low displacement.\nTo attempt to fix this, we will try using an interaction model, that is, instead of simply three intercepts and one slope, we will allow for three slopes. Again, we’ll let R take the wheel, (no pun intended) then figure out what model it has applied.\n(mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg))\r## ## Call:\r## lm(formula = mpg ~ disp * cyl, data = autompg)\r## ## Coefficients:\r## (Intercept) disp cyl6 cyl8 disp:cyl6 disp:cyl8 ## 43.59052 -0.13069 -13.20026 -20.85706 0.08299 0.10817\r# could also use mpg ~ disp + cyl + disp:cyl\rR has again chosen to use 4 cylinder cars as the reference level, but this also now has an effect on the interaction terms. R has fit the model.\n\\[\rY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon\r\\]\nWe’re using \\(\\gamma\\) like a \\(\\beta\\) parameter for simplicity, so that, for example \\(\\beta_2\\) and \\(\\gamma_2\\) are both associated with \\(v_2\\).\nNow, the three “sub models” are:\n\r4 Cylinder: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\).\r6 Cylinder: \\(Y = (\\beta_0 + \\beta_2) + (\\beta_1 + \\gamma_2) x + \\epsilon\\).\r8 Cylinder: \\(Y = (\\beta_0 + \\beta_3) + (\\beta_1 + \\gamma_3) x + \\epsilon\\).\r\rInterpreting some parameters and coefficients then:\n\r\\((\\beta_0 + \\beta_2)\\) is the average mpg of a 6 cylinder car with 0 disp\r\\((\\hat{\\beta}_1 + \\hat{\\gamma}_3) = -0.1306935 + 0.1081714 = -0.0225221\\) is the estimated change in average mpg for an increase of one disp, for an 8 cylinder car.\r\rSo, as we have seen before \\(\\beta_2\\) and \\(\\beta_3\\) change the intercepts for 6 and 8 cylinder cars relative to the reference level of \\(\\beta_0\\) for 4 cylinder cars.\nNow, similarly \\(\\gamma_2\\) and \\(\\gamma_3\\) change the slopes for 6 and 8 cylinder cars relative to the reference level of \\(\\beta_1\\) for 4 cylinder cars.\nOnce again, we extract the coefficients and plot the results.\nint_4cyl = coef(mpg_disp_int_cyl)[1]\rint_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3]\rint_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4]\rslope_4cyl = coef(mpg_disp_int_cyl)[2]\rslope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5]\rslope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6]\rplot_colors = c(\u0026quot;Darkorange\u0026quot;, \u0026quot;Darkgrey\u0026quot;, \u0026quot;Dodgerblue\u0026quot;)\rplot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))\rabline(int_4cyl, slope_4cyl, col = plot_colors[1], lty = 1, lwd = 2)\rabline(int_6cyl, slope_6cyl, col = plot_colors[2], lty = 2, lwd = 2)\rabline(int_8cyl, slope_8cyl, col = plot_colors[3], lty = 3, lwd = 2)\rlegend(\u0026quot;topright\u0026quot;, c(\u0026quot;4 Cylinder\u0026quot;, \u0026quot;6 Cylinder\u0026quot;, \u0026quot;8 Cylinder\u0026quot;),\rcol = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))\rThis looks much better! We can see that for medium displacement cars, 6 cylinder cars now perform better than 8 cylinder cars, which seems much more reasonable than before.\nTo completely justify the interaction model (i.e., a unique slope for each cyl level) compared to the additive model (single slope), we can perform an \\(F\\)-test. Notice first, that there is no \\(t\\)-test that will be able to do this since the difference between the two models is not a single parameter.\nWe will test,\n\\[\rH_0: \\gamma_2 = \\gamma_3 = 0\r\\]\nwhich represents the parallel regression lines we saw before,\n\\[\rY = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon.\r\\]\nAgain, this is a difference of two parameters, thus no \\(t\\)-test will be useful.\nanova(mpg_disp_add_cyl, mpg_disp_int_cyl)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp + cyl\r## Model 2: mpg ~ disp * cyl\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 379 7299.5 ## 2 377 6551.7 2 747.79 21.515 1.419e-09 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rAs expected, we see a very low p-value, and thus reject the null. We prefer the interaction model over the additive model.\nRecapping a bit:\n\rNull Model: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon\\)\r\rNumber of parameters: \\(q = 4\\)\r\rFull Model: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon\\)\r\rNumber of parameters: \\(p = 6\\)\r\r\rlength(coef(mpg_disp_int_cyl)) - length(coef(mpg_disp_add_cyl))\r## [1] 2\rWe see there is a difference of two parameters, which is also displayed in the resulting ANOVA table from R. Notice that the following two values also appear on the ANOVA table.\nnrow(autompg) - length(coef(mpg_disp_int_cyl))\r## [1] 377\rnrow(autompg) - length(coef(mpg_disp_add_cyl))\r## [1] 379\r\r\rParameterization\rSo far we have been simply letting R decide how to create the dummy variables, and thus R has been deciding the parameterization of the models. To illustrate the ability to use alternative parameterizations, we will recreate the data, but directly creating the dummy variables ourselves.\nnew_param_data = data.frame(\ry = autompg$mpg,\rx = autompg$disp,\rv1 = 1 * as.numeric(autompg$cyl == 4),\rv2 = 1 * as.numeric(autompg$cyl == 6),\rv3 = 1 * as.numeric(autompg$cyl == 8))\rhead(new_param_data, 20)\r## y x v1 v2 v3\r## 1 18 307 0 0 1\r## 2 15 350 0 0 1\r## 3 18 318 0 0 1\r## 4 16 304 0 0 1\r## 5 17 302 0 0 1\r## 6 15 429 0 0 1\r## 7 14 454 0 0 1\r## 8 14 440 0 0 1\r## 9 14 455 0 0 1\r## 10 15 390 0 0 1\r## 11 15 383 0 0 1\r## 12 14 340 0 0 1\r## 13 15 400 0 0 1\r## 14 14 455 0 0 1\r## 15 24 113 1 0 0\r## 16 22 198 0 1 0\r## 17 18 199 0 1 0\r## 18 21 200 0 1 0\r## 19 27 97 1 0 0\r## 20 26 97 1 0 0\rNow,\n\ry is mpg\rx is disp, the displacement in cubic inches,\rv1, v2, and v3 are dummy variables as defined above.\r\rFirst let’s try to fit an additive model using x as well as the three dummy variables.\nlm(y ~ x + v1 + v2 + v3, data = new_param_data)\r## ## Call:\r## lm(formula = y ~ x + v1 + v2 + v3, data = new_param_data)\r## ## Coefficients:\r## (Intercept) x v1 v2 v3 ## 32.96326 -0.05217 2.03603 -1.59722 NA\rWhat is happening here? Notice that R is essentially ignoring v3, but why? Well, because R uses an intercept, it cannot also use v3. This is because\n\\[\r\\boldsymbol{1} = v_1 + v_2 + v_3\r\\]\nwhich means that \\(\\boldsymbol{1}\\), \\(v_1\\), \\(v_2\\), and \\(v_3\\) are linearly dependent. This would make the \\(X^\\top X\\) matrix singular, but we need to be able to invert it to solve the normal equations and obtain \\(\\hat{\\beta}.\\) With the intercept, v1, and v2, R can make the necessary “three intercepts”. So, in this case v3 is the reference level.\nIf we remove the intercept, then we can directly obtain all “three intercepts” without a reference level.\nlm(y ~ 0 + x + v1 + v2 + v3, data = new_param_data)\r## ## Call:\r## lm(formula = y ~ 0 + x + v1 + v2 + v3, data = new_param_data)\r## ## Coefficients:\r## x v1 v2 v3 ## -0.05217 34.99929 31.36604 32.96326\rHere, we are fitting the model\n\\[\rY = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta x +\\epsilon.\r\\]\nThus we have:\n\r4 Cylinder: \\(Y = \\mu_1 + \\beta x + \\epsilon\\)\r6 Cylinder: \\(Y = \\mu_2 + \\beta x + \\epsilon\\)\r8 Cylinder: \\(Y = \\mu_3 + \\beta x + \\epsilon\\)\r\rWe could also do something similar with the interaction model, and give each line an intercept and slope, without the need for a reference level.\nlm(y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)\r## ## Call:\r## lm(formula = y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)\r## ## Coefficients:\r## v1 v2 v3 v1:x v2:x v3:x ## 43.59052 30.39026 22.73346 -0.13069 -0.04770 -0.02252\r\\[\rY = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta_1 x v_1 + \\beta_2 x v_2 + \\beta_3 x v_3 +\\epsilon\r\\]\n\r4 Cylinder: \\(Y = \\mu_1 + \\beta_1 x + \\epsilon\\)\r6 Cylinder: \\(Y = \\mu_2 + \\beta_2 x + \\epsilon\\)\r8 Cylinder: \\(Y = \\mu_3 + \\beta_3 x + \\epsilon\\)\r\rUsing the original data, we have (at least) three equivalent ways to specify the interaction model with R.\nlm(mpg ~ disp * cyl, data = autompg)\r## ## Call:\r## lm(formula = mpg ~ disp * cyl, data = autompg)\r## ## Coefficients:\r## (Intercept) disp cyl6 cyl8 disp:cyl6 disp:cyl8 ## 43.59052 -0.13069 -13.20026 -20.85706 0.08299 0.10817\rlm(mpg ~ 0 + cyl + disp : cyl, data = autompg)\r## ## Call:\r## lm(formula = mpg ~ 0 + cyl + disp:cyl, data = autompg)\r## ## Coefficients:\r## cyl4 cyl6 cyl8 cyl4:disp cyl6:disp cyl8:disp ## 43.59052 30.39026 22.73346 -0.13069 -0.04770 -0.02252\rlm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg)\r## ## Call:\r## lm(formula = mpg ~ 0 + disp + cyl + disp:cyl, data = autompg)\r## ## Coefficients:\r## disp cyl4 cyl6 cyl8 disp:cyl6 disp:cyl8 ## -0.13069 43.59052 30.39026 22.73346 0.08299 0.10817\rThey all fit the same model, importantly each using six parameters, but the coefficients mean slightly different things in each. However, once they are interpreted as slopes and intercepts for the “three lines” they will have the same result.\nUse ?all.equal to learn about the all.equal() function, and think about how the following code verifies that the residuals of the two models are the same.\nall.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)),\rfitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)))\r## [1] TRUE\r\rBuilding Larger Models\rNow that we have seen how to incorporate categorical predictors as well as interaction terms, we can start to build much larger, much more flexible models which can potentially fit data better.\nLet’s define a “big” model,\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon.\r\\]\nHere,\n\r\\(Y\\) is mpg.\r\\(x_1\\) is disp.\r\\(x_2\\) is hp.\r\\(x_3\\) is domestic, which is a dummy variable we defined, where 1 is a domestic vehicle.\r\rFirst thing to note here, we have included a new term \\(x_1 x_2 x_3\\) which is a three-way interaction. Interaction terms can be larger and larger, up to the number of predictors in the model.\nSince we are using the three-way interaction term, we also use all possible two-way interactions, as well as each of the first order (main effect) terms. This is the concept of a hierarchy. Any time a “higher-order” term is in a model, the related “lower-order” terms should also be included. Mathematically their inclusion or exclusion is sometimes irrelevant, but from an interpretation standpoint, it is best to follow the hierarchy rules.\nLet’s do some rearrangement to obtain a “coefficient” in front of \\(x_1\\).\n\\[\rY = \\beta_0 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_6 x_2 x_3 + (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)x_1 + \\epsilon.\r\\]\nSpecifically, the “coefficient” in front of \\(x_1\\) is\n\\[\r(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3).\r\\]\nLet’s discuss this “coefficient” to help us understand the idea of the flexibility of a model. Recall that,\n\r\\(\\beta_1\\) is the coefficient for a first order term,\r\\(\\beta_4\\) and \\(\\beta_5\\) are coefficients for two-way interactions,\r\\(\\beta_7\\) is the coefficient for the three-way interaction.\r\rIf the two and three way interactions were not in the model, the whole “coefficient” would simply be\n\\[\r\\beta_1.\r\\]\nThus, no matter the values of \\(x_2\\) and \\(x_3\\), \\(\\beta_1\\) would determine the relationship between \\(x_1\\) (disp) and \\(Y\\) (mpg).\nWith the addition of the two-way interactions, now the “coefficient” would be\n\\[\r(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3).\r\\]\nNow, changing \\(x_1\\) (disp) has a different effect on \\(Y\\) (mpg), depending on the values of \\(x_2\\) and \\(x_3\\).\nLastly, adding the three-way interaction gives the whole “coefficient”\n\\[\r(\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)\r\\]\nwhich is even more flexible. Now changing \\(x_1\\) (disp) has a different effect on \\(Y\\) (mpg), depending on the values of \\(x_2\\) and \\(x_3\\), but in a more flexible way which we can see with some more rearrangement. Now the “coefficient” in front of \\(x_3\\) in this “coefficient” is dependent on \\(x_2\\).\n\\[\r(\\beta_1 + \\beta_4 x_2 + (\\beta_5 + \\beta_7 x_2) x_3)\r\\]\nIt is so flexible, it is becoming hard to interpret!\nLet’s fit this three-way interaction model in R.\nbig_model = lm(mpg ~ disp * hp * domestic, data = autompg)\rsummary(big_model)\r## ## Call:\r## lm(formula = mpg ~ disp * hp * domestic, data = autompg)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -11.9410 -2.2147 -0.4008 1.9430 18.4094 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 6.065e+01 6.600e+00 9.189 \u0026lt; 2e-16 ***\r## disp -1.416e-01 6.344e-02 -2.232 0.0262 * ## hp -3.545e-01 8.123e-02 -4.364 1.65e-05 ***\r## domestic -1.257e+01 7.064e+00 -1.780 0.0759 . ## disp:hp 1.369e-03 6.727e-04 2.035 0.0426 * ## disp:domestic 4.933e-02 6.400e-02 0.771 0.4414 ## hp:domestic 1.852e-01 8.709e-02 2.126 0.0342 * ## disp:hp:domestic -9.163e-04 6.768e-04 -1.354 0.1766 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 3.88 on 375 degrees of freedom\r## Multiple R-squared: 0.76, Adjusted R-squared: 0.7556 ## F-statistic: 169.7 on 7 and 375 DF, p-value: \u0026lt; 2.2e-16\rDo we actually need this large of a model? Let’s first test for the necessity of the three-way interaction term. That is,\n\\[\rH_0: \\beta_7 = 0.\r\\]\nSo,\n\rFull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon\\)\rNull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\)\r\rWe fit the null model in R as two_way_int_mod, then use anova() to perform an \\(F\\)-test as usual.\ntwo_way_int_mod = lm(mpg ~ disp * hp + disp * domestic + hp * domestic, data = autompg)\r#two_way_int_mod = lm(mpg ~ (disp + hp + domestic) ^ 2, data = autompg)\ranova(two_way_int_mod, big_model)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp * hp + disp * domestic + hp * domestic\r## Model 2: mpg ~ disp * hp * domestic\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F)\r## 1 376 5673.2 ## 2 375 5645.6 1 27.599 1.8332 0.1766\rWe see the p-value is somewhat large, so we would fail to reject. We prefer the smaller, less flexible, null model, without the three-way interaction.\nA quick note here: the full model does still “fit better.” Notice that it has a smaller RMSE than the null model, which means the full model makes smaller (squared) errors on average.\nmean(resid(big_model) ^ 2)\r## [1] 14.74053\rmean(resid(two_way_int_mod) ^ 2)\r## [1] 14.81259\rHowever, it is not much smaller. We could even say that, the difference is insignificant. This is an idea we will return to later in greater detail.\nNow that we have chosen the model without the three-way interaction, can we go further? Do we need the two-way interactions? Let’s test\n\\[\rH_0: \\beta_4 = \\beta_5 = \\beta_6 = 0.\r\\]\nRemember we already chose \\(\\beta_7 = 0\\), so,\n\rFull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\)\rNull Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\)\r\rWe fit the null model in R as additive_mod, then use anova() to perform an \\(F\\)-test as usual.\nadditive_mod = lm(mpg ~ disp + hp + domestic, data = autompg)\ranova(additive_mod, two_way_int_mod)\r## Analysis of Variance Table\r## ## Model 1: mpg ~ disp + hp + domestic\r## Model 2: mpg ~ disp * hp + disp * domestic + hp * domestic\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 379 7369.7 ## 2 376 5673.2 3 1696.5 37.478 \u0026lt; 2.2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rHere the p-value is small, so we reject the null, and we prefer the full (alternative) model. Of the models we have considered, our final preference is for\n\\[\rY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon.\r\\]\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602332948,"objectID":"a66b8951081722f5755747f3314e83f0","permalink":"https://datavizm20.classes.andrewheiss.com/example/06-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/06-example/","section":"example","summary":"Preliminaries\rDummy Variables\rInteractions\rFactor Variables\rFactors with More Than Two Levels\r\rParameterization\rBuilding Larger Models\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\rPreliminaries\rSo far in each of our analyses, we have only used numeric variables as predictors.","tags":null,"title":"Linear Regression: Interpreting Coefficients","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rBackstory and Set Up\rLinear Models\r\rAssesing Model Accuracy\rModel Complexity\rTest-Train Split\rAdding Flexibility to Linear Models\r\r\r\rREAD THIS CAREFULLY\nThe content below describes both Lab 7 and Lab 8. Lab 7 is Exercise 1; Lab 8 is Exercise 2. Also, you may find some other tasks in the text…\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 26 for Lab 7; turn in Lab 8 by 11:59 PM Eastern Time on Monday, November 2.\n\rBackstory and Set Up\rYou still work for Zillow as a junior analyst (sorry). But you’re hunting a promotion. Your job is to present some more advanced predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nAs always, let’s load the data.\nAmes \u0026lt;- read.table(\u0026quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv\u0026quot;,\rheader = TRUE,\rsep = \u0026quot;,\u0026quot;)\r\rLinear Models\rWhen exploring linear models in other classes, we often emphasize asymptotic results under distributional assumptions. That is, we make assumptions about the model in order to derive properties of large samples. This general approach is useful for creating and performing hypothesis tests. Frequently, when developing a linear regression model, part of the goal is to explain a relationship. However, this isn’t always the case. And it’s often not a valid approach, as we discussed in this week’s content.\nSo, we will ignore much of what we have learned in other classes (sorry, EC420) and instead simply use regression as a tool to predict. Instead of a model which supposedly explains relationships, we seek a model which minimizes errors.\nTo discuss linear models in the context of prediction, we return to the Ames data. Accordingly, you should utilize some of the early code from Lab 2 to hasten your progress in this lab.\nAssesing Model Accuracy\rThere are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.\n\\[\r\\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\r\\]\nWhile for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be \\(n\\).\nFor a linear model , the estimate of \\(f\\), \\(\\hat{f}\\), is given by the fitted regression line.\n\\[\r\\hat{y}({\\bf{x}_i}) = \\hat{f}({\\bf{x}_i})\r\\]\nWe can write an R function that will be useful for performing this calculation.\nrmse = function(actual, predicted) {\rsqrt(mean((actual - predicted) ^ 2))\r}\r\rModel Complexity\rAside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, \\(p\\).\nWe write a simple R function to extract this information from a model.\nget_complexity = function(model) {\rlength(coef(model)) - 1\r}\rWhen deciding how complex of a model to use, we can utilize two techniques: forward selection or backward selection. Forward selection means that we start with the simplest model (with a single predictor) and then add one at a time until we decide to stop. Backward selection means that we start with the most complex model (with every available predictor) and then remove one at a time until we decide to stop. There are many criteria for “when to stop”. Below, we’ll try to give you some intuition on the model-building process.\nEXERCISE 1\nLoad the Ames data. Drop the variables OverallCond and OverallQual.\n\rUsing forward selection (that is, select one variable, then select another) create a series of models up to complexity length 15. You may use any variable within the dataset, including categorical variables.\n\rCreate a chart plotting the model complexity as the \\(x\\)-axis variable and RMSE as the \\(y\\)-axis variable. Describe any patterns you see. Do you think you should use the full-size model? Why or why not? What criterion are you using to make this statement?\n\r\r\rWeekly writing: After completing the exercise above, reflect on the process. Was this efficient? Was it enjoyable? Do you think you created an highly predictive model? Write a short paragraph. As always, submit this separately into the “weekly writing” assigment on D2L.\n\rTest-Train Split\rThere is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts: it is essentially cheating. As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down—or, in very specific cases where a new predictor is completely uncorrelated with the target, stay the same. This might seem to suggest that in order to predict well, we should use the largest possible model. However, in reality we have fit to a specific dataset, but as soon as we see new data, a large model may (in fact) predict poorly. This is called overfitting.\nThe most common approach to overfitting is to take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data. Test data should never be used to train a model—its pupose is to evaluate the fitted model once you’ve settled on something.1\nHere we use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to allow use to reproduce the same random split each time we perform this analysis. Sometimes we don’t want to do this; if we want to run lots of independent splits, then we do not need to set the initial seed.\nset.seed(9)\rnum_obs = nrow(Ames)\rtrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\rtrain_data = Ames[train_index, ]\rtest_data = Ames[-train_index, ]\rWe will look at two measures that assess how well a model is predicting: train RMSE and test RMSE.\n\\[\r\\text{RMSE}_\\text{Train} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\sum_{i \\in \\text{Train}}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\r\\]\nHere \\(n_{Tr}\\) is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.\n\\[\r\\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\sum_{i \\in \\text{Test}} \\left ( y_i - \\hat{f}(\\bf{x}_i) \\right ) ^2}\r\\]\nHere \\(n_{Te}\\) is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.\nWe will start with the simplest possible linear model, that is, a model with no predictors.\nfit_0 = lm(SalePrice ~ 1, data = train_data)\rget_complexity(fit_0)\r## [1] 0\r# train RMSE\rsqrt(mean((train_data$SalePrice - predict(fit_0, train_data)) ^ 2))\r## [1] 80875.98\r# test RMSE\rsqrt(mean((test_data$SalePrice - predict(fit_0, test_data)) ^ 2))\r## [1] 77928.62\rThe previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.\n# train RMSE\rrmse(actual = train_data$SalePrice, predicted = predict(fit_0, train_data))\r## [1] 80875.98\r# test RMSE\rrmse(actual = test_data$SalePrice, predicted = predict(fit_0, test_data))\r## [1] 77928.62\rThis function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.\nget_rmse = function(model, data, response) {\rrmse(actual = subset(data, select = response, drop = TRUE),\rpredicted = predict(model, data))\r}\rBy using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.\nget_rmse(model = fit_0, data = train_data, response = \u0026quot;SalePrice\u0026quot;) # train RMSE\r## [1] 80875.98\rget_rmse(model = fit_0, data = test_data, response = \u0026quot;SalePrice\u0026quot;) # test RMSE\r## [1] 77928.62\rTry it: Apply this basic function with different arguments. Do you understand how we’ve nested functions within functions?\nTry it: Define a total of five models using the first five models you fit in Exercise 1. Define these as fit_1 through fit_5\n\rAdding Flexibility to Linear Models\rEach successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting. To better understand the relationship between train RMSE, test RMSE, and model complexity, we’ll explore the results from Exercise 1.\nHopefully, you tried the in-line excercise above. If so, we can create a list of the models fit.\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\rWe then obtain train RMSE, test RMSE, and model complexity for each. In doing so, we’ll introduce a handy function from R called sapply(). You can likely intuit what it does by looking at the code below.\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \u0026quot;SalePrice\u0026quot;)\rtest_rmse = sapply(model_list, get_rmse, data = test_data, response = \u0026quot;SalePrice\u0026quot;)\rmodel_complexity = sapply(model_list, get_complexity)\rTry it: Run ?sapply() to understand what are valid arguments to the function.\nOnce you’ve done this, you’ll notice the following:\n# This is the same as the apply command above\rtest_rmse = c(get_rmse(fit_1, test_data, \u0026quot;SalePrice\u0026quot;),\rget_rmse(fit_2, test_data, \u0026quot;SalePrice\u0026quot;),\rget_rmse(fit_3, test_data, \u0026quot;SalePrice\u0026quot;),\rget_rmse(fit_4, test_data, \u0026quot;SalePrice\u0026quot;),\rget_rmse(fit_5, test_data, \u0026quot;SalePrice\u0026quot;))\rWe can plot the results. If you execute the code below, you’ll see the train RMSE in blue, while the test RMSE is given in orange.2\nplot(model_complexity, train_rmse, type = \u0026quot;b\u0026quot;,\rylim = c(min(c(train_rmse, test_rmse)) - 0.02,\rmax(c(train_rmse, test_rmse)) + 0.02),\rcol = \u0026quot;dodgerblue\u0026quot;,\rxlab = \u0026quot;Model Size\u0026quot;,\rylab = \u0026quot;RMSE\u0026quot;)\rlines(model_complexity, test_rmse, type = \u0026quot;b\u0026quot;, col = \u0026quot;darkorange\u0026quot;)\rWe could also summarize the results as a table. fit_1 is the least flexible, and fit_5 is the most flexible. If we were to do this (see the exercise below) we would see that Train RMSE decreases as flexibility increases forever. However, this may not be the case for the Test RMSE.\n\r\r\r\rModel\rTrain RMSE\rTest RMSE\rPredictors\r\r\r\rfit_1\rRMSE\\(_{\\text{train}}\\) for model 1\rRMSE\\(_{\\text{test}}\\) for model 1\rput predictors here\r\r…\r…\r….\r…\r\rfit_5\rRMSE\\(_{\\text{train}}\\) for model 5\rRMSE\\(_{\\text{train}}\\) for model 5\r\\(p\\) predictors\r\r\r\rTo summarize:\n\rUnderfitting models: In general High Train RMSE, High Test RMSE.\rOverfitting models: In general Low Train RMSE, High Test RMSE.\r\rSpecifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE.3 Then a model is underfitting if there exists a more complex model with lower Test RMSE.\nEXERCISE 2\n(AKA Lab 8)\nMake a table exactly like the table above for the 15 models you fit in Exercise 1.\n\rThis question should be the most time-consuming question. Using any method you choose and any number of regressors, predict SalePrice. Calculate the Train and Test RMSE. Your goal is to have a lower Test RMSE than others in the class.\n\rIn a short paragraph, describe the resulting model. Discuss how you arrived at this model, what interactions you’re using (if any) and how confident you are that your prediction will perform well, relative to other people in the class.\n\rDifficult; extra credit: Visualize your final model in a sensible way and provide a two-paragraph interpretation.\n\r\r\rA final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that predicted well, and paid no attention to a model for explaination. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)\n\r\r\rNote that sometimes the terms evaluation set and test set are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.↩︎\n\rThe train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. We often see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later. Also, we might intuitively expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get (un)lucky and this will not be true.↩︎\n\rThe labels of under and overfitting are relative to the best model we see. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.↩︎\n\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605796661,"objectID":"734e226209bb9037b527a2c610184860","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/07-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/07-assignment/","section":"assignment","summary":"Backstory and Set Up\rLinear Models\r\rAssesing Model Accuracy\rModel Complexity\rTest-Train Split\rAdding Flexibility to Linear Models\r\r\r\rREAD THIS CAREFULLY\nThe content below describes both Lab 7 and Lab 8. Lab 7 is Exercise 1; Lab 8 is Exercise 2. Also, you may find some other tasks in the text…\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 26 for Lab 7; turn in Lab 8 by 11:59 PM Eastern Time on Monday, November 2.","tags":null,"title":"Advanced Model Building","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rModel Selection\rAssesing Model Accuracy\rModel Complexity\rTest-Train Split\rAdding Flexibility to Linear Models\rChoosing a Model\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\rModel Selection\rOften when we are developing a linear regression model, part of our goal is to explain a relationship. Now, we will ignore much of what we have learned and instead simply use regression as a tool to predict. Instead of a model which explains relationships, we seek a model which minimizes errors.\nFirst, note that a linear model is one of many methods used in regression.\nTo discuss linear models in the context of prediction, we introduce the (very boring) Advertising data that is discussed in the ISL text (see supplemental readings).\nAdvertising\r## # A tibble: 200 x 4\r## TV Radio Newspaper Sales\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 230. 37.8 69.2 22.1\r## 2 44.5 39.3 45.1 10.4\r## 3 17.2 45.9 69.3 9.3\r## 4 152. 41.3 58.5 18.5\r## 5 181. 10.8 58.4 12.9\r## 6 8.7 48.9 75 7.2\r## 7 57.5 32.8 23.5 11.8\r## 8 120. 19.6 11.6 13.2\r## 9 8.6 2.1 1 4.8\r## 10 200. 2.6 21.2 10.6\r## # … with 190 more rows\rlibrary(caret)\rfeaturePlot(x = Advertising[ , c(\u0026quot;TV\u0026quot;, \u0026quot;Radio\u0026quot;, \u0026quot;Newspaper\u0026quot;)], y = Advertising$Sales)\r\rAssesing Model Accuracy\rThere are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.\n\\[\r\\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\r\\]\nWhile for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be \\(n\\).\nFor a linear model , the estimate of \\(f\\), \\(\\hat{f}\\), is given by the fitted regression line.\n\\[\r\\hat{y}({\\bf{x}_i}) = \\hat{f}({\\bf{x}_i})\r\\]\nWe can write an R function that will be useful for performing this calculation.\nrmse = function(actual, predicted) {\rsqrt(mean((actual - predicted) ^ 2))\r}\r\rModel Complexity\rAside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, \\(p\\).\nWe write a simple R function to extract this information from a model.\nget_complexity = function(model) {\rlength(coef(model)) - 1\r}\r\rTest-Train Split\rThere is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts. It is essentially cheating! As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down, or in very specific cases, stay the same.\nThis would suggest that to predict well, we should use the largest possible model! However, in reality we have hard fit to a specific dataset, but as soon as we see new data, a large model may in fact predict poorly. This is called overfitting.\nFrequently we will take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data. Test data should never be used to train a model.\nNote that sometimes the terms evaluation set and test set are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.\nHere we use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to allow use to reproduce the same random split each time we perform this analysis.\nset.seed(9)\rnum_obs = nrow(Advertising)\rtrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\rtrain_data = Advertising[train_index, ]\rtest_data = Advertising[-train_index, ]\rWe will look at two measures that assess how well a model is predicting, the train RMSE and the test RMSE.\n\\[\r\\text{RMSE}_{\\text{Train}} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\displaystyle\\sum_{i \\in \\text{Train}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\r\\]\nHere \\(n_{Tr}\\) is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.\n\\[\r\\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\displaystyle\\sum_{i \\in \\text{Test}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2}\r\\]\nHere \\(n_{Te}\\) is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.\nWe will start with the simplest possible linear model, that is, a model with no predictors.\nfit_0 = lm(Sales ~ 1, data = train_data)\rget_complexity(fit_0)\r## [1] 0\r# train RMSE\rsqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))\r## [1] 5.529258\r# test RMSE\rsqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2))\r## [1] 4.914163\rThe previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.\n# train RMSE\rrmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))\r## [1] 5.529258\r# test RMSE\rrmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))\r## [1] 4.914163\rThis function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.\nget_rmse = function(model, data, response) {\rrmse(actual = subset(data, select = response, drop = TRUE),\rpredicted = predict(model, data))\r}\rBy using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.\nget_rmse(model = fit_0, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 5.529258\rget_rmse(model = fit_0, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 4.914163\r\rAdding Flexibility to Linear Models\rEach successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.\nfit_1 = lm(Sales ~ ., data = train_data)\rget_complexity(fit_1)\r## [1] 3\rget_rmse(model = fit_1, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 1.888488\rget_rmse(model = fit_1, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 1.461661\rfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\rget_complexity(fit_2)\r## [1] 7\rget_rmse(model = fit_2, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 1.016822\rget_rmse(model = fit_2, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 0.9117228\rfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\rget_complexity(fit_3)\r## [1] 8\rget_rmse(model = fit_3, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 0.6553091\rget_rmse(model = fit_3, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 0.6633375\rfit_4 = lm(Sales ~ Radio * Newspaper * TV +\rI(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\rget_complexity(fit_4)\r## [1] 10\rget_rmse(model = fit_4, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 0.6421909\rget_rmse(model = fit_4, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 0.7465957\rfit_5 = lm(Sales ~ Radio * Newspaper * TV +\rI(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\rget_complexity(fit_5)\r## [1] 14\rget_rmse(model = fit_5, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE\r## [1] 0.6120887\rget_rmse(model = fit_5, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE\r## [1] 0.7864181\r\rChoosing a Model\rTo better understand the relationship between train RMSE, test RMSE, and model complexity, we summarize our results, as the above is somewhat cluttered.\nFirst, we recap the models that we have fit.\nfit_1 = lm(Sales ~ ., data = train_data)\rfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\rfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\rfit_4 = lm(Sales ~ Radio * Newspaper * TV +\rI(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\rfit_5 = lm(Sales ~ Radio * Newspaper * TV +\rI(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\rNext, we create a list of the models fit.\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\rWe then obtain train RMSE, test RMSE, and model complexity for each.\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \u0026quot;Sales\u0026quot;)\rtest_rmse = sapply(model_list, get_rmse, data = test_data, response = \u0026quot;Sales\u0026quot;)\rmodel_complexity = sapply(model_list, get_complexity)\rWe then plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\nplot(model_complexity, train_rmse, type = \u0026quot;b\u0026quot;,\rylim = c(min(c(train_rmse, test_rmse)) - 0.02,\rmax(c(train_rmse, test_rmse)) + 0.02),\rcol = \u0026quot;dodgerblue\u0026quot;,\rxlab = \u0026quot;Model Size\u0026quot;,\rylab = \u0026quot;RMSE\u0026quot;)\rlines(model_complexity, test_rmse, type = \u0026quot;b\u0026quot;, col = \u0026quot;darkorange\u0026quot;)\rWe also summarize the results as a table. fit_1 is the least flexible, and fit_5 is the most flexible. We see the Train RMSE decrease as flexibility increases. We see that the Test RMSE is smallest for fit_3, thus is the model we believe will perform the best on future data not used to train the model. Note this may not be the best model, but it is the best model of the models we have seen in this example.\n\r\rModel\rTrain RMSE\rTest RMSE\rPredictors\r\r\r\rfit_1\r1.8884884\r1.4616608\r3\r\rfit_2\r1.0168223\r0.9117228\r7\r\rfit_3\r0.6553091\r0.6633375\r8\r\rfit_4\r0.6421909\r0.7465957\r10\r\rfit_5\r0.6120887\r0.7864181\r14\r\r\r\rTo summarize:\n\rUnderfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\rOverfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.\r\rSpecifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE. Then a model is underfitting if there exists a more complex model with lower Test RMSE.\nA number of notes on these results:\n\rThe labels of under and overfitting are relative to the best model we see, fit_3. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.\rThe train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. Here we see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later.\rOften we expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get lucky and this will not be true.\r\rA final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that predicted well, and paid no attention to a model for explaination. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603200224,"objectID":"40eeabbfe9d21a7292000fc4fe538d43","permalink":"https://datavizm20.classes.andrewheiss.com/example/07-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/07-example/","section":"example","summary":"Model Selection\rAssesing Model Accuracy\rModel Complexity\rTest-Train Split\rAdding Flexibility to Linear Models\rChoosing a Model\r\r\rToday’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\rModel Selection\rOften when we are developing a linear regression model, part of our goal is to explain a relationship.","tags":null,"title":"Linear Regression: Model Selection","type":"docs"},{"authors":null,"categories":null,"content":"\r\rToday’s example will come from the “Content” tab.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604586903,"objectID":"66dae6a89dc933d1691fce47e0612205","permalink":"https://datavizm20.classes.andrewheiss.com/example/08-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/08-example/","section":"example","summary":"\r\rToday’s example will come from the “Content” tab.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n\r Ames.csv\r\r\r","tags":null,"title":"Nonparametric Regression","type":"docs"},{"authors":null,"categories":null,"content":"\r\rToday’s example will come from the “Content” tab.\nWe may use a new dataset (it is covered in the Lab). You can find it below.\n\r bank.csv\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604586903,"objectID":"4ca8d8f57d9585dcab132a17e9a5a6e7","permalink":"https://datavizm20.classes.andrewheiss.com/example/09-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/09-example/","section":"example","summary":"\r\rToday’s example will come from the “Content” tab.\nWe may use a new dataset (it is covered in the Lab). You can find it below.\n\r bank.csv\r\r\r","tags":null,"title":"Illustrating Bias vs. Variance","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rBackstory and Set Up\r\r\rYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, November 23.\n\rBackstory and Set Up\rYou work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.) They’ve given you a dataset on defaults (encoded as the variable y). You’re going to try to predict this.\nThis is some new data. The snippet below loads it.\nbank \u0026lt;- read.table(\u0026quot;https://msudataanalytics.github.io/SSC442/Labs/data/bank.csv\u0026quot;,\rheader = TRUE,\rsep = \u0026quot;,\u0026quot;)\rThere’s not going to be a whole lot of wind-up here. You should be well-versed in doing these sorts of things by now (if not, look back at the previous lab for sample code).\nEXERCISE 1\nSplit the data into an 80/20 train vs. test split. Make sure you explicitly set the seed for replicability, but do not share your seed with others in the class. (We may compare some results across people.)\n\rRun a series of KNN models with \\(k\\) ranging from 2 to 100. (You need not do every \\(k\\) between 2 and 100, but you can easily write a short function to do this; see the Content tab).\n\rCreate a chart plotting the model complexity as the \\(x\\)-axis variable and RMSE as the \\(y\\)-axis variable for both the training and test data. What do you think is the optimal \\(k\\)?\n\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605796661,"objectID":"31ea2e7e9d2a752ab8d50f8bdb0af971","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/09-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/09-assignment/","section":"assignment","summary":"Backstory and Set Up\r\r\rYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, November 23.\n\rBackstory and Set Up\rYou work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.","tags":null,"title":"Model Evaluation","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rAccessibility\rColors\rFonts\rGraphic assets\r\rImages\rVectors\rVectors, photos, videos, and other assets\r\r\r\rAccessibility\r\rVischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\rColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)\r\r\rColors\r\rAdobe Color: Create, share, and explore rule-based and custom color palettes.\rColourLovers: Like Facebook for color palettes.\rviridis: Percetually uniform color scales.\rScientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with scico.\rColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account.\rColorgorical: Create color palettes based on fancy mathematical rules for perceptual distance.\rColorpicker for data: More fancy mathematical rules for color palettes (explanation).\riWantHue: Yet another perceptual distance-based color palette builder.\rPhotochrome: Word-based color pallettes.\rPolicyViz Design Color Tools: Large collection of useful color resources\r\r\rFonts\r\rGoogle Fonts: Huge collection of free, well-made fonts.\rThe Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).\r\r\rGraphic assets\rImages\r\rUse the Creative Commons filters on Google Images or Flickr\rUnsplash\rPexels\rPixabay\rStockSnap.io\rBurst\rfreephotos.cc\r\r\rVectors\r\rNoun Project: Thousands of free simple vector images\raiconica: 1,000+ vector icons\rVecteezy: Thousands of free vector images\r\r\rVectors, photos, videos, and other assets\r\rStockio\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"16fd04c4714e3d096bffcf19e6c524ca","permalink":"https://datavizm20.classes.andrewheiss.com/resource/design/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/design/","section":"resource","summary":"Accessibility\rColors\rFonts\rGraphic assets\r\rImages\rVectors\rVectors, photos, videos, and other assets\r\r\r\rAccessibility\r\rVischeck: Simulate how your images look for people with different forms of colorblindness (web-based)\rColor Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)\r\r\rColors\r\rAdobe Color: Create, share, and explore rule-based and custom color palettes.\rColourLovers: Like Facebook for color palettes.","tags":null,"title":"Design","type":"docs"},{"authors":null,"categories":null,"content":"\r\rBasic Markdown formatting\rMath\rTables\rFootnotes\rFront matter\rCitations\rOther references\r\r\rMarkdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting\r\r\r\r\rType…\r…or…\r…to get\r\r\r\rSome text in a paragraph.\rMore text in the next paragraph. Always\ruse empty lines between paragraphs.\r\rSome text in a paragraph.\nMore text in the next paragraph. Always\ruse empty lines between paragraphs.\n\r\r*Italic*\r_Italic_\rItalic\r\r**Bold**\r__Bold__\rBold\r\r# Heading 1\r\rHeading 1\r\r\r## Heading 2\r\rHeading 2\r\r\r### Heading 3\r\rHeading 3\r\r\r(Go up to heading level 6 with ######)\r\r\r\r[Link text](http://www.example.com)\r\rLink text\r\r![Image caption](/path/to/image.png)\r\r\r\r`Inline code` with backticks\r\rInline code with backticks\r\r\u0026gt; Blockquote\r\r\rBlockquote\n\r\r- Things in\r- an unordered\r- list\r* Things in\r* an unordered\r* list\r\rThings in\ran unordered\rlist\r\r\r1. Things in\r2. an ordered\r3. list\r1) Things in\r2) an ordered\r3) list\rThings in\ran ordered\rlist\r\r\rHorizontal line\r---\rHorizontal line\r***\rHorizontal line\n\r\r\r\r\rMath\rMarkdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n\r\r\r\rType…\r…to get\r\r\r\rBased on the DAG, the regression model for\restimating the effect of education on wages\ris $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or\r$\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$.\rBased on the DAG, the regression model for\restimating the effect of education on wages\ris \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or\r\\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).\r\r\r\rTo put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math:\r$$\rx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\r$$\rBut now we just use computers to solve for $x$.\r…to get…\n\rThe quadratic equation was an important part of high school math:\n\\[\rx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\r\\]\nBut now we just use computers to solve for \\(x\\).\n\rBecause dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n\rTables\rThere are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default\r------- ------ ---------- -------\r12 12 12 12\r123 123 123 123\r1 1 1 1\rTable: Caption goes here\r…to get…\n\rCaption goes here\r\rRight\rLeft\rCenter\rDefault\r\r\r\r12\r12\r12\r12\r\r123\r123\r123\r123\r\r1\r1\r1\r1\r\r\r\rFor pipe tables, type…\n| Right | Left | Default | Center |\r|------:|:-----|---------|:------:|\r| 12 | 12 | 12 | 12 |\r| 123 | 123 | 123 | 123 |\r| 1 | 1 | 1 | 1 |\rTable: Caption goes here\r…to get…\n\rCaption goes here\r\rRight\rLeft\rDefault\rCenter\r\r\r\r12\r12\r12\r12\r\r123\r123\r123\r123\r\r1\r1\r1\r1\r\r\r\r\rFootnotes\rThere are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags].\r[^1]: This is a note.\r[^note-on-dags]: DAGs are neat.\rAnd here\u0026#39;s more of the document.\r…to get…\n\rHere is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n\r\rThis is a note.↩︎\r\r\rDAGs are neat.↩︎\r\r\r\r\rYou can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!]\r…to get…\n\rCausal inference is neat.1\n\r\rBut it can be hard too!↩︎\r\r\r\r\r\rFront matter\rYou can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n---\rtitle: Title of your document\rdate: \u0026quot;January 13, 2020\u0026quot;\rauthor: \u0026quot;Your name\u0026quot;\r---\rYou can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n---\rtitle: \u0026quot;My cool title: a subtitle\u0026quot;\r---\rIf you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n---\rtitle: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39;\r---\r\rCitations\rOne of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n---\rtitle: Title of your document\rdate: \u0026quot;January 13, 2020\u0026quot;\rauthor: \u0026quot;Your name\u0026quot;\rbibliography: name_of_file.bib\r---\rChoose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n---\rtitle: Title of your document\rdate: \u0026quot;January 13, 2020\u0026quot;\rauthor: \u0026quot;Your name\u0026quot;\rbibliography: name_of_file.bib\rcsl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot;\r---\rSome of the most common CSLs are:\n\rChicago author-date\rChicago note-bibliography\rChicago full note-bibliography (no shortened notes or ibids)\rAPA 7th edition\rMLA 8th edition\r\rCite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n\r\r\r\rType…\r…to get…\r\r\r\rCausal inference is neat [@Rohrer:2018; @AngristPischke:2015].\rCausal inference is neat (Rohrer 2018; Angrist and Pischke 2015).\r\rCausal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1].\rCausal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).\r\rAngrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018].\rAngrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).\r\r@AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees.\rAngrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.\r\r\r\rAfter compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n\rAngrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n\r\r\rOther references\rThese websites have additional details and examples and practice tools:\n\rCommonMark’s Markdown tutorial: A quick interactive Markdown tutorial.\rMarkdown tutorial: Another interactive tutorial to practice using Markdown.\rMarkdown cheatsheet: Useful one-page reminder of Markdown syntax.\rThe Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.\r\r\r","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594409288,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"https://datavizm20.classes.andrewheiss.com/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Basic Markdown formatting\rMath\rTables\rFootnotes\rFront matter\rCitations\rOther references\r\r\rMarkdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting\r\r\r\r\rType…\r…or…\r…to get\r\r\r\rSome text in a paragraph.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rInteresting and excellent real world examples\rHow to select the appropriate chart type\rGeneral resources\rVisualization in Excel\rVisualization in Tableau\r\r\rInteresting and excellent real world examples\r\rThe Stories Behind a Line\rAustralia as 100 people: You can make something like this with d3 and the potato project.\rMarrying Later, Staying Single Longer\r\r\rHow to select the appropriate chart type\rMany people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:\n\rThe Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations.\rThe Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.).\rFrom Data to Viz: A decision tree for dozens of chart types with links to R and Python code.\rThe Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R.\rR Graph Catalog: R code for 124 ggplot graphs.\rEmery’s Essentials: Descriptions and examples of 26 different chart types.\r\r\rGeneral resources\r\rStorytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic.\rAnn K. Emery’s blog: Blog and tutorials by Ann Emery.\rEvergreen Data: Helful resources by Stephanie Evergreen.\rPolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch.\rVisualising Data: Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk.\rInfo We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field.\rFlowingData: Blog by Nathan Yau.\rInformation is Beautiful: Blog by David McCandless.\rJunk Charts: Blog by Kaiser Fung.\rWTF Visualizations: Visualizations that make you ask “wtf?”\rThe Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic.\rData Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway.\rSeeing Data: A series of research projects about perceptions and visualizations.\r\r\rVisualization in Excel\r\rHow to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel.\rAnn Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel.\r\r\rVisualization in Tableau\rBecause it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well.\n\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"ca403ba352e0871f06b445d2470037b3","permalink":"https://datavizm20.classes.andrewheiss.com/resource/visualization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/visualization/","section":"resource","summary":"Interesting and excellent real world examples\rHow to select the appropriate chart type\rGeneral resources\rVisualization in Excel\rVisualization in Tableau\r\r\rInteresting and excellent real world examples\r\rThe Stories Behind a Line\rAustralia as 100 people: You can make something like this with d3 and the potato project.\rMarrying Later, Staying Single Longer\r\r\rHow to select the appropriate chart type\rMany people have created many useful tools for selecting the correct chart type for a given dataset or question.","tags":null,"title":"Visualization","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rKey terms\rAdd chunks\rChunk names\rChunk options\rInline chunks\rOutput formats\r\r\rR Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms\r\rDocument: A Markdown file where you type stuff\n\rChunk: A piece of R code that is included in your document. It looks like this:\n```{r}\r# Code goes here\r```\rThere must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n\rKnit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n\r\r\rAdd chunks\rThere are three ways to insert chunks:\n\rPress ⌘⌥I on macOS or control + alt + I on Windows\n\rClick on the “Insert” button at the top of the editor window\n\rManually type all the backticks and curly braces (don’t do this)\n\r\r\rChunk names\rYou can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk}\r# Code goes here\r```\r\rChunk options\rThere are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE}\r# Code goes here\r```\rThe most common chunk options are these:\n\rfig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures\recho=FALSE: The code is not shown in the final document, but the results are\rmessage=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted\rwarning=FALSE: Any warnings that R generates are omitted\rinclude=FALSE: The chunk still runs, but the code and results are not included in the final document\r\rYou can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n\rInline chunks\rYou can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE}\ravg_mpg \u0026lt;- mean(mtcars$mpg)\r```\rThe average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon.\r… would knit into this:\n\rThe average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n\r\rOutput formats\rYou can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot;\routput:\rhtml_document: default\rpdf_document: default\rword_document: default\rYou can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n---\rtitle: \u0026quot;My document\u0026quot;\rauthor: \u0026quot;My name\u0026quot;\rdate: \u0026quot;January 13, 2020\u0026quot;\routput: html_document: toc: yes\rfig_caption: yes\rfig_height: 8\rfig_width: 10\rpdf_document: latex_engine: xelatex # More modern PDF typesetting engine\rtoc: yes\rword_document: toc: yes\rfig_caption: yes\rfig_height: 4\rfig_width: 5\r---\r\r","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"https://datavizm20.classes.andrewheiss.com/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"Key terms\rAdd chunks\rChunk names\rChunk options\rInline chunks\rOutput formats\r\r\rR Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rR style conventions\rMain style things to pay attention to for this class\r\rSpacing\rLong lines\rPipes (%\u0026gt;%) and ggplot layers (+)\rComments\r\r\r\rR style conventions\rR is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;)\rfilter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rfilter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; )\rBut you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n\rMain style things to pay attention to for this class\r\rImportant note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n\rSpacing\r\rSee the “Spacing” section in the tidyverse style guide.\n\rPut spaces after commas (like in regular English):\n# Good\rfilter(mpg, cty \u0026gt; 10)\r# Bad\rfilter(mpg , cty \u0026gt; 10)\rfilter(mpg ,cty \u0026gt; 10)\rfilter(mpg,cty \u0026gt; 10)\rPut spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good\rfilter(mpg, cty \u0026gt; 10)\r# Bad\rfilter(mpg, cty\u0026gt;10)\rfilter(mpg, cty\u0026gt; 10)\rfilter(mpg, cty \u0026gt;10)\rDon’t put spaces around parentheses that are parts of functions:\n# Good\rfilter(mpg, cty \u0026gt; 10)\r# Bad\rfilter (mpg, cty \u0026gt; 10)\rfilter ( mpg, cty \u0026gt; 10)\rfilter( mpg, cty \u0026gt; 10 )\r\rLong lines\r\rSee the “Long lines” section in the tidyverse style guide.\n\rIt’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good\rfilter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\r# Good\rfilter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\r# Good\rfilter(mpg,\rcty \u0026gt; 10,\rclass == \u0026quot;compact\u0026quot;)\r# Bad\rfilter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))\r# Good\rfilter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))\r\rPipes (%\u0026gt;%) and ggplot layers (+)\rPut each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good\rggplot(mpg, aes(x = cty, y = hwy, color = class)) +\rgeom_point() +\rgeom_smooth() +\rtheme_bw()\r# Bad\rggplot(mpg, aes(x = cty, y = hwy, color = class)) +\rgeom_point() + geom_smooth() +\rtheme_bw()\r# Super bad\rggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw()\r# Super bad and won\u0026#39;t even work\rggplot(mpg, aes(x = cty, y = hwy, color = class))\r+ geom_point()\r+ geom_smooth() + theme_bw()\rPut each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good\rmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))\r# Bad\rmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))\r# Super bad\rmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))\r# Super bad and won\u0026#39;t even work\rmpg %\u0026gt;% filter(cty \u0026gt; 10)\r%\u0026gt;% group_by(class)\r%\u0026gt;% summarize(avg_hwy = mean(hwy))\r\rComments\r\rSee the “Comments” section in the tidyverse style guide.\n\rComments should start with a comment symbol and a single space: #\n# Good\r#Bad\r#Bad\rIf the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 +\rgroup_by(class) %\u0026gt;% # Divide into class groups\rsummarize(avg_hwy = mean(hwy)) # Find the average hwy in each group\rYou can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 +\rgroup_by(class) %\u0026gt;% # Divide into class groups\rsummarize(avg_hwy = mean(hwy)) # Find the average hwy in each group\rIf the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good\r# Happy families are all alike; every unhappy family is unhappy in its own way.\r# Everything was in confusion in the Oblonskys’ house. The wife had discovered\r# that the husband was carrying on an intrigue with a French girl, who had been\r# a governess in their family, and she had announced to her husband that she\r# could not go on living in the same house with him. This position of affairs\r# had now lasted three days, and not only the husband and wife themselves, but\r# all the members of their family and household, were painfully conscious of it.\r# Bad\r# Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.\rThough, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"https://datavizm20.classes.andrewheiss.com/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions\rMain style things to pay attention to for this class\r\rSpacing\rLong lines\rPipes (%\u0026gt;%) and ggplot layers (+)\rComments\r\r\r\rR style conventions\rR is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;)\rfilter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;)\rmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;)\rfilter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; )\rBut you’ll notice that only a few of those iterations (the first three) are easily readable.","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"\r\rBecause RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS\rDouble click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n\rUnzipping files on Windows\rtl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n\r","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"https://datavizm20.classes.andrewheiss.com/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"\r\rThere are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n\rData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n\rGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.\n\rKaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n\r360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\n\rUS City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n\rPolitical science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n\rFrançois Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.\rThomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).\rErik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"https://datavizm20.classes.andrewheiss.com/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n\rData is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n\rGoogle Dataset Search: Google indexes thousands of public datasets; search for them here.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"\r\rCourse Times and Zoom links\rLecture\rOffice hours\rTeaching Assistant\r\rWhat is This Course and Can / Should You Take It?\rWhat This Course is Not\rSuccess in this Course\rCourse materials\rR and RStudio\rOnline help\r\rEvaluations and Grades\rClass Participation\rAcademic honesty\rGrading\r\rResources\rAccommodations\rCOVID-19\rMandated Reporting\rAcknowledgements\rMiscellanea\rUsing Office Hours\rContacting Me\rLetters of Recommendation / References\r\r\r\rCourse Times and Zoom links\rPlease bookmark lecture and office hour Zoom links. As per MSU, we will not meet until Tuesday, January 19th at our regular time (1:00pm - 2:20pm).\nLecture\rTuesday/Thursday 1:00pm - 2:20pm\nLecture Zoom: https://msu.zoom.us/j/96223127673 (Passcode: GOGREEN)\n\rOffice hours\rWednesdays 4:00pm - 5:30pm\nOffice Hours Zoom: https://msu.zoom.us/j/95079959521 (Passcode: GOGREEN)\n\rTeaching Assistant\rAnh Do - doanh@msu.edu\nTA Office Hours: Monday 12:30pm - 2:00pm\nTA Office Hours Zoom: https://msu.zoom.us/j/96783872030 (Passcode: 525948)\n\r\rWhat is This Course and Can / Should You Take It?\rInnovations in statistical learning have created many engineering breakthroughs. From real time voice recognition to automatic categorization (and in some cases production) of news stories, machine learning is transforming the way we live our lives. These techniques are, at their heart, novel ways to work with data, and therefore they should have implications for social science. This course explores the intersection of statistical learning (or machine learning) and social science and aims to answer two primary questions about these new techniques:\nHow does statistical learning work and what kinds of statistical guarantees can be made about the performance of statistical-learning algorithms?\n\rHow can statistical learning be used to answer questions that interest social science researchers, such as testing theories or improving social policy?\n\r\rIn order to address these questions, we will cover so-called “standard” techniques such as supervised and unsupervised learning, statistical learning theory and nonparametric and Bayesian approaches. If it were up to me, this course would be titled “Statistical Learning for Social Scientists”—I believe this provides a more appropriate guide to the content of this course. And while this class will cover these novel statistical methodologies in some detail, it is not a substitute for the appropriate class in Computer Science or Statistics. Nor is this a class that teaches specific skills for the job market. Rather, this class will teach you to think about data analytics broadly. We will spend a great deal of time learning how to interpret the output of statistical learning algorithms and approaches, and will also spend a great deal of time on better understanding the basic ideas in statistical learning. This, of course, comes at some cost in terms of time spent on learning computational and/or programming skills.\nEnrollment for credit in this course is simply not suitable for those unprepared in or uninterested in elementary statistical theory no matter the intensity of interest in machine learning or “Big Data”. Really.\nYou will be required to understand elementary mathematics in this course and should have at least some exposure to statistical theory. The class is front-loaded technically: early lectures are more mathematically oriented, while later lectures are more applied.\nThe topics covered in this course are listed later in this document. I will assign readings sparingly from Introduction to Statistical Learning, henceforth referred to as ISL. This text is available for free online and, for those who like physical books, can be purchased for about $25. Importantly, the lectures deviate a fair bit from the reading, and thus you will rely on your course notes much more than you might in other classes.\nIf—after you have read this document and preferably after attending the first lecture—you have any questions about whether this course is appropriate for you, please come talk to me.\n\rWhat This Course is Not\rThe focus of this course is conceptual. The goal is to create a working understanding of when and how tools from computer science and statistics can be profitably applied to problems in social science. Though students will be required to apply some of these techniques themselves, this course is not…\n…a replacement for EC420 or a course in causal inference.\nAs social scientists, we are most often concerned with causal inference in order to analyze and write policies. Statistical learning and the other methods we will discuss in this course are generally not well-suited to these problems, and while I’ll give a short overview of standard methods, this is only to build intuitions. Ultimately, this course has a different focus and you should still pursue standard methodological insights from your home departments.\n…a course on the computational aspects of the underlying methods.\nThere are many important innovations that have made machine learning techniques computationally feasible. We will not discuss these, as there are computer science courses better equipped to cover them. When appropriate, we will discuss whether something is computable, and we will even give rough approximations of the amount of time required (e.g. P vs NP). But we will not discuss how optimizers work or best practices in programming.\n…a primer on the nitty-gritty of how to use these tools or a way to pad your resume.\nThe mechanics of implementation, whether it be programming languages or learning to use APIs, will not be covered in any satisfying level of depth. Students will be expected to learn most of the programming skills on their own. Specifically, while there will be some material to remind you of basic R commands, this is not a good course for people who are simply looking to learn the mechanics of programming. This course is designed to get you to use both traditional analytics and, eventually, machine learning tools. We will do some review of basic programming, and you will have the opportunity to explore topics that interest you through a final project, but ultimately this is a course that largely focuses on the theoretical and practical aspects of statistical learning as applied to social science and not a class on programming.\nPerhaps most importantly, this course is an attempt to push undergraduate education toward the frontiers in social science. Accordingly, please allow some messiness. Some topics may be underdeveloped for a given person’s passions, but given the wide variety of technical skills and overall interests, this is a near certainty. Both the challenge and opportunity of this area comes from the fact that there is no fully developed, wholly unifying framework. Our collective struggle—me from teaching, you from learning—will ultimately bear fruit.\n\rSuccess in this Course\rI promise, you are equipped to succeed in this course.\nLearning R can be difficult at first. Like learning a new language—Spanish, French, or Chinese—it takes dedication and perseverance. Hadley Wickham (the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like) ggplot2—made this wise observation:\n\rIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\rEven experienced programmers (like me) find themselves bashing their heads against seemingly intractable errors.1 If you’re finding yourself bashing your head against a wall and not making progress, try the following. First, take a break. Sometimes you just need space to see an error. Next, talk to classmates. Finally, if you genuinely cannot see the solution, e-mail the TA. But, honestly, it’s probably just a typo.\n\n\rCourse materials\rThe course website can be found at https://ssc442Kirkpatrick.netlify.app (but you know that. You’re on it right now.)\nAll of the readings and software in this class are free. There are free online version of all the texts including Introduction to Statistical Learning and R / RStudio are free. (Don’t pay for RStudio.) We will reference outside readings and there exist paper versions of some “books” but you won’t need to buy anything2\nR and RStudio\rYou will do all of your analysis with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R. We recommend this for those who may be switching between computers and are trying to get some work done. That said, while RStudio.cloud is convenient, it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis and graphics. You also can’t use your own custom fonts with RStudio.cloud.3 And, generally speaking, you should have (from the prerequisite course) sufficient experience to make your R work. If not, over the course of the semester, you’ll probably want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud. If you plan on making a career out of data science, you should consider this a necessary step.\nYou can find instructions for installing R, RStudio, and all the tidyverse packages here. And you may find some other goodies.\n\rOnline help\rData science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Likewise, whenever using a specific package, try searching for that package name instead of the letter “r” (e.g. “ggplot scatterplot”). Good, concise searches are generally more effective.\n\r\rEvaluations and Grades\rYour grade in this course will be based on attendance/participation, labs, weekly writings, and a final project.\nThe general breakdown will be approximately 55% for labs, participation, and weekly writings, and 45% for projects (see below for specific details). The primary focus of the course is a final project; this requires two “mini-projects” to ensure you’re making satisfactory progress. Assignment of numeric grades will follow the standard, where ties (e.g., 91.5%) are rounded to favor the student. Evaluations (read: grades) are designed not to deter anyone from taking this course who might otherwise be interested, but will be taken seriously.\nWeekly writings are intended to be an easy way to get some points. Labs will be short homework assignments that require you to do something practical using a basic statistical language. Support will be provided for the R language only. You must have access to computing resources and the ability to program basic statistical analyses. As mentioned above, this course will not teach you how to program or how to write code in a specific language. If you are unprepared to do implement basic statistical coding, please take (or retake) PLS202. I highly encourage seeking coding advice from those who instruct computer science courses – it’s their job and they are better at it than I am. I’ll try to provide a good service, but I’m really not an expert in computer science.\nMore in-depth descriptions for all the assignments are on the assignments page. As the course progresses, the assignments themselves will be posted within that page.\nTo Recap:\n\r\r\rAssignment\rPoints\rPercent\r\r\r\rClass Participation\r20\r4%\r\rWeekly Writings (11 x 10)\r110\r22%\r\rLabs (10 x 15)\r150\r29%\r\rMini project 1\r50\r10%\r\rMini project 2\r50\r10%\r\rFinal project\r130\r25%\r\rTotal\r510\r-\r\r\r\r\r\r\r\rGrade\rRange\rGrade\rRange\r\r\r\r4.0\r92-100%\r2.0\r72-76%\r\r3.5\r87-91%\r1.5\r67-72%\r\r3.0\r82-87%\r1.0\r62-67%\r\r2.5\r77-81%\r0.0\rbad-66%\r\r\r\r\rClass Participation\rParticipation can take many forms in an online synchronous course. Most preferred is active participation during class using the Zoom chat and hand-raising with camera and mic on. However, I know that myriad reasons may preclude you from this participation. The bare minimum can best be described as “showing your presence and having some engagement” – questions typed in chat and questions posed after class count towards participation. To encourage some form of participation, I will often pose questions to the class. I am not above bribery - your response to these extra credit questions will earn extra credit points, up to 5, for participation. Thus, you can easily pad your score by (1) meeting the minimum participation requirements such that I know you are present, and (2) earning extra credit by responding (through chat or mic, with or without video) to in-class extra credit prompts. I will clearly state which questions are extra credit. Wrong answers get the same credit as right answers. We are here to learn. If you knew everything already, you wouldn’t be in the class.\n\rAcademic honesty\rViolation of MSU’s Spartan Code of Honor will result in a grade of 0.0 in the course. Moreover, I am required by MSU policy to report suspected cases of academic dishonesty for possible disciplinary action.4\n\rGrading\rAll grades are considered final. Any request for a re-grade beyond simple point-tallying mistakes will require that the entire assignment be re-graded. Any points previously awarded may be changed in either direction in the re-grade.\n\r\rResources\rMental health concerns or stressful events may lead to diminished academic performance or reduce a student’s ability to participate in daily activities. Services are available to assist you with addressing these and other concerns you may be experiencing. You can learn more about the broad range of confidential mental health services available on campus via the Counseling \u0026amp; Psychiatric Services (CAPS) website at www.caps.msu.edu.\n\rAccommodations\rIf you need a special accommodation for a disability, religious observance, or have any other concerns about your ability to perform well in this course, please contact me immediately so that we can discuss the issue and make appropriate arrangements. MSU has a specific policy for religious observance available here.\nMichigan State University is committed to providing equal opportunity for participation in all programs, services and activities. Requests for accommodations by persons with disabilities may be made by contacting the Resource Center for Persons with Disabilities at 517-884-RCPD or on the web at rcpd.msu.edu. Once your eligibility for an accommodation has been determined, you will be issued a verified individual services accommodation (“VISA”) form. Please present this form to me at the start of the term and/or two weeks prior to the accommodation date (test, project, etc). Requests received after this date will be honored whenever possible.\n\rCOVID-19\rThings are hard right now. You most likely know people who have lost their jobs, have tested positive for COVID-19, have been hospitalized, or perhaps have even died. You all have increased (or possibly decreased) work responsibilities and increased family care responsibilities. You might be caring for extra people right now, and you are likely facing uncertain job prospects.\nI’m fully committed to making sure that you learn everything you were hoping to learn from this class! I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Under pandemic conditions, that flexibility and leniency is intensified.\nIf you feel like you’re behind or not understanding everything, do not suffer in silence! Please contact me. I’m available at e-mail.\n\rMandated Reporting\rEssays, journals, and other materials submitted for this class are generally considered confidential pursuant to the University’s student record policies. However, students should be aware that University employees, including instructors, may not be able to maintain confidentiality when it conflicts with their responsibility to report certain issues to protect the health and safety of MSU community members and others. As the instructor, I must report the following information to other University offices (including the Department of Police and Public Safety) if you share it with me:\r• Suspected child abuse/neglect, even if this maltreatment happened when you were a child;\r• Allegations of sexual assault, relationship violence, stalking, or sexual harassment; and\r• Credible threats of harm to oneself or to others.\rThese reports may trigger contact from a campus official who will want to talk with you about the incident that you have shared. In almost all cases, it will be your decision whether you wish to speak with that individual. If you would like to talk about these events in a more confidential setting, you are encouraged to make an appointment with the MSU Counseling and Psychiatric Services.\n\rAcknowledgements\rThis syllabus and course structure was developed by Prof. Ben Bushong with iterative refinements by myself. All credit goes to Prof. Bushong. All errors are my own.\n\rMiscellanea\rAll class material will be posted on https://ssc442kirkpatrick.netlify.app. D2L will be used sparingly for submission of weekly writings and assignments and distribution of grades.\nUsing Office Hours\rPlease use my office hours (Passcode: GOGREEN) Wednesdays 4-5:30pm and by appointment. It would be remarkable if you didn’t need some assistance with the material, and I am here to help. One of the benefits of open office hours is to accommodate many students at once; if fellow students are “in my office”, please join in and feel very free to show up in groups. (This bit of the syllabus obviously less critical in the COVID Era). Office hours will move around a little bit throughout the semester to attempt to meet the needs of all students.\nIn addition to drop-in office hours, I always have sign-up office hours for advising and other purposes. They are online, linked from my web page. As a general rule, please first seek course-related help from the drop-in office hours. However, if my scheduled office hours are always infeasible for you, let me know, and then I may encourage you to make appointments with me. I ask that you schedule your studying so that you are prepared to ask questions during office hours – office hours are not a lecture and if you’re not prepared with questions we will end up awkwardly staring at each other for an hour until you leave.\nSome gentle requests regarding office hours and on contacting me. First, my office hours end sharply at the end, so don’t arrive 10 minutes before the scheduled end and expect a full session. Please arrive early if you have lengthy questions, or if you don’t want to risk not having time due to others’ questions. You are free to ask me some stuff by e-mail, (e.g. a typo or something on a handout), but please know e-mail sucks for answering many types of questions. “How do I do this lab?” or “What’s up with R?” are short questions with long answers. Come to office hours.\n\rContacting Me\rEmail is a blessing and a curse. Instant communication is wonderful, but often email is the wrong medium to have a productive conversation about course material. Moreover, I get a lot of emails. This means that I am frequently triaging emails into two piles: “my house is burning down” and “everything else”. Your email is unlikely to make the former pile. So… asking questions about course material is always best done in-class or in office hours. Students always roll their eyes when professors say things like that, but it’s true that if you have a question, it’s very likely someone else has the same question.\nThat said, email is still useful. If you’re going to use it, you should at least use if effectively. There’s a running joke in academia that professors only read an email until they find a question. They then respond to that question and ignore the rest of the email. I won’t do this, but I do think it is helpful to assume that the person on the receiving end of an email will operate this way. By keeping this in mind, you will write a much more concise and easy to understand email.\nSome general tips:\n\rAlways include [SSC442] in your subject line (brackets included).\rUse a short but informative subject line. For example: [SSC442] Final Project Grading\rUse your University-supplied email for University business. This helps me know who you are.\rOne topic, one email. If you have multiple things to discuss, and you anticipate followup replies, it is best to split them into two emails so that the threads do not get cluttered.\rAsk direct questions. If you’re asking multiple questions in one email, use a bulleted list.\rDon’t ask questions that are answered by reading the syllabus! This drives me nuts.\rI’ve also found that students are overly polite in emails. I suppose it may be intimidating to email a professor, and you should try to match the style that the professor prefers, but I view email for a course as a casual form of communication. Said another way: get to the point. Students often send an entire paragraph introducing themselves, but if you use your University email address, and add the course name in the subject, I will already know who you are. Here’s an example of a perfectly reasonable email:\r\r\rSubject: [SSC442] Lab, Question 2, Typo\nHi Prof. Kirkpatrick,\nThere seems to be a typo in the Lab on Question 2. The problem says to use a column of data that doesn’t seem to exist. Can you correct this or which should we use?\nThanks,\rStudent McStudentFace\n\r\rLetters of Recommendation / References\rIf you are applying for further study or another pursuit that requires letters of recommendation and you’d like me to recommend you, I will be glad to write a letter on your behalf if your final grade is a 4.0. Grades below a 4.0 may be handled on a case-by-case basis. In addition, you should have held at least three substantial conversations with me about the course material or other academic subjects over the course of the semester.\n\r\r\rBy the end of the course, you will realize that 1) I make many many many errors; 2) that I frequently cannot remember a command or the correct syntax; and 3) that none of this matters too much in the big picture because I know the broad approaches I’m trying to take and I know how to Google stuff. Learn from my idiocy.↩︎\n\rIf you’ve got money to burn, you can buy me a burrito.↩︎\n\rThis bothers me way more than it should.↩︎\n\rSo just don’t cheat or plagiarize. This is an easy problem to avoid.↩︎\n\r\r\r","date":1609027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610286616,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"https://datavizm20.classes.andrewheiss.com/syllabus/","publishdate":"2020-12-27T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Course Times and Zoom links\rLecture\rOffice hours\rTeaching Assistant\r\rWhat is This Course and Can / Should You Take It?\rWhat This Course is Not\rSuccess in this Course\rCourse materials\rR and RStudio\rOnline help\r\rEvaluations and Grades\rClass Participation\rAcademic honesty\rGrading\r\rResources\rAccommodations\rCOVID-19\rMandated Reporting\rAcknowledgements\rMiscellanea\rUsing Office Hours\rContacting Me\rLetters of Recommendation / References\r\r\r\rCourse Times and Zoom links\rPlease bookmark lecture and office hour Zoom links.","tags":null,"title":"Syllabus","type":"page"},{"authors":null,"categories":null,"content":"\r\rRequirements\rTeams\rSuggested outline\rIntroduction\rTheory and Background\rData and Analyses\rConclusion\r\r\r\rRequirements\rData analytics is inherently a hands-on endeavor. Accordingly, the final project for this class is hands-on. As per the overview page, the final project has the following elements:\nFor your final project in this class, you will analyze existing data in some area of interest to you.1 Aggregating data from multiple sources is encouraged, but is not required.\r\rYou must visualize (at least) three interesting features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.[^4]\n\rYou must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.\n\rYou will submit three things via D2L:\n\r\r\rA PDF of your report (see the outline below for details of what this needs to contain). You should compile this with R Markdown. You might want to write the prose-heavy sections in a word processor like Word or Google Docs and copy/paste the text into your R Markdown document, since RStudio doesn’t have a nice spell checker or grammar checker. This should have no visible R code, warnings, or messages in it. To do this, you must set echo = FALSE at the beginning of your document before you knit.\rThe same PDF as above, but with all the R code in it (set echo = TRUE at the beginning of your document and reknit the file). Please label files in an obvious way.\rA CSV file of your data; or a link to the data online if your code pulls from the internet. This must be a separate file titled “data.csv” or “data.txt” as applicable.\r\rThis project is due by 7:00 PM on Monday, December 14, 2020. No late work will be accepted.\nYou can either run the analysis in RStudio locally on your computer (highly recommended!), since you won’t have to worry about keeping all your work on RStudio’s servers), or use an RStudio.cloud project.\nThere is no final exam. This project is your final exam.\nThe project will not be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:\nTechnical skills: Was the project easy? Does it showcase mastery of data analysis?\rVisual design: Was the information smartly conveyed and usable? Was it beautiful?\rAnalytic design: Was the analysis appropriate? Was it sensible, given the dataset?\rStory: Did we learn something?\rFollowing instructions: Did you surpress R code as asked? Did you submit a separate datafile and label it correctly?\r\rIf you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.\n\rTeams\rMost importantly, you must work with classmates. You will work in groups of four people on your project. (There may be some groups of three). Your team must come up with a name and a Github site for your project and labs. Your team will earn the same scores on all projects. To combat additional freeloading, we will use a reporting system. Any team member can email me to report another team member’s lack of participation secretly. See below for details. Two strikes will result in a 25% grade deduction on the mini projects and final project; three strikes will result in a 50% deduction.\nHere’s how we will select teams:\nIf you choose to work in teams of your choosing, your group will receive 0 bonus points.\nIf you choose to work in a team with a partner, you will be randomly matched with another pair of students and your group will receive 20 bonus points.\nIf you choose to work in a randomly assigned team, your group will receive 40 bonus points.\nYou must make this selection by the end of the second full week of class.\n\rMy team sucks; how can I switch teams?\n\rLife is full of small disappointments. While we would love to spend 12 weeks carefully optimizing groups, that would require a collosal amount of effort that would ultimately not yield anything fruitful. You’re stuck.\n\rMy team sucks; how can I punish them for their lack of effort?\n\rOn this front, we will be more supportive. While you have to put up with your team regardless of their quality, you can indicate that your team members are not carrying their fair share by issuing a strike. This processs works as follows:\r1. A team member systematically fails to exert effort on collaborative projects (for example, by not showing up for meetings or not communicating, or by simply leeching off others without contributing.)\r2. Your frustration reaches a boiling point. You decide this has to stop. You decide to issue a strike\r3. You send an email with the following information:\r- Subject line: [SSC442] Strike against [Last name of Recipient]\r- Body: You do not need to provide detailed reasoning. However, you must discuss the actions (plural) you took to remedy the situation before sending the strike email.\nA strike is a serious matter, and will reduce that team member’s grade on joint work by 10%. If any team-member gets strikes from all other members of his or her team, their grade will be reduced by 50%.\nStrikes are anonymous so that you do not need to fear social retaliation. However, they are not anonymous to allow you to issue them without thoughtful consideration. Perhaps the other person has a serious issue that is preventing them from completing work (e.g., a relative passing away). Please be thoughtful in using this remedy and consider it a last resort.\n\rDo I really need to create a team GitHub repository? I don’t like GitHub / programming/ work.\n\rYes, you need to become familiar with GitHub and you and your team will work in a central repository for mini-projects and your final project.\nThis is for two reasons. First, computer scientists spent a huge amount of time coming up with the solutions that are implemented in GitHub (and other flavors of git). Their efforts are largely dedicated toward solving a very concrete goal: how can two people edit the same thing at the same time without creating a ton of new issues. While you could use a paid variant of GitHub (e.g., you could all collaborate over the Microsoft Office suite as implemented by the 360 software that MSU provides), you’d ultimately have the following issues:\r1. The software doesn’t support some file types.\r2. The software doesn’t autosave versions.2 If someone accidentally deletes something, you’re in trouble.\r3. You have to learn an entirely new system every time you change classes / universities / jobs, because said institute doesn’t buy the product you love.3\n\rI’m on a smaller-than-normal team. Does this mean that I have to do more work?\n\rYour instructors are able to count and are aware the teams are imbalanced. Evaluations of final projects will take this into account. While your final product should reflect the best ability of your team, we do not anticipate that the uneven teams will lead to substantively different outputs.\n\rSuggested outline\rYou must write and present your analysis as if presenting to a C-suite executive. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.4\r- Concretely, this requires a written memo, which describes the data, analyses, and results. This must be clear and easy to understand for a non-expert in your field. Figures and tables do not apply to the page limit.\nBelow is a very loose guide to the sort of content that we expect for the final project. Word limits are suggestions only. Note your final report will be approximately\nIntroduction\rDescribe the motivation for this analysis. Briefly describe the dataset, and explain why the analysis you’re undertaking matters for society. (Or matters for some decision-making. You should not feel constrained to asking only “big questions.” The best projects will be narrow-scope but well-defined.) (≈300 words)\n\rTheory and Background\rProvide in-depth background about the data of interest and about your analytics question. (≈300 words)\n“Theory”\rProvide some theoretical guidance to the functional relationship you hope to explore. If you’re interested on how, say, height affects scoring in the NBA, write down a proposed function that might map height to scoring. Describe how you might look for this unknown relationship in the data.(≈300 words)\n\rHypotheses\rMake predictions. Declare what you think will happen. (Note, this may carry over from second project.) (≈250 words)\n\r\rData and Analyses\rData\rGiven your motivations, limits on feasibility, and hypotheses, describe the data you use. (≈100 words)\n\rAnalyses\rGenerate the analyses relevant to your hypotheses and interests. Here you must include three figures and must describe what they contain in simple, easy to digest language. Why did you visualize these elements? Your analyses also must include brief discussion.\n(As many words as you need to fully describe your analysis and results)\n\r\rConclusion\rWhat caveats should we consider? Do you believe this is a truly causal relationship? Why does any of this matter to the decision-maker? (≈75 words)\n\r\r\rNote that existing is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged. But you cannot stand with a clipboard outside a store and count visitors (for instance).↩︎\n\rSome products, of course, solve this problem a little bit. For example, Dropbox allows users to share files with ease (of any file type) and saves a (coarse) version history. However, Dropbox does not allow multiple users to work on the same file, and has no way of merging edits together.↩︎\n\rThis logic is also why we utilize only free software in this course. It sucks to get really good at, say, SAS (as I did many years ago) only to realize that the software costs about $10000 and many firms are unwilling to spent that. We will try our best to avoid giving you dead-end skills.↩︎\n\rThis exercise provides you with an opportunity to identify your marketable skills and to practice them. I encourage those who will be looking for jobs soon to take this exercise seriously.↩︎\n\r\r\r","date":1607904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604325415,"objectID":"8d16837a0c729f9c31150a71deaf1f1e","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/final-project/","publishdate":"2020-12-14T00:00:00Z","relpermalink":"/assignment/final-project/","section":"assignment","summary":"Requirements\rTeams\rSuggested outline\rIntroduction\rTheory and Background\rData and Analyses\rConclusion\r\r\r\rRequirements\rData analytics is inherently a hands-on endeavor. Accordingly, the final project for this class is hands-on. As per the overview page, the final project has the following elements:\nFor your final project in this class, you will analyze existing data in some area of interest to you.1 Aggregating data from multiple sources is encouraged, but is not required.","tags":null,"title":"Final project","type":"docs"},{"authors":null,"categories":null,"content":"\r\rRequired Reading\rGuiding Questions\r\rString processing\rThe stringr package\rCase study 1: US murders data\rCase study 2: self-reported heights\rHow to escape when defining strings\rRegular expressions\rStrings are a regexp\rSpecial characters\rCharacter classes\rAnchors\rQuantifiers\rWhite space \\s\rQuantifiers: *, ?, +\rNot\rGroups\r\rSearch and replace with regex\rSearch and replace using groups\r\rTesting and improving\rTrimming\rChanging lettercase\rCase study 2: self-reported heights (continued)\rThe extract function\rPutting it all together\r\rString splitting\rCase study 3: extracting tables from a PDF\rRecoding\r\rWeb scraping\rHTML\rThe rvest package\rCSS selectors\rJSON\r\r\r\rRequired Reading\r\rThis page.\r\rGuiding Questions\r\rWhat are some common issues with string data?\rWhat are the key ways to wrangle strings?\rWhat are regular expressions and why are they magic\r\r\r\rString processing\rOne of the most common data wrangling challenges involves extracting numeric data contained in character strings and converting them into the numeric representations required to make plots, compute summaries, or fit models in R. Also common is processing unorganized text into meaningful variable names or categorical variables. Many of the string processing challenges a data scientist faces are unique and often unexpected. It is therefore quite ambitious to write a comprehensive section on this topic. Here we use a series of case studies that help us demonstrate how string processing is a necessary step for many data wrangling challenges. Specifically, we describe the process of converting the not yet shown original raw data from which we extracted the murders, heights, and research_funding_rates example into the data frames we have studied in this book.\nBy going over these case studies, we will cover some of the most common tasks in string processing including\rextracting numbers from strings,\rremoving unwanted characters from text,\rfinding and replacing characters,\rextracting specific parts of strings,\rconverting free form text to more uniform formats, and\rsplitting strings into multiple values.\nBase R includes functions to perform all these tasks. However, they don’t follow a unifying convention, which makes them a bit hard to memorize and use. The stringr package basically repackages this functionality, but uses a more consistent approach of naming functions and ordering their arguments. For example, in stringr, all the string processing functions start with str_. This means that if you type str_ and hit tab, R will auto-complete and show all the available functions. As a result, we don’t necessarily have to memorize all the function names. Another advantage is that in the functions in this package the string being processed is always the first argument, which means we can more easily use the pipe. Therefore, we will start by describing how to use the functions in the stringr package.\nMost of the examples will come from the second case study which deals with self-reported heights by students and most of the chapter is dedicated to learning regular expressions (regex), and functions in the stringr package.\nThe stringr package\rlibrary(tidyverse)\rlibrary(stringr)\rIn general, string processing tasks can be divided into detecting, locating, extracting, or replacing patterns in strings. We will see several examples. The table below includes the functions available to you in the stringr package. We split them by task. We also include the R-base equivalent when available.\nAll these functions take a character vector as first argument. Also, for each function, operations are vectorized: the operation gets applied to each string in the vector.\nFinally, note that in this table we mention groups. These will be explained in Section ??.\n\r\r\r\rstringr\rTask\rDescription\rR-base\r\r\r\rstr_detect\rDetect\rIs the pattern in the string?\rgrepl\r\rstr_which\rDetect\rReturns the index of entries that contain the pattern.\rgrep\r\rstr_subset\rDetect\rReturns the subset of strings that contain the pattern.\rgrep with value = TRUE\r\rstr_locate\rLocate\rReturns positions of first occurrence of pattern in a string.\rregexpr\r\rstr_locate_all\rLocate\rReturns position of all occurrences of pattern in a string.\rgregexpr\r\rstr_view\rLocate\rShow the first part of the string that matches pattern.\r\r\rstr_view_all\rLocate\rShow me all the parts of the string that match the pattern.\r\r\rstr_extract\rExtract\rExtract the first part of the string that matches the pattern.\r\r\rstr_extract_all\rExtract\rExtract all parts of the string that match the pattern.\r\r\rstr_match\rExtract\rExtract first part of the string that matches the groups and the patterns defined by the groups.\r\r\rstr_match_all\rExtract\rExtract all parts of the string that matches the groups and the patterns defined by the groups.\r\r\rstr_sub\rExtract\rExtract a substring.\rsubstring\r\rstr_split\rExtract\rSplit a string into a list with parts separated by pattern.\rstrsplit\r\rstr_split_fixed\rExtract\rSplit a string into a matrix with parts separated by pattern.\rstrsplit with fixed = TRUE\r\rstr_count\rDescribe\rCount number of times a pattern appears in a string.\r\r\rstr_length\rDescribe\rNumber of character in string.\rnchar\r\rstr_replace\rReplace\rReplace first part of a string matching a pattern with another.\r\r\rstr_replace_all\rReplace\rReplace all parts of a string matching a pattern with another.\rgsub\r\rstr_to_upper\rReplace\rChange all characters to upper case.\rtoupper\r\rstr_to_lower\rReplace\rChange all characters to lower case.\rtolower\r\rstr_to_title\rReplace\rChange first character to upper and rest to lower.\r\r\rstr_replace_na\rReplace\rReplace all NAs to a new value.\r\r\rstr_trim\rReplace\rRemove white space from start and end of string.\r\r\rstr_c\rManipulate\rJoin multiple strings.\rpaste0\r\rstr_conv\rManipulate\rChange the encoding of the string.\r\r\rstr_sort\rManipulate\rSort the vector in alphabetical order.\rsort\r\rstr_order\rManipulate\rIndex needed to order the vector in alphabetical order.\rorder\r\rstr_trunc\rManipulate\rTruncate a string to a fixed size.\r\r\rstr_pad\rManipulate\rAdd white space to string to make it a fixed size.\r\r\rstr_dup\rManipulate\rRepeat a string.\rrep then paste\r\rstr_wrap\rManipulate\rWrap things into formatted paragraphs.\r\r\rstr_interp\rManipulate\rString interpolation.\rsprintf\r\r\r\r\rCase study 1: US murders data\rIn this section we introduce some of the more simple string processing challenges with the following datasets as an example:\nlibrary(rvest)\rurl \u0026lt;- paste0(\u0026quot;https://en.wikipedia.org/w/index.php?title=\u0026quot;,\r\u0026quot;Gun_violence_in_the_United_States_by_state\u0026quot;,\r\u0026quot;\u0026amp;direction=prev\u0026amp;oldid=810166167\u0026quot;)\rmurders_raw \u0026lt;- read_html(url) %\u0026gt;%\rhtml_node(\u0026quot;table\u0026quot;) %\u0026gt;%\rhtml_table() %\u0026gt;%\rsetNames(c(\u0026quot;state\u0026quot;, \u0026quot;population\u0026quot;, \u0026quot;total\u0026quot;, \u0026quot;murder_rate\u0026quot;))\rThe code above shows the first step in constructing the dataset\nlibrary(dslabs)\rdata(murders)\rfrom the raw data, which was extracted from a Wikipedia page.\nIn general, string processing involves a string and a pattern. In R, we usually store strings in a character vector such as murders$population. The first three strings in this vector defined by the population variable are:\nmurders_raw$population[1:3]\r## [1] \u0026quot;4,853,875\u0026quot; \u0026quot;737,709\u0026quot; \u0026quot;6,817,565\u0026quot;\rThe usual coercion does not work here:\nas.numeric(murders_raw$population[1:3])\r## Warning: NAs introduced by coercion\r## [1] NA NA NA\rThis is because of the commas ,. The string processing we want to do here is remove the pattern, ,, from the strings in murders_raw$population and then coerce to numbers.\rWe can use the str_detect function to see that two of the three columns have commas in the entries:\ncommas \u0026lt;- function(x) any(str_detect(x, \u0026quot;,\u0026quot;))\rmurders_raw %\u0026gt;% summarize_all(commas)\r## state population total murder_rate\r## 1 FALSE TRUE TRUE FALSE\rWe can then use the str_replace_all function to remove them:\ntest_1 \u0026lt;- str_replace_all(murders_raw$population, \u0026quot;,\u0026quot;, \u0026quot;\u0026quot;)\rtest_1 \u0026lt;- as.numeric(test_1)\rWe can then use mutate_all to apply this operation to each column, since it won’t affect the columns without commas.\nIt turns out that this operation is so common that readr includes the function parse_number specifically meant to remove non-numeric characters before coercing:\ntest_2 \u0026lt;- parse_number(murders_raw$population)\ridentical(test_1, test_2)\r## [1] TRUE\rSo we can obtain our desired table using:\nmurders_new \u0026lt;- murders_raw %\u0026gt;% mutate_at(2:3, parse_number)\rhead(murders_new)\r## state population total murder_rate\r## 1 Alabama 4853875 348 7.2\r## 2 Alaska 737709 59 8.0\r## 3 Arizona 6817565 309 4.5\r## 4 Arkansas 2977853 181 6.1\r## 5 California 38993940 1861 4.8\r## 6 Colorado 5448819 176 3.2\rThis case is relatively simple compared to the string processing challenges that we typically face in data science. The next example is a rather complex one and it provides several challenges that will permit us to learn many string processing techniques.\n\rCase study 2: self-reported heights\rThe dslabs package includes the raw data from which the heights dataset was obtained. You can load it like this:\ndata(reported_heights)\rThese heights were obtained using a web form in which students were asked to enter their heights. They could enter anything, but the instructions asked for height in inches, a number. We compiled 1,095 submissions, but unfortunately the column vector with the reported heights had several non-numeric entries and as a result became a character vector:\nclass(reported_heights$height)\r## [1] \u0026quot;character\u0026quot;\rIf we try to parse it into numbers, we get a warning:\nx \u0026lt;- as.numeric(reported_heights$height)\r## Warning: NAs introduced by coercion\rAlthough most values appear to be height in inches as requested:\nhead(x)\r## [1] 75 70 68 74 61 65\rwe do end up with many NAs:\nsum(is.na(x))\r## [1] 81\rWe can see some of the entries that are not successfully converted by using filter to keep only the entries resulting in NAs:\nreported_heights %\u0026gt;%\rdplyr::mutate(new_height = as.numeric(height)) %\u0026gt;%\rdplyr::filter(is.na(new_height)) %\u0026gt;%\rhead(n=10)\r## time_stamp sex height new_height\r## 1 2014-09-02 15:16:28 Male 5\u0026#39; 4\u0026quot; NA\r## 2 2014-09-02 15:16:37 Female 165cm NA\r## 3 2014-09-02 15:16:52 Male 5\u0026#39;7 NA\r## 4 2014-09-02 15:16:56 Male \u0026gt;9000 NA\r## 5 2014-09-02 15:16:56 Male 5\u0026#39;7\u0026quot; NA\r## 6 2014-09-02 15:17:09 Female 5\u0026#39;3\u0026quot; NA\r## 7 2014-09-02 15:18:00 Male 5 feet and 8.11 inches NA\r## 8 2014-09-02 15:19:48 Male 5\u0026#39;11 NA\r## 9 2014-09-04 00:46:45 Male 5\u0026#39;9\u0026#39;\u0026#39; NA\r## 10 2014-09-04 10:29:44 Male 5\u0026#39;10\u0026#39;\u0026#39; NA\rWe immediately see what is happening. Some of the students did not report their heights in inches as requested. We could discard these data and continue. However, many of the entries follow patterns that, in principle, we can easily convert to inches. For example, in the output above, we see various cases that use the format x'y'' with x and y representing feet and inches, respectively. Each one of these cases can be read and converted to inches by a human, for example 5'4'' is 5*12 + 4 = 64. So we could fix all the problematic entries by hand. However, humans are prone to making mistakes, so an automated approach is preferable. Also, because we plan on continuing to collect data, it will be convenient to write code that automatically does this.\nA first step in this type of task is to survey the problematic entries and try to define specific patterns followed by a large groups of entries. The larger these groups, the more entries we can fix with a single programmatic approach. We want to find patterns that can be accurately described with a rule, such as “a digit, followed by a feet symbol, followed by one or two digits, followed by an inches symbol”.\nTo look for such patterns, it helps to remove the entries that are consistent with being in inches and to view only the problematic entries. We thus write a function to automatically do this. We keep entries that either result in NAs when applying as.numeric or are outside a range of plausible heights. We permit a range that covers about 99.9999% of the adult population. We also use suppressWarnings to avoid the warning message we know as.numeric will gives us.\nnot_inches \u0026lt;- function(x, smallest = 50, tallest = 84){\rinches \u0026lt;- suppressWarnings(as.numeric(x))\rind \u0026lt;- is.na(inches) | inches \u0026lt; smallest | inches \u0026gt; tallest\rind\r}\rWe apply this function and find the number of problematic entries:\nproblems \u0026lt;- reported_heights %\u0026gt;%\rdplyr::filter(not_inches(height)) %\u0026gt;%\rpull(height)\rlength(problems)\r## [1] 292\rWe can now view all the cases by simply printing them. We don’t do that here because there are length(problems), but after surveying them carefully, we see that three patterns can be used to define three large groups within these exceptions.\n1. A pattern of the form x'y or x' y'' or x'y\" with x and y representing feet and inches, respectively. Here are ten examples:\n## 5\u0026#39; 4\u0026quot; 5\u0026#39;7 5\u0026#39;7\u0026quot; 5\u0026#39;3\u0026quot; 5\u0026#39;11 5\u0026#39;9\u0026#39;\u0026#39; 5\u0026#39;10\u0026#39;\u0026#39; 5\u0026#39; 10 5\u0026#39;5\u0026quot; 5\u0026#39;2\u0026quot;\r2. A pattern of the form x.y or x,y with x feet and y inches. Here are ten examples:\n## 5.3 5.5 6.5 5.8 5.6 5,3 5.9 6,8 5.5 6.2\r3. Entries that were reported in centimeters rather than inches. Here are ten examples:\n## 150 175 177 178 163 175 178 165 165 180\rOnce we see these large groups following specific patterns, we can develop a plan of attack. Remember that there is rarely just one way to perform these tasks. Here we pick one that helps us teach several useful techniques. But surely there is a more efficient way of performing the task.\nPlan of attack: we will convert entries fitting the first two patterns into a standardized one. We will then leverage the standardization to extract the feet and inches and convert to inches. We will then define a procedure for identifying entries that are in centimeters and convert them to inches. After applying these steps, we will then check again to see what entries were not fixed and see if we can tweak our approach to be more comprehensive.\nAt the end, we hope to have a script that makes web-based data collection methods robust to the most common user mistakes.\nTo achieve our goal, we will use a technique that enables us to accurately detect patterns and extract the parts we want: regular expressions (regex). But first, we quickly describe how to escape the function of certain characters so that they can be included in strings.\n\rHow to escape when defining strings\rTo define strings in R, we can use either double quotes:\ns \u0026lt;- \u0026quot;Hello!\u0026quot;\ror single quotes:\ns \u0026lt;- \u0026#39;Hello!\u0026#39;\rMake sure you choose the correct single quote since using the back quote will give you an error:\ns \u0026lt;- `Hello`\rError: object \u0026#39;Hello\u0026#39; not found\rNow, what happens if the string we want to define includes double quotes? For example, if we want to write 10 inches like this 10\"?\rIn this case you can’t use:\ns \u0026lt;- \u0026quot;10\u0026quot;\u0026quot;\rbecause this is just the string 10 followed by a double quote. If you type this into R, you get an error because you have an unclosed double quote. To avoid this, we can use the single quotes:\ns \u0026lt;- \u0026#39;10\u0026quot;\u0026#39;\rIf we print out s we see that the double quotes are escaped with the backslash \\.\ns\r## [1] \u0026quot;10\\\u0026quot;\u0026quot;\rIn fact, escaping with the backslash provides a way to define the string while still using the double quotes to define strings:\ns \u0026lt;- \u0026quot;10\\\u0026quot;\u0026quot;\rIn R, the function cat lets us see what the string actually looks like:\ncat(s)\r## 10\u0026quot;\rNow, what if we want our string to be 5 feet written like this 5'? In this case, we can use the double quotes:\ns \u0026lt;- \u0026quot;5\u0026#39;\u0026quot;\rcat(s)\r## 5\u0026#39;\rSo we’ve learned how to write 5 feet and 10 inches separately, but what if we want to write them together to represent 5 feet and 10 inches like this 5'10\"? In this case, neither the single nor double quotes will work. This:\ns \u0026lt;- \u0026#39;5\u0026#39;10\u0026quot;\u0026#39;\rcloses the string after 5 and this:\ns \u0026lt;- \u0026quot;5\u0026#39;10\u0026quot;\u0026quot;\rcloses the string after 10. Keep in mind that if we type one of the above code snippets into R, it will get stuck waiting for you to close the open quote and you will have to exit the execution with the esc button.\nIn this situation, we need to escape the function of the quotes with the backslash \\. You can escape either character like this:\ns \u0026lt;- \u0026#39;5\\\u0026#39;10\u0026quot;\u0026#39;\rcat(s)\r## 5\u0026#39;10\u0026quot;\ror like this:\ns \u0026lt;- \u0026quot;5\u0026#39;10\\\u0026quot;\u0026quot;\rcat(s)\r## 5\u0026#39;10\u0026quot;\rEscaping characters is something we often have to use when processing strings.\n\rRegular expressions\rA regular expression (regex) is a way to describe specific patterns of characters of text. They can be used to determine if a given string matches the pattern. A set of rules has been defined to do this efficiently and precisely and here we show some examples. We can learn more about these rules by reading a detailed tutorials1 2. This RStudio cheat sheet3 is also very useful.\nThe patterns supplied to the stringr functions can be a regex rather than a standard string. We will learn how this works through a series of examples.\nThroughout this section you will see that we create strings to test out our regex. To do this, we define patterns that we know should match and also patterns that we know should not. We will call them yes and no, respectively. This permits us to check for the two types of errors: failing to match and incorrectly matching.\nStrings are a regexp\rTechnically any string is a regex, perhaps the simplest example is a single character. So the comma , used in the next code example is a simple example of searching with regex.\npattern \u0026lt;- \u0026quot;,\u0026quot;\rstr_detect(murders_raw$total, pattern)\rWe suppress the output which is logical vector telling us which entries have commas.\nAbove, we noted that an entry included a cm. This is also a simple example of a regex. We can show all the entries that used cm like this:\nstr_subset(reported_heights$height, \u0026quot;cm\u0026quot;)\r## [1] \u0026quot;165cm\u0026quot; \u0026quot;170 cm\u0026quot;\r\rSpecial characters\rNow let’s consider a slightly more complicated example. Which of the following strings contain the pattern cm or inches?\nyes \u0026lt;- c(\u0026quot;180 cm\u0026quot;, \u0026quot;70 inches\u0026quot;)\rno \u0026lt;- c(\u0026quot;180\u0026quot;, \u0026quot;70\u0026#39;\u0026#39;\u0026quot;)\rs \u0026lt;- c(yes, no)\rstr_detect(s, \u0026quot;cm\u0026quot;) | str_detect(s, \u0026quot;inches\u0026quot;)\r## [1] TRUE TRUE FALSE FALSE\rHowever, we don’t need to do this. The main feature that distinguishes the regex language from plain strings is that we can use special characters. These are characters with a meaning. We start by introducing | which means or. So if we want to know if either cm or inches appears in the strings, we can use the regex cm|inches:\nstr_detect(s, \u0026quot;cm|inches\u0026quot;)\r## [1] TRUE TRUE FALSE FALSE\rand obtain the correct answer.\nAnother special character that will be useful for identifying feet and inches values is \\d which means any digit: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. The backslash is used to distinguish it from the character d. In R, we have to escape the backslash \\ so we actually have to use \\\\d to represent digits. Here is an example:\nyes \u0026lt;- c(\u0026quot;5\u0026quot;, \u0026quot;6\u0026quot;, \u0026quot;5\u0026#39;10\u0026quot;, \u0026quot;5 feet\u0026quot;, \u0026quot;4\u0026#39;11\u0026quot;)\rno \u0026lt;- c(\u0026quot;\u0026quot;, \u0026quot;.\u0026quot;, \u0026quot;Five\u0026quot;, \u0026quot;six\u0026quot;)\rs \u0026lt;- c(yes, no)\rpattern \u0026lt;- \u0026quot;\\\\d\u0026quot;\rstr_detect(s, pattern)\r## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE\rWe take this opportunity to introduce the str_view function, which is helpful for troubleshooting as it shows us the first match for each string:\nstr_view(s, pattern)\rand str_view_all shows us all the matches, so 3'2 has two matches and 5'10 has three.\nstr_view_all(s, pattern)\rThere are many other special characters. We will learn some others below, but you can see most or all of them in the cheat sheet4 mentioned earlier.\n\rCharacter classes\rCharacter classes are used to define a series of characters that can be matched. We define character classes with square brackets []. So, for example, if we want the pattern to match only if we have a 5 or a 6, we use the regex [56]:\nstr_view(s, \u0026quot;[56]\u0026quot;)\rSuppose we want to match values between 4 and 7. A common way to define character classes is with ranges. So, for example, [0-9] is equivalent to \\\\d. The pattern we want is therefore [4-7].\nyes \u0026lt;- as.character(4:7)\rno \u0026lt;- as.character(1:3)\rs \u0026lt;- c(yes, no)\rstr_detect(s, \u0026quot;[4-7]\u0026quot;)\r## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE\rHowever, it is important to know that in regex everything is a character; there are no numbers. So 4 is the character 4 not the number four. Notice, for example, that [1-20] does not mean 1 through 20, it means the characters 1 through 2 or the character 0. So [1-20] simply means the character class composed of 0, 1, and 2.\nKeep in mind that characters do have an order and the digits do follow the numeric order. So 0 comes before 1 which comes before 2 and so on. For the same reason, we can define lower case letters as [a-z], upper case letters as [A-Z], and [a-zA-z] as both.\n\rAnchors\rWhat if we want a match when we have exactly 1 digit? This will be useful in our case study since feet are never more than 1 digit so a restriction will help us. One way to do this with regex is by using anchors, which let us define patterns that must start or end at a specific place. The two most common anchors are\r^ and $ which represent the beginning and end of a string, respectively. So the pattern ^\\\\d$ is read as “start of the string followed by one digit followed by end of string”.\nThis pattern now only detects the strings with exactly one digit:\npattern \u0026lt;- \u0026quot;^\\\\d$\u0026quot;\ryes \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;9\u0026quot;)\rno \u0026lt;- c(\u0026quot;12\u0026quot;, \u0026quot;123\u0026quot;, \u0026quot; 1\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;b\u0026quot;)\rs \u0026lt;- c(yes, no)\rstr_view_all(s, pattern)\rThe 1 does not match because it does not start with the digit but rather with a space, which is actually not easy to see.\n\rQuantifiers\rFor the inches part, we can have one or two digits. This can be specified in regex with quantifiers. This is done by following the pattern with curly brackets containing the number of times the previous entry can be repeated. We use an example to illustrate. The pattern for one or two digits is:\npattern \u0026lt;- \u0026quot;^\\\\d{1,2}$\u0026quot;\ryes \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;9\u0026quot;, \u0026quot;12\u0026quot;)\rno \u0026lt;- c(\u0026quot;123\u0026quot;, \u0026quot;a4\u0026quot;, \u0026quot;b\u0026quot;)\rstr_view(c(yes, no), pattern)\rIn this case, 123 does not match, but 12 does. So to look for our feet and inches pattern, we can add the symbols for feet ' and inches \" after the digits.\nWith what we have learned, we can now construct an example for the pattern x'y\\\" with x feet and y inches.\npattern \u0026lt;- \u0026quot;^[4-7]\u0026#39;\\\\d{1,2}\\\u0026quot;$\u0026quot;\rThe pattern is now getting complex, but you can look at it carefully and break it down:\n\r^ = start of the string\r[4-7] = one digit, either 4,5,6 or 7\r' = feet symbol\r\\\\d{1,2} = one or two digits\r\\\" = inches symbol\r$ = end of the string\r\rLet’s test it out:\nyes \u0026lt;- c(\u0026quot;5\u0026#39;7\\\u0026quot;\u0026quot;, \u0026quot;6\u0026#39;2\\\u0026quot;\u0026quot;, \u0026quot;5\u0026#39;12\\\u0026quot;\u0026quot;)\rno \u0026lt;- c(\u0026quot;6,2\\\u0026quot;\u0026quot;, \u0026quot;6.2\\\u0026quot;\u0026quot;,\u0026quot;I am 5\u0026#39;11\\\u0026quot;\u0026quot;, \u0026quot;3\u0026#39;2\\\u0026quot;\u0026quot;, \u0026quot;64\u0026quot;)\rstr_detect(yes, pattern)\r## [1] TRUE TRUE TRUE\rstr_detect(no, pattern)\r## [1] FALSE FALSE FALSE FALSE FALSE\rFor now, we are permitting the inches to be 12 or larger. We will add a restriction later as the regex for this is a bit more complex than we are ready to show.\n\rWhite space \\s\rAnother problem we have are spaces. For example, our pattern does not match 5' 4\" because there is a space between ' and 4 which our pattern does not permit. Spaces are characters and R does not ignore them:\nidentical(\u0026quot;Hi\u0026quot;, \u0026quot;Hi \u0026quot;)\r## [1] FALSE\rIn regex, \\s represents white space. To find patterns like 5' 4, we can change our pattern to:\npattern_2 \u0026lt;- \u0026quot;^[4-7]\u0026#39;\\\\s\\\\d{1,2}\\\u0026quot;$\u0026quot;\rstr_subset(problems, pattern_2)\r## [1] \u0026quot;5\u0026#39; 4\\\u0026quot;\u0026quot; \u0026quot;5\u0026#39; 11\\\u0026quot;\u0026quot; \u0026quot;5\u0026#39; 7\\\u0026quot;\u0026quot;\rHowever, this will not match the patterns with no space. So do we need more than one regex pattern? It turns out we can use a quantifier for this as well.\n\rQuantifiers: *, ?, +\rWe want the pattern to permit spaces but not require them. Even if there are several spaces, like in this example 5' 4, we still want it to match. There is a quantifier for exactly this purpose. In regex, the character * means zero or more instances of the previous character. Here is an example:\nyes \u0026lt;- c(\u0026quot;AB\u0026quot;, \u0026quot;A1B\u0026quot;, \u0026quot;A11B\u0026quot;, \u0026quot;A111B\u0026quot;, \u0026quot;A1111B\u0026quot;)\rno \u0026lt;- c(\u0026quot;A2B\u0026quot;, \u0026quot;A21B\u0026quot;)\rstr_detect(yes, \u0026quot;A1*B\u0026quot;)\r## [1] TRUE TRUE TRUE TRUE TRUE\rstr_detect(no, \u0026quot;A1*B\u0026quot;)\r## [1] FALSE FALSE\rThe above matches the first string which has zero 1s and all the strings with one or more 1. We can then improve our pattern by adding the * after the space character \\s.\nThere are two other similar quantifiers. For none or once, we can use ?, and for one or more, we can use +. You can see how they differ with this example:\ndata.frame(string = c(\u0026quot;AB\u0026quot;, \u0026quot;A1B\u0026quot;, \u0026quot;A11B\u0026quot;, \u0026quot;A111B\u0026quot;, \u0026quot;A1111B\u0026quot;),\rnone_or_more = str_detect(yes, \u0026quot;A1*B\u0026quot;),\rnore_or_once = str_detect(yes, \u0026quot;A1?B\u0026quot;),\ronce_or_more = str_detect(yes, \u0026quot;A1+B\u0026quot;))\r## string none_or_more nore_or_once once_or_more\r## 1 AB TRUE TRUE FALSE\r## 2 A1B TRUE TRUE TRUE\r## 3 A11B TRUE FALSE TRUE\r## 4 A111B TRUE FALSE TRUE\r## 5 A1111B TRUE FALSE TRUE\rWe will actually use all three in our reported heights example, but we will see these in a later section.\n\rNot\rTo specify patterns that we do not want to detect, we can use the ^ symbol but only inside square brackets. Remember that outside the square bracket ^ means the start of the string. So, for example, if we want to detect digits that are preceded by anything except a letter we can do the following:\npattern \u0026lt;- \u0026quot;[^a-zA-Z]\\\\d\u0026quot;\ryes \u0026lt;- c(\u0026quot;.3\u0026quot;, \u0026quot;+2\u0026quot;, \u0026quot;-0\u0026quot;,\u0026quot;*4\u0026quot;)\rno \u0026lt;- c(\u0026quot;A3\u0026quot;, \u0026quot;B2\u0026quot;, \u0026quot;C0\u0026quot;, \u0026quot;E4\u0026quot;)\rstr_detect(yes, pattern)\r## [1] TRUE TRUE TRUE TRUE\rstr_detect(no, pattern)\r## [1] FALSE FALSE FALSE FALSE\rAnother way to generate a pattern that searches for everything except is to use the upper case of the special character. For example \\\\D means anything other than a digit, \\\\S means anything except a space, and so on.\n\rGroups\rGroups are a powerful aspect of regex that permits the extraction of values. Groups are defined using parentheses. They don’t affect the pattern matching per se. Instead, it permits tools to identify specific parts of the pattern so we can extract them.\nWe want to change heights written like 5.6 to 5'6.\nTo avoid changing patterns such as 70.2, we will require that the first digit be between 4 and 7 [4-7] and that the second be none or more digits \\\\d*.\rLet’s start by defining a simple pattern that matches this:\npattern_without_groups \u0026lt;- \u0026quot;^[4-7],\\\\d*$\u0026quot;\rWe want to extract the digits so we can then form the new version using a period. These are our two groups, so we encapsulate them with parentheses:\npattern_with_groups \u0026lt;- \u0026quot;^([4-7]),(\\\\d*)$\u0026quot;\rWe encapsulate the part of the pattern that matches the parts we want to keep for later use. Adding groups does not affect the detection, since it only signals that we want to save what is captured by the groups. Note that both patterns return the same result when using str_detect:\nyes \u0026lt;- c(\u0026quot;5,9\u0026quot;, \u0026quot;5,11\u0026quot;, \u0026quot;6,\u0026quot;, \u0026quot;6,1\u0026quot;)\rno \u0026lt;- c(\u0026quot;5\u0026#39;9\u0026quot;, \u0026quot;,\u0026quot;, \u0026quot;2,8\u0026quot;, \u0026quot;6.1.1\u0026quot;)\rs \u0026lt;- c(yes, no)\rstr_detect(s, pattern_without_groups)\r## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE\rstr_detect(s, pattern_with_groups)\r## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE\rOnce we define groups, we can use the function str_match to extract the values these groups define:\nstr_match(s, pattern_with_groups)\r## [,1] [,2] [,3]\r## [1,] \u0026quot;5,9\u0026quot; \u0026quot;5\u0026quot; \u0026quot;9\u0026quot; ## [2,] \u0026quot;5,11\u0026quot; \u0026quot;5\u0026quot; \u0026quot;11\u0026quot;\r## [3,] \u0026quot;6,\u0026quot; \u0026quot;6\u0026quot; \u0026quot;\u0026quot; ## [4,] \u0026quot;6,1\u0026quot; \u0026quot;6\u0026quot; \u0026quot;1\u0026quot; ## [5,] NA NA NA ## [6,] NA NA NA ## [7,] NA NA NA ## [8,] NA NA NA\rNotice that the second and third columns contain feet and inches, respectively. The first column is the part of the string matching the pattern. If no match occurred, we see an NA.\nNow we can understand the difference between the functions str_extract and str_match: str_extract extracts only strings that match a pattern, not the values defined by groups:\nstr_extract(s, pattern_with_groups)\r## [1] \u0026quot;5,9\u0026quot; \u0026quot;5,11\u0026quot; \u0026quot;6,\u0026quot; \u0026quot;6,1\u0026quot; NA NA NA NA\r\r\rSearch and replace with regex\rEarlier we defined the object problems containing the strings that do not appear to be in inches. We can see that not too many of our problematic strings match the pattern:\npattern \u0026lt;- \u0026quot;^[4-7]\u0026#39;\\\\d{1,2}\\\u0026quot;$\u0026quot;\rsum(str_detect(problems, pattern))\r## [1] 14\rTo see why this is, we show some examples that expose why we don’t have more matches:\nproblems[c(2, 10, 11, 12, 15)] %\u0026gt;% str_view(pattern)\rAn initial problem we see immediately is that some students wrote out the words “feet” and “inches”. We can see the entries that did this with the str_subset function:\nstr_subset(problems, \u0026quot;inches\u0026quot;)\r## [1] \u0026quot;5 feet and 8.11 inches\u0026quot; \u0026quot;Five foot eight inches\u0026quot; \u0026quot;5 feet 7inches\u0026quot; ## [4] \u0026quot;5ft 9 inches\u0026quot; \u0026quot;5 ft 9 inches\u0026quot; \u0026quot;5 feet 6 inches\u0026quot;\rWe also see that some entries used two single quotes '' instead of a double quote \".\nstr_subset(problems, \u0026quot;\u0026#39;\u0026#39;\u0026quot;)\r## [1] \u0026quot;5\u0026#39;9\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;10\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;10\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;3\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;7\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;6\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;7.5\u0026#39;\u0026#39;\u0026quot;\r## [8] \u0026quot;5\u0026#39;7.5\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;10\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;11\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;10\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;5\u0026#39;\u0026#39;\u0026quot;\rTo correct this, we can replace the different ways of representing inches and feet with a uniform symbol. We will use ' for feet, whereas for inches we will simply not use a symbol since some entries were of the form x'y. Now, if we no longer use the inches symbol, we have to change our pattern accordingly:\npattern \u0026lt;- \u0026quot;^[4-7]\u0026#39;\\\\d{1,2}$\u0026quot;\rIf we do this replacement before the matching, we get many more matches:\nproblems %\u0026gt;%\rstr_replace(\u0026quot;feet|ft|foot\u0026quot;, \u0026quot;\u0026#39;\u0026quot;) %\u0026gt;% # replace feet, ft, foot with \u0026#39;\rstr_replace(\u0026quot;inches|in|\u0026#39;\u0026#39;|\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;% # remove all inches symbols\rstr_detect(pattern) %\u0026gt;%\rsum()\r## [1] 48\rHowever, we still have many cases to go.\nNote that in the code above, we leveraged the stringr consistency and used the pipe.\nFor now, we improve our pattern by adding \\\\s* in front of and after the feet symbol ' to permit space between the feet symbol and the numbers. Now we match a few more entries:\npattern \u0026lt;- \u0026quot;^[4-7]\\\\s*\u0026#39;\\\\s*\\\\d{1,2}$\u0026quot;\rproblems %\u0026gt;%\rstr_replace(\u0026quot;feet|ft|foot\u0026quot;, \u0026quot;\u0026#39;\u0026quot;) %\u0026gt;% # replace feet, ft, foot with \u0026#39;\rstr_replace(\u0026quot;inches|in|\u0026#39;\u0026#39;|\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;% # remove all inches symbols\rstr_detect(pattern) %\u0026gt;%\rsum\r## [1] 53\rWe might be tempted to avoid doing this by removing all the spaces with str_replace_all. However, when doing such an operation we need to make sure that it does not have unintended effects. In our reported heights examples, this will be a problem because some entries are of the form x y with space separating the feet from the inches. If we remove all spaces, we will incorrectly turn x y into xy which implies that a 6 1 would become 61 inches instead of 73 inches.\nThe second large type of problematic entries were of the form x.y, x,y and x y. We want to change all these to our common format x'y. But we can’t just do a search and replace because we would change values such as 70.5 into 70'5.\rOur strategy will therefore be to search for a very specific pattern that assures us feet and inches are being provided and then, for those that match, replace appropriately.\nSearch and replace using groups\rAnother powerful aspect of groups is that you can refer to the extracted values in a regex when searching and replacing.\nThe regex special character for the i-th group is \\\\i. So \\\\1 is the value extracted from the first group, \\\\2 the value from the second and so on. As a simple example, note that the following code will replace a comma with period, but only if it is between two digits:\npattern_with_groups \u0026lt;- \u0026quot;^([4-7]),(\\\\d*)$\u0026quot;\ryes \u0026lt;- c(\u0026quot;5,9\u0026quot;, \u0026quot;5,11\u0026quot;, \u0026quot;6,\u0026quot;, \u0026quot;6,1\u0026quot;)\rno \u0026lt;- c(\u0026quot;5\u0026#39;9\u0026quot;, \u0026quot;,\u0026quot;, \u0026quot;2,8\u0026quot;, \u0026quot;6.1.1\u0026quot;)\rs \u0026lt;- c(yes, no)\rstr_replace(s, pattern_with_groups, \u0026quot;\\\\1\u0026#39;\\\\2\u0026quot;)\r## [1] \u0026quot;5\u0026#39;9\u0026quot; \u0026quot;5\u0026#39;11\u0026quot; \u0026quot;6\u0026#39;\u0026quot; \u0026quot;6\u0026#39;1\u0026quot; \u0026quot;5\u0026#39;9\u0026quot; \u0026quot;,\u0026quot; \u0026quot;2,8\u0026quot; \u0026quot;6.1.1\u0026quot;\rWe can use this to convert cases in our reported heights.\nWe are now ready to define a pattern that helps us convert all the x.y, x,y and x y to our preferred format. We need to adapt pattern_with_groups to be a bit more flexible and capture all the cases.\npattern_with_groups \u0026lt;-\u0026quot;^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\u0026quot;\rLet’s break this one down:\n\r^ = start of the string\r[4-7] = one digit, either 4, 5, 6, or 7\r\\\\s* = none or more white space\r[,\\\\.\\\\s+] = feet symbol is either ,, . or at least one space\r\\\\s* = none or more white space\r\\\\d* = none or more digits\r$ = end of the string\r\rWe can see that it appears to be working:\nstr_subset(problems, pattern_with_groups) %\u0026gt;% head()\r## [1] \u0026quot;5.3\u0026quot; \u0026quot;5.25\u0026quot; \u0026quot;5.5\u0026quot; \u0026quot;6.5\u0026quot; \u0026quot;5.8\u0026quot; \u0026quot;5.6\u0026quot;\rand will be able to perform the search and replace:\nstr_subset(problems, pattern_with_groups) %\u0026gt;%\rstr_replace(pattern_with_groups, \u0026quot;\\\\1\u0026#39;\\\\2\u0026quot;) %\u0026gt;% head\r## [1] \u0026quot;5\u0026#39;3\u0026quot; \u0026quot;5\u0026#39;25\u0026quot; \u0026quot;5\u0026#39;5\u0026quot; \u0026quot;6\u0026#39;5\u0026quot; \u0026quot;5\u0026#39;8\u0026quot; \u0026quot;5\u0026#39;6\u0026quot;\rAgain, we will deal with the inches-larger-than-twelve challenge later.\n\r\rTesting and improving\rDeveloping the right regex on the first try is often difficult. Trial and error is a common approach to finding the regex pattern that satisfies all desired conditions. In the previous sections, we have developed a powerful string processing technique that can help us catch many of the problematic entries. Here we will test our approach, search for further problems, and tweak our approach for possible improvements. Let’s write a function that captures all the entries that can’t be converted into numbers remembering that some are in centimeters (we will deal with those later):\nnot_inches_or_cm \u0026lt;- function(x, smallest = 50, tallest = 84){\rinches \u0026lt;- suppressWarnings(as.numeric(x))\rind \u0026lt;- !is.na(inches) \u0026amp;\r((inches \u0026gt;= smallest \u0026amp; inches \u0026lt;= tallest) |\r(inches/2.54 \u0026gt;= smallest \u0026amp; inches/2.54 \u0026lt;= tallest))\r!ind\r}\rproblems \u0026lt;- reported_heights %\u0026gt;%\rdplyr::filter(not_inches_or_cm(height)) %\u0026gt;%\rpull(height)\rlength(problems)\r## [1] 200\rLet’s see what proportion of these fit our pattern after the processing steps we developed above:\nconverted \u0026lt;- problems %\u0026gt;%\rstr_replace(\u0026quot;feet|foot|ft\u0026quot;, \u0026quot;\u0026#39;\u0026quot;) %\u0026gt;% # convert feet symbols to \u0026#39;\rstr_replace(\u0026quot;inches|in|\u0026#39;\u0026#39;|\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;% # remove inches symbols\rstr_replace(\u0026quot;^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\u0026quot;, \u0026quot;\\\\1\u0026#39;\\\\2\u0026quot;)# change format\rpattern \u0026lt;- \u0026quot;^[4-7]\\\\s*\u0026#39;\\\\s*\\\\d{1,2}$\u0026quot;\rindex \u0026lt;- str_detect(converted, pattern)\rmean(index)\r## [1] 0.615\rNote how we leveraged the pipe, one of the advantages of using stringr. This last piece of code shows that we have matched well over half of the strings. Let’s examine the remaining cases:\nconverted[!index]\r## [1] \u0026quot;6\u0026quot; \u0026quot;165cm\u0026quot; \u0026quot;511\u0026quot; \u0026quot;6\u0026quot; ## [5] \u0026quot;2\u0026quot; \u0026quot;\u0026gt;9000\u0026quot; \u0026quot;5 \u0026#39; and 8.11 \u0026quot; \u0026quot;11111\u0026quot; ## [9] \u0026quot;6\u0026quot; \u0026quot;103.2\u0026quot; \u0026quot;19\u0026quot; \u0026quot;5\u0026quot; ## [13] \u0026quot;300\u0026quot; \u0026quot;6\u0026#39;\u0026quot; \u0026quot;6\u0026quot; \u0026quot;Five \u0026#39; eight \u0026quot;\r## [17] \u0026quot;7\u0026quot; \u0026quot;214\u0026quot; \u0026quot;6\u0026quot; \u0026quot;0.7\u0026quot; ## [21] \u0026quot;6\u0026quot; \u0026quot;2\u0026#39;33\u0026quot; \u0026quot;612\u0026quot; \u0026quot;1,70\u0026quot; ## [25] \u0026quot;87\u0026quot; \u0026quot;5\u0026#39;7.5\u0026quot; \u0026quot;5\u0026#39;7.5\u0026quot; \u0026quot;111\u0026quot; ## [29] \u0026quot;5\u0026#39; 7.78\u0026quot; \u0026quot;12\u0026quot; \u0026quot;6\u0026quot; \u0026quot;yyy\u0026quot; ## [33] \u0026quot;89\u0026quot; \u0026quot;34\u0026quot; \u0026quot;25\u0026quot; \u0026quot;6\u0026quot; ## [37] \u0026quot;6\u0026quot; \u0026quot;22\u0026quot; \u0026quot;684\u0026quot; \u0026quot;6\u0026quot; ## [41] \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; \u0026quot;6*12\u0026quot; \u0026quot;87\u0026quot; ## [45] \u0026quot;6\u0026quot; \u0026quot;1.6\u0026quot; \u0026quot;120\u0026quot; \u0026quot;120\u0026quot; ## [49] \u0026quot;23\u0026quot; \u0026quot;1.7\u0026quot; \u0026quot;6\u0026quot; \u0026quot;5\u0026quot; ## [53] \u0026quot;69\u0026quot; \u0026quot;5\u0026#39; 9 \u0026quot; \u0026quot;5 \u0026#39; 9 \u0026quot; \u0026quot;6\u0026quot; ## [57] \u0026quot;6\u0026quot; \u0026quot;86\u0026quot; \u0026quot;708,661\u0026quot; \u0026quot;5 \u0026#39; 6 \u0026quot; ## [61] \u0026quot;6\u0026quot; \u0026quot;649,606\u0026quot; \u0026quot;10000\u0026quot; \u0026quot;1\u0026quot; ## [65] \u0026quot;728,346\u0026quot; \u0026quot;0\u0026quot; \u0026quot;6\u0026quot; \u0026quot;6\u0026quot; ## [69] \u0026quot;6\u0026quot; \u0026quot;100\u0026quot; \u0026quot;88\u0026quot; \u0026quot;6\u0026quot; ## [73] \u0026quot;170 cm\u0026quot; \u0026quot;7,283,465\u0026quot; \u0026quot;5\u0026quot; \u0026quot;5\u0026quot; ## [77] \u0026quot;34\u0026quot;\rFour clear patterns arise:\nMany students measuring exactly 5 or 6 feet did not enter any inches, for example 6', and our pattern requires that inches be included.\rSome students measuring exactly 5 or 6 feet entered just that number.\rSome of the inches were entered with decimal points. For example 5'7.5''. Our pattern only looks for two digits.\rSome entries have spaces at the end, for example 5 ' 9.\r\rAlthough not as common, we also see the following problems:\nSome entries are in meters and some of these use European decimals: 1.6, 1,70.\rTwo students added cm.\rA student spelled out the numbers: Five foot eight inches.\r\rIt is not necessarily clear that it is worth writing code to handle these last three cases since they might be rare enough. However, some of them provide us with an opportunity to learn a few more regex techniques, so we will build a fix.\nFor case 1, if we add a '0 after the first digit, for example, convert all 6 to 6'0, then our previously defined pattern will match. This can be done using groups:\nyes \u0026lt;- c(\u0026quot;5\u0026quot;, \u0026quot;6\u0026quot;, \u0026quot;5\u0026quot;)\rno \u0026lt;- c(\u0026quot;5\u0026#39;\u0026quot;, \u0026quot;5\u0026#39;\u0026#39;\u0026quot;, \u0026quot;5\u0026#39;4\u0026quot;)\rs \u0026lt;- c(yes, no)\rstr_replace(s, \u0026quot;^([4-7])$\u0026quot;, \u0026quot;\\\\1\u0026#39;0\u0026quot;)\r## [1] \u0026quot;5\u0026#39;0\u0026quot; \u0026quot;6\u0026#39;0\u0026quot; \u0026quot;5\u0026#39;0\u0026quot; \u0026quot;5\u0026#39;\u0026quot; \u0026quot;5\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;4\u0026quot;\rThe pattern says it has to start (^) with a digit between 4 and 7 and end there ($). The parenthesis defines the group that we pass as \\\\1 to generate the replacement regex string.\nWe can adapt this code slightly to handle the case 2 as well, which covers the entry 5'. Note 5' is left untouched. This is because the extra ' makes the pattern not match since we have to end with a 5 or 6. We want to permit the 5 or 6 to be followed by 0 or 1 feet sign. So we can simply add '{0,1} after the ' to do this. However, we can use the none or once special character ?. As we saw above, this is different from * which is none or more. We now see that the fourth case is also converted:\nstr_replace(s, \u0026quot;^([56])\u0026#39;?$\u0026quot;, \u0026quot;\\\\1\u0026#39;0\u0026quot;)\r## [1] \u0026quot;5\u0026#39;0\u0026quot; \u0026quot;6\u0026#39;0\u0026quot; \u0026quot;5\u0026#39;0\u0026quot; \u0026quot;5\u0026#39;0\u0026quot; \u0026quot;5\u0026#39;\u0026#39;\u0026quot; \u0026quot;5\u0026#39;4\u0026quot;\rHere we only permit 5 and 6, but not 4 and 7. This is because 5 and 6 feet tall is quite common, so we assume those that typed 5 or 6 really meant 60 or 72 inches. However, 4 and 7 feet tall are so rare that, although we accept 84 as a valid entry, we assume 7 was entered in error.\nWe can use quantifiers to deal with case 3. These entries are not matched because the inches include decimals and our pattern does not permit this. We need to allow the second group to include decimals not just digits. This means we must permit zero or one period . then zero or more digits. So we will be using both ? and *.\rAlso remember that, for this particular case, the period needs to be escaped since it is a special character (it means any character except line break). Here is a simple example of how we can use *.\nSo we can adapt our pattern, currently ^[4-7]\\\\s*'\\\\s*\\\\d{1,2}$ to permit a decimal at the end:\npattern \u0026lt;- \u0026quot;^[4-7]\\\\s*\u0026#39;\\\\s*(\\\\d+\\\\.?\\\\d*)$\u0026quot;\rCase 4, meters using commas, we can approach similarly to how we converted the x.y to x'y. A difference is that we require that the first digit be 1 or 2:\nyes \u0026lt;- c(\u0026quot;1,7\u0026quot;, \u0026quot;1, 8\u0026quot;, \u0026quot;2, \u0026quot; )\rno \u0026lt;- c(\u0026quot;5,8\u0026quot;, \u0026quot;5,3,2\u0026quot;, \u0026quot;1.7\u0026quot;)\rs \u0026lt;- c(yes, no)\rstr_replace(s, \u0026quot;^([12])\\\\s*,\\\\s*(\\\\d*)$\u0026quot;, \u0026quot;\\\\1\\\\.\\\\2\u0026quot;)\r## [1] \u0026quot;1.7\u0026quot; \u0026quot;1.8\u0026quot; \u0026quot;2.\u0026quot; \u0026quot;5,8\u0026quot; \u0026quot;5,3,2\u0026quot; \u0026quot;1.7\u0026quot;\rWe will later check if the entries are meters using their numeric values. We will come back to the case study after introducing two widely used functions in string processing that will come in handy when developing our final solution for the self-reported heights.\n\rTrimming\rIn general, spaces at the start or end of the string are uninformative.\rThese can be particularly deceptive because sometimes they can be hard to see:\ns \u0026lt;- \u0026quot;Hi \u0026quot;\rcat(s)\r## Hi\ridentical(s, \u0026quot;Hi\u0026quot;)\r## [1] FALSE\rThis is a general enough problem that there is a function dedicated to removing them:\rstr_trim.\nstr_trim(\u0026quot;5 \u0026#39; 9 \u0026quot;)\r## [1] \u0026quot;5 \u0026#39; 9\u0026quot;\r\rChanging lettercase\rNotice that regex is case sensitive. Often we want to match a word regardless of case. One approach to doing this is to first change everything to lower case and then proceeding ignoring case. As an example, note that one of the entries writes out numbers as words Five foot eight inches. Although not efficient, we could add 13 extra str_replace calls to convert zero to 0, one to 1, and so on. To avoid having to write two separate operations for Zero and zero, One and one, etc., we can use the str_to_lower function to make all works lower case first:\ns \u0026lt;- c(\u0026quot;Five feet eight inches\u0026quot;)\rstr_to_lower(s)\r## [1] \u0026quot;five feet eight inches\u0026quot;\rOther related functions are str_to_upper and str_to_title. We are now ready to define a procedure that converts all the problematic cases to inches.\n\rCase study 2: self-reported heights (continued)\rWe now put all of what we have learned together into a function that takes a string vector and tries to convert as many strings as possible to one format. We write a function that puts together what we have done above.\nconvert_format \u0026lt;- function(s){\rs %\u0026gt;%\rstr_replace(\u0026quot;feet|foot|ft\u0026quot;, \u0026quot;\u0026#39;\u0026quot;) %\u0026gt;%\rstr_replace_all(\u0026quot;inches|in|\u0026#39;\u0026#39;|\\\u0026quot;|cm|and\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstr_replace(\u0026quot;^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\u0026quot;, \u0026quot;\\\\1\u0026#39;\\\\2\u0026quot;) %\u0026gt;%\rstr_replace(\u0026quot;^([56])\u0026#39;?$\u0026quot;, \u0026quot;\\\\1\u0026#39;0\u0026quot;) %\u0026gt;%\rstr_replace(\u0026quot;^([12])\\\\s*,\\\\s*(\\\\d*)$\u0026quot;, \u0026quot;\\\\1\\\\.\\\\2\u0026quot;) %\u0026gt;%\rstr_trim()\r}\rWe can also write a function that converts words to numbers:\nlibrary(english)\rwords_to_numbers \u0026lt;- function(s){\rs \u0026lt;- str_to_lower(s)\rfor(i in 0:11)\rs \u0026lt;- str_replace_all(s, words(i), as.character(i))\rs\r}\rNote that we can perform the above operation more efficiently with the function recode, which we learn about in Section ??.\rNow we can see which problematic entries remain:\nconverted \u0026lt;- problems %\u0026gt;% words_to_numbers() %\u0026gt;% convert_format()\rremaining_problems \u0026lt;- converted[not_inches_or_cm(converted)]\rpattern \u0026lt;- \u0026quot;^[4-7]\\\\s*\u0026#39;\\\\s*\\\\d+\\\\.?\\\\d*$\u0026quot;\rindex \u0026lt;- str_detect(remaining_problems, pattern)\rremaining_problems[!index]\r## [1] \u0026quot;511\u0026quot; \u0026quot;2\u0026quot; \u0026quot;\u0026gt;9000\u0026quot; \u0026quot;11111\u0026quot; \u0026quot;103.2\u0026quot; \u0026quot;19\u0026quot; ## [7] \u0026quot;300\u0026quot; \u0026quot;7\u0026quot; \u0026quot;214\u0026quot; \u0026quot;0.7\u0026quot; \u0026quot;2\u0026#39;33\u0026quot; \u0026quot;612\u0026quot; ## [13] \u0026quot;1.70\u0026quot; \u0026quot;87\u0026quot; \u0026quot;111\u0026quot; \u0026quot;12\u0026quot; \u0026quot;yyy\u0026quot; \u0026quot;89\u0026quot; ## [19] \u0026quot;34\u0026quot; \u0026quot;25\u0026quot; \u0026quot;22\u0026quot; \u0026quot;684\u0026quot; \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; ## [25] \u0026quot;6*12\u0026quot; \u0026quot;87\u0026quot; \u0026quot;1.6\u0026quot; \u0026quot;120\u0026quot; \u0026quot;120\u0026quot; \u0026quot;23\u0026quot; ## [31] \u0026quot;1.7\u0026quot; \u0026quot;86\u0026quot; \u0026quot;708,661\u0026quot; \u0026quot;649,606\u0026quot; \u0026quot;10000\u0026quot; \u0026quot;1\u0026quot; ## [37] \u0026quot;728,346\u0026quot; \u0026quot;0\u0026quot; \u0026quot;100\u0026quot; \u0026quot;88\u0026quot; \u0026quot;7,283,465\u0026quot; \u0026quot;34\u0026quot;\rapart from the cases reported as meters, which we will fix below, they all seem to be cases that are impossible to fix.\nThe extract function\rThe extract function is a useful tidyverse function for string processing that we will use in our final solution, so we introduce it here. In a previous section, we constructed a regex that lets us identify which elements of a character vector match the feet and inches pattern. However, we want to do more. We want to extract and save the feet and number values so that we can convert them to inches when appropriate.\nIf we have a simpler case like this:\ns \u0026lt;- c(\u0026quot;5\u0026#39;10\u0026quot;, \u0026quot;6\u0026#39;1\u0026quot;)\rtab \u0026lt;- data.frame(x = s)\rIn Section ?? we learned about the separate function, which can be used to achieve our current goal:\ntab %\u0026gt;% separate(x, c(\u0026quot;feet\u0026quot;, \u0026quot;inches\u0026quot;), sep = \u0026quot;\u0026#39;\u0026quot;)\r## feet inches\r## 1 5 10\r## 2 6 1\rThe extract function from the tidyr package lets us use regex groups to extract the desired values. Here is the equivalent to the code above using separate but using extract:\nlibrary(tidyr)\rtab %\u0026gt;% tidyr::extract(x, c(\u0026quot;feet\u0026quot;, \u0026quot;inches\u0026quot;), regex = \u0026quot;(\\\\d)\u0026#39;(\\\\d{1,2})\u0026quot;)\r## feet inches\r## 1 5 10\r## 2 6 1\rSo why do we even need the new function extract? We have seen how small changes can throw off exact pattern matching. Groups in regex give us more flexibility. For example, if we define:\ns \u0026lt;- c(\u0026quot;5\u0026#39;10\u0026quot;, \u0026quot;6\u0026#39;1\\\u0026quot;\u0026quot;,\u0026quot;5\u0026#39;8inches\u0026quot;)\rtab \u0026lt;- data.frame(x = s)\rand we only want the numbers, separate fails:\ntab %\u0026gt;% separate(x, c(\u0026quot;feet\u0026quot;,\u0026quot;inches\u0026quot;), sep = \u0026quot;\u0026#39;\u0026quot;, fill = \u0026quot;right\u0026quot;)\r## feet inches\r## 1 5 10\r## 2 6 1\u0026quot;\r## 3 5 8inches\rHowever, we can use extract. The regex here is a bit more complicated since we have to permit ' with spaces and feet. We also do not want the \" included in the value, so we do not include that in the group:\ntab %\u0026gt;% tidyr::extract(x, c(\u0026quot;feet\u0026quot;, \u0026quot;inches\u0026quot;), regex = \u0026quot;(\\\\d)\u0026#39;(\\\\d{1,2})\u0026quot;)\r## feet inches\r## 1 5 10\r## 2 6 1\r## 3 5 8\r\rPutting it all together\rWe are now ready to put it all together and wrangle our reported heights data to try to recover as many heights as possible. The code is complex, but we will break it down into parts.\nWe start by cleaning up the height column so that the heights are closer to a feet’inches format. We added an original heights column so we can compare before and after.\nNow we are ready to wrangle our reported heights dataset:\npattern \u0026lt;- \u0026quot;^([4-7])\\\\s*\u0026#39;\\\\s*(\\\\d+\\\\.?\\\\d*)$\u0026quot;\rsmallest \u0026lt;- 50\rtallest \u0026lt;- 84\rnew_heights \u0026lt;- reported_heights %\u0026gt;%\rdplyr::mutate(original = height,\rheight = words_to_numbers(height) %\u0026gt;% convert_format()) %\u0026gt;%\rtidyr::extract(height, c(\u0026quot;feet\u0026quot;, \u0026quot;inches\u0026quot;), regex = pattern, remove = FALSE) %\u0026gt;%\rdplyr::mutate_at(c(\u0026quot;height\u0026quot;, \u0026quot;feet\u0026quot;, \u0026quot;inches\u0026quot;), as.numeric) %\u0026gt;%\rdplyr::mutate(guess = 12 * feet + inches) %\u0026gt;%\rdplyr::mutate(height = case_when(\ris.na(height) ~ as.numeric(NA),\rbetween(height, smallest, tallest) ~ height, #inches\rbetween(height/2.54, smallest, tallest) ~ height/2.54, #cm\rbetween(height*100/2.54, smallest, tallest) ~ height*100/2.54, #meters\rTRUE ~ as.numeric(NA))) %\u0026gt;%\rdplyr::mutate(height = ifelse(is.na(height) \u0026amp;\rinches \u0026lt; 12 \u0026amp; between(guess, smallest, tallest),\rguess, height)) %\u0026gt;%\rdplyr::select(-guess)\rWe can check all the entries we converted by typing:\nnew_heights %\u0026gt;%\rdplyr::filter(not_inches(original)) %\u0026gt;%\rdplyr::select(original, height) %\u0026gt;%\rarrange(height) %\u0026gt;%\rView()\rA final observation is that if we look at the shortest students in our course:\nnew_heights %\u0026gt;% arrange(height) %\u0026gt;% head(n=7)\r## time_stamp sex height feet inches original\r## 1 2017-07-04 01:30:25 Male 50.00 NA NA 50\r## 2 2017-09-07 10:40:35 Male 50.00 NA NA 50\r## 3 2014-09-02 15:18:30 Female 51.00 NA NA 51\r## 4 2016-06-05 14:07:20 Female 52.00 NA NA 52\r## 5 2016-06-05 14:07:38 Female 52.00 NA NA 52\r## 6 2014-09-23 03:39:56 Female 53.00 NA NA 53\r## 7 2015-01-07 08:57:29 Male 53.77 NA NA 53.77\rWe see heights of 53, 54, and 55. In the originals, we also have 51 and 52. These short heights are rare and it is likely that the students actually meant 5'1, 5'2, 5'3, 5'4, and 5'5. Because we are not completely sure, we will leave them as reported. The object new_heights contains our final solution for this case study.\n\r\rString splitting\rAnother very common data wrangling operation is string splitting. To illustrate how this comes up, we start with an illustrative example. Suppose we did not have the function read_csv or read.csv available to us. We instead have to read a csv file using the base R function readLines like this:\nfilename \u0026lt;- system.file(\u0026quot;extdata/murders.csv\u0026quot;, package = \u0026quot;dslabs\u0026quot;)\rlines \u0026lt;- readLines(filename)\rThis function reads-in the data line-by-line to create a vector of strings. In this case, one string for each row in the spreadsheet. The first six lines are:\nlines %\u0026gt;% head()\r## [1] \u0026quot;state,abb,region,population,total\u0026quot; \u0026quot;Alabama,AL,South,4779736,135\u0026quot; ## [3] \u0026quot;Alaska,AK,West,710231,19\u0026quot; \u0026quot;Arizona,AZ,West,6392017,232\u0026quot; ## [5] \u0026quot;Arkansas,AR,South,2915918,93\u0026quot; \u0026quot;California,CA,West,37253956,1257\u0026quot;\rWe want to extract the values that are separated by a comma for each string in the vector. The command str_split does exactly this:\nx \u0026lt;- str_split(lines, \u0026quot;,\u0026quot;)\rx %\u0026gt;% head(2)\r## [[1]]\r## [1] \u0026quot;state\u0026quot; \u0026quot;abb\u0026quot; \u0026quot;region\u0026quot; \u0026quot;population\u0026quot; \u0026quot;total\u0026quot; ## ## [[2]]\r## [1] \u0026quot;Alabama\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;South\u0026quot; \u0026quot;4779736\u0026quot; \u0026quot;135\u0026quot;\rNote that the first entry has the column names, so we can separate that out:\ncol_names \u0026lt;- x[[1]]\rx \u0026lt;- x[-1]\rTo convert our list into a data frame, we can use a shortcut provided by the map functions in the purrr package. The map function applies the same function to each element in a list. So if we want to extract the first entry of each element in x, we can write:\nlibrary(purrr)\rmap(x, function(y) y[1]) %\u0026gt;% head(2)\r## [[1]]\r## [1] \u0026quot;Alabama\u0026quot;\r## ## [[2]]\r## [1] \u0026quot;Alaska\u0026quot;\rHowever, because this is such a common task, purrr provides a shortcut. If the second argument receives an integer instead of a function, it assumes we want that entry. So the code above can be written more efficiently like this:\nmap(x, 1)\rTo force map to return a character vector instead of a list, we can use map_chr. Similarly, map_int returns integers. So to create our data frame, we can use:\ndat \u0026lt;- tibble(map_chr(x, 1),\rmap_chr(x, 2),\rmap_chr(x, 3),\rmap_chr(x, 4),\rmap_chr(x, 5)) %\u0026gt;%\rmutate_all(parse_guess) %\u0026gt;%\rsetNames(col_names)\rdat %\u0026gt;% head\r## # A tibble: 6 x 5\r## state abb region population total\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Alabama AL South 4779736 135\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\r## 4 Arkansas AR South 2915918 93\r## 5 California CA West 37253956 1257\r## 6 Colorado CO West 5029196 65\rIf you learn more about the purrr package, you will learn that you perform the above with the following, more efficient, code:\ndat \u0026lt;- x %\u0026gt;%\rtranspose() %\u0026gt;%\rmap( ~ parse_guess(unlist(.))) %\u0026gt;%\rsetNames(col_names) %\u0026gt;%\ras_tibble()\rIt turns out that we can avoid all the work shown above after the call to str_split. Specifically, if we know that the data we are extracting can be represented as a table, we can use the argument simplify=TRUE and str_split returns a matrix instead of a list:\nx \u0026lt;- str_split(lines, \u0026quot;,\u0026quot;, simplify = TRUE)\rcol_names \u0026lt;- x[1,]\rx \u0026lt;- x[-1,]\rcolnames(x) \u0026lt;- col_names\rx %\u0026gt;% as_tibble() %\u0026gt;%\rmutate_all(parse_guess) %\u0026gt;%\rhead(5)\r## # A tibble: 5 x 5\r## state abb region population total\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Alabama AL South 4779736 135\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\r## 4 Arkansas AR South 2915918 93\r## 5 California CA West 37253956 1257\r\rCase study 3: extracting tables from a PDF\rOne of the datasets provided in dslabs shows scientific funding rates by gender in the Netherlands:\nlibrary(dslabs)\rdata(\u0026quot;research_funding_rates\u0026quot;)\rresearch_funding_rates %\u0026gt;%\rdplyr::select(\u0026quot;discipline\u0026quot;, \u0026quot;success_rates_men\u0026quot;, \u0026quot;success_rates_women\u0026quot;)\r## discipline success_rates_men success_rates_women\r## 1 Chemical sciences 26.5 25.6\r## 2 Physical sciences 19.3 23.1\r## 3 Physics 26.9 22.2\r## 4 Humanities 14.3 19.3\r## 5 Technical sciences 15.9 21.0\r## 6 Interdisciplinary 11.4 21.8\r## 7 Earth/life sciences 24.4 14.3\r## 8 Social sciences 15.3 11.5\r## 9 Medical sciences 18.8 11.2\rThe data comes from a paper published in the Proceedings of the National Academy of Science (PNAS)5, a widely read scientific journal. However, the data is not provided in a spreadsheet; it is in a table in a PDF document. Here is a screenshot of the table:\n(Source: Romy van der Lee and Naomi Ellemers, PNAS 2015 112 (40) 12349-123536.)\nWe could extract the numbers by hand, but this could lead to human error. Instead, we can try to wrangle the data using R. We start by downloading the pdf document, then importing into R:\nlibrary(\u0026quot;pdftools\u0026quot;)\rtemp_file \u0026lt;- tempfile()\rurl \u0026lt;- paste0(\u0026quot;https://www.pnas.org/content/suppl/2015/09/16/\u0026quot;,\r\u0026quot;1510159112.DCSupplemental/pnas.201510159SI.pdf\u0026quot;)\rdownload.file(url, temp_file)\rtxt \u0026lt;- pdf_text(temp_file)\rfile.remove(temp_file)\rIf we examine the object text, we notice that it is a character vector with an entry for each page. So we keep the page we want:\nraw_data_research_funding_rates \u0026lt;- txt[2]\rThe steps above can actually be skipped because we include this raw data in the dslabs package as well:\ndata(\u0026quot;raw_data_research_funding_rates\u0026quot;)\rExamining the object raw_data_research_funding_rates\rwe see that it is a long string and each line on the page, including the table rows, are separated by the symbol for newline: \\n. We therefore can create a list with the lines of the text as elements as follows:\ntab \u0026lt;- str_split(raw_data_research_funding_rates, \u0026quot;\\n\u0026quot;)\rBecause we start off with just one element in the string, we end up with a list with just one entry.\ntab \u0026lt;- tab[[1]]\rBy examining tab we see that the information for the column names is the third and fourth entries:\nthe_names_1 \u0026lt;- tab[3]\rthe_names_2 \u0026lt;- tab[4]\rThe first of these rows looks like this:\n## Applications, n\r## Awards, n Success rates, %\rWe want to create one vector with one name for each column. Using some of the functions we have just learned, we do this. Let’s start with the_names_1, shown above. We want to remove the leading space and anything following the comma. We use regex for the latter. Then we can obtain the elements by splitting strings separated by space. We want to split only when there are 2 or more spaces to avoid splitting Success rates. So we use the regex \\\\s{2,}\nthe_names_1 \u0026lt;- the_names_1 %\u0026gt;%\rstr_trim() %\u0026gt;%\rstr_replace_all(\u0026quot;,\\\\s.\u0026quot;, \u0026quot;\u0026quot;) %\u0026gt;%\rstr_split(\u0026quot;\\\\s{2,}\u0026quot;, simplify = TRUE)\rthe_names_1\r## [,1] [,2] [,3] ## [1,] \u0026quot;Applications\u0026quot; \u0026quot;Awards\u0026quot; \u0026quot;Success rates\u0026quot;\rNow we will look at the_names_2:\n## Discipline Total Men Women\r## Total Men Women Total Men Women\rHere we want to trim the leading space and then split by space as we did for the first line:\nthe_names_2 \u0026lt;- the_names_2 %\u0026gt;%\rstr_trim() %\u0026gt;%\rstr_split(\u0026quot;\\\\s+\u0026quot;, simplify = TRUE)\rthe_names_2\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] \u0026quot;Discipline\u0026quot; \u0026quot;Total\u0026quot; \u0026quot;Men\u0026quot; \u0026quot;Women\u0026quot; \u0026quot;Total\u0026quot; \u0026quot;Men\u0026quot; \u0026quot;Women\u0026quot; \u0026quot;Total\u0026quot; \u0026quot;Men\u0026quot;\r## [,10] ## [1,] \u0026quot;Women\u0026quot;\rWe can then join these to generate one name for each column:\ntmp_names \u0026lt;- str_c(rep(the_names_1, each = 3), the_names_2[-1], sep = \u0026quot;_\u0026quot;)\rthe_names \u0026lt;- c(the_names_2[1], tmp_names) %\u0026gt;%\rstr_to_lower() %\u0026gt;%\rstr_replace_all(\u0026quot;\\\\s\u0026quot;, \u0026quot;_\u0026quot;)\rthe_names\r## [1] \u0026quot;discipline\u0026quot; \u0026quot;applications_total\u0026quot; \u0026quot;applications_men\u0026quot; ## [4] \u0026quot;applications_women\u0026quot; \u0026quot;awards_total\u0026quot; \u0026quot;awards_men\u0026quot; ## [7] \u0026quot;awards_women\u0026quot; \u0026quot;success_rates_total\u0026quot; \u0026quot;success_rates_men\u0026quot; ## [10] \u0026quot;success_rates_women\u0026quot;\rNow we are ready to get the actual data. By examining the tab object, we notice that the information is in lines 6 through 14. We can use str_split again to achieve our goal:\nnew_research_funding_rates \u0026lt;- tab[6:14] %\u0026gt;%\rstr_trim %\u0026gt;%\rstr_split(\u0026quot;\\\\s{2,}\u0026quot;, simplify = TRUE) %\u0026gt;%\rdata.frame(stringsAsFactors = FALSE) %\u0026gt;%\rsetNames(the_names) %\u0026gt;%\rmutate_at(-1, parse_number)\rnew_research_funding_rates %\u0026gt;% as_tibble()\r## # A tibble: 9 x 10\r## discipline applications_to~ applications_men applications_wo~ awards_total\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Chemical ~ 122 83 39 32\r## 2 Physical ~ 174 135 39 35\r## 3 Physics 76 67 9 20\r## 4 Humanities 396 230 166 65\r## 5 Technical~ 251 189 62 43\r## 6 Interdisc~ 183 105 78 29\r## 7 Earth/lif~ 282 156 126 56\r## 8 Social sc~ 834 425 409 112\r## 9 Medical s~ 505 245 260 75\r## # ... with 5 more variables: awards_men \u0026lt;dbl\u0026gt;, awards_women \u0026lt;dbl\u0026gt;,\r## # success_rates_total \u0026lt;dbl\u0026gt;, success_rates_men \u0026lt;dbl\u0026gt;,\r## # success_rates_women \u0026lt;dbl\u0026gt;\rWe can see that the objects are identical:\nidentical(research_funding_rates, new_research_funding_rates)\r## [1] TRUE\r\rRecoding\rAnother common operation involving strings is recoding the names of categorical variables. Let’s say you have really long names for your levels and you will be displaying them in plots, you might want to use shorter versions of these names. For example, in character vectors with country names, you might want to change “United States of America” to “USA” and “United Kingdom” to UK, and so on. We can do this with case_when, although the tidyverse offers an option that is specifically designed for this task: the recode function.\nHere is an example that shows how to rename countries with long names:\nlibrary(dslabs)\rdata(\u0026quot;gapminder\u0026quot;)\rSuppose we want to show life expectancy time series by country for the Caribbean:\ngapminder %\u0026gt;%\rdplyr::filter(region == \u0026quot;Caribbean\u0026quot;) %\u0026gt;%\rggplot(aes(year, life_expectancy, color = country)) +\rgeom_line()\rThe plot is what we want, but much of the space is wasted to accommodate some of the long country names.\r%\rdplyr::filter(region == \"Caribbean\") %%\rdplyr::filter(str_length(country) = 12) %%\rdistinct(country)\r```\r```\r## country\r## 1 Antigua and Barbuda\r## 2 Dominican Republic\r## 3 St. Vincent and the Grenadines\r## 4 Trinidad and Tobago\r```\r--\rWe have four countries with names longer than 12 characters. These names appear once for each year in the Gapminder dataset. Once we pick nicknames, we need to change them all consistently. The recode function can be used to do this:\ngapminder %\u0026gt;% dplyr::filter(region==\u0026quot;Caribbean\u0026quot;) %\u0026gt;%\rmutate(country = recode(country,\r`Antigua and Barbuda` = \u0026quot;Barbuda\u0026quot;,\r`Dominican Republic` = \u0026quot;DR\u0026quot;,\r`St. Vincent and the Grenadines` = \u0026quot;St. Vincent\u0026quot;,\r`Trinidad and Tobago` = \u0026quot;Trinidad\u0026quot;)) %\u0026gt;%\rggplot(aes(year, life_expectancy, color = country)) +\rgeom_line()\rThere are other similar functions in other R packages, such as recode_factor and fct_recoder in the forcats package.\nTRY IT\nComplete all lessons and exercises in the https://regexone.com/ online interactive tutorial.\n\rIn the extdata directory of the dslabs package, you will find a PDF file containing daily mortality data for Puerto Rico from Jan 1, 2015 to May 31, 2018. You can find the file like this:\n\r\rfn \u0026lt;- system.file(\u0026quot;extdata\u0026quot;, \u0026quot;RD-Mortality-Report_2015-18-180531.pdf\u0026quot;,\rpackage=\u0026quot;dslabs\u0026quot;)\rFind and open the file or open it directly from RStudio. On a Mac, you can type:\nsystem2(\u0026quot;open\u0026quot;, args = fn)\rand on Windows, you can type:\nsystem(\u0026quot;cmd.exe\u0026quot;, input = paste(\u0026quot;start\u0026quot;, fn))\rWhich of the following best describes this file:\nIt is a table. Extracting the data will be easy.\rIt is a report written in prose. Extracting the data will be impossible.\rIt is a report combining graphs and tables. Extracting the data seems possible.\rIt shows graphs of the data. Extracting the data will be difficult.\r\rWe are going to create a tidy dataset with each row representing one observation. The variables in this dataset will be year, month, day, and deaths.\rStart by installing and loading the pdftools package:\r\rinstall.packages(\u0026quot;pdftools\u0026quot;)\rlibrary(pdftools)\rNow read-in fn using the pdf_text function and store the results in an object called txt. Which of the following best describes what you see in txt?\nA table with the mortality data.\rA character string of length 12. Each entry represents the text in each page. The mortality data is in there somewhere.\rA character string with one entry containing all the information in the PDF file.\rAn html document.\r\rExtract the ninth page of the PDF file from the object txt, then use the str_split from the stringr package so that you have each line in a different entry. Call this string vector s. Then look at the result and choose the one that best describes what you see.\r\rIt is an empty string.\rI can see the figure shown in page 1.\rIt is a tidy table.\rI can see the table! But there is a bunch of other stuff we need to get rid of.\r\rWhat kind of object is s and how many entries does it have?\n\rWe see that the output is a list with one component. Redefine s to be the first entry of the list. What kind of object is s and how many entries does it have?\n\rWhen inspecting the string we obtained above, we see a common problem: white space before and after the other characters. Trimming is a common first step in string processing. These extra spaces will eventually make splitting the strings hard so we start by removing them. We learned about the command str_trim that removes spaces at the start or end of the strings. Use this function to trim s.\n\rWe want to extract the numbers from the strings stored in s. However, there are many non-numeric characters that will get in the way. We can remove these, but before doing this we want to preserve the string with the column header, which includes the month abbreviation.\rUse the str_which function to find the rows with a header. Save these results to header_index. Hint: find the first string that matches the pattern 2015 using the str_which function.\n\rNow we are going to define two objects: month will store the month and header will store the column names. Identify which row contains the header of the table. Save the content of the row into an object called header, then use str_split to help define the two objects we need. Hints: the separator here is one or more spaces. Also, consider using the simplify argument.\n\rNotice that towards the end of the page you see a totals row followed by rows with other summary statistics. Create an object called tail_index with the index of the totals entry.\n\rBecause our PDF page includes graphs with numbers, some of our rows have just one number (from the y-axis of the plot). Use the str_count function to create an object n with the number of numbers in each each row. Hint: you can write a regex for number like this \\\\d+.\n\rWe are now ready to remove entries from rows that we know we don’t need. The entry header_index and everything before it should be removed. Entries for which n is 1 should also be removed, and the entry tail_index and everything that comes after it should be removed as well.\n\rNow we are ready to remove all the non-numeric entries. Do this using regex and the str_remove_all function. Hint: remember that in regex, using the upper case version of a special character usually means the opposite. So \\\\D means “not a digit”. Remember you also want to keep spaces.\n\rTo convert the strings into a table, use the str_split_fixed function. Convert s into a data matrix with just the day and death count data. Hints: note that the separator is one or more spaces. Make the argument n a value that limits the number of columns to the values in the 4 columns and the last column captures all the extra stuff. Then keep only the first four columns.\n\rNow you are almost ready to finish. Add column names to the matrix, including one called day. Also, add a column with the month. Call the resulting object dat. Finally, make sure the day is an integer not a character. Hint: use only the first five columns.\n\rNow finish it up by tidying tab with the gather function.\n\rMake a plot of deaths versus day with color to denote year. Exclude 2018 since we do not have data for the entire year.\n\rNow that we have wrangled this data step-by-step, put it all together in one R chunk, using the pipe as much as possible. Hint: first define the indexes, then write one line of code that does all the string processing.\n\rAdvanced: let’s return to the MLB Payroll example from the web scraping section. Use what you have learned in the web scraping and string processing chapters to extract the payroll for the New York Yankees, Boston Red Sox, and Oakland A’s and plot them as a function of time.\n\r\r\r\r\rWeb scraping\rThe data we need to answer a question is not always in a spreadsheet ready for us to read. For example, the US murders dataset we used in the R Basics chapter originally comes from this Wikipedia page:\nurl \u0026lt;- paste0(\u0026quot;https://en.wikipedia.org/w/index.php?title=\u0026quot;,\r\u0026quot;Gun_violence_in_the_United_States_by_state\u0026quot;,\r\u0026quot;\u0026amp;direction=prev\u0026amp;oldid=810166167\u0026quot;)\rYou can see the data table when you visit the webpage:\n(Web page courtesy of Wikipedia7. CC-BY-SA-3.0 license8. Screenshot of part of the page.)\nUnfortunately, there is no link to a data file. To make the data frame that is loaded when we type data(murders), we had to do some web scraping.\nWeb scraping, or web harvesting, is the term we use to describe the process of extracting data from a website. The reason we can do this is because the information used by a browser to render webpages is received as a text file from a server. The text is code written in hyper text markup language (HTML). Every browser has a way to show the html source code for a page, each one different. On Chrome, you can use Control-U on a PC and command+alt+U on a Mac. You will see something like this:\nHTML\rBecause this code is accessible, we can download the HTML file, import it into R, and then write programs to extract the information we need from the page. However, once we look at HTML code, this might seem like a daunting task. But we will show you some convenient tools to facilitate the process. To get an idea of how it works, here are a few lines of code from the Wikipedia page that provides the US murders data:\n\u0026lt;table class=\u0026quot;wikitable sortable\u0026quot;\u0026gt;\r\u0026lt;tr\u0026gt;\r\u0026lt;th\u0026gt;State\u0026lt;/th\u0026gt;\r\u0026lt;th\u0026gt;\u0026lt;a href=\u0026quot;/wiki/List_of_U.S._states_and_territories_by_population\u0026quot;\rtitle=\u0026quot;List of U.S. states and territories by population\u0026quot;\u0026gt;Population\u0026lt;/a\u0026gt;\u0026lt;br /\u0026gt;\r\u0026lt;small\u0026gt;(total inhabitants)\u0026lt;/small\u0026gt;\u0026lt;br /\u0026gt;\r\u0026lt;small\u0026gt;(2015)\u0026lt;/small\u0026gt; \u0026lt;sup id=\u0026quot;cite_ref-1\u0026quot; class=\u0026quot;reference\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#cite_note-1\u0026quot;\u0026gt;[1]\u0026lt;/a\u0026gt;\u0026lt;/sup\u0026gt;\u0026lt;/th\u0026gt;\r\u0026lt;th\u0026gt;Murders and Nonnegligent\r\u0026lt;p\u0026gt;Manslaughter\u0026lt;br /\u0026gt;\r\u0026lt;small\u0026gt;(total deaths)\u0026lt;/small\u0026gt;\u0026lt;br /\u0026gt;\r\u0026lt;small\u0026gt;(2015)\u0026lt;/small\u0026gt; \u0026lt;sup id=\u0026quot;cite_ref-2\u0026quot; class=\u0026quot;reference\u0026quot;\u0026gt;\r\u0026lt;a href=\u0026quot;#cite_note-2\u0026quot;\u0026gt;[2]\u0026lt;/a\u0026gt;\u0026lt;/sup\u0026gt;\u0026lt;/p\u0026gt;\r\u0026lt;/th\u0026gt;\r\u0026lt;th\u0026gt;Murder and Nonnegligent\r\u0026lt;p\u0026gt;Manslaughter Rate\u0026lt;br /\u0026gt;\r\u0026lt;small\u0026gt;(per 100,000 inhabitants)\u0026lt;/small\u0026gt;\u0026lt;br /\u0026gt;\r\u0026lt;small\u0026gt;(2015)\u0026lt;/small\u0026gt;\u0026lt;/p\u0026gt;\r\u0026lt;/th\u0026gt;\r\u0026lt;/tr\u0026gt;\r\u0026lt;tr\u0026gt;\r\u0026lt;td\u0026gt;\u0026lt;a href=\u0026quot;/wiki/Alabama\u0026quot; title=\u0026quot;Alabama\u0026quot;\u0026gt;Alabama\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\r\u0026lt;td\u0026gt;4,853,875\u0026lt;/td\u0026gt;\r\u0026lt;td\u0026gt;348\u0026lt;/td\u0026gt;\r\u0026lt;td\u0026gt;7.2\u0026lt;/td\u0026gt;\r\u0026lt;/tr\u0026gt;\r\u0026lt;tr\u0026gt;\r\u0026lt;td\u0026gt;\u0026lt;a href=\u0026quot;/wiki/Alaska\u0026quot; title=\u0026quot;Alaska\u0026quot;\u0026gt;Alaska\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt;\r\u0026lt;td\u0026gt;737,709\u0026lt;/td\u0026gt;\r\u0026lt;td\u0026gt;59\u0026lt;/td\u0026gt;\r\u0026lt;td\u0026gt;8.0\u0026lt;/td\u0026gt;\r\u0026lt;/tr\u0026gt;\r\u0026lt;tr\u0026gt;\rYou can actually see the data, except data values are surrounded by html code such as \u0026lt;td\u0026gt;. We can also see a pattern of how it is stored. If you know HTML, you can write programs that leverage knowledge of these patterns to extract what we want. We also take advantage of a language widely used to make webpages look “pretty” called Cascading Style Sheets (CSS). We say more about this in Section ??.\nAlthough we provide tools that make it possible to scrape data without knowing HTML, as a data scientist it is quite useful to learn some HTML and CSS. Not only does this improve your scraping skills, but it might come in handy if you are creating a webpage to showcase your work. There are plenty of online courses and tutorials for learning these. Two examples are Codeacademy9 and W3schools10.\n\rThe rvest package\rThe tidyverse provides a web harvesting package called rvest. The first step using this package is to import the webpage into R. The package makes this quite simple:\nlibrary(tidyverse)\rlibrary(rvest)\rh \u0026lt;- read_html(url)\rNote that the entire Murders in the US Wikipedia webpage is now contained in h. The class of this object is:\nclass(h)\r## [1] \u0026quot;xml_document\u0026quot; \u0026quot;xml_node\u0026quot;\rThe rvest package is actually more general; it handles XML documents. XML is a general markup language (that’s what the ML stands for) that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing webpages. Here we focus on HTML documents.\nNow, how do we extract the table from the object h? If we print h, we don’t really see much:\nh\r## {html_document}\r## \u0026lt;html class=\u0026quot;client-nojs\u0026quot; lang=\u0026quot;en\u0026quot; dir=\u0026quot;ltr\u0026quot;\u0026gt;\r## [1] \u0026lt;head\u0026gt;\\n\u0026lt;meta http-equiv=\u0026quot;Content-Type\u0026quot; content=\u0026quot;text/html; charset=UTF-8 ...\r## [2] \u0026lt;body class=\u0026quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject ...\rWe can see all the code that defines the downloaded webpage using the html_text function like this:\nhtml_text(h)\rWe don’t show the output here because it includes thousands of characters, but if we look at it, we can see the data we are after are stored in an HTML table: you can see this in this line of the HTML code above \u0026lt;table class=\"wikitable sortable\"\u0026gt;. The different parts of an HTML document, often defined with a message in between \u0026lt; and \u0026gt; are referred to as nodes. The rvest package includes functions to extract nodes of an HTML document: html_nodes extracts all nodes of different types and html_node extracts the first one. To extract the tables from the html code we use:\ntab \u0026lt;- h %\u0026gt;% html_nodes(\u0026quot;table\u0026quot;)\rNow, instead of the entire webpage, we just have the html code for the tables in the page:\ntab\r## {xml_nodeset (2)}\r## [1] \u0026lt;table class=\u0026quot;wikitable sortable\u0026quot;\u0026gt;\u0026lt;tbody\u0026gt;\\n\u0026lt;tr\u0026gt;\\n\u0026lt;th\u0026gt;State\\n\u0026lt;/th\u0026gt;\\n\u0026lt;th\u0026gt;\\n ...\r## [2] \u0026lt;table class=\u0026quot;nowraplinks hlist mw-collapsible mw-collapsed navbox-inner\u0026quot; ...\rThe table we are interested is the first one:\ntab[[1]]\r## {html_node}\r## \u0026lt;table class=\u0026quot;wikitable sortable\u0026quot;\u0026gt;\r## [1] \u0026lt;tbody\u0026gt;\\n\u0026lt;tr\u0026gt;\\n\u0026lt;th\u0026gt;State\\n\u0026lt;/th\u0026gt;\\n\u0026lt;th\u0026gt;\\n\u0026lt;a href=\u0026quot;/wiki/List_of_U.S._states ...\rThis is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, rvest includes a function just for converting HTML tables into data frames:\ntab \u0026lt;- tab[[1]] %\u0026gt;% html_table\rclass(tab)\r## [1] \u0026quot;data.frame\u0026quot;\rWe are now much closer to having a usable data table:\ntab \u0026lt;- tab %\u0026gt;% setNames(c(\u0026quot;state\u0026quot;, \u0026quot;population\u0026quot;, \u0026quot;total\u0026quot;, \u0026quot;murder_rate\u0026quot;))\rhead(tab)\r## state population total murder_rate\r## 1 Alabama 4,853,875 348 7.2\r## 2 Alaska 737,709 59 8.0\r## 3 Arizona 6,817,565 309 4.5\r## 4 Arkansas 2,977,853 181 6.1\r## 5 California 38,993,940 1,861 4.8\r## 6 Colorado 5,448,819 176 3.2\rWe still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites.\n\rCSS selectors\rThe default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS to define the look and style of webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as selectors. An example of such a pattern, which we used above, is table, but there are many, many more.\nIf we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the html_nodes function. However, knowing which selector can be quite complicated.\rIn fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the nodes that define a particular piece of data. However, selector gadgets actually make this possible.\nSelectorGadget11 is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables from html pages, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including rvest author Hadley Wickham’s\rvignette12 and other tutorials based on the vignette13 14.\n\rJSON\rSharing data on the internet has become more and more common. Unfortunately, providers use different formats, which makes it harder for data scientists to wrangle data into R. Yet there are some standards that are also becoming more common. Currently, a format that is widely being adopted is the JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. This JSON file looks more like the code you use to define a list. Here is an example of information stored in a JSON format:\n## ## Attaching package: \u0026#39;jsonlite\u0026#39;\r## The following object is masked from \u0026#39;package:purrr\u0026#39;:\r## ## flatten\r## [\r## {\r## \u0026quot;name\u0026quot;: \u0026quot;Miguel\u0026quot;,\r## \u0026quot;student_id\u0026quot;: 1,\r## \u0026quot;exam_1\u0026quot;: 85,\r## \u0026quot;exam_2\u0026quot;: 86\r## },\r## {\r## \u0026quot;name\u0026quot;: \u0026quot;Sofia\u0026quot;,\r## \u0026quot;student_id\u0026quot;: 2,\r## \u0026quot;exam_1\u0026quot;: 94,\r## \u0026quot;exam_2\u0026quot;: 93\r## },\r## {\r## \u0026quot;name\u0026quot;: \u0026quot;Aya\u0026quot;,\r## \u0026quot;student_id\u0026quot;: 3,\r## \u0026quot;exam_1\u0026quot;: 87,\r## \u0026quot;exam_2\u0026quot;: 88\r## },\r## {\r## \u0026quot;name\u0026quot;: \u0026quot;Cheng\u0026quot;,\r## \u0026quot;student_id\u0026quot;: 4,\r## \u0026quot;exam_1\u0026quot;: 90,\r## \u0026quot;exam_2\u0026quot;: 91\r## }\r## ]\rThe file above actually represents a data frame. To read it, we can use the function fromJSON from the jsonlite package. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and obtain data. Here is an example:\nlibrary(jsonlite)\rres \u0026lt;- fromJSON(\u0026#39;http://ergast.com/api/f1/2004/1/results.json\u0026#39;)\rciti_bike \u0026lt;- fromJSON(\u0026quot;http://citibikenyc.com/stations/json\u0026quot;)\rThis downloads a list. The first argument tells you when you downloaded it:\nciti_bike$executionTime\rand the second is a data table:\nciti_bike$stationBeanList %\u0026gt;% as_tibble()\rYou can learn much more by examining tutorials and help files from the jsonlite package. This package is intended for relatively simple tasks such as converting data into tables. For more flexibility, we recommend rjson.\n\r\r\rhttps://www.regular-expressions.info/tutorial.html↩︎\n\rhttp://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions↩︎\n\rhttps://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf↩︎\n\rhttps://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf↩︎\n\rhttp://www.pnas.org/content/112/40/12349.abstract↩︎\n\rhttp://www.pnas.org/content/112/40/12349↩︎\n\rhttps://en.wikipedia.org/w/index.php?title=Gun_violence_in_the_United_States_by_state\u0026amp;direction=prev\u0026amp;oldid=810166167↩︎\n\rhttps://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License↩︎\n\rhttps://www.codecademy.com/learn/learn-html↩︎\n\rhttps://www.w3schools.com/↩︎\n\rhttp://selectorgadget.com/↩︎\n\rhttps://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html↩︎\n\rhttps://stat4701.github.io/edav/2015/04/02/rvest_tutorial/↩︎\n\rhttps://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/↩︎\n\r\r\r","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610286616,"objectID":"9eee4da015dc05af8a4bee2b0e8794b9","permalink":"https://datavizm20.classes.andrewheiss.com/content/12-content/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/content/12-content/","section":"content","summary":"Required Reading\rGuiding Questions\r\rString processing\rThe stringr package\rCase study 1: US murders data\rCase study 2: self-reported heights\rHow to escape when defining strings\rRegular expressions\rStrings are a regexp\rSpecial characters\rCharacter classes\rAnchors\rQuantifiers\rWhite space \\s\rQuantifiers: *, ?, +\rNot\rGroups\r\rSearch and replace with regex\rSearch and replace using groups\r\rTesting and improving\rTrimming\rChanging lettercase\rCase study 2: self-reported heights (continued)\rThe extract function\rPutting it all together\r\rString splitting\rCase study 3: extracting tables from a PDF\rRecoding\r\rWeb scraping\rHTML\rThe rvest package\rCSS selectors\rJSON\r\r\r\rRequired Reading\r\rThis page.","tags":null,"title":"Text as Data","type":"docs"},{"authors":null,"categories":null,"content":"\r\rRequired Reading\rGuiding Questions\r\rIntroduction to data wrangling\rReshaping data\rgather\rspread\rseparate\runite\r\rJoining tables\rJoins\rLeft join\rRight join\rInner join\rFull join\rSemi join\rAnti join\r\rBinding\rBinding columns\rBinding by rows\r\rSet operators\rIntersect\rUnion\rsetdiff\rsetequal\r\r\rParsing dates and times\rThe date data type\rThe lubridate package\r\r\r\rRequired Reading\r\rThis page.\r\rGuiding Questions\r\rHow can we reshape data into a useable tidy form?\rWhat is a join and why is it a common data wrangling maneuver?\rWhat is a primary key and why is it important to think about our data in this way?\rHow do we deal with messy date variables?\r\r\r\rIntroduction to data wrangling\rMany of the datasets used in this class have been made available to you as R objects, specifically as data frames. The US murders data, the reported heights data, and the Gapminder data were all data frames. Furthermore, much of the data is available in what is referred to as tidy form. The tidyverse packages and functions assume that the data is tidy and this assumption is a big part of the reason these packages work so well together.\nHowever, very rarely in a data science project is data easily available as part of a package. People did quite a bit of work “behind the scenes” to get the original raw data into the tidy tables. Much more typical is for the data to be in a file, a database, or extracted from a document, including web pages, tweets, or PDFs. In these cases, the first step is to import the data into R and, when using the tidyverse, tidy up the data. This initial step in the data analysis process usually involves several, often complicated, steps to convert data from its raw form to the tidy form that greatly facilitates the rest of the analysis. We refer to this process as data wrangling.\nHere we cover several common steps of the data wrangling process including tidying data, string processing, html parsing, working with dates and times, and text mining. Rarely are all these wrangling steps necessary in a single analysis, but data scientists will likely face them all at some point.\n\rReshaping data\rAs we have seen through the class, having data in tidy format is what makes the tidyverse flow. After the first step in the data analysis process, importing data, a common next step is to reshape the data into a form that facilitates the rest of the analysis. The tidyr package includes several functions that are useful for tidying data.\nlibrary(tidyverse)\rlibrary(dslabs)\rpath \u0026lt;- system.file(\u0026quot;extdata\u0026quot;, package=\u0026quot;dslabs\u0026quot;)\rfilename \u0026lt;- file.path(path, \u0026quot;fertility-two-countries-example.csv\u0026quot;)\rwide_data \u0026lt;- read_csv(filename)\rgather\rOne of the most used functions in the tidyr package is gather, which is useful for converting wide data into tidy data.\nAs with most tidyverse functions, the gather function’s first argument is the data frame that will be converted. Here we want to reshape the wide_data dataset so that each row represents a fertility observation, which implies we need three columns to store the year, country, and the observed value. In its current form, data from different years are in different columns with the year values stored in the column names. Through the second and third argument we will tell gather the column names we want to assign to the columns containing the current column names and observations, respectively. In this case a good choice for these two arguments would be year and fertility. Note that nowhere in the data file does it tell us this is fertility data. Instead, we deciphered this from the file name. Through the fourth argument we specify the columns containing observed values; these are the columns that will be gathered. The default is to gather all columns so, in most cases, we have to specify the columns. In our example we want columns 1960, 1961 up to 2015.\nThe code to gather the fertility data therefore looks like this:\nnew_tidy_data \u0026lt;- gather(wide_data, year, fertility, `1960`:`2015`)\rWe can also use the pipe like this:\nnew_tidy_data \u0026lt;- wide_data %\u0026gt;% gather(year, fertility, `1960`:`2015`)\rWe can see that the data have been converted to tidy format with columns year and fertility:\nhead(new_tidy_data)\r## # A tibble: 6 x 3\r## country year fertility\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 1960 2.41\r## 2 South Korea 1960 6.16\r## 3 Germany 1961 2.44\r## 4 South Korea 1961 5.99\r## 5 Germany 1962 2.47\r## 6 South Korea 1962 5.79\rand that each year resulted in two rows since we have two countries and this column was not gathered.\rA somewhat quicker way to write this code is to specify which column will not be gathered, rather than all the columns that will be gathered:\nnew_tidy_data \u0026lt;- wide_data %\u0026gt;%\rgather(year, fertility, -country)\rThe new_tidy_data object looks like the original tidy_data we defined this way\ndata(\u0026quot;gapminder\u0026quot;)\rtidy_data \u0026lt;- gapminder %\u0026gt;%\rdplyr::filter(country %in% c(\u0026quot;South Korea\u0026quot;, \u0026quot;Germany\u0026quot;) \u0026amp; !is.na(fertility)) %\u0026gt;%\rdplyr::select(country, year, fertility)\rwith just one minor difference. Can you spot it? Look at the data type of the year column:\nclass(tidy_data$year)\r## [1] \u0026quot;integer\u0026quot;\rclass(new_tidy_data$year)\r## [1] \u0026quot;character\u0026quot;\rThe gather function assumes that column names are characters. So we need a bit more wrangling before we are ready to make a plot. We need to convert the year column to be numbers. The gather function includes the convert argument for this purpose:\nnew_tidy_data \u0026lt;- wide_data %\u0026gt;%\rgather(year, fertility, -country, convert = TRUE)\rclass(new_tidy_data$year)\r## [1] \u0026quot;integer\u0026quot;\rNote that we could have also used the mutate and as.numeric.\nNow that the data is tidy, we can use this relatively simple ggplot code:\nnew_tidy_data %\u0026gt;% ggplot(aes(year, fertility, color = country)) +\rgeom_point()\r\rspread\rAs we will see in later examples, it is sometimes useful for data wrangling purposes to convert tidy data into wide data. We often use this as an intermediate step in tidying up data. The spread function is basically the inverse of gather. The first argument is for the data, but since we are using the pipe, we don’t show it. The second argument tells spread which variable will be used as the column names. The third argument specifies which variable to use to fill out the cells:\nnew_wide_data \u0026lt;- new_tidy_data %\u0026gt;% spread(year, fertility)\rdplyr::select(new_wide_data, country, `1960`:`1967`)\r## # A tibble: 2 x 9\r## country `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967`\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 2.41 2.44 2.47 2.49 2.49 2.48 2.44 2.37\r## 2 South Korea 6.16 5.99 5.79 5.57 5.36 5.16 4.99 4.85\rThe following diagram can help remind you how these two functions work:\n(Image courtesy of RStudio1. CC-BY-4.0 license2. Cropped from original.)\r\rseparate\rThe data wrangling shown above was simple compared to what is usually required. In our example spreadsheet files, we include an illustration that is slightly more complicated. It contains two variables: life expectancy and fertility. However, the way it is stored is not tidy and, as we will explain, not optimal.\npath \u0026lt;- system.file(\u0026quot;extdata\u0026quot;, package = \u0026quot;dslabs\u0026quot;)\rfilename \u0026lt;- \u0026quot;life-expectancy-and-fertility-two-countries-example.csv\u0026quot;\rfilename \u0026lt;- file.path(path, filename)\rraw_dat \u0026lt;- read_csv(filename)\rdplyr::select(raw_dat, 1:5)\r## # A tibble: 2 x 5\r## country `1960_fertility` `1960_life_expec~ `1961_fertility` `1961_life_expec~\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 2.41 69.3 2.44 69.8\r## 2 South K~ 6.16 53.0 5.99 53.8\rFirst, note that the data is in wide format. Second, notice that this table includes values for two variables, fertility and life expectancy, with the column name encoding which column represents which variable. Encoding information in the column names is not recommended but, unfortunately, it is quite common. We will put our wrangling skills to work to extract this information and store it in a tidy fashion.\nWe can start the data wrangling with the gather function, but we should no longer use the column name year for the new column since it also contains the variable type. We will call it key, the default, for now:\ndat \u0026lt;- raw_dat %\u0026gt;% gather(key, value, -country)\rhead(dat)\r## # A tibble: 6 x 3\r## country key value\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 1960_fertility 2.41\r## 2 South Korea 1960_fertility 6.16\r## 3 Germany 1960_life_expectancy 69.3 ## 4 South Korea 1960_life_expectancy 53.0 ## 5 Germany 1961_fertility 2.44\r## 6 South Korea 1961_fertility 5.99\rThe result is not exactly what we refer to as tidy since each observation is associated with two, not one, rows. We want to have the values from the two variables, fertility and life expectancy, in two separate columns. The first challenge to achieve this is to separate the key column into the year and the variable type. Notice that the entries in this column separate the year from the variable name with an underscore:\ndat$key[1:5]\r## [1] \u0026quot;1960_fertility\u0026quot; \u0026quot;1960_fertility\u0026quot; \u0026quot;1960_life_expectancy\u0026quot;\r## [4] \u0026quot;1960_life_expectancy\u0026quot; \u0026quot;1961_fertility\u0026quot;\rEncoding multiple variables in a column name is such a common problem that the readr package includes a function to separate these columns into two or more. Apart from the data, the separate function takes three arguments: the name of the column to be separated, the names to be used for the new columns, and the character that separates the variables. So, a first attempt at this is:\ndat %\u0026gt;% separate(key, c(\u0026quot;year\u0026quot;, \u0026quot;variable_name\u0026quot;), \u0026quot;_\u0026quot;)\rBecause _ is the default separator assumed by separate, we do not have to include it in the code:\ndat %\u0026gt;% separate(key, c(\u0026quot;year\u0026quot;, \u0026quot;variable_name\u0026quot;))\r## Warning: Expected 2 pieces. Additional pieces discarded in 112 rows [3, 4, 7, 8,\r## 11, 12, 15, 16, 19, 20, 23, 24, 27, 28, 31, 32, 35, 36, 39, 40, ...].\r## # A tibble: 224 x 4\r## country year variable_name value\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 1960 fertility 2.41\r## 2 South Korea 1960 fertility 6.16\r## 3 Germany 1960 life 69.3 ## 4 South Korea 1960 life 53.0 ## 5 Germany 1961 fertility 2.44\r## 6 South Korea 1961 fertility 5.99\r## 7 Germany 1961 life 69.8 ## 8 South Korea 1961 life 53.8 ## 9 Germany 1962 fertility 2.47\r## 10 South Korea 1962 fertility 5.79\r## # ... with 214 more rows\rThe function does separate the values, but we run into a new problem. We receive the warning Too many values at 112 locations: and that the life_expectancy variable is truncated to life. This is because the _ is used to separate life and expectancy, not just year and variable name! We could add a third column to catch this and let the separate function know which column to fill in with missing values, NA, when there is no third value. Here we tell it to fill the column on the right:\nvar_names \u0026lt;- c(\u0026quot;year\u0026quot;, \u0026quot;first_variable_name\u0026quot;, \u0026quot;second_variable_name\u0026quot;)\rdat %\u0026gt;% separate(key, var_names, fill = \u0026quot;right\u0026quot;)\r## # A tibble: 224 x 5\r## country year first_variable_name second_variable_name value\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 1960 fertility \u0026lt;NA\u0026gt; 2.41\r## 2 South Korea 1960 fertility \u0026lt;NA\u0026gt; 6.16\r## 3 Germany 1960 life expectancy 69.3 ## 4 South Korea 1960 life expectancy 53.0 ## 5 Germany 1961 fertility \u0026lt;NA\u0026gt; 2.44\r## 6 South Korea 1961 fertility \u0026lt;NA\u0026gt; 5.99\r## 7 Germany 1961 life expectancy 69.8 ## 8 South Korea 1961 life expectancy 53.8 ## 9 Germany 1962 fertility \u0026lt;NA\u0026gt; 2.47\r## 10 South Korea 1962 fertility \u0026lt;NA\u0026gt; 5.79\r## # ... with 214 more rows\rHowever, if we read the separate help file, we find that a better approach is to merge the last two variables when there is an extra separation:\ndat %\u0026gt;% separate(key, c(\u0026quot;year\u0026quot;, \u0026quot;variable_name\u0026quot;), extra = \u0026quot;merge\u0026quot;)\r## # A tibble: 224 x 4\r## country year variable_name value\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 1960 fertility 2.41\r## 2 South Korea 1960 fertility 6.16\r## 3 Germany 1960 life_expectancy 69.3 ## 4 South Korea 1960 life_expectancy 53.0 ## 5 Germany 1961 fertility 2.44\r## 6 South Korea 1961 fertility 5.99\r## 7 Germany 1961 life_expectancy 69.8 ## 8 South Korea 1961 life_expectancy 53.8 ## 9 Germany 1962 fertility 2.47\r## 10 South Korea 1962 fertility 5.79\r## # ... with 214 more rows\rThis achieves the separation we wanted. However, we are not done yet. We need to create a column for each variable. As we learned, the spread function can do this:\ndat %\u0026gt;%\rseparate(key, c(\u0026quot;year\u0026quot;, \u0026quot;variable_name\u0026quot;), extra = \u0026quot;merge\u0026quot;) %\u0026gt;%\rspread(variable_name, value)\r## # A tibble: 112 x 4\r## country year fertility life_expectancy\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 1960 2.41 69.3\r## 2 Germany 1961 2.44 69.8\r## 3 Germany 1962 2.47 70.0\r## 4 Germany 1963 2.49 70.1\r## 5 Germany 1964 2.49 70.7\r## 6 Germany 1965 2.48 70.6\r## 7 Germany 1966 2.44 70.8\r## 8 Germany 1967 2.37 71.0\r## 9 Germany 1968 2.28 70.6\r## 10 Germany 1969 2.17 70.5\r## # ... with 102 more rows\rThe data is now in tidy format with one row for each observation with three variables: year, fertility, and life expectancy.\n\runite\rIt is sometimes useful to do the inverse of separate, unite two columns into one. To demonstrate how to use unite, we show code that, although not the optimal approach, serves as an illustration. Suppose that we did not know about extra and used this command to separate:\ndat %\u0026gt;%\rseparate(key, var_names, fill = \u0026quot;right\u0026quot;)\r## # A tibble: 224 x 5\r## country year first_variable_name second_variable_name value\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 1960 fertility \u0026lt;NA\u0026gt; 2.41\r## 2 South Korea 1960 fertility \u0026lt;NA\u0026gt; 6.16\r## 3 Germany 1960 life expectancy 69.3 ## 4 South Korea 1960 life expectancy 53.0 ## 5 Germany 1961 fertility \u0026lt;NA\u0026gt; 2.44\r## 6 South Korea 1961 fertility \u0026lt;NA\u0026gt; 5.99\r## 7 Germany 1961 life expectancy 69.8 ## 8 South Korea 1961 life expectancy 53.8 ## 9 Germany 1962 fertility \u0026lt;NA\u0026gt; 2.47\r## 10 South Korea 1962 fertility \u0026lt;NA\u0026gt; 5.79\r## # ... with 214 more rows\rWe can achieve the same final result by uniting the second and third columns, then spreading the columns and renaming fertility_NA to fertility:\ndat %\u0026gt;%\rseparate(key, var_names, fill = \u0026quot;right\u0026quot;) %\u0026gt;%\runite(variable_name, first_variable_name, second_variable_name) %\u0026gt;%\rspread(variable_name, value) %\u0026gt;%\rrename(fertility = fertility_NA)\r## # A tibble: 112 x 4\r## country year fertility life_expectancy\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Germany 1960 2.41 69.3\r## 2 Germany 1961 2.44 69.8\r## 3 Germany 1962 2.47 70.0\r## 4 Germany 1963 2.49 70.1\r## 5 Germany 1964 2.49 70.7\r## 6 Germany 1965 2.48 70.6\r## 7 Germany 1966 2.44 70.8\r## 8 Germany 1967 2.37 71.0\r## 9 Germany 1968 2.28 70.6\r## 10 Germany 1969 2.17 70.5\r## # ... with 102 more rows\rTRY IT\nRun the following command to define the co2_wide object:\r\rco2_wide \u0026lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) %\u0026gt;%\rsetNames(1:12) %\u0026gt;%\rmutate(year = as.character(1959:1997))\rUse the gather function to wrangle this into a tidy dataset. Call the column with the CO2 measurements co2 and call the month column month. Call the resulting object co2_tidy.\nPlot CO2 versus month with a different curve for each year using this code:\r\rco2_tidy %\u0026gt;% ggplot(aes(month, co2, color = year)) + geom_line()\rIf the expected plot is not made, it is probably because co2_tidy$month is not numeric:\nclass(co2_tidy$month)\rRewrite the call to gather using an argument that assures the month column will be numeric. Then make the plot.\nWhat do we learn from this plot?\r\rCO2 measures increase monotonically from 1959 to 1997.\rCO2 measures are higher in the summer and the yearly average increased from 1959 to 1997.\rCO2 measures appear constant and random variability explains the differences.\rCO2 measures do not have a seasonal trend.\r\rNow load the admissions data set, which contains admission information for men and women across six majors and keep only the admitted percentage column:\r\rload(admissions)\rdat \u0026lt;- admissions %\u0026gt;% dplyr::select(-applicants)\rIf we think of an observation as a major, and that each observation has two variables (men admitted percentage and women admitted percentage) then this is not tidy. Use the spread function to wrangle into tidy shape: one row for each major.\nNow we will try a more advanced wrangling challenge. We want to wrangle the admissions data so that for each major we have 4 observations: admitted_men, admitted_women, applicants_men and applicants_women. The trick we perform here is actually quite common: first gather to generate an intermediate data frame and then spread to obtain the tidy data we want. We will go step by step in this and the next two exercises.\r\rUse the gather function to create a tmp data.frame with a column containing the type of observation admitted or applicants. Call the new columns key and value.\nNow you have an object tmp with columns major, gender, key and value. Note that if you combine the key and gender, we get the column names we want: admitted_men, admitted_women, applicants_men and applicants_women. Use the function unite to create a new column called column_name.\n\rNow use the spread function to generate the tidy data with four variables for each major.\n\rNow use the pipe to write a line of code that turns admissions to the table produced in the previous exercise.\n\r\r\r\r\rJoining tables\rThe information we need for a given analysis may not be just in one table. For example, when forecasting elections we used the function left_join to combine the information from two tables. Here we use a simpler example to illustrate the general challenge of combining tables.\nSuppose we want to explore the relationship between population size for US states and electoral votes. We have the population size in this table:\nlibrary(tidyverse)\rlibrary(dslabs)\rdata(murders)\rhead(murders)\r## state abb region population total\r## 1 Alabama AL South 4779736 135\r## 2 Alaska AK West 710231 19\r## 3 Arizona AZ West 6392017 232\r## 4 Arkansas AR South 2915918 93\r## 5 California CA West 37253956 1257\r## 6 Colorado CO West 5029196 65\rand electoral votes in this one:\ndata(polls_us_election_2016)\rhead(results_us_election_2016)\r## state electoral_votes clinton trump others\r## 1 California 55 61.7 31.6 6.7\r## 2 Texas 38 43.2 52.2 4.5\r## 3 Florida 29 47.8 49.0 3.2\r## 4 New York 29 59.0 36.5 4.5\r## 5 Illinois 20 55.8 38.8 5.4\r## 6 Pennsylvania 20 47.9 48.6 3.6\rJust concatenating these two tables together will not work since the order of the states is not the same.\nidentical(results_us_election_2016$state, murders$state)\r## [1] FALSE\rThe join functions, described below, are designed to handle this challenge.\nJoins\rThe join functions in the dplyr package make sure that the tables are combined so that matching rows are together. If you know SQL, you will see that the approach and syntax is very similar. The general idea is that one needs to identify one or more columns that will serve to match the two tables. Then a new table with the combined information is returned. Notice what happens if we join the two tables above by state using left_join (we will remove the others column and rename electoral_votes so that the tables fit on the page):\ntab \u0026lt;- left_join(murders, results_us_election_2016, by = \u0026quot;state\u0026quot;) %\u0026gt;%\rdplyr::select(-others) %\u0026gt;% rename(ev = electoral_votes)\rhead(tab)\r## state abb region population total ev clinton trump\r## 1 Alabama AL South 4779736 135 9 34.4 62.1\r## 2 Alaska AK West 710231 19 3 36.6 51.3\r## 3 Arizona AZ West 6392017 232 11 45.1 48.7\r## 4 Arkansas AR South 2915918 93 6 33.7 60.6\r## 5 California CA West 37253956 1257 55 61.7 31.6\r## 6 Colorado CO West 5029196 65 9 48.2 43.3\rThe data has been successfully joined and we can now, for example, make a plot to explore the relationship:\nlibrary(ggrepel)\rtab %\u0026gt;% ggplot(aes(population/10^6, ev, label = abb)) +\rgeom_point() +\rgeom_text_repel() +\rscale_x_continuous(trans = \u0026quot;log2\u0026quot;) +\rscale_y_continuous(trans = \u0026quot;log2\u0026quot;) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE)\rWe see the relationship is close to linear with about 2 electoral votes for every million persons, but with very small states getting higher ratios.\nIn practice, it is not always the case that each row in one table has a matching row in the other. For this reason, we have several versions of join. To illustrate this challenge, we will take subsets of the tables above. We create the tables tab1 and tab2 so that they have some states in common but not all:\ntab_1 \u0026lt;- slice(murders, 1:6) %\u0026gt;% dplyr::select(state, population)\rtab_1\r## state population\r## 1 Alabama 4779736\r## 2 Alaska 710231\r## 3 Arizona 6392017\r## 4 Arkansas 2915918\r## 5 California 37253956\r## 6 Colorado 5029196\rtab_2 \u0026lt;- results_us_election_2016 %\u0026gt;%\rdplyr::filter(state%in%c(\u0026quot;Alabama\u0026quot;, \u0026quot;Alaska\u0026quot;, \u0026quot;Arizona\u0026quot;,\r\u0026quot;California\u0026quot;, \u0026quot;Connecticut\u0026quot;, \u0026quot;Delaware\u0026quot;)) %\u0026gt;%\rdplyr::select(state, electoral_votes) %\u0026gt;% rename(ev = electoral_votes)\rtab_2\r## state ev\r## 1 California 55\r## 2 Arizona 11\r## 3 Alabama 9\r## 4 Connecticut 7\r## 5 Alaska 3\r## 6 Delaware 3\rWe will use these two tables as examples in the next sections.\nLeft join\rSuppose we want a table like tab_1, but adding electoral votes to whatever states we have available. For this, we use left_join with tab_1 as the first argument. We specify which column to use to match with the by argument.\nleft_join(tab_1, tab_2, by = \u0026quot;state\u0026quot;)\r## state population ev\r## 1 Alabama 4779736 9\r## 2 Alaska 710231 3\r## 3 Arizona 6392017 11\r## 4 Arkansas 2915918 NA\r## 5 California 37253956 55\r## 6 Colorado 5029196 NA\rNote that NAs are added to the two states not appearing in tab_2. Also, notice that this function, as well as all the other joins, can receive the first arguments through the pipe:\ntab_1 %\u0026gt;% left_join(tab_2, by = \u0026quot;state\u0026quot;)\r\rRight join\rIf instead of a table with the same rows as first table, we want one with the same rows as second table, we can use right_join:\ntab_1 %\u0026gt;% right_join(tab_2, by = \u0026quot;state\u0026quot;)\r## state population ev\r## 1 Alabama 4779736 9\r## 2 Alaska 710231 3\r## 3 Arizona 6392017 11\r## 4 California 37253956 55\r## 5 Connecticut NA 7\r## 6 Delaware NA 3\rNow the NAs are in the column coming from tab_1.\n\rInner join\rIf we want to keep only the rows that have information in both tables, we use inner_join. You can think of this as an intersection:\ninner_join(tab_1, tab_2, by = \u0026quot;state\u0026quot;)\r## state population ev\r## 1 Alabama 4779736 9\r## 2 Alaska 710231 3\r## 3 Arizona 6392017 11\r## 4 California 37253956 55\r\rFull join\rIf we want to keep all the rows and fill the missing parts with NAs, we can use full_join. You can think of this as a union:\nfull_join(tab_1, tab_2, by = \u0026quot;state\u0026quot;)\r## state population ev\r## 1 Alabama 4779736 9\r## 2 Alaska 710231 3\r## 3 Arizona 6392017 11\r## 4 Arkansas 2915918 NA\r## 5 California 37253956 55\r## 6 Colorado 5029196 NA\r## 7 Connecticut NA 7\r## 8 Delaware NA 3\r\rSemi join\rThe semi_join function lets us keep the part of first table for which we have information in the second. It does not add the columns of the second:\nsemi_join(tab_1, tab_2, by = \u0026quot;state\u0026quot;)\r## state population\r## 1 Alabama 4779736\r## 2 Alaska 710231\r## 3 Arizona 6392017\r## 4 California 37253956\r\rAnti join\rThe function anti_join is the opposite of semi_join. It keeps the elements of the first table for which there is no information in the second:\nanti_join(tab_1, tab_2, by = \u0026quot;state\u0026quot;)\r## state population\r## 1 Arkansas 2915918\r## 2 Colorado 5029196\rThe following diagram summarizes the above joins:\n(Image courtesy of RStudio3. CC-BY-4.0 license4. Cropped from original.)\n\r\rBinding\rAlthough we have yet to use it in this book, another common way in which datasets are combined is by binding them. Unlike the join function, the binding functions do not try to match by a variable, but instead simply combine datasets. If the datasets don’t match by the appropriate dimensions, one obtains an error.\nBinding columns\rThe dplyr function bind_cols binds two objects by making them columns in a tibble. For example, we quickly want to make a data frame consisting of numbers we can use.\nbind_cols(a = 1:3, b = 4:6)\r## # A tibble: 3 x 2\r## a b\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 4\r## 2 2 5\r## 3 3 6\rThis function requires that we assign names to the columns. Here we chose a and b.\nNote that there is an R-base function cbind with the exact same functionality. An important difference is that cbind can create different types of objects, while bind_cols always produces a data frame.\nbind_cols can also bind two different data frames. For example, here we break up the tab data frame and then bind them back together:\ntab_1 \u0026lt;- tab[, 1:3]\rtab_2 \u0026lt;- tab[, 4:6]\rtab_3 \u0026lt;- tab[, 7:8]\rnew_tab \u0026lt;- bind_cols(tab_1, tab_2, tab_3)\rhead(new_tab)\r## state abb region population total ev clinton trump\r## 1 Alabama AL South 4779736 135 9 34.4 62.1\r## 2 Alaska AK West 710231 19 3 36.6 51.3\r## 3 Arizona AZ West 6392017 232 11 45.1 48.7\r## 4 Arkansas AR South 2915918 93 6 33.7 60.6\r## 5 California CA West 37253956 1257 55 61.7 31.6\r## 6 Colorado CO West 5029196 65 9 48.2 43.3\r\rBinding by rows\rThe bind_rows function is similar to bind_cols, but binds rows instead of columns:\ntab_1 \u0026lt;- tab[1:2,]\rtab_2 \u0026lt;- tab[3:4,]\rbind_rows(tab_1, tab_2)\r## state abb region population total ev clinton trump\r## 1 Alabama AL South 4779736 135 9 34.4 62.1\r## 2 Alaska AK West 710231 19 3 36.6 51.3\r## 3 Arizona AZ West 6392017 232 11 45.1 48.7\r## 4 Arkansas AR South 2915918 93 6 33.7 60.6\rThis is based on an R-base function rbind.\n\r\rSet operators\rAnother set of commands useful for combining datasets are the set operators. When applied to vectors, these behave as their names suggest. Examples are intersect, union, setdiff, and setequal. However, if the tidyverse, or more specifically dplyr, is loaded, these functions can be used on data frames as opposed to just on vectors.\nIntersect\rYou can take intersections of vectors of any type, such as numeric:\nintersect(1:10, 6:15)\r## [1] 6 7 8 9 10\ror characters:\nintersect(c(\u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;), c(\u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;d\u0026quot;))\r## [1] \u0026quot;b\u0026quot; \u0026quot;c\u0026quot;\rThe dplyr package includes an intersect function that can be applied to tables with the same column names. This function returns the rows in common between two tables. To make sure we use the dplyr version of intersect rather than the base package version, we can use dplyr::intersect like this:\ntab_1 \u0026lt;- tab[1:5,]\rtab_2 \u0026lt;- tab[3:7,]\rdplyr::intersect(tab_1, tab_2)\r## state abb region population total ev clinton trump\r## 1 Arizona AZ West 6392017 232 11 45.1 48.7\r## 2 Arkansas AR South 2915918 93 6 33.7 60.6\r## 3 California CA West 37253956 1257 55 61.7 31.6\r\rUnion\rSimilarly union takes the union of vectors. For example:\nunion(1:10, 6:15)\r## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\runion(c(\u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;), c(\u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;d\u0026quot;))\r## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;d\u0026quot;\rThe dplyr package includes a version of union that combines all the rows of two tables with the same column names.\ntab_1 \u0026lt;- tab[1:5,]\rtab_2 \u0026lt;- tab[3:7,]\rdplyr::union(tab_1, tab_2)\r## state abb region population total ev clinton trump\r## 1 Alabama AL South 4779736 135 9 34.4 62.1\r## 2 Alaska AK West 710231 19 3 36.6 51.3\r## 3 Arizona AZ West 6392017 232 11 45.1 48.7\r## 4 Arkansas AR South 2915918 93 6 33.7 60.6\r## 5 California CA West 37253956 1257 55 61.7 31.6\r## 6 Colorado CO West 5029196 65 9 48.2 43.3\r## 7 Connecticut CT Northeast 3574097 97 7 54.6 40.9\r\rsetdiff\rThe set difference between a first and second argument can be obtained with setdiff. Unlike intersect and union, this function is not symmetric:\nsetdiff(1:10, 6:15)\r## [1] 1 2 3 4 5\rsetdiff(6:15, 1:10)\r## [1] 11 12 13 14 15\rAs with the functions shown above, dplyr has a version for data frames:\ntab_1 \u0026lt;- tab[1:5,]\rtab_2 \u0026lt;- tab[3:7,]\rdplyr::setdiff(tab_1, tab_2)\r## state abb region population total ev clinton trump\r## 1 Alabama AL South 4779736 135 9 34.4 62.1\r## 2 Alaska AK West 710231 19 3 36.6 51.3\r\rsetequal\rFinally, the function setequal tells us if two sets are the same, regardless of order. So notice that:\nsetequal(1:5, 1:6)\r## [1] FALSE\rbut:\nsetequal(1:5, 5:1)\r## [1] TRUE\rWhen applied to data frames that are not equal, regardless of order, the dplyr version provides a useful message letting us know how the sets are different:\ndplyr::setequal(tab_1, tab_2)\r## [1] FALSE\rTRY IT\nInstall and load the Lahman library. This database includes data related to baseball teams. It includes summary statistics about how the players performed on offense and defense for several years. It also includes personal information about the players.\r\rThe Batting data frame contains the offensive statistics for all players for many years. You can see, for example, the top 10 hitters by running this code:\nlibrary(Lahman)\rtop \u0026lt;- Batting %\u0026gt;%\rdplyr::filter(yearID == 2016) %\u0026gt;%\rarrange(desc(HR)) %\u0026gt;%\rslice(1:10)\rtop %\u0026gt;% as_tibble()\rBut who are these players? We see an ID, but not the names. The player names are in this table\nMaster %\u0026gt;% as_tibble()\rWe can see column names nameFirst and nameLast. Use the left_join function to create a table of the top home run hitters. The table should have playerID, first name, last name, and number of home runs (HR). Rewrite the object top with this new table.\nNow use the Salaries data frame to add each player’s salary to the table you created in exercise 1. Note that salaries are different every year so make sure to filter for the year 2016, then use right_join. This time show first name, last name, team, HR, and salary.\n\rIn a previous exercise, we created a tidy version of the co2 dataset:\n\r\rco2_wide \u0026lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) %\u0026gt;%\rsetNames(1:12) %\u0026gt;%\rmutate(year = 1959:1997) %\u0026gt;%\rgather(month, co2, -year, convert = TRUE)\rWe want to see if the monthly trend is changing so we are going to remove the year effects and then plot the results. We will first compute the year averages. Use the group_by and summarize to compute the average co2 for each year. Save in an object called yearly_avg.\nNow use the left_join function to add the yearly average to the co2_wide dataset. Then compute the residuals: observed co2 measure - yearly average.\n\rMake a plot of the seasonal trends by year but only after removing the year effect.\n\r\r\r\r\r\rParsing dates and times\rThe date data type\rWe have described three main types of vectors: numeric, character, and logical. In data science projects, we very often encounter variables that are dates. Although we can represent a date with a string, for example November 2, 2017, once we pick a reference day, referred to as the epoch, they can be converted to numbers by calculating the number of days since the epoch. Computer languages usually use January 1, 1970, as the epoch. So, for example, January 2, 2017 is day 1, December 31, 1969 is day -1, and November 2, 2017, is day 17,204.\nNow how should we represent dates and times when analyzing data in R? We could just use days since the epoch, but then it is almost impossible to interpret. If I tell you it’s November 2, 2017, you know what this means immediately. If I tell you it’s day 17,204, you will be quite confused. Similar problems arise with times and even more complications can appear due to time zones.\nFor this reason, R defines a data type just for dates and times. We saw an example in the polls data:\nlibrary(tidyverse)\rlibrary(dslabs)\rdata(\u0026quot;polls_us_election_2016\u0026quot;)\rpolls_us_election_2016$startdate %\u0026gt;% head\r## [1] \u0026quot;2016-11-03\u0026quot; \u0026quot;2016-11-01\u0026quot; \u0026quot;2016-11-02\u0026quot; \u0026quot;2016-11-04\u0026quot; \u0026quot;2016-11-03\u0026quot;\r## [6] \u0026quot;2016-11-03\u0026quot;\rThese look like strings, but they are not:\nclass(polls_us_election_2016$startdate)\r## [1] \u0026quot;Date\u0026quot;\rLook at what happens when we convert them to numbers:\nas.numeric(polls_us_election_2016$startdate) %\u0026gt;% head\r## [1] 17108 17106 17107 17109 17108 17108\rIt turns them into days since the epoch. The as.Date function can convert a character into a date. So to see that the epoch is day 0 we can type\nas.Date(\u0026quot;1970-01-01\u0026quot;) %\u0026gt;% as.numeric\r## [1] 0\rPlotting functions, such as those in ggplot, are aware of the date format. This means that, for example, a scatterplot can use the numeric representation to decide on the position of the point, but include the string in the labels:\npolls_us_election_2016 %\u0026gt;% dplyr::filter(pollster == \u0026quot;Ipsos\u0026quot; \u0026amp; state ==\u0026quot;U.S.\u0026quot;) %\u0026gt;%\rggplot(aes(startdate, rawpoll_trump)) +\rgeom_line()\rNote in particular that the month names are displayed, a very convenient feature.\n\rThe lubridate package\rThe tidyverse includes functionality for dealing with dates through the lubridate package.\nlibrary(lubridate)\rWe will take a random sample of dates to show some of the useful things one can do:\nset.seed(2002)\rdates \u0026lt;- sample(polls_us_election_2016$startdate, 10) %\u0026gt;% sort\rdates\r## [1] \u0026quot;2016-05-31\u0026quot; \u0026quot;2016-08-08\u0026quot; \u0026quot;2016-08-19\u0026quot; \u0026quot;2016-09-22\u0026quot; \u0026quot;2016-09-27\u0026quot;\r## [6] \u0026quot;2016-10-12\u0026quot; \u0026quot;2016-10-24\u0026quot; \u0026quot;2016-10-26\u0026quot; \u0026quot;2016-10-29\u0026quot; \u0026quot;2016-10-30\u0026quot;\rThe functions year, month and day extract those values:\ntibble(date = dates,\rmonth = month(dates),\rday = day(dates),\ryear = year(dates))\r## # A tibble: 10 x 4\r## date month day year\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2016-05-31 5 31 2016\r## 2 2016-08-08 8 8 2016\r## 3 2016-08-19 8 19 2016\r## 4 2016-09-22 9 22 2016\r## 5 2016-09-27 9 27 2016\r## 6 2016-10-12 10 12 2016\r## 7 2016-10-24 10 24 2016\r## 8 2016-10-26 10 26 2016\r## 9 2016-10-29 10 29 2016\r## 10 2016-10-30 10 30 2016\rWe can also extract the month labels:\nmonth(dates, label = TRUE)\r## [1] May Aug Aug Sep Sep Oct Oct Oct Oct Oct\r## 12 Levels: Jan \u0026lt; Feb \u0026lt; Mar \u0026lt; Apr \u0026lt; May \u0026lt; Jun \u0026lt; Jul \u0026lt; Aug \u0026lt; Sep \u0026lt; ... \u0026lt; Dec\rAnother useful set of functions are the parsers that convert strings into dates. The function ymd assumes the dates are in the format YYYY-MM-DD and tries to parse as well as possible.\nx \u0026lt;- c(20090101, \u0026quot;2009-01-02\u0026quot;, \u0026quot;2009 01 03\u0026quot;, \u0026quot;2009-1-4\u0026quot;,\r\u0026quot;2009-1, 5\u0026quot;, \u0026quot;Created on 2009 1 6\u0026quot;, \u0026quot;200901 !!! 07\u0026quot;)\rymd(x)\r## [1] \u0026quot;2009-01-01\u0026quot; \u0026quot;2009-01-02\u0026quot; \u0026quot;2009-01-03\u0026quot; \u0026quot;2009-01-04\u0026quot; \u0026quot;2009-01-05\u0026quot;\r## [6] \u0026quot;2009-01-06\u0026quot; \u0026quot;2009-01-07\u0026quot;\rA further complication comes from the fact that dates often come in different formats in which the order of year, month, and day are different. The preferred format is to show year (with all four digits), month (two digits), and then day, or what is called the ISO 8601. Specifically we use YYYY-MM-DD so that if we order the string, it will be ordered by date. You can see the function ymd returns them in this format.\nBut, what if you encounter dates such as “09/01/02”? This could be September 1, 2002 or January 2, 2009 or January 9, 2002.\rIn these cases, examining the entire vector of dates will help you determine what format it is by process of elimination. Once you know, you can use the many parses provided by lubridate.\nFor example, if the string is:\nx \u0026lt;- \u0026quot;09/01/02\u0026quot;\rThe ymd function assumes the first entry is the year, the second is the month, and the third is the day, so it converts it to:\nymd(x)\r## [1] \u0026quot;2009-01-02\u0026quot;\rThe mdy function assumes the first entry is the month, then the day, then the year:\nmdy(x)\r## [1] \u0026quot;2002-09-01\u0026quot;\rThe lubridate package provides a function for every possibility:\nydm(x)\r## [1] \u0026quot;2009-02-01\u0026quot;\rmyd(x)\r## [1] \u0026quot;2001-09-02\u0026quot;\rdmy(x)\r## [1] \u0026quot;2002-01-09\u0026quot;\rdym(x)\r## [1] \u0026quot;2001-02-09\u0026quot;\rThe lubridate package is also useful for dealing with times. In R base, you can get the current time typing Sys.time(). The lubridate package provides a slightly more advanced function, now, that permits you to define the time zone:\nnow()\r## [1] \u0026quot;2020-12-30 14:04:05 EST\u0026quot;\rnow(\u0026quot;GMT\u0026quot;)\r## [1] \u0026quot;2020-12-30 19:04:05 GMT\u0026quot;\rYou can see all the available time zones with OlsonNames() function.\nWe can also extract hours, minutes, and seconds:\nnow() %\u0026gt;% hour()\r## [1] 14\rnow() %\u0026gt;% minute()\r## [1] 4\rnow() %\u0026gt;% second()\r## [1] 5.452146\rThe package also includes a function to parse strings into times as well as parsers for time objects that include dates:\nx \u0026lt;- c(\u0026quot;12:34:56\u0026quot;)\rhms(x)\r## [1] \u0026quot;12H 34M 56S\u0026quot;\rx \u0026lt;- \u0026quot;Nov/2/2012 12:34:56\u0026quot;\rmdy_hms(x)\r## [1] \u0026quot;2012-11-02 12:34:56 UTC\u0026quot;\rThis package has many other useful functions. We describe two of these here that we find particularly useful.\nThe make_date function can be used to quickly create a date object. It takes three arguments: year, month, day, hour, minute, seconds, and time zone defaulting to the epoch values on UTC time. So create an date object representing, for example, July 6, 2019 we write:\nmake_date(2019, 7, 6)\r## [1] \u0026quot;2019-07-06\u0026quot;\rTo make a vector of January 1 for the 80s we write:\nmake_date(1980:1989)\r## [1] \u0026quot;1980-01-01\u0026quot; \u0026quot;1981-01-01\u0026quot; \u0026quot;1982-01-01\u0026quot; \u0026quot;1983-01-01\u0026quot; \u0026quot;1984-01-01\u0026quot;\r## [6] \u0026quot;1985-01-01\u0026quot; \u0026quot;1986-01-01\u0026quot; \u0026quot;1987-01-01\u0026quot; \u0026quot;1988-01-01\u0026quot; \u0026quot;1989-01-01\u0026quot;\rAnother very useful function is the round_date. It can be used to round dates to nearest year, quarter, month, week, day, hour, minutes, or seconds. So if we want to group all the polls by week of the year we can do the following:\npolls_us_election_2016 %\u0026gt;%\rmutate(week = round_date(startdate, \u0026quot;week\u0026quot;)) %\u0026gt;%\rgroup_by(week) %\u0026gt;%\rsummarize(margin = mean(rawpoll_clinton - rawpoll_trump)) %\u0026gt;%\rqplot(week, margin, data = .)\r## `summarise()` ungrouping output (override with `.groups` argument)\r\r\r\rhttps://github.com/rstudio/cheatsheets↩︎\n\rhttps://github.com/rstudio/cheatsheets/blob/master/LICENSE↩︎\n\rhttps://github.com/rstudio/cheatsheets↩︎\n\rhttps://github.com/rstudio/cheatsheets/blob/master/LICENSE↩︎\n\r\r\r","date":1605744000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610286616,"objectID":"d9c34011e152a6099f13882fb144b965","permalink":"https://datavizm20.classes.andrewheiss.com/content/11-content/","publishdate":"2021-01-15T00:00:00Z","relpermalink":"/content/11-content/","section":"content","summary":"Required Reading\rGuiding Questions\r\rIntroduction to data wrangling\rReshaping data\rgather\rspread\rseparate\runite\r\rJoining tables\rJoins\rLeft join\rRight join\rInner join\rFull join\rSemi join\rAnti join\r\rBinding\rBinding columns\rBinding by rows\r\rSet operators\rIntersect\rUnion\rsetdiff\rsetequal\r\r\rParsing dates and times\rThe date data type\rThe lubridate package\r\r\r\rRequired Reading\r\rThis page.\r\rGuiding Questions\r\rHow can we reshape data into a useable tidy form?","tags":null,"title":"Data Wrangling","type":"docs"},{"authors":null,"categories":null,"content":"\r\r\rPart 1: Hypotheses\rPast 2: Instructions\r\rEvaluation\r\rData cleaning code\rData to possibly use in your plot\r\rCountry totals over time\rCumulative country totals over time\rContinent totals over time\rCumulative continent totals over time\r\r\r\rEach member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.\n\rThe United States has resettled more than 600,000 refugees from 60 different countries since 2006.1\nIn this project, you will use R, ggplot and some form of graphics editor to explore where these refugees have come from.\nPart 1: Hypotheses\rFor this part of the assignment, you need to provide five hypotheses about the relationship between variables in a dataset. You can (and should) consider making hypotheses about the dataset that you plan to use for your final project. However, this is not a requirement. All that is required is that you provide five hypotheses about some data. Your write-up should have an enumerated list of questions (e.g., “1. Are there more murders in states that have high unemployment.”). You will receive 2 points for each hypothesis.\n\rPast 2: Instructions\rHere’s what you need to do:\n\rDownload the Department of Homeland Security’s annual count of people granted refugee status between 2006-2015:\n DHS refugees, 2006-2015\nSave this somewhere on your computer (you might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text). This data was originally uploaded by the Department of Homeland Security to Kaggle, and is provided with a public domain license.\n\rClean the data using the code we’ve given you below. As always, this code is presented without guarantee. You may need to deal with a few issues, depending on your computer’s setup.\n\rSummarize the data somehow. There is data for 60 countries over 10 years, so you’ll probably need to aggregate or reshape the data somehow (unless you do a 60-country sparkline). I’ve included some examples down below.\n\rCreate an appropriate time-based visualization based on the data. I’ve shown a few different ways to summarize the data so that it’s plottable down below. Don’t just calculate overall averages or totals per country—the visualization needs to deal with change over time. Do as much polishing and refining in R—make adjustments to the colors, scales, labels, grid lines, and even fonts, etc.\n\rRefine and polish the saved image, adding annotations, changing colors, and otherwise enhancing it.\n\rDesign and write a poster (no word limit). Your poster should look like a polished image that you might see in a newspaper. You can (and should consider) integrating other images like national flags or arrows to convey some semantic meaning.\n\rUpload the following outputs to D2L:\n\rYour code (.Rmd) that generates the unpolished graphic.\rYour final poster, saved as a PDF.\r\r\rFor this assignment, we are less concerned with the code (that’s why we gave most of it to you), and more concerned with the design. Choose good colors based on palettes. Choose good, clean fonts. Use the heck out of theme(). Add informative design elements. Make it look beautiful. Refer to the design resources here.\nPlease seek out help when you need it! You know enough R (and have enough examples of code from class and your readings) to be able to do this. You can do this, and you’ll feel like a budding dataviz witch/wizard when you’re done.\nEvaluation\rI will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:\nPart 1\nEach hypothesis is worth 2 points. (This is intended to be some free points for all; 10 points)\nPart 2\nTechnical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (15 points)\n\rProfessionalism of visuals: Does the visualizations look like something you might see on TV or in the newspaper? (15 points)\n\rPoster clarity: Does your poster clearly convey some point? (10 points)\n\r\r\r\rData cleaning code\rThe data isn’t perfectly clean and tidy, but it’s real world data, so this is normal. Because the emphasis for this assignment is on design, not code, we’ve provided code to help you clean up the data.\nThese are the main issues with the data:\n\rThere are non-numeric values in the data, like -, X, and D. The data isn’t very well documented; we’re assuming - indicates a missing value, but we’re not sure what X and D mean, so for this assignment, we’ll just assume they’re also missing.\n\rThe data generally includes rows for dozens of countries, but there are also rows for some continents, “unknown,” “other,” and a total row. Because Africa is not a country, and neither are the other continents, we want to exclude all non-countries.\n\rMaintaining consistent country names across different datasets is literally the woooooooorst. Countries have different formal official names and datasets are never consistent in how they use those names.2 It’s such a tricky problem that social scientists have spent their careers just figuring out how to properly name and code countries. Really.3 There are international standards for country codes, though, like ISO 3166-1 alpha 3 (my favorite), also known as ISO3. It’s not perfect—it omits microstates (some Polynesian countries) and gray area states (Palestine, Kosovo)—but it’s an international standard, so it has that going for it.\n\rTo ensure that country names are consistent in this data, we use the countrycode package (install it if you don’t have it), which is amazing. The countrycode() function will take a country name in a given coding scheme and convert it to a different coding scheme using this syntax:\n countrycode(variable, \u0026quot;current-coding-scheme\u0026quot;, \u0026quot;new-coding-scheme\u0026quot;)\rIt also does a farily good job at guessing and parsing inconsistent country names (e.g. it will recognize “Congo, Democratic Republic”, even though it should technically be “Democratic Republic of the Congo”). Here, we use countrycode() to convert the inconsistent country names into ISO3 codes. We then create a cleaner version of the origin_country column by converting the ISO3 codes back into country names. Note that the function chokes on North Korea initially, since it’s included as “Korea, North”—we use the custom_match argument to help the function out.\n\rThe data isn’t tidy—there are individual columns for each year. gather() takes every column and changes it to a row. We exclude the country, region, continent, and ISO3 code from the change-into-rows transformation with -origin_country, -iso3, -origin_region, -origin_continent.\n\rCurrently, the year is being treated as a number, but it’s helpful to also treat it as an actual date. We create a new variable named year_date that converts the raw number (e.g. 2009) into a date. The date needs to have at least a month, day, and year, so we actually convert it to January 1, 2009 with ymd(paste0(year, \"-01-01\")).\n\r\rlibrary(tidyverse) # For ggplot, dplyr, and friends\rlibrary(countrycode) # For dealing with country names, abbreviations, and codes\rlibrary(lubridate) # For dealing with dates\rrefugees_raw \u0026lt;- read_csv(\u0026quot;data/refugee_status.csv\u0026quot;, na = c(\u0026quot;-\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;D\u0026quot;))\rnon_countries \u0026lt;- c(\u0026quot;Africa\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Europe\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;Oceania\u0026quot;,\r\u0026quot;South America\u0026quot;, \u0026quot;Unknown\u0026quot;, \u0026quot;Other\u0026quot;, \u0026quot;Total\u0026quot;)\rrefugees_clean \u0026lt;- refugees_raw %\u0026gt;%\r# Make this column name easier to work with\rrename(origin_country = `Continent/Country of Nationality`) %\u0026gt;%\r# Get rid of non-countries\rfilter(!(origin_country %in% non_countries)) %\u0026gt;%\r# Convert country names to ISO3 codes\rmutate(iso3 = countrycode(origin_country, \u0026quot;country.name\u0026quot;, \u0026quot;iso3c\u0026quot;,\rcustom_match = c(\u0026quot;Korea, North\u0026quot; = \u0026quot;PRK\u0026quot;))) %\u0026gt;%\r# Convert ISO3 codes to country names, regions, and continents\rmutate(origin_country = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;country.name\u0026quot;),\rorigin_region = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;region\u0026quot;),\rorigin_continent = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;continent\u0026quot;)) %\u0026gt;%\r# Make this data tidy\rgather(year, number, -origin_country, -iso3, -origin_region, -origin_continent) %\u0026gt;%\r# Make sure the year column is numeric + make an actual date column for years\rmutate(year = as.numeric(year),\ryear_date = ymd(paste0(year, \u0026quot;-01-01\u0026quot;)))\r\rData to possibly use in your plot\rHere are some possible summaries of the data you might use…\nCountry totals over time\rThis is just the refugees_clean data frame I gave you. You’ll want to filter it and select specific countries, though—you won’t really be able to plot 60 countries all at once unless you use sparklines.\n## # A tibble: 6 x 7\r## origin_country iso3 origin_region origin_continent year number year_date ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; ## 1 Afghanistan AFG South Asia Asia 2006 651 2006-01-01\r## 2 Angola AGO Sub-Saharan Afr… Africa 2006 13 2006-01-01\r## 3 Armenia ARM Europe \u0026amp; Centra… Asia 2006 87 2006-01-01\r## 4 Azerbaijan AZE Europe \u0026amp; Centra… Asia 2006 77 2006-01-01\r## 5 Belarus BLR Europe \u0026amp; Centra… Europe 2006 350 2006-01-01\r## 6 Bhutan BTN South Asia Asia 2006 3 2006-01-01\r\rCumulative country totals over time\rNote the cumsum() function—it calculates the cumulative sum of a column.\nrefugees_countries_cumulative \u0026lt;- refugees_clean %\u0026gt;%\rarrange(year_date) %\u0026gt;%\rgroup_by(origin_country) %\u0026gt;%\rmutate(cumulative_total = cumsum(number))\r## # A tibble: 6 x 7\r## # Groups: origin_country [1]\r## origin_country iso3 origin_continent year number year_date cumulative_total\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan AFG Asia 2006 651 2006-01-01 651\r## 2 Afghanistan AFG Asia 2007 441 2007-01-01 1092\r## 3 Afghanistan AFG Asia 2008 576 2008-01-01 1668\r## 4 Afghanistan AFG Asia 2009 349 2009-01-01 2017\r## 5 Afghanistan AFG Asia 2010 515 2010-01-01 2532\r## 6 Afghanistan AFG Asia 2011 428 2011-01-01 2960\r\rContinent totals over time\rNote the na.rm = TRUE argument in sum(). This makes R ignore any missing data when calculating the total. Without it, if R finds a missing value in the column, it will mark the final sum as NA too, which we don’t want.\nrefugees_continents \u0026lt;- refugees_clean %\u0026gt;%\rgroup_by(origin_continent, year_date) %\u0026gt;%\rsummarize(total = sum(number, na.rm = TRUE))\r## `summarise()` regrouping output by \u0026#39;origin_continent\u0026#39; (override with `.groups` argument)\r## # A tibble: 6 x 3\r## # Groups: origin_continent [1]\r## origin_continent year_date total\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 2006-01-01 18116\r## 2 Africa 2007-01-01 17473\r## 3 Africa 2008-01-01 8931\r## 4 Africa 2009-01-01 9664\r## 5 Africa 2010-01-01 13303\r## 6 Africa 2011-01-01 7677\r\rCumulative continent totals over time\rNote that there are two group_by() functions here. First we get the total number of refugees per continent per year, then we group by continent only to get the cumulative sum of refugees across continents.\nrefugees_continents_cumulative \u0026lt;- refugees_clean %\u0026gt;%\rgroup_by(origin_continent, year_date) %\u0026gt;%\rsummarize(total = sum(number, na.rm = TRUE)) %\u0026gt;%\rarrange(year_date) %\u0026gt;%\rgroup_by(origin_continent) %\u0026gt;%\rmutate(cumulative_total = cumsum(total))\r## `summarise()` regrouping output by \u0026#39;origin_continent\u0026#39; (override with `.groups` argument)\r## # A tibble: 6 x 4\r## # Groups: origin_continent [1]\r## origin_continent year_date total cumulative_total\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Africa 2006-01-01 18116 18116\r## 2 Africa 2007-01-01 17473 35589\r## 3 Africa 2008-01-01 8931 44520\r## 4 Africa 2009-01-01 9664 54184\r## 5 Africa 2010-01-01 13303 67487\r## 6 Africa 2011-01-01 7677 75164\r\r\r\rYou found the footnote! Week 8’s writing assignment has three parts. First, write down your guess (a probability; stated as a percentage) for the odds that Joe Biden is elected President of the United States. Second, write a short paragraph about why you made that guess. Are you integrating data into your guess? If so, what data? Do you believe that personal experience is useful for making such predictions, or that personal experience can be misleading. As always, the total writing should be a couple hundred words.↩︎\n\rFor instance, “North Korea”, “Korea, North”, “DPRK”, “Korea, Democratic People’s Republic of”, and “Democratic People’s Republic of Korea”, and “Korea (DPRK)” are all perfectly normal versions of the country’s name and you’ll find them all in the wild.↩︎\n\rSee Gleditsch, Kristian S. \u0026amp; Michael D. Ward. 1999. “Interstate System Membership: A Revised List of the Independent States since 1816.” International Interactions 25: 393-413; or the “ICOW Historical State Names Data Set”.↩︎\n\r\r\r","date":1605225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605796661,"objectID":"5b66ec5f9db457ac4a522cdf36fd69ce","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/project2/","publishdate":"2020-11-13T00:00:00Z","relpermalink":"/assignment/project2/","section":"assignment","summary":"Part 1: Hypotheses\rPast 2: Instructions\r\rEvaluation\r\rData cleaning code\rData to possibly use in your plot\r\rCountry totals over time\rCumulative country totals over time\rContinent totals over time\rCumulative continent totals over time\r\r\r\rEach member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.","tags":null,"title":"Project 2","type":"docs"},{"authors":null,"categories":null,"content":"\r\rPart 1: Rats, rats, rats.\rInstructions\rStarter code\r\rPart 2: Data Hunting\rEvaluations\r\r\rEach member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.\n\rPart 1: Rats, rats, rats.\rNew York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.\n  NYC keeps incredibly detailed data regarding animal sightings, including rats, and it makes this data publicly available.\nFor this first project, you will use R and ggplot2 to tell an interesting story hidden in the data. You must create a story by looking carefully at the data.\nInstructions\rHere’s what you need to do:\nDownload New York City’s database of rat sightings since 2010:\n\r Rat_sightings.csv\r\rSummarize the data somehow. The raw data has more than 100,000 rows, which means you’ll need to aggregate the data (filter(), group_by(), and summarize() will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2016.\n\rCreate an appropriate visualization based on the data you summarized.\n\rWrite a memo explaining your process. We are specifically looking for a discussion of the following:\n\rWhat story are you telling with your new graphic?\rHow have you applied reasonable standards in visual storytelling?\rWhat policy implication is there (if any)?\r\rUpload the following outputs to D2L:\n\rA PDF file of your memo with your final code and graphic embedded in it.1 This means you’ll need to do all your coding in an R Markdown file and embed your code in chunks. Note that Part 2 of this project should be included in this PDF (see below).\rA standalone PDF version of your graphic. Use ggsave(plot_name, filename = \"output/blah.pdf\", width = XX, height = XX)\r\r\r\rStarter code\rI’ve provided some starter code below. A couple comments about it:\n\rBy default, read_csv() treats cells that are empty or “NA” as missing values. This rat dataset uses “N/A” to mark missing values, so we need to add that as a possible marker of missingness (hence na = c(\"\", \"NA\", \"N/A\"))\rTo make life easier, I’ve renamed some of the key variables you might work with. You can rename others if you want.\rI’ve also created a few date-related variables (sighting_year, sighting_month, sighting_day, and sighting_weekday). You don’t have to use them, but they’re there if you need them. The functions that create these, like year() and wday() are part of the lubridate library.\rThe date/time variables are formatted like 04/03/2017 12:00:00 AM, which R is not able to automatically parse as a date when reading the CSV file. You can use the mdy_hms() function in the lubridate library to parse dates that are structured as “month-day-year-hour-minute”. There are also a bunch of other iterations of this function, like ymd(), dmy(), etc., for other date formats.\rThere’s one row with an unspecified borough, so I filter that out.\r\rlibrary(tidyverse)\rlibrary(lubridate)\rrats_raw \u0026lt;- read_csv(\u0026quot;data/Rat_Sightings.csv\u0026quot;, na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;N/A\u0026quot;))\r# If you get an error that says \u0026quot;All formats failed to parse. No formats\r# found\u0026quot;, it\u0026#39;s because the mdy_hms function couldn\u0026#39;t parse the date. The date\r# variable *should* be in this format: \u0026quot;04/03/2017 12:00:00 AM\u0026quot;, but in some\r# rare instances, it might load without the seconds as \u0026quot;04/03/2017 12:00 AM\u0026quot;.\r# If there are no seconds, use mdy_hm() instead of mdy_hms().\rrats_clean \u0026lt;- rats_raw %\u0026gt;%\rrename(created_date = `Created Date`,\rlocation_type = `Location Type`,\rborough = Borough) %\u0026gt;%\rmutate(created_date = mdy_hms(created_date)) %\u0026gt;%\rmutate(sighting_year = year(created_date),\rsighting_month = month(created_date),\rsighting_day = day(created_date),\rsighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %\u0026gt;%\rfilter(borough != \u0026quot;Unspecified\u0026quot;)\rYou’ll summarize the data with functions from dplyr, including stuff like count(), arrange(), filter(), group_by(), summarize(), and mutate(). Here are some examples of ways to summarize the data:\n# See the count of rat sightings by weekday\rrats_clean %\u0026gt;%\rcount(sighting_weekday)\r# Assign a summarized data frame to an object to use it in a plot\rrats_by_weekday \u0026lt;- rats_clean %\u0026gt;%\rcount(sighting_weekday, sighting_year)\rggplot(rats_by_weekday, aes(x = fct_rev(sighting_weekday), y = n)) +\rgeom_col() +\rcoord_flip() +\rfacet_wrap(~ sighting_year)\r# See the count of rat sightings by weekday and borough\rrats_clean %\u0026gt;%\rcount(sighting_weekday, borough, sighting_year)\r# An alternative to count() is to specify the groups with group_by() and then\r# be explicit about how you\u0026#39;re summarizing the groups, such as calculating the\r# mean, standard deviation, or number of observations (we do that here with\r# `n()`).\rrats_clean %\u0026gt;%\rgroup_by(sighting_weekday, borough) %\u0026gt;%\rsummarize(n = n())\r\r\rPart 2: Data Hunting\rFor the second part of the project, your task is simple. Your group must identify three different data sources2 for potential use in your final project. You are not bound to this decision.\nFor each, you must write a single paragraph about what about this data interests you. Add this to the memo from Part 1.\n\rEvaluations\rI will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:\nPart 1\nTechnical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (10 points)\n\rAppropriateness of visuals: Do the visualizations tell a clear story? Have we learned something? (10 points)\n\rStorytelling: Does your memo clearly convey what you’re doing and why? (9 points)\n\r\rPart 2\nEach piece of data (and description) is worth 7 points. (21 points total)\n\r\rYou can approach this in a couple different ways—you can write the memo and then include the full figure and code at the end, similar to this blog post, or you can write the memo in an incremental way, describing the different steps of creating the figure, ultimately arriving at a clean final figure, like this blog post.↩︎\n\rThe three different sources need not be different websites or from different organizations. For example, three different tables from the US Census would be sufficient↩︎\n\r\r\r","date":1602806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604325415,"objectID":"1ee2ebbd4938399d5199cb912763ad52","permalink":"https://datavizm20.classes.andrewheiss.com/assignment/project1/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/assignment/project1/","section":"assignment","summary":"Part 1: Rats, rats, rats.\rInstructions\rStarter code\r\rPart 2: Data Hunting\rEvaluations\r\r\rEach member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.\n\rPart 1: Rats, rats, rats.\rNew York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots.","tags":null,"title":"Project 1","type":"docs"},{"authors":null,"categories":null,"content":"\rBelow is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon. Accordingly, you should visit this page frequently throughout the term.\nAs mentioned in the syllabus, the course is structured by topics; each week introduces a new topic. Moreover, every week is divided into three important sections that you should engage with.\nOverview\rThe class is structured with three distinct bits. First, the Tuesday lecture will give an overview of the topic for the week. Next, the Thursday lecture will have a short, practical lecture and an activity which is designed to give you hands-on experience and a greater understanding of the broader material. Finally, you will complete weekly writings (short) and labs (also short; requiring coding in R). Out of class, you will complete readings and can watch (short) recorded lectures on the week’s topic. Note, though, that you are not required to view supplemental recorded videos.\n\rContent (): This page contains the readings and recorded lectures for the topic. These pages should be read completely. Lectures are not an exact replication of the written content; on the contrary, the lectures are intended to keep you focused on the high-level ideas, while the readings are broader and more comprehensive. Accordingly, lectures are shorter than the (often quite lengthy) written content.\n\rExamples (): This page the material that we will discuss in Thursday classes. In addition to teaching specific content, there are many more R code examples. These are intended as a useful reference to various functions that you will need when working on (nearly) weekly labs and your group project.\n\rAssignments (): This page contains the instructions for the weekly lab (1–3 brief tasks) and for the two mini projects + final project. Labs are due by 11:59 PM (Eastern) on the Monday after they’re posted.\n\r\r\rLab Hours (TA): Fridays 9:30 - 10:50 AM via Zoom\rThe teaching assistant for this course (Anh Do; doanh@msu.edu) will host a (very short) supplemental lecture each week to help promote additional understanding. This will be followed by (or preceeded by) open Q\u0026amp;A about the week’s content. I highly encourage you to utilize this resource, especially if you struggle with basic R programming. Passcode: 242626.\n\rLink To TA Office Hours\rtl;dr: You should follow this general process (in order) each week:\n\rDo everything on the content () page before Tuesday\rCome to the lecture on Tuesday.\rWhile “in class” on Thursday, work through the example () page\rComplete the lab () and the weekly writing (assigned in class) before the next Tuesday.\rAs needed, attend the lab hours hosted by the TA.\r\r\r\r\r\rProgramming Foundations\rContent\rExample\rAssignment\r\r\rWeek 0\r(Re-) Introduction to R\r\r\r\r\r\rWeek 1\rProgramming Basics, the tidyverse, and Visualization\r\r\r\r\r\rWeek 2\rVisualization II\r\r\r\r\r\r\r\r\rWeek 3\rVisualization III\r\r\r\r\r\r\r\r\r\rData Analysis Foundations\rContent\rExample\rAssignment\r\r\rWeek 4\rProbability and Statistics in R\r\r\r\r\r\r\r\r\rWeek 5\rLinear Regression I\r\r\r\r\r\r\r\r\rWeek 6\rLinear Regression II\r\r\r\r\r\r\r\r\rEnd of Week 6\r Project 1 Due\r\r\r\r\r\r\r\rWeek 7\rLinear Regression III\r\r\r\r\r\r\r\r\r\rApplications of Data Analysis\rContent\rExample\rAssignment\r\r\rWeek 8\rNonlinear Regression\r\r\r\r\r\r\r\r\rWeek 9\rBias vs Variance\r\r\r\r\r\r\r\r\rWeek 10\rClassification\r\r\r\r\r\r\r\r\rEnd of Week 10\r Project 2 Due\r\r\r\r\r\r\r\r\rWeek 11\rWrangling Data\r\r\r\r\r\r\r\r\r\rFurther Extensions\rContent\rExample\rAssignment\r\r\rWeek 12\rText as Data\r\r\r\r\r\r\r\r\r\rConclusions\rContent\rExample\rAssignment\r\r\rFinals Week\rConcluding Thoughts\r\r\r\r\r\r\r\r\rDate TBA; around end of finals week\r Final Project Due\r\r\r\r\r\r\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600098724,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"https://datavizm20.classes.andrewheiss.com/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"Below is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon. Accordingly, you should visit this page frequently throughout the term.\nAs mentioned in the syllabus, the course is structured by topics; each week introduces a new topic. Moreover, every week is divided into three important sections that you should engage with.","tags":null,"title":"Schedule","type":"page"}]