[{"authors":["andrew"],"categories":null,"content":"Andrew Heiss is an assistant professor at the Andrew Young School of Policy Studies at Georgia State University, researching international NGOs and teaching data science, program evaluation, and economics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cbc5591315bd1828d0bcb005e8207892","permalink":"/authors/ben/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ben/","section":"authors","summary":"Andrew Heiss is an assistant professor at the Andrew Young School of Policy Studies at Georgia State University, researching international NGOs and teaching data science, program evaluation, and economics.","tags":null,"title":"Andrew Heiss","type":"authors"},{"authors":["Ben"],"categories":null,"content":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron. Prior to coming to Michigan State University, I worked with the U.S. Army to help soldiers become more psychologically resilient.\nI hold a Ph.D. in Social Science (Economics) from the California Institute of Technology (Caltech), and a B.S. in Economics from the University of Oregon.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592849563,"objectID":"bf008f22d9b0754cde4f6972811c28b7","permalink":"/authors/ben/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ben/","section":"authors","summary":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron.","tags":null,"title":"Ben Bushong","type":"authors"},{"authors":null,"categories":null,"content":"  I have included a bunch of extra resources and guides related to graphic design, visualization, R, data, and other relevant topics. Enjoy!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592849563,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"I have included a bunch of extra resources and guides related to graphic design, visualization, R, data, and other relevant topics. Enjoy!","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":" Each week has a set of required readings that you should complete before watching the lecture. If you are attending in-person on Tuesdays, you should complete the reading, attend class, then watch the lecture and do the associated content on Thursday. If you are attending on Thursdays, you should read first, watch the lecture, then attend the lecture. Both groups will be working on the last part of the labs between Thursday afternoon and Monday at 11:59 PM (when the labs are due).\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts. These questions can serve as helpful starting places for your You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read.\nEvery weekly session also has a YouTube playlist of short recorded videos associated with each of the lecture sections. The lecture slides are HTML files made with the R package xaringan. For each of the weekly pages, you’ll buttons for opening the presentation in a new tab.1.\n View all slides in new window\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).\n I aspire to include a link for downloading a PDF of the slides in case you want to print them or store them on your computer. However… this seems ambitious.↩︎\n   ","date":1598918400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592869554,"objectID":"1413f960ec974b9863bc45d887efa8bd","permalink":"/content/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Each week has a set of required readings that you should complete before watching the lecture. If you are attending in-person on Tuesdays, you should complete the reading, attend class, then watch the lecture and do the associated content on Thursday. If you are attending on Thursdays, you should read first, watch the lecture, then attend the lecture. Both groups will be working on the last part of the labs between Thursday afternoon and Monday at 11:59 PM (when the labs are due).","tags":null,"title":"Readings, lectures, and videos","type":"docs"},{"authors":null,"categories":null,"content":"  This section contains fully annotated R code that you can use as a reference for creating your own visualizations. In the lessons section, you sequentially build up your understanding of R and ggplot2; here you can see how all the pieces work together.\nVisit this section after you have finished the readings, lecture videos, and lesson. The examples here will be indispensable for you as you work on your assignments and mini projects.\nEach section also contains videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592849563,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"This section contains fully annotated R code that you can use as a reference for creating your own visualizations. In the lessons section, you sequentially build up your understanding of R and ggplot2; here you can see how all the pieces work together.\nVisit this section after you have finished the readings, lecture videos, and lesson. The examples here will be indispensable for you as you work on your assignments and mini projects.","tags":null,"title":"Code examples","type":"docs"},{"authors":null,"categories":null,"content":"  In-Class Warm-Ups Labs Projects Final project   This course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nEngage with the readings and lecture materials Regularly use R (aka engage daily or almost every day in some way)  Each type of assignment in this class helps with one of these strategies.\nIn-Class Warm-Ups To encourage you to actively engage with the course content, you will write a ≈150 word reflection about the reading or lecture each week. That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced). You must complete a total of ten of these; there are more than ten weeks in the course, so you have some flexibility.1 Your actual prompt will be assigned in class, so you must attend on your assigned day.\nYou can do a lot of different things with this memo: discuss something you learned from the course content, write about the best or worst data visualization you saw recently, connect the course content to your own work, etc. These reflections let you explore and answer some of the key questions of this course, including:\n When is a link correlational vs causal? How can we still make useful statements about non-causal things? Why do we visualize data? What makes a great data analysis? What makes a bad analysis? How do you choose which kind of analysis method to use? What is the role of the data structure in choosing an analysis? Can we be flexible?  The course content for each day will also include a set of questions specific to that topic. You do not have to answer all (or any) of these questions. That would be impossible. They exist to guide your thinking and to make complex reading more digestible. The specific topic for each week will be assigned in class. (I can’t emphasize this enough.)\nThe TA will grade these mini-exercises using a very simple system:\n ✔+: (11.5 points (115%) in gradebook) Work shows phenomenal thought and engagement with the course content. We will not assign these often. ✔: (10 points (100%) in gradebook) Work is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Work is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.  (There is an implicit 0 above for work that is not turned in on-time). Notice that this is essentially a pass/fail or completion-based system. We’re not grading your writing ability; we’re not counting the exact number of words you’re writing; and we’re not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. We are looking for thoughtful engagement. Read the material, engage with the work and you’ll get a ✓.\nYou will turn these reflections in via D2L. You will write them using R Markdown and must knit your work to a PDF document (this will be what you turn in).\n Labs Each week of the course has fully annotated examples of code that teach and demonstrate how to do specific tasks in R. However, without practicing these principles and making graphics on your own, you won’t remember what you learn.\n Practice, uh, makes, whatever.\nBen Bushong\n For example, to practice working with ggplot2 and making data-based graphics, you will complete a brief set of exercises over a few class sessions. These exercises will have 1–3 short tasks that are directly related to the topic for the week. You need to show that you made a good faith effort to work each question. There will also be a final question which requires significantly more thought and work. This will be where you get to show some creativity and stretch your abilities. Overall, labs will be graded the same check system:\n ✔+: (11.5 points (115%) in gradebook) Exercises are complete. Every task was attempted and answered, and most answers are correct. Knitted document is clean and easy to follow. Work on the final problem shows creativity or is otherwise exceptional. We will not assign these often. ✔: (10 points (100%) in gradebook) Exercises are complete and most answers are correct. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. We will hopefully not assign these often, but subpar work can expect a ✔−.  Note that this is also essentially a pass/fail system. As noted in the syllabus, we are not grading your coding ability. We are not checking each line of code to make sure it produces some exact final figure, and we do not expect perfection. Also note that a ✓ does not require 100% success. You will sometimes get stuck with weird errors that you can’t solve, or the demands of pandemic living might occasionally become overwhelming. We are looking for good faith effort. Try hard, engage with the task, and you’ll get a ✓.\nYou may work together on the labs, but you must turn in your own answers. You will turn these labs in via D2L. You will write them using R Markdown and must knit your work to a PDF document.\n Projects To give you practice with the data and design principles you’ll learn in this class, you will complete two projects en route to the overarching final project of the course. Both these mini projects and the final project must be completed in groups.\nThe two (mini) projects are checkpoints to ensure you’re working on your project seriously. They will be graded using a check system:\n ✔+: (55 points (≈115%) in gradebook) Project is phenomenally well-designed and uses advanced R techniques. The project uncovers an important story that is not readily apparent from just looking at the raw data. I will not assign these often. ✔: (50 points (100%) in gradebook) Project is fine, follows most design principles, answers a question from the data, and uses R correctly. This is the expected level of performance. ✔−: (25 points (50%) in gradebook) Project is missing large components, is poorly designed, does not answer a relevant question, and/or uses R incorrectly. This indicates that you need to improve next time. I will hopefully not assign these often.  Because these mini projects give you practice for the final project, we will provide you with substantial feedback on your design and code.\n Final project At the end of the course, you will demonstrate your skills by completing a final project. Complete details for the final project (including past examples of excellent projects) are here. In brief, the final project has the following elements:\nYou must find existing data to analyze.2 Aggregating data from multiple sources is encouraged, but is not required.  You must visualize (at least) three interesting features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.3  You must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.4  You must present your analysis as if presenting to a C-suite executive. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.5  Concretely, this requires one of the two following options: At least one member of the group presents from a slide deck (preferably as a PDF; alternatively as either a Powerpoint or Presentation file). If your group chooses this option, presentations will be 10 minutes in duration (preferrably, within about 30 seconds on either side of that time). This is hard: you will need to be able to quickly and clearly present a few results.   Whether or not a given person presents (or does not present) will not affect their grade if this alternative is chosen. Accordingly, groups should divide their work to capitalize on the skills of the members.  The group writes a memo—approximately 5 pages—which describes their data, analyses, and results. This must be clear and easy to understand for a non-expert in your field. Figures and tables do not apply to the page limit.    There is no final exam. This project is your final exam.\nThe project will not be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:\nTechnical skills: Was the project easy? Does it showcase mastery of data analysis? Visual design: Was the information smartly conveyed and usable? Was it beautiful? Analytic design: Was the analysis appropriate? Was it sensible, given the dataset? Story: Did we learn something?  If you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.\n  Note that sometimes the memo takes the form of a very short coding assignment.↩︎\n Note that existing is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged.↩︎\n Pie charts of any kind will result in a 25% grade deduction.↩︎\n This is an extremely dumb idea for a number of reasons. Moreover, it’s worth mentioning that sports data, while rich, can be overwhelming due to its sheer magnitude and the variety of approaches that can be applied. Use with caution.↩︎\n This exercise provides you with an opportunity to identify your marketable skills and to practice them. I encourage those who will be looking for jobs soon to take this exercise seriously.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592869554,"objectID":"f2303119cf11053d8da6abe3b0cd9610","permalink":"/lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/lab/","section":"lab","summary":"In-Class Warm-Ups Labs Projects Final project   This course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nEngage with the readings and lecture materials Regularly use R (aka engage daily or almost every day in some way)  Each type of assignment in this class helps with one of these strategies.","tags":null,"title":"Labs and Evaluations","type":"docs"},{"authors":null,"categories":null,"content":"  Task 1: Make an RStudio Project Task 2: Make an R Markdown file with a plot in it   Task 1: Make an RStudio Project Use either RStudio.cloud or RStudio on your computer (preferably RStudio on your computer! Follow these instructions to get started!) to create a new RStudio Project.\n Create a folder named “data” in the project folder you just made.\n Download this CSV file and place it in that folder:\n  cars.csv  In RStudio, go to “File” \u0026gt; “New File…” \u0026gt; “R Markdown…” and click “OK” in the dialog without changing anything.\n Delete all the placeholder text in that new file and replace it with this:\n--- title: \u0026quot;Exercise 1\u0026quot; author: \u0026quot;Put your name here\u0026quot; output: html_document --- # Reflection Replace this text with your reflection # My first plot ```{r load-libraries-data, warning=FALSE, message=FALSE} library(tidyverse) cars \u0026lt;- read_csv(\u0026quot;data/cars.csv\u0026quot;) ``` Replace this line with a code chunk and use it to create a plot.  Save the R Markdown file with some sort of name (without any spaces!)\n Your project folder should look something like this:\n   Task 2: Make an R Markdown file with a plot in it Add your reading reflection to the appropriate place in the R Markdown file. You can type directly in RStudio if you want (though there’s no spell checker), or you can type it in Word or Google Docs and then paste it into RStudio.\n Remove the text that says “Replace this line with a code chunk” and insert a new R code chunk. Either type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS, or use the “Insert Chunk” menu:\n Use ggplot() to create a scatterplot using the mpg dataset. Use whatever variables you want. Type the code to create the plot in the new empty chunk.\n Knit your document as a Word file (or PDF if you’re brave and installed LaTeX). Use the “Knit” menu:\n Upload the knitted document to iCollege.\n 🎉 Party! 🎉\n  You’ll be doing this same process for all your future exercises. Each exercise will involve an R Markdown file. You can either create a new RStudio Project directory for all your work:\nOr you can create individual projects for each assignment and mini-project:\n  ","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"5d0daa128f2d0803c0e0d1a44e04bf6a","permalink":"/lab/01-exercise/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/lab/01-exercise/","section":"lab","summary":"Task 1: Make an RStudio Project Task 2: Make an R Markdown file with a plot in it   Task 1: Make an RStudio Project Use either RStudio.cloud or RStudio on your computer (preferably RStudio on your computer! Follow these instructions to get started!) to create a new RStudio Project.\n Create a folder named “data” in the project folder you just made.\n Download this CSV file and place it in that folder:","tags":null,"title":"Introduction to R and the tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"  Instructions Data cleaning code Data to possibly use in your plot Country totals over time Cumulative country totals over time Continent totals over time Cumulative continent totals over time  Visualization ideas   The United States has resettled more than 600,000 refugees from 60 different countries since 2006.\nIn this mini project, you will use R, ggplot, and Illustrator, Inkscape, or Gravit Designer to explore where these refugees have come from.\nInstructions Here’s what you need to do:\n Create a new RStudio project and place it on your computer somewhere. Open that new folder in Windows File Explorer or macOS Finder (however you navigate around the files on your computer), and create two subfolders there named data and output.\n Download the Department of Homeland Security’s annual count of people granted refugee status between 2006-2015:\n DHS refugees, 2006-2015\nPlace this in the data subfolder you created in step 1. You might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text. This data was originally uploaded by the Department of Homeland Security to Kaggle, and is provided with a public domain license.\n Create a new R Markdown file and save it in your project. In RStudio go to File \u0026gt; New File \u0026gt; R Markdown…, choose the default options, and delete all the placeholder text in the new file except for the metadata at the top, which is between --- and ---.\n Verify that your project folder is structured like this:\nyour-project-name/ your-analysis.Rmd your-project-name.Rproj data/ refugee_status.csv output/ NOTHING Clean the data using the code I’ve given you below.\n Summarize the data somehow. There is data for 60 countries over 10 years, so you’ll probably need to aggregate or reshape the data somehow (unless you do a 60-country sparkline). I’ve included some examples down below.\n Create an appropriate time-based visualization based on the data. I’ve shown a few different ways to summarize the data so that it’s plottable down below. Don’t just calculate overall averages or totals per country—the visualization needs to deal with change over time. Do as much polishing and refining in R—make adjustments to the colors, scales, labels, grid lines, and even fonts, etc.\n Save the figure as a PDF. Use ggsave(plot_name, filename = \"output/blah.pdf\", width = XX, height = XX)\n Refine and polish the saved PDF in Illustrator or Inkscape or Gravit Designer, adding annotations, changing colors, and otherwise enhancing it.\n Export the polished image as a PDF and a PNG file.\n Write a memo (no word limit) explaining your process. I’m specifically looking for the following:\n What story are you telling with your graphic? How did you apply the principles of CRAP? How did you apply Kieran Healy’s principles of great visualizations or Alberto Cairo’s five qualities of great visualizations?  Upload the following outputs to iCollege:\n A PDF or Word file of your memo with your final code, intermediate graphic (the one you create in R), and final graphic (the one you enhance) in it. Remember to use ![Caption](path/to/figure/here) to place external images in Markdown. A standalone PNG version of your graphic. You’ll export this from Illustrator or Inkscape. A standalone PDF version of your graphic. You’ll export this from Illustrator or Inkscape.   You will be graded based on completion using the standard ✓ system, but I’ll provide comments on how you use R and ggplot2, how well you apply the principles of CRAP, The Truthful Art, and Effective Data Visualization, and how appropriate the graph is for the data and the story you’re telling. I will use this rubric to make comments and provide you with a simulated grade.\n  mini-project-2-rubric.pdf  For this assignment, I am less concerned with the code (that’s why I gave most of it to you), and more concerned with the design. Choose good colors based on palettes. Choose good, clean fonts. Use the heck out of theme(). Add informative design elements in Illustrator/Inkscape/Gravit Designer. Make it look beautiful and CRAPpy. Refer to the design resources here.\nThe assignment is due by 11:59 PM on Friday, May 29.\nPlease seek out help when you need it! You know enough R (and have enough examples of code from class and your readings) to be able to do this. Your project has to be turned in individually, and your visualization should be your own (i.e. if you work with others, don’t all turn in the same graph), but you should work with others! Reach out to me for help too—I’m here to help!\nYou can do this, and you’ll feel like a budding dataviz witch/wizard when you’re done.\n Data cleaning code The data isn’t perfectly clean and tidy, but it’s real world data, so this is normal. Because the emphasis for this assignment is on design, not code, I’ve provided code to help you clean up the data.\nThese are the main issues with the data:\n There are non-numeric values in the data, like -, X, and D. The data isn’t very well documented; I’m assuming - indicates a missing value, but I’m not sure what X and D mean, so for this assignment, we’ll just assume they’re also missing.\n The data generally includes rows for dozens of countries, but there are also rows for some continents, “unknown,” “other,” and a total row. Because Africa is not a country, and neither are the other continents, we want to exclude all non-countries.\n Maintaining consistent country names across different datasets is literally the woooooooorst. Countries have different formal official names and datasets are never consistent in how they use those names.1 It’s such a tricky problem that social scientists have spent their careers just figuring out how to properly name and code countries. Really.2 There are international standards for country codes, though, like ISO 3166-1 alpha 3 (my favorite), also known as ISO3. It’s not perfect—it omits microstates (some Polynesian countries) and gray area states (Palestine, Kosovo)—but it’s an international standard, so it has that going for it.\n To ensure that country names are consistent in this data, we use the countrycode package (install it if you don’t have it), which is amazing. The countrycode() function will take a country name in a given coding scheme and convert it to a different coding scheme using this syntax:\n countrycode(variable, \u0026quot;current-coding-scheme\u0026quot;, \u0026quot;new-coding-scheme\u0026quot;) It also does a farily good job at guessing and parsing inconsistent country names (e.g. it will recognize “Congo, Democratic Republic”, even though it should technically be “Democratic Republic of the Congo”). Here, we use countrycode() to convert the inconsistent country names into ISO3 codes. We then create a cleaner version of the origin_country column by converting the ISO3 codes back into country names. Note that the function chokes on North Korea initially, since it’s included as “Korea, North”—we use the custom_match argument to help the function out.\n The data isn’t tidy—there are individual columns for each year. gather() takes every column and changes it to a row. We exclude the country, region, continent, and ISO3 code from the change-into-rows transformation with -origin_country, -iso3, -origin_region, -origin_continent.\n Currently, the year is being treated as a number, but it’s helpful to also treat it as an actual date. We create a new variable named year_date that converts the raw number (e.g. 2009) into a date. The date needs to have at least a month, day, and year, so we actually convert it to January 1, 2009 with ymd(paste0(year, \"-01-01\")).\n  library(tidyverse) # For ggplot, dplyr, and friends library(countrycode) # For dealing with country names, abbreviations, and codes library(lubridate) # For dealing with dates refugees_raw \u0026lt;- read_csv(\u0026quot;data/refugee_status.csv\u0026quot;, na = c(\u0026quot;-\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;D\u0026quot;)) non_countries \u0026lt;- c(\u0026quot;Africa\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Europe\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;Oceania\u0026quot;, \u0026quot;South America\u0026quot;, \u0026quot;Unknown\u0026quot;, \u0026quot;Other\u0026quot;, \u0026quot;Total\u0026quot;) refugees_clean \u0026lt;- refugees_raw %\u0026gt;% # Make this column name easier to work with rename(origin_country = `Continent/Country of Nationality`) %\u0026gt;% # Get rid of non-countries filter(!(origin_country %in% non_countries)) %\u0026gt;% # Convert country names to ISO3 codes mutate(iso3 = countrycode(origin_country, \u0026quot;country.name\u0026quot;, \u0026quot;iso3c\u0026quot;, custom_match = c(\u0026quot;Korea, North\u0026quot; = \u0026quot;PRK\u0026quot;))) %\u0026gt;% # Convert ISO3 codes to country names, regions, and continents mutate(origin_country = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;country.name\u0026quot;), origin_region = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;region\u0026quot;), origin_continent = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;continent\u0026quot;)) %\u0026gt;% # Make this data tidy gather(year, number, -origin_country, -iso3, -origin_region, -origin_continent) %\u0026gt;% # Make sure the year column is numeric + make an actual date column for years mutate(year = as.numeric(year), year_date = ymd(paste0(year, \u0026quot;-01-01\u0026quot;)))  Data to possibly use in your plot Here are some possible summaries of the data you might use…\nCountry totals over time This is just the refugees_clean data frame I gave you. You’ll want to filter it and select specific countries, though—you won’t really be able to plot 60 countries all at once unless you use sparklines.\n## # A tibble: 6 x 7 ## origin_country iso3 origin_region origin_continent year number year_date ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; ## 1 Afghanistan AFG South Asia Asia 2006 651 2006-01-01 ## 2 Angola AGO Sub-Saharan Afr… Africa 2006 13 2006-01-01 ## 3 Armenia ARM Europe \u0026amp; Centra… Asia 2006 87 2006-01-01 ## 4 Azerbaijan AZE Europe \u0026amp; Centra… Asia 2006 77 2006-01-01 ## 5 Belarus BLR Europe \u0026amp; Centra… Europe 2006 350 2006-01-01 ## 6 Bhutan BTN South Asia Asia 2006 3 2006-01-01  Cumulative country totals over time Note the cumsum() function—it calculates the cumulative sum of a column.\nrefugees_countries_cumulative \u0026lt;- refugees_clean %\u0026gt;% arrange(year_date) %\u0026gt;% group_by(origin_country) %\u0026gt;% mutate(cumulative_total = cumsum(number)) ## # A tibble: 6 x 7 ## # Groups: origin_country [1] ## origin_country iso3 origin_continent year number year_date cumulative_total ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan AFG Asia 2006 651 2006-01-01 651 ## 2 Afghanistan AFG Asia 2007 441 2007-01-01 1092 ## 3 Afghanistan AFG Asia 2008 576 2008-01-01 1668 ## 4 Afghanistan AFG Asia 2009 349 2009-01-01 2017 ## 5 Afghanistan AFG Asia 2010 515 2010-01-01 2532 ## 6 Afghanistan AFG Asia 2011 428 2011-01-01 2960  Continent totals over time Note the na.rm = TRUE argument in sum(). This makes R ignore any missing data when calculating the total. Without it, if R finds a missing value in the column, it will mark the final sum as NA too, which we don’t want.\nrefugees_continents \u0026lt;- refugees_clean %\u0026gt;% group_by(origin_continent, year_date) %\u0026gt;% summarize(total = sum(number, na.rm = TRUE)) ## `summarise()` regrouping output by \u0026#39;origin_continent\u0026#39; (override with `.groups` argument) ## # A tibble: 6 x 3 ## # Groups: origin_continent [1] ## origin_continent year_date total ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 2006-01-01 18116 ## 2 Africa 2007-01-01 17473 ## 3 Africa 2008-01-01 8931 ## 4 Africa 2009-01-01 9664 ## 5 Africa 2010-01-01 13303 ## 6 Africa 2011-01-01 7677  Cumulative continent totals over time Note that there are two group_by() functions here. First we get the total number of refugees per continent per year, then we group by continent only to get the cumulative sum of refugees across continents.\nrefugees_continents_cumulative \u0026lt;- refugees_clean %\u0026gt;% group_by(origin_continent, year_date) %\u0026gt;% summarize(total = sum(number, na.rm = TRUE)) %\u0026gt;% arrange(year_date) %\u0026gt;% group_by(origin_continent) %\u0026gt;% mutate(cumulative_total = cumsum(total)) ## `summarise()` regrouping output by \u0026#39;origin_continent\u0026#39; (override with `.groups` argument) ## # A tibble: 6 x 4 ## # Groups: origin_continent [1] ## origin_continent year_date total cumulative_total ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 2006-01-01 18116 18116 ## 2 Africa 2007-01-01 17473 35589 ## 3 Africa 2008-01-01 8931 44520 ## 4 Africa 2009-01-01 9664 54184 ## 5 Africa 2010-01-01 13303 67487 ## 6 Africa 2011-01-01 7677 75164   Visualization ideas You can redesign one of these ugly, less-than-helpful graphs, or create a brand new visualization (like a map!).\nOr be super brave and make a map!\n  For instance, “North Korea”, “Korea, North”, “DPRK”, “Korea, Democratic People’s Republic of”, and “Democratic People’s Republic of Korea”, and “Korea (DPRK)” are all perfectly normal versions of the country’s name and you’ll find them all in the wild.↩︎\n See Gleditsch, Kristian S. \u0026amp; Michael D. Ward. 1999. “Interstate System Membership: A Revised List of the Independent States since 1816.” International Interactions 25: 393-413; or the “ICOW Historical State Names Data Set”.↩︎\n   ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"b67556c14e90a56d3f0c0e54605d8a80","permalink":"/lab/02-mini-project/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/lab/02-mini-project/","section":"lab","summary":"Instructions Data cleaning code Data to possibly use in your plot Country totals over time Cumulative country totals over time Continent totals over time Cumulative continent totals over time  Visualization ideas   The United States has resettled more than 600,000 refugees from 60 different countries since 2006.\nIn this mini project, you will use R, ggplot, and Illustrator, Inkscape, or Gravit Designer to explore where these refugees have come from.","tags":null,"title":"Mini project 2","type":"docs"},{"authors":null,"categories":null,"content":"  Instructions Starter code   New York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.\n  NYC keeps incredibly detailed data regarding animal sightings, including rats, and it makes this data publicly available.\nFor this first mini project, you will use R and ggplot2 to tell an interesting story hidden in the data. You can recreate one of these ugly, less-than-helpful graphs, or create a new story by looking at other variables in the data:\nInstructions Here’s what you need to do:\nCreate a new RStudio project and place it on your computer somewhere. Open that new folder in Windows File Explorer or macOS Finder (however you navigate around the files on your computer), and create two subfolders there named data and output.\n Download New York City’s database of rat sightings since 2010:\n  Rat_Sightings.csv\n Place this in the data subfolder you created in step 1. You might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text. The data was originally uploaded by the City of New York to Kaggle, and is provided with a public domain license.\n  Create a new R Markdown file and save it in your project. In RStudio go to File \u0026gt; New File \u0026gt; R Markdown…, choose the default options, and delete all the placeholder text in the new file except for the metadata at the top, which is between --- and ---.\n Verify that your project folder is structured like this:\nyour-project-name/ your-analysis.Rmd your-project-name.Rproj data/ Rat_Sightings.csv output/ NOTHING Summarize the data somehow. The raw data has more than 100,000 rows, which means you’ll need to aggregate the data (filter(), group_by(), and summarize() will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2016.\n Create an appropriate visualization based on the data you summarized.\n Write a memo (no word limit) explaining your process. I’m specifically looking for a discussion of the following:\n What was wrong with the original graphic (if you’re fixing one of the original figures)? What story are you telling with your new graphic? How did you apply the principles of CRAP? How did you apply Kieran Healy’s principles of great visualizations or Alberto Cairo’s five qualities of great visualizations?  Upload the following outputs to iCollege:\n A PDF or Word file of your memo with your final code and graphic embedded in it.1 This means you’ll need to do all your coding in an R Markdown file and embed your code in chunks. A standalone PNG version of your graphic. Use ggsave(plot_name, filename = \"output/blah.png\", width = XX, height = XX) A standalone PDF version of your graphic. Use ggsave(plot_name, filename = \"output/blah.pdf\", width = XX, height = XX)   You will be graded based on completion using the standard ✓ system, but I’ll provide comments on how you use R and ggplot2, how well you apply the principles of CRAP, The Truthful Art, and Effective Data Visualization, and how appropriate the graph is for the data and the story you’re telling. I will use this rubric to make comments and provide you with a simulated grade.\n  mini-project-1-rubric.pdf  For this assignment, I am less concerned with detailed graphic design principles—select appropriate colors, change fonts if you’re brave, and choose a nice ggplot theme and make some adjustments like moving the legend around (theme(legend.position = \"bottom\")).\nThe assignment is due by 11:59 PM on Friday, May 226.\nPlease seek out help when you need it! You know enough R (and have enough examples of code from class and your readings) to be able to do this. Your project has to be turned in individually, and your visualization should be your own (i.e. if you work with others, don’t all turn in the same graph), but you should work with others! Reach out to me for help too—I’m here to help!\nYou can do this, and you’ll feel like a budding dataviz witch/wizard when you’re done.\n Starter code I’ve provided some starter code below. A couple comments about it:\n By default, read_csv() treats cells that are empty or “NA” as missing values. This rat dataset uses “N/A” to mark missing values, so we need to add that as a possible marker of missingness (hence na = c(\"\", \"NA\", \"N/A\")) To make life easier, I’ve renamed some of the key variables you might work with. You can rename others if you want. I’ve also created a few date-related variables (sighting_year, sighting_month, sighting_day, and sighting_weekday). You don’t have to use them, but they’re there if you need them. The functions that create these, like year() and wday() are part of the lubridate library. The date/time variables are formatted like 04/03/2017 12:00:00 AM, which R is not able to automatically parse as a date when reading the CSV file. You can use the mdy_hms() function in the lubridate library to parse dates that are structured as “month-day-year-hour-minute”. There are also a bunch of other iterations of this function, like ymd(), dmy(), etc., for other date formats. There’s one row with an unspecified borough, so I filter that out.  library(tidyverse) library(lubridate) rats_raw \u0026lt;- read_csv(\u0026quot;data/Rat_Sightings.csv\u0026quot;, na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;N/A\u0026quot;)) # If you get an error that says \u0026quot;All formats failed to parse. No formats # found\u0026quot;, it\u0026#39;s because the mdy_hms function couldn\u0026#39;t parse the date. The date # variable *should* be in this format: \u0026quot;04/03/2017 12:00:00 AM\u0026quot;, but in some # rare instances, it might load without the seconds as \u0026quot;04/03/2017 12:00 AM\u0026quot;. # If there are no seconds, use mdy_hm() instead of mdy_hms(). rats_clean \u0026lt;- rats_raw %\u0026gt;% rename(created_date = `Created Date`, location_type = `Location Type`, borough = Borough) %\u0026gt;% mutate(created_date = mdy_hms(created_date)) %\u0026gt;% mutate(sighting_year = year(created_date), sighting_month = month(created_date), sighting_day = day(created_date), sighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %\u0026gt;% filter(borough != \u0026quot;Unspecified\u0026quot;) You’ll summarize the data with functions from dplyr, including stuff like count(), arrange(), filter(), group_by(), summarize(), and mutate(). Here are some examples of ways to summarize the data:\n# See the count of rat sightings by weekday rats_clean %\u0026gt;% count(sighting_weekday) # Assign a summarized data frame to an object to use it in a plot rats_by_weekday \u0026lt;- rats_clean %\u0026gt;% count(sighting_weekday, sighting_year) ggplot(rats_by_weekday, aes(x = fct_rev(sighting_weekday), y = n)) + geom_col() + coord_flip() + facet_wrap(~ sighting_year) # See the count of rat sightings by weekday and borough rats_clean %\u0026gt;% count(sighting_weekday, borough, sighting_year) # An alternative to count() is to specify the groups with group_by() and then # be explicit about how you\u0026#39;re summarizing the groups, such as calculating the # mean, standard deviation, or number of observations (we do that here with # `n()`). rats_clean %\u0026gt;% group_by(sighting_weekday, borough) %\u0026gt;% summarize(n = n())   You can approach this in a couple different ways—you can write the memo and then include the full figure and code at the end, similar to this blog post, or you can write the memo in an incremental way, describing the different steps of creating the figure, ultimately arriving at a clean final figure, like this blog post.↩︎\n   ","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"234b90c127fe693c65be123ead0c665b","permalink":"/lab/01-mini-project/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/lab/01-mini-project/","section":"lab","summary":"Instructions Starter code   New York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.\n  NYC keeps incredibly detailed data regarding animal sightings, including rats, and it makes this data publicly available.\nFor this first mini project, you will use R and ggplot2 to tell an interesting story hidden in the data.","tags":null,"title":"Mini project 1","type":"docs"},{"authors":null,"categories":null,"content":"   Basic process for working with RStudio   Basic process for working with RStudio For this example, I’m going to create a new RStudio project, download some data, put the data in the project, and make a graph of it using R Markdown. You’ll follow this same process any time you start a new project or exercise.\nTo follow along, download this CSV file here (you may need to right click on it and select “Save As…”):\n  gapminder.csv  Here’s a video walkthrough of how to get started:\n   ","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"059bb398e999a9d10b388c3df2b5644f","permalink":"/example/01-example/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/example/01-example/","section":"example","summary":"Basic process for working with RStudio   Basic process for working with RStudio For this example, I’m going to create a new RStudio project, download some data, put the data in the project, and make a graph of it using R Markdown. You’ll follow this same process any time you start a new project or exercise.\nTo follow along, download this CSV file here (you may need to right click on it and select “Save As…”):","tags":null,"title":"Introduction to R and the tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"   RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse Install tinytex    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nRStudio.cloud R is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service initially, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for labs and problem sets.\nGo to https://rstudio.cloud/ and create an account. You’ll receive a link to join the shared class workspace separately. If you don’t get this link, let me know and I will invite you.\n RStudio on your computer RStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets, more complicated analysis, or fancier graphics. Over the course of the semester, you should wean yourself off of RStudio.cloud and install all these things locally. This is also important if you want to customize fonts, since RStudio.cloud has extremely limited support for fonts other than Helvetica.\nHere’s how you install all these things\nInstall R First you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n Click on “Download R for XXX”, where XXX is either Mac or Windows:\n If you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.\n If you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n  Double click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n If you use macOS, download and install XQuartz. You do not need to do this on Windows.\n   Install RStudio Next, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n Double click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n  Double click on RStudio to run it (check your applications folder or start menu).\n Install tidyverse R packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel.\n Install tinytex When you knit to PDF, R uses a special scientific typesetting program named LaTeX (pronounced “lay-tek” or “lah-tex”; for goofy nerdy reasons, the x is technically the “ch” sound in “Bach”, but most people just say it as “k”—saying “layteks” is frowned on for whatever reason).\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB! To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console. Run tinytex::install_tinytex() in the console. Wait for a bit while R downloads and installs everything you need. The end! You should now be able to knit to PDF.     It’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n   ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"/resource/install/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"RStudio.cloud RStudio on your computer  Install R Install RStudio Install tidyverse Install tinytex    You will do all of your work in this class with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"   Accessibility Colors Fonts Graphic assets  Images Vectors Vectors, photos, videos, and other assets    Accessibility  Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based) Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes. ColourLovers: Like Facebook for color palettes. viridis: Percetually uniform color scales. Scientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with scico. ColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account. Colorgorical: Create color palettes based on fancy mathematical rules for perceptual distance. Colorpicker for data: More fancy mathematical rules for color palettes (explanation). iWantHue: Yet another perceptual distance-based color palette builder. Photochrome: Word-based color pallettes. PolicyViz Design Color Tools: Large collection of useful color resources   Fonts  Google Fonts: Huge collection of free, well-made fonts. The Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).   Graphic assets Images  Use the Creative Commons filters on Google Images or Flickr Unsplash Pexels Pixabay StockSnap.io Burst freephotos.cc   Vectors  Noun Project: Thousands of free simple vector images aiconica: 1,000+ vector icons Vecteezy: Thousands of free vector images   Vectors, photos, videos, and other assets  Stockio    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"16fd04c4714e3d096bffcf19e6c524ca","permalink":"/resource/design/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/design/","section":"resource","summary":"Accessibility Colors Fonts Graphic assets  Images Vectors Vectors, photos, videos, and other assets    Accessibility  Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based) Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes. ColourLovers: Like Facebook for color palettes.","tags":null,"title":"Design","type":"docs"},{"authors":null,"categories":null,"content":"   Critique Redesign in Canva Redesign in Illustrator Final versions   For this example, I’m going to critique and improve this random flyer I found posted in the BYU library in September 2018:\nIt’s not the best designed poster, but it’s incredibly typical of what you see in the real world. By applying the principles of CRAP, we can improve the poster significantly.\nIf you download and unzip this file, you can follow along too (but you don’t have to—you can just sit back and enjoy the ride).\n  02-example.zip  Critique    Redesign in Canva    Redesign in Illustrator    Final versions  ","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"ce879375c9ab42490f4d0d112e48c07c","permalink":"/example/02-example/","publishdate":"2020-05-12T00:00:00Z","relpermalink":"/example/02-example/","section":"example","summary":"Critique Redesign in Canva Redesign in Illustrator Final versions   For this example, I’m going to critique and improve this random flyer I found posted in the BYU library in September 2018:\nIt’s not the best designed poster, but it’s incredibly typical of what you see in the real world. By applying the principles of CRAP, we can improve the poster significantly.\nIf you download and unzip this file, you can follow along too (but you don’t have to—you can just sit back and enjoy the ride).","tags":null,"title":"Graphic design","type":"docs"},{"authors":null,"categories":null,"content":"  Task 1: Reflection Task 2: CRAP critique Task 3: CRAP redesign Turning everything in   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: CRAP critique Critique the design of the poster for the BYU Student Wellness Center workshop below. Go through the CRAP checklist and analyze how well or poorly the poster follows each of the principles. Discuss how the poster’s adherence to (or non-adherence to) these principles influences its effectiveness.\n(This would have been some random poster from GSU, but I haven’t been on campus since mid-March 😭)\n Task 3: CRAP redesign Redesign the poster for the BYU Student Wellness Center workshop. Use whatever program you want—even PowerPoint if you’re most comfortable with that, though it’ll probably be easier to use something like Canva or Adobe Illustrator. If you use Canva, don’t use any of the built-in templates—start from scratch with a blank page.\nTo save you from retyping everything, I’ve included all the text and Student Wellness hex logo in the zip file below:\n  02-exercise.zip  I didn’t include the Instagram logo. If you want to use that, go find one online. You don’t have to use it. You don’t have to use the big paragraph of text either—you can rewrite it to shrink it down if you want.\nCritique your new design using the CRAP checklist. How did you use contrast, repetition, alignment, and proximity in your improved design?\n Turning everything in You don’t need to worry about using R Markdown for this assignment (unless you really want to). On iCollege, submit a PDF of your new poster, along with a PDF of your reflection and your critiques of the original poster and your new poster.\n ","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"8a76e0e11f7dca670d02a859ef947eb3","permalink":"/lab/02-exercise/","publishdate":"2020-05-12T00:00:00Z","relpermalink":"/lab/02-exercise/","section":"lab","summary":"Task 1: Reflection Task 2: CRAP critique Task 3: CRAP redesign Turning everything in   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: CRAP critique Critique the design of the poster for the BYU Student Wellness Center workshop below. Go through the CRAP checklist and analyze how well or poorly the poster follows each of the principles. Discuss how the poster’s adherence to (or non-adherence to) these principles influences its effectiveness.","tags":null,"title":"Graphic design","type":"docs"},{"authors":null,"categories":null,"content":"   Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph. More text in the next paragraph. Always use empty lines between paragraphs.  Some text in a paragraph.\nMore text in the next paragraph. Always use empty lines between paragraphs.\n  *Italic* _Italic_ Italic  **Bold** __Bold__ Bold  # Heading 1  Heading 1   ## Heading 2  Heading 2   ### Heading 3  Heading 3   (Go up to heading level 6 with ######)    [Link text](http://www.example.com)  Link text  ![Image caption](/path/to/image.png)    `Inline code` with backticks  Inline code with backticks  \u0026gt; Blockquote   Blockquote\n  - Things in - an unordered - list * Things in * an unordered * list  Things in an unordered list   1. Things in 2. an ordered 3. list 1) Things in 2) an ordered 3) list Things in an ordered list   Horizontal line --- Horizontal line *** Horizontal line\n     Math Markdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n    Type… …to get    Based on the DAG, the regression model for estimating the effect of education on wages is $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or $\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$. Based on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).    To put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math: $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ But now we just use computers to solve for $x$. …to get…\n The quadratic equation was an important part of high school math:\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\nBut now we just use computers to solve for \\(x\\).\n Because dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n Tables There are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: Caption goes here …to get…\n Caption goes here  Right Left Center Default    12 12 12 12  123 123 123 123  1 1 1 1    For pipe tables, type…\n| Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Table: Caption goes here …to get…\n Caption goes here  Right Left Default Center    12 12 12 12  123 123 123 123  1 1 1 1     Footnotes There are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags]. [^1]: This is a note. [^note-on-dags]: DAGs are neat. And here\u0026#39;s more of the document. …to get…\n Here is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n  This is a note.↩︎   DAGs are neat.↩︎     You can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!] …to get…\n Causal inference is neat.1\n  But it can be hard too!↩︎      Front matter You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; --- You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n--- title: \u0026quot;My cool title: a subtitle\u0026quot; --- If you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n--- title: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39; ---  Citations One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib --- Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib csl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot; --- Some of the most common CSLs are:\n Chicago author-date Chicago note-bibliography Chicago full note-bibliography (no shortened notes or ibids) APA 7th edition MLA 8th edition  Cite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n    Type… …to get…    Causal inference is neat [@Rohrer:2018; @AngristPischke:2015]. Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).  Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1]. Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).  Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018]. Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).  @AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees. Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.    After compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n Angrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n   Other references These websites have additional details and examples and practice tools:\n CommonMark’s Markdown tutorial: A quick interactive Markdown tutorial. Markdown tutorial: Another interactive tutorial to practice using Markdown. Markdown cheatsheet: Useful one-page reminder of Markdown syntax. The Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.   ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  The Stories Behind a Line Australia as 100 people: You can make something like this with d3 and the potato project. Marrying Later, Staying Single Longer   How to select the appropriate chart type Many people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:\n The Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations. The Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.). From Data to Viz: A decision tree for dozens of chart types with links to R and Python code. The Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R. R Graph Catalog: R code for 124 ggplot graphs. Emery’s Essentials: Descriptions and examples of 26 different chart types.   General resources  Storytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic. Ann K. Emery’s blog: Blog and tutorials by Ann Emery. Evergreen Data: Helful resources by Stephanie Evergreen. PolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch. Visualising Data: Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk. Info We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field. FlowingData: Blog by Nathan Yau. Information is Beautiful: Blog by David McCandless. Junk Charts: Blog by Kaiser Fung. WTF Visualizations: Visualizations that make you ask “wtf?” The Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic. Data Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway. Seeing Data: A series of research projects about perceptions and visualizations.   Visualization in Excel  How to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel. Ann Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel.   Visualization in Tableau Because it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"ca403ba352e0871f06b445d2470037b3","permalink":"/resource/visualization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/visualization/","section":"resource","summary":"Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  The Stories Behind a Line Australia as 100 people: You can make something like this with d3 and the potato project. Marrying Later, Staying Single Longer   How to select the appropriate chart type Many people have created many useful tools for selecting the correct chart type for a given dataset or question.","tags":null,"title":"Visualization","type":"docs"},{"authors":null,"categories":null,"content":"  Data from the internet Nonprofit management Federal, state, and local government management Business management  Instructions Final deliverables Past examples Travel runs in Yellowstone Firefighter fatalities Scripture use by The Killers Utah nonprofits Buckethead    You made it to the end of our whirlwind tour of data visualization principles! Congratulations!\nNow you get to show off all the tools you learned with a beautiful, truthful, narrative visualization.\nFor your final project, you will take a dataset, explore it, tinker with it, and tell a nuanced story about it using at least three graphs.\nI want this project to be as useful for you and your future career as possible—you’ll hopefully want to show off your final project in a portfolio or during job interviews.\nAccordingly, you have some choice in what data you can use for this project. I’ve found several different high-quality datasets online related to the core MPA/MPP tracks. You do not have to choose a dataset in your given field (especially if you’re not an MPA or MPP student!) Choose whatever one you are most interested in or will have the most fun with.\nData from the internet Go to this list of data sources and find something interesting! The things in the “Data is Plural” newsletter are often especially interesting and fun. Here are some different high-quality datasets that students have worked with before:\nNonprofit management  U.S. Charities and Non-profits: All of the charities and nonprofits registered with the IRS. This is actually split into six separate files. You can combine them all into one massive national database with bind_rows(), or filter the data to include specific states (or a single state). It all depends on the story you’re telling. Source: IRS. Nonprofit Grants 2010 to 2016: Nonprofit grants made in the US as listed in Schedule I of the IRS 990 tax form between 2010 to 2016. Source: IRS.   Federal, state, and local government management  Deadly traffic accidents in the UK (2015): List of all traffic-related deaths in the UK in 2015. Source: data.gov.uk. Firefighter Fatalities in the United States: Name, rank, and cause of death for all firefighters killed since 2000. Source: FEMA. Federal Emergencies and Disasters, 1953–Present: Every federal emergency or disaster declared by the President of the United States since 1953. Source: FEMA. Global Terrorism Database (1970–2016): 170,000 terrorist attacks worldwide, 1970-2016. Source: National Consortium for the Study of Terrorism and Responses to Terrorism (START), University of Maryland. City of Austin 311 Unified Data: All 311 calls to the City of Austin since 2014. Source: City of Austin.   Business management  515K Hotel Reviews Data in Europe: 515,000 customer reviews and scoring of 1,493 luxury hotels across Europe. Source: Booking.com. Chase Bank Branch Deposits, 2010–2016: Records for every branch of Chase Bank in the United States. This dataset is not quite tidy and will require a little bit of reshaping with gather() or pivot_longer(), since there are individual columns of deposits per year. Source: Chase Bank.    Instructions Here’s what you’ll need to do:\nDownload a dataset and explore it. Many of these datasets are large and will not open (well) in Excel, so you’ll need to load the CSV file into R with read_csv(). Most of these datasets have nice categorical variables that you can use for grouping and summarizing, and many have time components too, so you can look at trends. Your past problem sets and in-class examples will come in handy here.\n Find a story in the data. Explore that story and make sure it’s true and insightful.\n Use R to create multiple graphs to tell the story. You can make as many graphs as you want, but you must use at least three different chart types (i.e. don’t just make three scatterplots or three maps).\n Export these figures as PDF files, place them in Adobe Illustrator (or InDesign or Gravit Designer or Inkscape), and make one combined graphic or handout where you tell the complete story. You have a lot of latitude in how you do this. You can make a graphic-heavy one-page handout. You can make something along the lines of the this, with one big graphic + smaller subgraphics + explanatory text. Just don’t make a goofy infographic. Whatever you do, the final figure must include all the graphics, must have some explanatory text to help summarize the narrative, and must be well designed.\n Export the final graphic from Illustrator as a PDF and a PNG.\n Write a memo using R Markdown to introduce, frame, and describe your story and figure. Use this template to get started. You should include the following in the memo:\n Executive summary Background information and summary of the data Explanation, description, and code for each individual figure Explanation and description for the final figure Final figure should be included as an image (remember ![Caption goes here](path/to/file.png))   Remember to follow R Markdown etiquette rules and style—don’t have it output extraneous messages or warnings, include summary tables in nice tables, adjust the dimensions for your figures, and remove the placeholder text that’s in the template already (i.e. I don’t want to see stuff like “Describe and show how you cleaned and reshaped the data” in the final report.)\nYou should download a full example of what a final project might look like (but don’t make your final combined visualization look exactly like this—show some creativity!)\n Final deliverables Upload the following files to iCollege:\nA memo introducing and describing your final graphic (see full instructions above) A standalone PDF of your graphic exported from Illustrator A standalone PNG of your graphic exported from Illustrator  No late work will be accepted for this project since it’s the last project and it counts as your final.\nI will use this rubric to grade the final product:\n  final-project-rubric.xlsx  I am happy to give feedback and help along the way—please don’t hesitate to get help! My goal is for you to have a beautiful graphic in the end that you’ll want to show off to all your friends, family, neighbors, employers, and strangers on the street—I’m not trying to trip you up or give you trick questions!\nAnd that’s it. You’re done! Go out into the world now and make beautiful, insightful, and truthful graphics.\nGo forth and make awesomeness.\n Past examples Download a full example of what a final project might look like.\nHere are some great examples of student projects from past versions of this class.\nTravel runs in Yellowstone  Project description  Final PDF  Final PNG\n\n Firefighter fatalities  Project description  Final PDF  Final PNG\n\n Scripture use by The Killers  Project description  Final PDF  Final PNG\n\n Utah nonprofits  Project description  Final PDF  Final PNG\n\n Buckethead  Project description  Final PDF  Final PNG\n\n  ","date":1591315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"18b68d98c58d200bc2f4f14ab76a474a","permalink":"/lab/final-project/","publishdate":"2020-06-05T00:00:00Z","relpermalink":"/lab/final-project/","section":"lab","summary":"Data from the internet Nonprofit management Federal, state, and local government management Business management  Instructions Final deliverables Past examples Travel runs in Yellowstone Firefighter fatalities Scripture use by The Killers Utah nonprofits Buckethead    You made it to the end of our whirlwind tour of data visualization principles! Congratulations!\nNow you get to show off all the tools you learned with a beautiful, truthful, narrative visualization.","tags":null,"title":"Final project","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load and clean data Histograms Points Boxplots Summaries    For this example, I’m going to use real world data to demonstrate the typical process for loading data, cleaning it up a bit, and mapping specific columns of the data onto the parts of a graph using the grammar of graphics and ggplot().\nThe data I’ll use comes from the BBC’s corporate charity, BBC Children in Need, which makes grants to smaller UK nonprofit organizations that work on issues related to childhood poverty. An organization in the UK named 360Giving helps nonprofits and foundations publish data about their grant giving activities in an open and standardized way, and (as of May 2020) they list data from 126 different charities, including BBC Children in Need.\nIf you want to follow along with this example (highly recommended!), you can download the data directly from 360Giving or by using this link:\n  360-giving-data.xlsx  Live coding example Warning: I got carried away with this because I wanted to make it as comprehensive and detailed as possible, so it starts off with nothing and walks through the process of downloading data, creating a new project, and getting everything started. As such, it is ridiculously long (1 hour 😱 😱). Remember that there’s no requirement that you watch these things—they’re simply for your reference so you can see what doing this R stuff looks like in real time. The content all below the video is roughly the same (more polished even).\nThat said, it is a useful demonstration of how to get everything started and what it looks like to do an entire analysis, so there is value in it. Watch just the first part, or watch it on 2x or something.\nAnd I promise future examples will not be this long!\n   Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and clean data First, we need to load a few libraries: tidyverse (as always), along with readxl for reading Excel files and lubridate for working with dates:\n# Load libraries library(tidyverse) # For ggplot, dplyr, and friends library(readxl) # For reading Excel files library(lubridate) # For working with dates We’ll then load the original Excel file. I placed this file in a folder named data in my RStudio Project folder for this example. I like to read original data into an object named whatever_raw just in case it takes a long time to load (that way I don’t have to keep reloading it every time I add a new column or do anything else with it). It’s also good practice to keep a pristine, untouched copy of your data.\n# Load the original Excel file bbc_raw \u0026lt;- read_excel(\u0026quot;data/360-giving-data.xlsx\u0026quot;) There may be some errors reading the file—you can ignore those in this case.\nNext we’ll add a couple columns and clean up the data a little. In the video I did this non-linearly—I came back to the top of the document to add columns when I needed them and then reran the chunk to create the data.\nWe’ll extract the year from the Award Date column, rename some of the longer-named columns, and make a new column that shows the duration of grants. We’ll also get rid of 2015 since there are so few observations then.\nNote the strange use of `s around column names like `Award Date`. This is because R technically doesn’t allow special characters like spaces in column names. If there are spaces, you have to wrap the column names in backticks. Because typing backticks all the time gets tedious, we’ll use rename() to rename some of the columns:\nbbc \u0026lt;- bbc_raw %\u0026gt;% # Extract the year from the award date mutate(grant_year = year(`Award Date`)) %\u0026gt;% # Rename some columns rename(grant_amount = `Amount Awarded`, grant_program = `Grant Programme:Title`, grant_duration = `Planned Dates:Duration (months)`) %\u0026gt;% # Make a new text-based version of the duration column, recoding months # between 12-23, 23-35, and 36+. The case_when() function here lets us use # multiple if/else conditions at the same time. mutate(grant_duration_text = case_when( grant_duration \u0026gt;= 12 \u0026amp; grant_duration \u0026lt; 24 ~ \u0026quot;1 year\u0026quot;, grant_duration \u0026gt;= 24 \u0026amp; grant_duration \u0026lt; 36 ~ \u0026quot;2 years\u0026quot;, grant_duration \u0026gt;= 36 ~ \u0026quot;3 years\u0026quot; )) %\u0026gt;% # Get rid of anything before 2016 filter(grant_year \u0026gt; 2015) %\u0026gt;% # Make a categorical version of the year column mutate(grant_year_category = factor(grant_year))  Histograms First let’s look at the distribution of grant amounts with a histogram. Map grant_amount to the x-axis and don’t map anything to the y-axis, since geom_histogram() will calculate the y-axis values for us:\nggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Notice that ggplot warns you about bin widths. By default it will divide the data into 30 equally spaced bins, which will most likely not be the best for your data. You should always set your own bin width to something more appropriate. There are no rules for correct bin widths. Just don’t have them be too wide:\nggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram(binwidth = 100000) Or too small:\nggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram(binwidth = 500) £10,000 seems to fit well. It’s often helpful to add a white border to the histogram bars, too:\nggplot(data = bbc, mapping = aes(x = grant_amount)) + geom_histogram(binwidth = 10000, color = \u0026quot;white\u0026quot;) We can map other variables onto the plot, like mapping grant_year_category to the fill aesthetic:\nggplot(bbc, aes(x = grant_amount, fill = grant_year_category)) + geom_histogram(binwidth = 10000, color = \u0026quot;white\u0026quot;) That gets really hard to interpret though, so we can facet by year with facet_wrap():\nggplot(bbc, aes(x = grant_amount, fill = grant_year_category)) + geom_histogram(binwidth = 10000, color = \u0026quot;white\u0026quot;) + facet_wrap(vars(grant_year)) Neat!\n Points Next let’s look at the data using points, mapping year to the x-axis and grant amount to the y-axis:\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point() We have some serious overplotting here, with dots so thick that it looks like lines. We can fix this a couple different ways. First, we can make the points semi-transparent using alpha, which ranges from 0 (completely invisible) to 1 (completely solid).\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point(alpha = 0.1) We can also randomly space the points to spread them out using position_jitter():\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point(position = position_jitter()) One issue with this, though, is that the points are jittered along the x-axis (which is fine, since they’re all within the same year) and the y-axis (which is bad, since the amounts are actual numbers). We can tell ggplot to only jitter in one direction by specifying the height argument—we don’t want any up-and-down jittering:\nggplot(bbc, aes(x = grant_year_category, y = grant_amount)) + geom_point(position = position_jitter(height = 0)) There are some weird clusters around £30,000 and below. Let’s map grant_program to the color aesthetic, which has two categories—regular grants and small grants—and see if that helps explain why:\nggplot(bbc, aes(x = grant_year_category, y = grant_amount, color = grant_program)) + geom_point(position = position_jitter(height = 0)) It does! We appear to have two different distributions of grants: small grants have a limit of £30,000, while regular grants have a much higher average amount.\n Boxplots We can add summary information to the plot by only changing the geom we’re using. Switch from geom_point() to geom_boxplot():\nggplot(bbc, aes(x = grant_year_category, y = grant_amount, color = grant_program)) + geom_boxplot()  Summaries We can also make smaller summarized datasets with dplyr functions like group_by() and summarize() and plot those. First let’s look at grant totals, averages, and counts over time:\nbbc_by_year \u0026lt;- bbc %\u0026gt;% group_by(grant_year) %\u0026gt;% # Make invisible subgroups for each year summarize(total = sum(grant_amount), # Find the total awarded in each group avg = mean(grant_amount), # Find the average awarded in each group number = n()) # n() is a special function that shows the number of rows in each group # Look at our summarized data bbc_by_year ## # A tibble: 4 x 4 ## grant_year total avg number ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2016 17290488 78238. 221 ## 2 2017 62394278 59765. 1044 ## 3 2018 61349392 60205. 1019 ## 4 2019 41388816 61136. 677 Because we used summarize(), R shrank our data down significantly. We now only have a row for each of the subgroups we made: one for each year. We can plot this smaller data. We’ll use geom_col() for now (but in tomorrow’s session you’ll learn why this is actually bad for averages!)\n# Plot our summarized data ggplot(bbc_by_year, aes(x = grant_year, y = avg)) + geom_col() ggplot(bbc_by_year, aes(x = grant_year, y = total)) + geom_col() ggplot(bbc_by_year, aes(x = grant_year, y = number)) + geom_col() Based on these charts, it looks like 2016 saw the largest average grant amount. In all other years, grants averaged around £60,000, but in 2016 it jumped up to £80,000. If we look at total grants, though, we can see that there were far fewer grants awarded in 2016—only 221! 2017 and 2018 were much bigger years with far more money awarded.\nWe can also use multiple aesthetics to reveal more information from the data. First we’ll make a new small summary dataset and group by both year and grant program. With those groups, we’ll again calculate the total, average, and number.\nbbc_year_size \u0026lt;- bbc %\u0026gt;% group_by(grant_year, grant_program) %\u0026gt;% summarize(total = sum(grant_amount), avg = mean(grant_amount), number = n()) bbc_year_size ## # A tibble: 8 x 5 ## # Groups: grant_year [4] ## grant_year grant_program total avg number ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2016 Main Grants 16405586 86345. 190 ## 2 2016 Small Grants 884902 28545. 31 ## 3 2017 Main Grants 48502923 90154. 538 ## 4 2017 Small Grants 13891355 27453. 506 ## 5 2018 Main Grants 47347789 95652. 495 ## 6 2018 Small Grants 14001603 26721. 524 ## 7 2019 Main Grants 33019492 96267. 343 ## 8 2019 Small Grants 8369324 25058. 334 Next we’ll plot the data, mapping the grant_program column to the fill aesthetic:\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col() By default, ggplot will stack the different fill colors within the same bar, but this makes it a little hard to make comparisons. While we can see that the average small grant amount was a little bigger in 2017 than in 2019, it’s harder to compare average main grant amount, since the bottoms of those sections don’t align.\nTo fix this, we can use position_dodge() to tell the columns to fit side-by-side:\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col(position = position_dodge()) Instead of dodging, we can also facet by grant_program to separate the bars:\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col() + facet_wrap(vars(grant_program)) We can put these in one column if we want:\nggplot(bbc_year_size, aes(x = grant_year, y = total, fill = grant_program)) + geom_col() + facet_wrap(vars(grant_program), ncol = 1) Finally, we can include even more variables! We have a lot of aesthetics we can work with (size, alpha, color, fill, linetype, etc.), as well as facets, so let’s add one more to show the duration of the awarded grant.\nFirst we’ll make another smaller summarized dataset, grouping by year, program, and duration and summarizing the total, average, and number of awards.\nbbc_year_size_duration \u0026lt;- bbc %\u0026gt;% group_by(grant_year, grant_program, grant_duration_text) %\u0026gt;% summarize(total = sum(grant_amount), avg = mean(grant_amount), number = n()) bbc_year_size_duration ## # A tibble: 21 x 6 ## # Groups: grant_year, grant_program [8] ## grant_year grant_program grant_duration_text total avg number ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 2016 Main Grants 2 years 97355 48678. 2 ## 2 2016 Main Grants 3 years 16308231 86746. 188 ## 3 2016 Small Grants 3 years 884902 28545. 31 ## 4 2017 Main Grants 1 year 59586 29793 2 ## 5 2017 Main Grants 2 years 825732 82573. 10 ## 6 2017 Main Grants 3 years 47617605 90528. 526 ## 7 2017 Small Grants 1 year 10000 10000 1 ## 8 2017 Small Grants 2 years 245227 18864. 13 ## 9 2017 Small Grants 3 years 13636128 27716. 492 ## 10 2018 Main Grants 1 year 118134 59067 2 ## # … with 11 more rows Next, we’ll fill by grant program and facet by duration and show the total number of grants awarded\nggplot(bbc_year_size_duration, aes(x = grant_year, y = number, fill = grant_program)) + geom_col(position = position_dodge(preserve = \u0026quot;single\u0026quot;)) + facet_wrap(vars(grant_duration_text), ncol = 1) The vast majority of BBC Children in Need’s grants last for 3 years. Super neat.\n  ","date":1589328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"73664027ae41c739f0c70a62c901a4c5","permalink":"/example/03-example/","publishdate":"2020-05-13T00:00:00Z","relpermalink":"/example/03-example/","section":"example","summary":"Live coding example Complete code  Load and clean data Histograms Points Boxplots Summaries    For this example, I’m going to use real world data to demonstrate the typical process for loading data, cleaning it up a bit, and mapping specific columns of the data onto the parts of a graph using the grammar of graphics and ggplot().\nThe data I’ll use comes from the BBC’s corporate charity, BBC Children in Need, which makes grants to smaller UK nonprofit organizations that work on issues related to childhood poverty.","tags":null,"title":"Mapping data to graphics","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Lord of the Rings Turning everything in   Getting started You’ll be doing all your R work in R Markdown this time (and from now on). You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nYou’ll need to download three CSV files and put them somewhere on your computer or upload them to RStudio.cloud—preferably in a folder named data in your project folder:\n  The_Fellowship_Of_The_Ring.csv  The_Two_Towers.csv  The_Return_Of_The_King.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise. Download that here and include it in your project:\n  03-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 03-exercise.Rmd your-project-name.Rproj data\\ The_Fellowship_Of_The_Ring.csv The_Two_Towers.csv The_Return_Of_The_King.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  03-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Lord of the Rings Answer the following questions:\n Use group_by() and summarize() on the lotr data to find the total number of words spoken by race. Don’t worry about plotting it. How many words did male hobbits say in the movies?\n Use group_by() and summarize() to answer these questions with bar plots (geom_col())\n Does a certain race dominate the entire trilogy? (hint: group by Race)\n Does a certain gender dominate a movie? (lolz of course it does, but still, graph it) (Hint: group by both Gender and Film.) Experiment with filling by Gender or Film and faceting by Gender or Film.\n Does the dominant race differ across the three movies? (Hint: group by both Race and Film.) Experiment with filling by Race or Film and faceting by Race or Film.\n  Create a plot that visualizes the number of words spoken by race, gender, and film simultaneously. Use the complete tidy lotr data frame. You don’t need to create a new summarized dataset (with group_by(Race, Gender, Film)) because the original data already has a row for each of those (you could make a summarized dataset, but it would be identical to the full version).\nYou need to show Race, Gender, and Film at the same time, but you only have two possible aesthetics (x and fill), so you’ll also need to facet by the third. Play around with different combinations (e.g. try x = Race, then x = Film) until you find one that tells the clearest story. For fun, add a labs() layer to add a title and subtitle and caption.\n  You’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1589328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"1007e2675b69de9f8d004a8637b0393b","permalink":"/lab/03-exercise/","publishdate":"2020-05-13T00:00:00Z","relpermalink":"/lab/03-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Lord of the Rings Turning everything in   Getting started You’ll be doing all your R work in R Markdown this time (and from now on). You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.","tags":null,"title":"Mapping data to graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms  Document: A Markdown file where you type stuff\n Chunk: A piece of R code that is included in your document. It looks like this:\n```{r} # Code goes here ``` There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n Knit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n   Add chunks There are three ways to insert chunks:\n Press ⌘⌥I on macOS or control + alt + I on Windows\n Click on the “Insert” button at the top of the editor window\n Manually type all the backticks and curly braces (don’t do this)\n   Chunk names You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk} # Code goes here ```  Chunk options There are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE} # Code goes here ``` The most common chunk options are these:\n fig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures echo=FALSE: The code is not shown in the final document, but the results are message=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted warning=FALSE: Any warnings that R generates are omitted include=FALSE: The chunk still runs, but the code and results are not included in the final document  You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n Inline chunks You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE} avg_mpg \u0026lt;- mean(mtcars$mpg) ``` The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon. … would knit into this:\n The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n  Output formats You can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot; output: html_document: default pdf_document: default word_document: default You can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n--- title: \u0026quot;My document\u0026quot; author: \u0026quot;My name\u0026quot; date: \u0026quot;January 13, 2020\u0026quot; output: html_document: toc: yes fig_caption: yes fig_height: 8 fig_width: 10 pdf_document: latex_engine: xelatex # More modern PDF typesetting engine toc: yes word_document: toc: yes fig_caption: yes fig_height: 4 fig_width: 5 ---  ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   Learning R R in the wild   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work in this class will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful. Also check out StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nThese resources are also really really helpful:\n R for Data Science: A free online book for learning the basics of R and the tidyverse. R and RStudio cheat sheets: A large collection of simple cheat sheets for RStudio, ggplot2, and other R-related things. Stat 545: Dr. Jenny Bryan at RStudio has an entire introductory course in R, visualization, and data analysis online. STA 112FS: Data Science: Dr. Mine Çetinkaya-Rundel at the University of Edinburgh / Duke University has an entire introductory course in R, visualization, and data science online. CSE 631: Principles \u0026amp; Practice of Data Visualization: Yet another introductory course for R and ggplot2 by Dr. Alison Presmanes Hill at RStudio.   R in the wild A popular (and increasingly standard) way for sharing your analyses and visualizations is to post an annotated explanation of your process somewhere online. RStudio allows you to publish knitted HTML files directly to RPubs, but you can also post your output to a blog or other type of website.1 Reading these kinds of posts is one of the best ways to learn R, since they walk you through each step of the process and show the code and output.\nHere are some of the best examples I’ve come across:\n Text analysis of Trump’s tweets confirms he writes only the (angrier) Android half (with a follow-up) Bob Ross - Joy of Painting Bechdel analysis using the tidyverse: There are also a bunch of other examples using data from FiveThirtyEight. Sexism on the Silver Screen: Exploring film’s gender divide Comparison of Quentin Tarantino Movies by Box Office and the Bechdel Test Who came to vote in Utah’s caucuses? Health care indicators in Utah counties Song lyrics across the United States A decade (ish) of listening to Sigur Rós When is Tom peeping these days?: There are a also bunch of final projects from other R and data visualization classes here and here. Mapping Fall Foliage General (Attys) Distributions Disproving Approval    If you want to be really fancy, you can use blogdown, which makes a complete website with R Markdown files. That’s actually how this site is built (see the source code). You can build your own site with this tutorial.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"0f6270d48011ac62645a8455a86a24bf","permalink":"/resource/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/r/","section":"resource","summary":"Learning R R in the wild   Learning R I highly recommend subscribing to the R Weekly newsletter. This e-mail is sent every Monday and is full of helpful tutorials about how to do stuff with R.\nSearching for help with R on Google can sometimes be tricky because the program name is a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.","tags":null,"title":"R","type":"docs"},{"authors":null,"categories":null,"content":"   R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n Main style things to pay attention to for this class  Important note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n Spacing  See the “Spacing” section in the tidyverse style guide.\n Put spaces after commas (like in regular English):\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg , cty \u0026gt; 10) filter(mpg ,cty \u0026gt; 10) filter(mpg,cty \u0026gt; 10) Put spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg, cty\u0026gt;10) filter(mpg, cty\u0026gt; 10) filter(mpg, cty \u0026gt;10) Don’t put spaces around parentheses that are parts of functions:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter (mpg, cty \u0026gt; 10) filter ( mpg, cty \u0026gt; 10) filter( mpg, cty \u0026gt; 10 )  Long lines  See the “Long lines” section in the tidyverse style guide.\n It’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Bad filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;)) # Good filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))  Pipes (%\u0026gt;%) and ggplot layers (+) Put each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad and won\u0026#39;t even work ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() Put each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad and won\u0026#39;t even work mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))  Comments  See the “Comments” section in the tidyverse style guide.\n Comments should start with a comment symbol and a single space: #\n# Good #Bad #Bad If the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group You can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good # Happy families are all alike; every unhappy family is unhappy in its own way. # Everything was in confusion in the Oblonskys’ house. The wife had discovered # that the husband was carrying on an intrigue with a French girl, who had been # a governess in their family, and she had announced to her husband that she # could not go on living in the same house with him. This position of affairs # had now lasted three days, and not only the husband and wife themselves, but # all the members of their family and household, were painfully conscious of it. # Bad # Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Though, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load data Wrangle data Bar plot Lollipop chart Strip plot Beeswarm plot Heatmap    For this example, we’re going to use real world data to demonstrate some different ways to visualize amounts and proportions. We’ll use data from the CDC and the Social Security Administration about the number of daily births in the United States from 1994–2014. FiveThirtyEight reported a story using this data in 2016 and they posted relatively CSV files on GitHub, so we can download and use those.\nIf you want to follow along with this example, you can download the data directly from GitHub or by using these links (you’ll likely need to right click on these and choose “Save Link As…”):\n  US_births_1994-2003_CDC_NCHS.csv  US_births_2000-2014_SSA.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad data There are two CSV files:\n US_births_1994-2003_CDC_NCHS.csv contains U.S. births data for the years 1994 to 2003, as provided by the Centers for Disease Control and Prevention’s National Center for Health Statistics. US_births_2000-2014_SSA.csv contains U.S. births data for the years 2000 to 2014, as provided by the Social Security Administration.  Since the two datasets overlap in 2000–2003, we use Social Security Administration data for those years.\nWe downloaded the data from GitHub and placed the CSV files in a folder named data. We’ll then load them with read_csv() and combine them into one data frame.\nlibrary(tidyverse) library(scales) # For nice labels in charts births_1994_1999 \u0026lt;- read_csv(\u0026quot;data/US_births_1994-2003_CDC_NCHS.csv\u0026quot;) %\u0026gt;% # Ignore anything after 2000 filter(year \u0026lt; 2000) births_2000_2014 \u0026lt;- read_csv(\u0026quot;data/US_births_2000-2014_SSA.csv\u0026quot;) births_combined \u0026lt;- bind_rows(births_1994_1999, births_2000_2014)  Wrangle data Let’s look at the first few rows of the data to see what we’re working with:\nhead(births_combined) ## # A tibble: 6 x 5 ## year month date_of_month day_of_week births ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1994 1 1 6 8096 ## 2 1994 1 2 7 7772 ## 3 1994 1 3 1 10142 ## 4 1994 1 4 2 11248 ## 5 1994 1 5 3 11053 ## 6 1994 1 6 4 11406 The columns for year and births seem straightforward and ready to use. The columns for month and day of the week could be improved if we changed them to text (i.e. January instead of 1; Tuesday instead of 3). To fix this, we can convert these columns to categorical variables, or factors in R. We can also specify that these categories (or factors) are ordered, meaning that Feburary comes after January, etc. Without ordering, R will plot them alphabetically, which isn’t very helpful.\nWe’ll make a new dataset named births that’s based on the combined births data, but with some new columns added:\n# The c() function lets us make a list of values month_names \u0026lt;- c(\u0026quot;January\u0026quot;, \u0026quot;February\u0026quot;, \u0026quot;March\u0026quot;, \u0026quot;April\u0026quot;, \u0026quot;May\u0026quot;, \u0026quot;June\u0026quot;, \u0026quot;July\u0026quot;, \u0026quot;August\u0026quot;, \u0026quot;September\u0026quot;, \u0026quot;October\u0026quot;, \u0026quot;November\u0026quot;, \u0026quot;December\u0026quot;) day_names \u0026lt;- c(\u0026quot;Monday\u0026quot;, \u0026quot;Tuesday\u0026quot;, \u0026quot;Wednesday\u0026quot;, \u0026quot;Thursday\u0026quot;, \u0026quot;Friday\u0026quot;, \u0026quot;Saturday\u0026quot;, \u0026quot;Sunday\u0026quot;) births \u0026lt;- births_combined %\u0026gt;% # Make month an ordered factor, using the month_name list as labels mutate(month = factor(month, labels = month_names, ordered = TRUE)) %\u0026gt;% mutate(day_of_week = factor(day_of_week, labels = day_names, ordered = TRUE), date_of_month_categorical = factor(date_of_month)) %\u0026gt;% # Add a column indicating if the day is on a weekend mutate(weekend = ifelse(day_of_week %in% c(\u0026quot;Saturday\u0026quot;, \u0026quot;Sunday\u0026quot;), TRUE, FALSE)) head(births) ## # A tibble: 6 x 7 ## year month date_of_month day_of_week births date_of_month_categori… weekend ## \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;lgl\u0026gt; ## 1 1994 January 1 Saturday 8096 1 TRUE ## 2 1994 January 2 Sunday 7772 2 TRUE ## 3 1994 January 3 Monday 10142 3 FALSE ## 4 1994 January 4 Tuesday 11248 4 FALSE ## 5 1994 January 5 Wednesday 11053 5 FALSE ## 6 1994 January 6 Thursday 11406 6 FALSE If you look at the data now, you can see the columns are changed and have different types. year and date_of_month are still numbers, but month, and day_of_week are ordered factors (ord) and date_of_month_categorical is a regular factor (fct). Technically it’s also ordered, but because it’s already alphabetical (i.e. 2 naturally comes after 1), we don’t need to force it to be in the right order.\nOur births data is now clean and ready to go!\n Bar plot First we can look at a bar chart showing the total number of births each day. We need to make a smaller summarized dataset and then we’ll plot it:\ntotal_births_weekday \u0026lt;- births %\u0026gt;% group_by(day_of_week) %\u0026gt;% summarize(total = sum(births)) ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, fill = day_of_week)) + geom_col() + # Turn off the fill legend because it\u0026#39;s redundant guides(fill = FALSE) If we fill by day of the week, we get 7 different colors, which is fine (I guess), but doesn’t really help tell a story. The main story here is that there are far fewer births during weekends. If we create a new column that flags if a row is Saturday or Sunday, we can fill by that column instead:\ntotal_births_weekday \u0026lt;- births %\u0026gt;% group_by(day_of_week) %\u0026gt;% summarize(total = sum(births)) %\u0026gt;% mutate(weekend = ifelse(day_of_week %in% c(\u0026quot;Saturday\u0026quot;, \u0026quot;Sunday\u0026quot;), TRUE, FALSE)) ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, fill = weekend)) + geom_col() Neat! Those default colors are kinda ugly, though, so let’s use the principles of preattentive processing and contrast to highlight the weekend bars:\nggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, fill = weekend)) + geom_col() + # Use grey and orange scale_fill_manual(values = c(\u0026quot;grey70\u0026quot;, \u0026quot;#f2ad22\u0026quot;)) + # Use commas instead of scientific notation scale_y_continuous(labels = comma) + # Turn off the legend since the title shows what the orange is guides(fill = FALSE) + labs(title = \u0026quot;Weekends are unpopular times for giving birth\u0026quot;, x = NULL, y = \u0026quot;Total births\u0026quot;)  Lollipop chart Since the ends of the bars are often the most important part of the graph, we can use a lollipop chart to emphasize them. We’ll keep all the same code from our bar chart and make a few changes:\n Color by weekend instead of fill by weekend, since points and lines are colored in ggplot, not filled Switch scale_fill_manual() to scale_color_manual() and turn off the color legend in the guides() layer Switch geom_col() to geom_pointrange(). The geom_pointrange() layer requires two additional aesthetics: ymin and ymax for the ends of the lines that come out of the point. Here we’ll set ymin to 0 so it starts at the x-axis, and we’ll set ymax to total so it ends at the point.  ggplot(data = total_births_weekday, mapping = aes(x = day_of_week, y = total, color = weekend)) + geom_pointrange(aes(ymin = 0, ymax = total), # Make the lines a little thicker and the dots a little bigger fatten = 5, size = 1.5) + # Use grey and orange scale_color_manual(values = c(\u0026quot;grey70\u0026quot;, \u0026quot;#f2ad22\u0026quot;)) + # Use commas instead of scientific notation scale_y_continuous(labels = comma) + # Turn off the legend since the title shows what the orange is guides(color = FALSE) + labs(title = \u0026quot;Weekends are unpopular times for giving birth\u0026quot;, x = NULL, y = \u0026quot;Total births\u0026quot;)  Strip plot However, we want to #barbarplots! (Though they’re arguably okay here, since they show totals and not averages). Let’s show all the data with points. We’ll use the full dataset now, map x to weekday, y to births, and change geom_col() to geom_point(). We’ll tell geom_point() to jitter the points randomly.\nggplot(data = births, mapping = aes(x = day_of_week, y = births, color = weekend)) + scale_color_manual(values = c(\u0026quot;grey70\u0026quot;, \u0026quot;#f2ad22\u0026quot;)) + geom_point(size = 0.5, position = position_jitter(height = 0)) + guides(color = FALSE) There are some interesting points in the low ends, likely because of holidays like Labor Day and Memorial Day (for the Mondays) and Thanksgiving (for the Thursday). If we had a column that indicated whether a day was a holiday, we could color by that and it would probably explain most of those low numbers. Unfortunately we don’t have that column, and it’d be hard to make. Some holidays are constant (Halloween is always October 31), but some aren’t (Thanksgiving is the fourth Thursday in November, so we’d need to find out which November 20-somethingth each year is the fourth Thursday, and good luck doing that at scale).\n Beeswarm plot We can add some structure to these points if we use the ggbeeswarm package, with either geom_beeswarm() or geom_quasirandom(). geom_quasirandom() actually works better here since there are so many points—geom_beeswarm() makes the clusters of points way too wide.\nlibrary(ggbeeswarm) ggplot(data = births, mapping = aes(x = day_of_week, y = births, color = weekend)) + scale_color_manual(values = c(\u0026quot;grey70\u0026quot;, \u0026quot;#f2ad22\u0026quot;)) + # Make these points suuuper tiny geom_quasirandom(size = 0.0001) + guides(color = FALSE)  Heatmap Finally, let’s use something non-traditional to show the average births by day in a somewhat proportional way. We can calculate the average number of births every day and then make a heatmap that fills each square by that average, thus showing the relative differences in births per day.\nTo do this, we need to make a summarized data frame with group_by() %\u0026gt;% summarize() to calculate the average number of births by month and day of the month (i.e. average for January 1, January 2, etc.).\nWe’ll then make a sort of calendar with date of the month on the x axis, month on the y axis, with heat map squares filled by the daily average. We’ll use geom_tile() to add squares for each day, and then add some extra scale, coordinates, and theme layers to clean up the plot:\navg_births_month_day \u0026lt;- births %\u0026gt;% group_by(month, date_of_month_categorical) %\u0026gt;% summarize(avg_births = mean(births)) ggplot(data = avg_births_month_day, # By default, the y-axis will have December at the top, so use fct_rev() to reverse it mapping = aes(x = date_of_month_categorical, y = fct_rev(month), fill = avg_births)) + geom_tile() + # Add viridis colors scale_fill_viridis_c(option = \u0026quot;inferno\u0026quot;, labels = comma) + # Add nice labels labs(x = \u0026quot;Day of the month\u0026quot;, y = NULL, title = \u0026quot;Average births per day\u0026quot;, subtitle = \u0026quot;1994-2014\u0026quot;, fill = \u0026quot;Average births\u0026quot;) + # Force all the tiles to have equal widths and heights coord_equal() + # Use a cleaner theme theme_minimal() Neat! There are some really interesting trends here. Most obvious, probably, is that very few people are born on New Year’s Day, July 4th, Halloween, Thanksgiving, and Christmas.\navg_births_month_day %\u0026gt;% arrange(avg_births) ## # A tibble: 366 x 3 ## # Groups: month [12] ## month date_of_month_categorical avg_births ## \u0026lt;ord\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 December 25 6601. ## 2 January 1 7827. ## 3 December 24 8103. ## 4 July 4 8825. ## 5 January 2 9356. ## 6 December 26 9599. ## 7 November 27 9770. ## 8 November 23 9919. ## 9 November 25 10001 ## 10 October 31 10030. ## # … with 356 more rows The days with the highest average are in mid-September (lol my birthday is #2), likely because that’s about 9 months after the first week of January. July 7th at #7 is odd and I have no idea why it might be so popular 🤷.\navg_births_month_day %\u0026gt;% arrange(desc(avg_births)) ## # A tibble: 366 x 3 ## # Groups: month [12] ## month date_of_month_categorical avg_births ## \u0026lt;ord\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 September 9 12344. ## 2 September 19 12285. ## 3 September 12 12282. ## 4 September 17 12201. ## 5 September 10 12190. ## 6 September 20 12162. ## 7 July 7 12147. ## 8 September 15 12126. ## 9 September 16 12114. ## 10 September 18 12112. ## # … with 356 more rows The funniest trend is the very visible dark column for the 13th of every month. People really don’t want to give birth on the 13th.\n  ","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"c977a82d5f9c9c6bf14f43f56de0b41e","permalink":"/example/04-example/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/example/04-example/","section":"example","summary":"Live coding example Complete code  Load data Wrangle data Bar plot Lollipop chart Strip plot Beeswarm plot Heatmap    For this example, we’re going to use real world data to demonstrate some different ways to visualize amounts and proportions. We’ll use data from the CDC and the Social Security Administration about the number of daily births in the United States from 1994–2014. FiveThirtyEight reported a story using this data in 2016 and they posted relatively CSV files on GitHub, so we can download and use those.","tags":null,"title":"Amounts and proportions","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Essential pandemic construction Turning everything in   Getting started The New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided an interactive map here where you can see the different projects. There’s also a link there to download the complete dataset.\nFor this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).\nYou’ll be doing all your R work in R Markdown this time (and from now on). You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nYou’ll need to download one CSV file and put it somewhere on your computer or upload it to RStudio.cloud—preferably in a folder named data in your project folder. You can download the data from the DOB’s map, or use this link to get it directly:\n  EssentialConstruction.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  04-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 04-exercise.Rmd your-project-name.Rproj data\\ EssentialConstruction.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  04-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Essential pandemic construction Make the following plots and briefly explain what they show:\n Show the count or proportion of approved projects by borough using a bar chart\n Show the count or proportion of approved projects by category using a lollipop chart\n Show the proportion of approved projects by borough and category simultaneously using a heatmap\n  The example for today’s session will be incredibly helpful for this exercise. Reference it.\nYou don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing fill colors with scale_fill_manual() or with viridis palettes.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"bac4074c6784201e7a5698eed092fcc9","permalink":"/lab/04-exercise/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/lab/04-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Essential pandemic construction Turning everything in   Getting started The New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided an interactive map here where you can see the different projects. There’s also a link there to download the complete dataset.\nFor this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).","tags":null,"title":"Amounts and proportions","type":"docs"},{"authors":null,"categories":null,"content":"  Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n Unzipping files on Windows tl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n ","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"  There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.\n Kaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n 360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\n US City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n Political science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n François Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities. Thomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.). Erik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"   Basic plot Nice pre-built themes Bonus: ggthemeassist Saving plots   The lesson for today’s session is a fairly comprehensive introduction to using the theme() function in ggplot, and this page by Henry Wang is a good cheat sheet for remembering which theme elements are which on a plot.\nFor your exercise, you’re going to create the world’s ugliest plot. For this example, we’ll use the principles of CRAP to make a great theme.\nI’m going to build the theme semi-incrementally here. Instead of showing how the plot updates with each change in setting, I do most of the updates all at once, with tons of comments explaining what each line does. Importantly, I did not write this all at once. When you’re tinkering with themes, you generally start with something like theme_minimal() or theme_bw() and then gradually add new things to theme(), like modifying plot.title, then plot.subtitle, etc. It’s a very iterative process with lots of tinkering. Because of this, there is no live-coding video for this example—it would be incredibly long and boring. Instead, look through each of the lines and see what they’re doing.\nFor this example, I’m going to use the gapminder dataset that we’ve been using throughout this week. You can get it if you install the gapminder package in R, or you can download this CSV file (you may need to right click on it and select “Save As…”):\n  gapminder.csv  I’m also going to use the Roboto Condensed font in the theme. Download and install it on your computer if you don’t have it.\nBasic plot When I’m creating a theme, I like to use a basic plot with everything that might show up, complete with a title, subtitle, caption, legend, facets, and other elements.\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(gapminder) # For gapminder data library(scales) # For nice axis labels gapminder_filtered \u0026lt;- gapminder %\u0026gt;% filter(year \u0026gt; 2000) base_plot \u0026lt;- ggplot(data = gapminder_filtered, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + geom_point() + # Use dollars, and get rid of the cents part (i.e. $300 instead of $300.00) scale_x_log10(labels = dollar_format(accuracy = 1)) + # Format with commas scale_size_continuous(labels = comma) + # Use viridis scale_color_viridis_d(option = \u0026quot;plasma\u0026quot;, end = 0.9) + labs(x = \u0026quot;GDP per capita\u0026quot;, y = \u0026quot;Life expectancy\u0026quot;, color = \u0026quot;Continent\u0026quot;, size = \u0026quot;Population\u0026quot;, title = \u0026quot;Here\u0026#39;s a cool title\u0026quot;, subtitle = \u0026quot;And here\u0026#39;s a neat subtitle\u0026quot;, caption = \u0026quot;Source: The Gapminder Project\u0026quot;) + facet_wrap(vars(year)) base_plot Now we have base_plot to work with. Here’s what it looks like with theme_minimal() applied to it:\nbase_plot + theme_minimal() That gets rid of the grey background and is a good start, but we can make lots of improvements. First let’s deal with the gridlines. There are too many. We can get rid of the minor gridlines with by setting them to element_blank():\nbase_plot + theme_minimal() + theme(panel.grid.minor = element_blank()) Next let’s add some typographic contrast. We’ll use Roboto Condensed Regular as the base font. Before trying this, make sure you do the following:\nOn macOS:\n Run capabilities() in your console and verify that TRUE shows up under cairo If not, download and install XQuartz  On Windows:\n Run windowsFonts() in your console and you’ll see a list of all the fonts you can use with R. It’s not a very big list.\n#\u0026gt; $serif #\u0026gt; [1] \u0026quot;TT Times New Roman\u0026quot; #\u0026gt; #\u0026gt; $sans #\u0026gt; [1] \u0026quot;TT Arial\u0026quot; #\u0026gt; #\u0026gt; $mono #\u0026gt; [1] \u0026quot;TT Courier New\u0026quot; You can add Roboto Condensed to your current R session by running this in your console:\nwindowsFonts(`Roboto Condensed` = windowsFont(\u0026quot;Roboto Condensed\u0026quot;)) Now if you run windowsFonts(), you’ll see it in the list:\n#\u0026gt; $serif #\u0026gt; [1] \u0026quot;TT Times New Roman\u0026quot; #\u0026gt; #\u0026gt; $sans #\u0026gt; [1] \u0026quot;TT Arial\u0026quot; #\u0026gt; #\u0026gt; $mono #\u0026gt; [1] \u0026quot;TT Courier New\u0026quot; #\u0026gt; #\u0026gt; $`Roboto Condensed` #\u0026gt; [1] \u0026quot;Roboto Condensed\u0026quot; This only takes effect for your current R session, so if you are knitting a document or if you ever plan on closing RStudio, you’ll need to incorporate this font creation code into your script.\n  We’ll use the font as the base_family argument. Note how I make it bold with face and change the size with rel(). Instead of manually setting some arbitrary size, I use rel() to resize the text in relation to the base_size argument. Using rel(1.7) means 1.7 × base_size, or 20.4 That will rescale according to whatever base_size is—if I shrink it to base_size = 8, the title will scale down accordingly.\nplot_with_good_typography \u0026lt;- base_plot + theme_minimal(base_family = \u0026quot;Roboto Condensed\u0026quot;, base_size = 12) + theme(panel.grid.minor = element_blank(), # Bold, bigger title plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.7)), # Plain, slightly bigger subtitle that is grey plot.subtitle = element_text(face = \u0026quot;plain\u0026quot;, size = rel(1.3), color = \u0026quot;grey70\u0026quot;), # Italic, smaller, grey caption that is left-aligned plot.caption = element_text(face = \u0026quot;italic\u0026quot;, size = rel(0.7), color = \u0026quot;grey70\u0026quot;, hjust = 0), # Bold legend titles legend.title = element_text(face = \u0026quot;bold\u0026quot;), # Bold, slightly larger facet titles that are left-aligned for the sake of repetition strip.text = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.1), hjust = 0), # Bold axis titles axis.title = element_text(face = \u0026quot;bold\u0026quot;), # Add some space above the x-axis title and make it left-aligned axis.title.x = element_text(margin = margin(t = 10), hjust = 0), # Add some space to the right of the y-axis title and make it top-aligned axis.title.y = element_text(margin = margin(r = 10), hjust = 1)) plot_with_good_typography Whoa. That gets us most of the way there! We have good contrast with the typography, with the strong bold and the lighter regular font (✓ contrast). Everything is aligned left (✓ alignment and ✓ repetition). By moving the axis titles a little bit away from the labels, we’ve enhanced proximity, since they were too close together (✓ proximity). We repeat grey in both the caption and the subtitle (✓ repetition).\nThe only thing I don’t like is that the 2002 isn’t quite aligned with the title and subtitle. This is because the facet labels are in boxes along the top of each plot, and in some themes (like theme_grey() and theme_bw()) those facet labels have grey backgrounds. We can turn off the margin in those boxes, or we can add a background, which will then be perfectly aligned with the title and subtitle.\nplot_with_good_typography + # Add a light grey background to the facet titles, with no borders theme(strip.background = element_rect(fill = \u0026quot;grey90\u0026quot;, color = NA), # Add a thin grey border around all the plots to tie in the facet titles panel.border = element_rect(color = \u0026quot;grey90\u0026quot;, fill = NA)) 👩‍🍳 💋! That looks great!\nTo save ourselves time in the future, we can store this whole thing as an object that we can then reuse on other plots:\nmy_pretty_theme \u0026lt;- theme_minimal(base_family = \u0026quot;Roboto Condensed\u0026quot;, base_size = 12) + theme(panel.grid.minor = element_blank(), # Bold, bigger title plot.title = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.7)), # Plain, slightly bigger subtitle that is grey plot.subtitle = element_text(face = \u0026quot;plain\u0026quot;, size = rel(1.3), color = \u0026quot;grey70\u0026quot;), # Italic, smaller, grey caption that is left-aligned plot.caption = element_text(face = \u0026quot;italic\u0026quot;, size = rel(0.7), color = \u0026quot;grey70\u0026quot;, hjust = 0), # Bold legend titles legend.title = element_text(face = \u0026quot;bold\u0026quot;), # Bold, slightly larger facet titles that are left-aligned for the sake of repetition strip.text = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1.1), hjust = 0), # Bold axis titles axis.title = element_text(face = \u0026quot;bold\u0026quot;), # Add some space above the x-axis title and make it left-aligned axis.title.x = element_text(margin = margin(t = 10), hjust = 0), # Add some space to the right of the y-axis title and make it top-aligned axis.title.y = element_text(margin = margin(r = 10), hjust = 1), # Add a light grey background to the facet titles, with no borders strip.background = element_rect(fill = \u0026quot;grey90\u0026quot;, color = NA), # Add a thin grey border around all the plots to tie in the facet titles panel.border = element_rect(color = \u0026quot;grey90\u0026quot;, fill = NA)) Now we can use it on any plot. Remember that first plot you made in your exercise from session 1 with the cars dataset? Let’s throw this theme on it! (only here the dataset is named mpg instead of cars; the mpg dataset is loaded invisibly whenever you load ggplot)\nmpg_example \u0026lt;- ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = class)) + geom_point(size = 3) + scale_color_viridis_d() + facet_wrap(vars(drv)) + labs(x = \u0026quot;Displacement\u0026quot;, y = \u0026quot;Highway MPG\u0026quot;, color = \u0026quot;Car class\u0026quot;, title = \u0026quot;Heavier cars get worse mileage\u0026quot;, subtitle = \u0026quot;Except two-seaters?\u0026quot;, caption = \u0026quot;Here\u0026#39;s a caption\u0026quot;) + my_pretty_theme mpg_example Super neat!\n Nice pre-built themes This custom theme we just made is just one iteration of a theme. There are countless ways to tinker with a theme and have it meet the different CRAP principles. People have even published their own themes in different R packages. Check these out to see lots of different examples:\n hrbrthemes ggthemes ggthemr ggtech tvthemes ggpomological (this one is incredible!)  Check this blog post for examples of a bunch of others\n Bonus: ggthemeassist If you’re intimidated by constantly referring to the documentation and figuring out what little line of code affects which part of the graph, install and check out the ggthemeassist package. It provides an interactive menu for manipulating different theme elements, and then generates all the corresponding code, which is really magical.\nHere’s a brief example of how to use it.\n   Saving plots If we want to save these plots, we can use ggsave(). For that to work, we need to store the plot as an object, which I already did in the examples above:\nname_of_plot_object \u0026lt;- ggplot(...) We then feed our saved plot object to ggsave() and specify the filename and dimensions we want to use. If we’re using PNG, we don’t need to worry about any extra options. If we’re using PDF, we need to tell R to use the Cairo PDF writing engine instead of R’s normal one, since R’s normal one can’t deal with custom fonts.\n# Add my_pretty_theme to the gapminder base_plot and save as an object final_gampinder_plot \u0026lt;- base_plot + my_pretty_theme # Save as PNG and PDF ggsave(\u0026quot;fancy_gapminder.png\u0026quot;, final_gampinder_plot, width = 8, height = 5, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;fancy_gapminder.pdf\u0026quot;, final_gampinder_plot, width = 8, height = 5, units = \u0026quot;in\u0026quot;, device = cairo_pdf) # Save the mpg plot as PNG and PDF ggsave(\u0026quot;fancy_mpg.png\u0026quot;, mpg_example, width = 8, height = 5, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;fancy_mpg.pdf\u0026quot;, mpg_example, width = 8, height = 5, units = \u0026quot;in\u0026quot;, device = cairo_pdf)  ","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"ac11eb4b300887ac6e4fc5c1906670ba","permalink":"/example/05-example/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/example/05-example/","section":"example","summary":"Basic plot Nice pre-built themes Bonus: ggthemeassist Saving plots   The lesson for today’s session is a fairly comprehensive introduction to using the theme() function in ggplot, and this page by Henry Wang is a good cheat sheet for remembering which theme elements are which on a plot.\nFor your exercise, you’re going to create the world’s ugliest plot. For this example, we’ll use the principles of CRAP to make a great theme.","tags":null,"title":"Themes","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: The ugliest plot in the world Turning everything in   Getting started For this assignment, you’re going to work with data compiled by data journalist Duncan Greere related to 48 Soviet dogs who flew as test subjects in USSR’s space program in the 1950s and 60s. The original data can be found here.\nYou’ll need to download one CSV file and put them somewhere on your computer or upload them to RStudio.cloud—preferably in a folder named data in your project folder:\n  Dogs-Database.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to clean up the data a little. Download that here and include it in your project:\n  05-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 05-exercise.Rmd your-project-name.Rproj data\\ Dogs-Database.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  05-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: The ugliest plot in the world For this assignment, you’re going to forget all the wonderful CRAP design principles you just learned and try your hardest to make the ugliest plot in the world. Modify the color scale and change theme elements to make this plot truly hideous.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex and you followed the instructions here for how to make Cairo fonts work with knitted PDFs) of your document.\nInclude a chunk that uses ggsave() to save the plot to your computer as a PNG file.\nUpload the knitted document and the saved PNG file of your plot to iCollege.\n ","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"ef178b1245accaa34fdedbbb18cbede3","permalink":"/lab/05-exercise/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/lab/05-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: The ugliest plot in the world Turning everything in   Getting started For this assignment, you’re going to work with data compiled by data journalist Duncan Greere related to 48 Soviet dogs who flew as test subjects in USSR’s space program in the 1950s and 60s. The original data can be found here.\nYou’ll need to download one CSV file and put them somewhere on your computer or upload them to RStudio.","tags":null,"title":"Themes","type":"docs"},{"authors":null,"categories":null,"content":"  You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"68be32a8da6a38dd54a9e724ab3904a0","permalink":"/resource/citations/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/citations/","section":"resource","summary":"You can download a BibTeX file of all the non-web-based readings in the course:\n  references.bib  You can open the file in BibDesk on macOS, JabRef on Windows, or Zotero or Mendeley online.","tags":null,"title":"Citations and bibliography","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load and clean data Histograms Density plots Box, violin, and rain cloud plots    For this example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  atl-weather-2019.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) library(lubridate) library(ggridges) library(gghalves) Then we load the data with read_csv(). Here I assume that the CSV file lives in a subfolder in my project named data:\nweather_atl_raw \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;) We’ll add a couple columns that we can use for faceting and filling using the month() and wday() functions from lubridate for extracting parts of the date:\nweather_atl \u0026lt;- weather_atl_raw %\u0026gt;% mutate(Month = month(time, label = TRUE, abbr = FALSE), Day = wday(time, label = TRUE, abbr = FALSE)) Now we’re ready to go!\n Histograms We can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:\nggplot(weather_atl, aes(x = windSpeed)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) This is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the boundary argument for that. We also add scale_x_continuous() to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:\nggplot(weather_atl, aes(x = windSpeed)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) We can show the distribution of wind speed by month if we map the Month column we made onto the fill aesthetic:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) This is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) + guides(fill = FALSE) + facet_wrap(vars(Month)) Neat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.\n Density plots The code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the geom layer:\nggplot(weather_atl, aes(x = windSpeed)) + geom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;) If we want, we can mess with some of the calculus options like the kernel and bandwidth:\nggplot(weather_atl, aes(x = windSpeed)) + geom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;, bw = 0.1, kernel = \u0026quot;epanechnikov\u0026quot;) We can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_density(alpha = 0.5) Even with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_density(alpha = 0.5) + guides(fill = FALSE) + facet_wrap(vars(Month)) Or we can stack the density plots behind each other with ggridges. For that to work, we also need to map Month to the y-axis. We can reverse the y-axis so that January is at the top if we use the fct_rev() function:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) + geom_density_ridges() + guides(fill = FALSE) We can add some extra information to geom_density_ridges() with some other arguments like quantile_lines. We can use the quantiles argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) + geom_density_ridges(quantile_lines = TRUE, quantiles = 2) + guides(fill = FALSE) Now that we have good working code, we can easily substitute in other variables by changing the x mapping:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) + geom_density_ridges(quantile_lines = TRUE, quantiles = 2) + guides(fill = FALSE) We can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use geom_density_ridges_gradient(), and we need to change the fill mapping to the strange looking ..x.., which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, fill = temperatureHigh doesn’t work 🤷:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) + geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) + scale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) + labs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;) And finally, we can get extra fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: temperatureLow and temperatureHigh. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like linetype), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using pivot_longer() from tidyr, which was already loaded with library(tidyverse). In the RStudio primers, you did this same thing with gather()—pivot_longer() is the newer version of gather():\nweather_atl_long \u0026lt;- weather_atl %\u0026gt;% pivot_longer(cols = c(temperatureLow, temperatureHigh), names_to = \u0026quot;temp_type\u0026quot;, values_to = \u0026quot;temp\u0026quot;) %\u0026gt;% # Clean up the new temp_type column so that \u0026quot;temperatureHigh\u0026quot; becomes \u0026quot;High\u0026quot;, etc. mutate(temp_type = recode(temp_type, temperatureHigh = \u0026quot;High\u0026quot;, temperatureLow = \u0026quot;Low\u0026quot;)) %\u0026gt;% # This is optional—just select a handful of columns select(time, temp_type, temp, Month) # Show the first few rows head(weather_atl_long) ## # A tibble: 6 x 4 ## time temp_type temp Month ## \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; ## 1 2019-01-01 05:00:00 Low 50.6 January ## 2 2019-01-01 05:00:00 High 63.9 January ## 3 2019-01-02 05:00:00 Low 49.0 January ## 4 2019-01-02 05:00:00 High 57.4 January ## 5 2019-01-03 05:00:00 Low 53.1 January ## 6 2019-01-03 05:00:00 High 55.3 January Now we have a column for the temperature (temp) and a column indicating if it is high or low (temp_type). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the linetype aesthetic to show high/low in the border of the plots:\nggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month), fill = ..x.., linetype = temp_type)) + geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) + scale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) + labs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;) Super neat! We can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.\n Box, violin, and rain cloud plots Finally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the Day variable we made indicating weekday:\nggplot(weather_atl, aes(y = windSpeed, fill = Day)) + geom_boxplot() We can switch this to a violin plot by just changing the geom layer and mapping Day to the x-axis:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() With violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) We can also add larger points for the daily averages. We’ll use a special layer for this: stat_summary(). It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here \"mean\") and then plotting that result:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + stat_summary(geom = \u0026quot;point\u0026quot;, fun = \u0026quot;mean\u0026quot;, size = 5, color = \u0026quot;white\u0026quot;) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) We can also show the mean and confidence interval at the same time by changing the summary function:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, size = 1, color = \u0026quot;white\u0026quot;) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) Overlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use the gghalves package, we can use special halved versions of some of these geoms like so:\nggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) + geom_half_boxplot(aes(fill = Day), side = \u0026quot;r\u0026quot;) + guides(color = FALSE, fill = FALSE) Note the side argument for specifying which half of the column the geom goes. We can also use geom_half_violin():\nggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) + geom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) + guides(color = FALSE, fill = FALSE) If we flip the plot, we can make a rain cloud plot:\nggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_boxplot(aes(fill = Day), side = \u0026quot;l\u0026quot;, width = 0.5, nudge = 0.1) + geom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) + geom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) + guides(color = FALSE, fill = FALSE) + coord_flip() Neat!\n  ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"a66b8951081722f5755747f3314e83f0","permalink":"/example/06-example/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/example/06-example/","section":"example","summary":"Live coding example Complete code  Load and clean data Histograms Density plots Box, violin, and rain cloud plots    For this example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):","tags":null,"title":"Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Visualizing uncertainty with gapminder Turning everything in   Getting started For this exercise you’ll revisit Hans Rosling’s gapminder data on health and wealth.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nYou don’t need to download any CSV files for this assignment. If you run library(gapminder) you’ll have access to a data frame named gapminder that contains all the data.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  06-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 06-exercise.Rmd your-project-name.Rproj To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  06-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Visualizing uncertainty with gapminder Make the following plots and briefly explain what they show:\n Make a histogram of logged GDP per capita for 1997 only, across all five continents\n Make a ridge plot of global life expectancy over time, from 1952 to 2007. You’ll need to use the full gapminder data, not the 1997-only data. Each ridge should show the distribution of the world’s life expectancy for each given year (similar to the temperature ridge plot in the example).\nImportant note: year will be on the y-axis, but it must be a categorical variable to work with ggridges, so you’ll either need to wrap it in as.factor() like aes(..., y = as.factor(year)), or add a new categorical/factor year column to the gapminder dataset with mutate().\n Make a filtered dataset that selects data from only 2007 and removes Oceania. Show the distribution of logged GDP per capita across the four continents using some combination of boxplots and/or violin plots and/or strip plots, either overlaid on top of each other, or using their geom_half_*() counterparts from gghalves.\n  The example for today’s session will be incredibly helpful for this exercise. Reference it.\nYou don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"2bc7c28b3388fae714cf12e0ad4070bc","permalink":"/lab/06-exercise/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/lab/06-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Visualizing uncertainty with gapminder Turning everything in   Getting started For this exercise you’ll revisit Hans Rosling’s gapminder data on health and wealth.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.","tags":null,"title":"Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load and clean data Legal dual y-axes Combining plots Scatterplot matrices Correlograms Simple regression Coefficient plots Marginal effects plots    For this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  atl-weather-2019.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(patchwork) # For combining ggplot plots library(GGally) # For scatterplot matrices library(broom) # For converting model objects to data frames Then we load the data with read_csv(). Here I assume that the CSV file lives in a subfolder in my project named data:\nweather_atl \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;)  Legal dual y-axes It is fine (and often helpful!) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.\nTo do this, you need to add an argument (sec.axis) to scale_y_continuous() to tell it to use a second axis. This sec.axis argument takes a sec_axis() function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a ~, which indicates that it’s a function, and it needs to use . to stand in for the original value in the original axis.\nSince the equation for converting Fahrenheit to Celsius is this…\n\\[ \\text{C} = (32 - \\text{F}) \\times -\\frac{5}{9} \\]\n…we can specify this with code like so (where . stands for the Fahrenheit value):\n~ (32 - .) * -5 / 9 Here’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) + geom_line() + scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9, name = \u0026quot;Celsius\u0026quot;)) + labs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) + theme_minimal() For fun, we could also convert it to Kelvin, which uses this formula:\n\\[ \\text{K} = (\\text{F} - 32) \\times \\frac{5}{9} + 273.15 \\]\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) + geom_line() + scale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15, name = \u0026quot;Kelvin\u0026quot;)) + labs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) + theme_minimal()  Combining plots A good alternative to using two y-axes is to use two plots instead. The patchwork package makes this really easy to do with R. There are other similar packages that do this, like cowplot and gridExtra, but I’ve found that patchwork is the easiest to use and it actually aligns the different plot elements like axis lines and legends (yay alignment in CRAP!). The documentation for patchwork is really great and full of examples—you should check it out to see all the things you can do with it!\nTo use patchwork, we need to (1) save our plots as objects and (2) add them together with +.\nFor instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:\n# Temperature in Atlanta temp_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) + geom_line() + geom_smooth() + scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9, name = \u0026quot;Celsius\u0026quot;)) + labs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) + theme_minimal() temp_plot # Humidity in Atlanta humidity_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = humidity)) + geom_line() + geom_smooth() + labs(x = NULL, y = \u0026quot;Humidity\u0026quot;) + theme_minimal() humidity_plot Right now, these are two separate plots, but we can combine them with + if we load patchwork:\nlibrary(patchwork) temp_plot + humidity_plot By default, patchwork will put these side-by-side, but we can change that with the plot_layout() function:\ntemp_plot + humidity_plot + plot_layout(ncol = 1) We can also play with other arguments in plot_layout(). If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:\ntemp_plot + humidity_plot + plot_layout(ncol = 1, heights = c(0.7, 0.3))  Scatterplot matrices We can visualize the correlations between pairs of variables with the ggpairs() function in the GGally package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into ggpairs() to see all the correlation information:\nlibrary(GGally) weather_correlations \u0026lt;- weather_atl %\u0026gt;% select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) ggpairs(weather_correlations) It looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind spped and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).\nEven though ggpairs() doesn’t use the standard ggplot(...) + geom_whatever() syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:\nggpairs(weather_correlations) + labs(title = \u0026quot;Correlations!\u0026quot;) + theme_dark()  Correlograms Scatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In the readings for today, Claus Wilke showed a type of plot called a correlogram which is more appropriate for publication.\nThese are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.\nFirst we need to build a correlation matrix of the main variables we care about. Ordinarily the cor() function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into cor() though, it’ll calculate the correlation between each pair of columns\n# Create a correlation matrix things_to_correlate \u0026lt;- weather_atl %\u0026gt;% select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %\u0026gt;% cor() things_to_correlate ## temperatureHigh temperatureLow humidity windSpeed precipProbability ## temperatureHigh 1.00 0.920 -0.030 -0.377 -0.124 ## temperatureLow 0.92 1.000 0.112 -0.450 -0.026 ## humidity -0.03 0.112 1.000 0.011 0.722 ## windSpeed -0.38 -0.450 0.011 1.000 0.196 ## precipProbability -0.12 -0.026 0.722 0.196 1.000 The two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to NA):\n# Get rid of the lower triangle things_to_correlate[lower.tri(things_to_correlate)] \u0026lt;- NA things_to_correlate ## temperatureHigh temperatureLow humidity windSpeed precipProbability ## temperatureHigh 1 0.92 -0.03 -0.377 -0.124 ## temperatureLow NA 1.00 0.11 -0.450 -0.026 ## humidity NA NA 1.00 0.011 0.722 ## windSpeed NA NA NA 1.000 0.196 ## precipProbability NA NA NA NA 1.000 Finally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the things_to_correlate matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named measure1, and take all the correlation numbers and put them in a column named cor In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)\nthings_to_correlate_long \u0026lt;- things_to_correlate %\u0026gt;% # Convert from a matrix to a data frame as.data.frame() %\u0026gt;% # Matrixes have column names that don\u0026#39;t get converted to columns when using # as.data.frame(), so this adds those names as a column rownames_to_column(\u0026quot;measure2\u0026quot;) %\u0026gt;% # Make this long. Take all the columns except measure2 and put their names in # a column named measure1 and their values in a column named cor pivot_longer(cols = -measure2, names_to = \u0026quot;measure1\u0026quot;, values_to = \u0026quot;cor\u0026quot;) %\u0026gt;% # Make a new column with the rounded version of the correlation value mutate(nice_cor = round(cor, 2)) %\u0026gt;% # Remove rows where the two measures are the same (like the correlation # between humidity and humidity) filter(measure2 != measure1) %\u0026gt;% # Get rid of the empty triangle filter(!is.na(cor)) %\u0026gt;% # Put these categories in order mutate(measure1 = fct_inorder(measure1), measure2 = fct_inorder(measure2)) things_to_correlate_long ## # A tibble: 10 x 4 ## measure2 measure1 cor nice_cor ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 temperatureHigh temperatureLow 0.920 0.92 ## 2 temperatureHigh humidity -0.0301 -0.03 ## 3 temperatureHigh windSpeed -0.377 -0.38 ## 4 temperatureHigh precipProbability -0.124 -0.12 ## 5 temperatureLow humidity 0.112 0.11 ## 6 temperatureLow windSpeed -0.450 -0.45 ## 7 temperatureLow precipProbability -0.0255 -0.03 ## 8 humidity windSpeed 0.0108 0.01 ## 9 humidity precipProbability 0.722 0.72 ## 10 windSpeed precipProbability 0.196 0.2 Phew. With the data all tidied like that, we can make a correlogram with a heatmap. This is just like the heatmap you made in session 4, but here we manipulate the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.\nggplot(things_to_correlate_long, aes(x = measure2, y = measure1, fill = cor)) + geom_tile() + geom_text(aes(label = nice_cor)) + scale_fill_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;, limits = c(-1, 1)) + labs(x = NULL, y = NULL) + coord_equal() + theme_minimal() + theme(panel.grid = element_blank()) Instead of using a heatmap, we can also use points, which encode the correlation information both as color and as size. To do that, we just need to switch geom_tile() to geom_point() and set the size = cor mapping:\nggplot(things_to_correlate_long, aes(x = measure2, y = measure1, color = cor)) + # Size by the absolute value so that -0.7 and 0.7 are the same size geom_point(aes(size = abs(cor))) + scale_color_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;, limits = c(-1, 1)) + scale_size_area(max_size = 15, limits = c(-1, 1), guide = FALSE) + labs(x = NULL, y = NULL) + coord_equal() + theme_minimal() + theme(panel.grid = element_blank())  Simple regression We can also visualize the relationships between variables using regression. Simple regression is easy to visualize, since you’re only working with an X and a Y. For instance, what’s the relationship between humidity and high temperatures during the summer?\nFirst, let’s filter the data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.\nweather_atl_summer \u0026lt;- weather_atl %\u0026gt;% filter(time \u0026gt;= \u0026quot;2019-05-01\u0026quot;, time \u0026lt;= \u0026quot;2019-09-30\u0026quot;) %\u0026gt;% mutate(humidity_scaled = humidity * 100, moonPhase_scaled = moonPhase * 100, precipProbability_scaled = precipProbability * 100, cloudCover_scaled = cloudCover * 100) Then we can build a simple regression model:\nmodel_simple \u0026lt;- lm(temperatureHigh ~ humidity_scaled, data = weather_atl_summer) tidy(model_simple, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 104. 2.35 44.3 1.88e-88 99.5 109. ## 2 humidity_scaled -0.241 0.0358 -6.74 3.21e-10 -0.312 -0.170 We can interpret these coefficients like so:\n The intercept shows that the average temperature when there’s 0% humidity is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line. The coefficient for humidity_scaled shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.  Visualizing this model is simple, since there are only two variables:\nggplot(weather_atl_summer, aes(x = humidity_scaled, y = temperatureHigh)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; And indeed, as humidity increases, temperatures decrease.\n Coefficient plots But if we use multiple variables in the model, it gets really hard to visualize the results since we’re working with multiple dimensions. Instead, we can use coefficient plots to see the individual coefficients in the model.\nFirst, let’s build a more complex model:\nmodel_complex \u0026lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled + precipProbability_scaled + windSpeed + pressure + cloudCover_scaled, data = weather_atl_summer) tidy(model_complex, conf.int = TRUE) ## # A tibble: 7 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 262. 125. 2.09 0.0380 14.8 510. ## 2 humidity_scaled -0.111 0.0757 -1.47 0.143 -0.261 0.0381 ## 3 moonPhase_scaled 0.0116 0.0126 0.917 0.360 -0.0134 0.0366 ## 4 precipProbability_scaled 0.0356 0.0203 1.75 0.0820 -0.00458 0.0758 ## 5 windSpeed -1.78 0.414 -4.29 0.0000326 -2.59 -0.958 ## 6 pressure -0.157 0.122 -1.28 0.203 -0.398 0.0854 ## 7 cloudCover_scaled -0.0952 0.0304 -3.14 0.00207 -0.155 -0.0352 We can interpret these coefficients like so:\n Holding everything else constant, a 1% increase in humidity is associated with a 0.11° decrease in the high temperature, on average, but the effect is not statistically significant Holding everything else constant, a 1% increase in moon visibility is associated with a 0.01° increase in the high temperature, on average, and the effect is not statistically significant Holding everything else constant, a 1% increase in the probability of precipitation is associated with a 0.04° increase in the high temperature, on average, and the effect is not statistically significant Holding everything else constant, a 1 mph increase in the wind speed is associated with a 1.8° decrease in the high temperature, on average, and the effect is statistically significant Holding everything else constant, a 1 unit increase in barometric pressure is associated with a 0.15° decrease in the high temperature, on average, and the effect is not statistically significant Holding everything else constant, a 1% increase in cloud cover is associated with a 0.01° decrease in the high temperature, on average, and the effect is statistically significant The intercept is pretty useless. It shows that the predicted temperature will be 262° when humidity is 0%, the moon is invisible, there’s no chance of precipitation, no wind, no barometric pressure, and no cloud cover. Yikes.  To plot all these things at once, we’ll store the results of tidy(model_complex) as a data frame, remove the useless intercept, and plot it using geom_pointrange():\nmodel_tidied \u0026lt;- tidy(model_complex, conf.int = TRUE) %\u0026gt;% filter(term != \u0026quot;(Intercept)\u0026quot;) ggplot(model_tidied, aes(x = estimate, y = term)) + geom_vline(xintercept = 0, color = \u0026quot;red\u0026quot;, linetype = \u0026quot;dotted\u0026quot;) + geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) + labs(x = \u0026quot;Coefficient estimate\u0026quot;, y = NULL) + theme_minimal() Neat! Now we can see how big these different coefficients are and how close they are to zero. Wind speed has a big significant effect on temperature. The others are all very close to zero.\n Marginal effects plots Instead of just looking at the coefficients, we can also see the effect of moving different variables up and down like sliders and switches. Remember that regression coefficients allow us to build actual mathematical formulas that predict the value of Y. The coefficients from model_compex yield the following big hairy ugly equation:\n\\[ \\begin{aligned} \\hat{\\text{High temperature}} =\u0026amp; 262 - 0.11 \\times \\text{humidity_scaled } \\\\ \u0026amp; + 0.01 \\times \\text{moonPhase_scaled } + 0.04 \\times \\text{precipProbability_scaled } \\\\ \u0026amp; - 1.78 \\times \\text{windSpeed} - 0.16 \\times \\text{pressure} - 0.095 \\times \\text{cloudCover_scaled} \\end{aligned} \\]\nIf we plug in values for each of the explanatory variables, we can get the predicted value of high temperature, or \\(\\hat{y}\\).\nThe augment() function in the broom library allows us to take a data frame of explanatory variable values, plug them into the model equation, and get predictions out. For example, let’s set each of the variables to some arbitrary values (50% for humidity, moon phase, chance of rain, and cloud cover; 1000 for pressure, and 1 MPH for wind speed).\nnewdata_example \u0026lt;- tibble(humidity_scaled = 50, moonPhase_scaled = 50, precipProbability_scaled = 50, windSpeed = 1, pressure = 1000, cloudCover_scaled = 50) newdata_example ## # A tibble: 1 x 6 ## humidity_scaled moonPhase_scaled precipProbability_scaled windSpeed pressure cloudCover_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 50 50 50 1 1000 50 We can plug these values into the model with augment():\n# I use select() here because augment() returns columns for all the explanatory # variables, and the .fitted column with the predicted value is on the far right # and gets cut off augment(model_complex, newdata = newdata_example) %\u0026gt;% select(.fitted, .se.fit) ## # A tibble: 1 x 2 ## .fitted .se.fit ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 96.2 3.19 Given these weather conditions, the predicted high temperature is 96.2°. Now you’re an armchair meteorologist!\nWe can follow the same pattern to show how the predicted temperature changes as specific variables change across a whole range. Here, we create a data frame of possible wind speeds and keep all the other explanatory variables at their means:\nnewdata \u0026lt;- tibble(windSpeed = seq(0, 8, 0.5), pressure = mean(weather_atl_summer$pressure), precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled), moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled), humidity_scaled = mean(weather_atl_summer$humidity_scaled), cloudCover_scaled = mean(weather_atl_summer$cloudCover_scaled)) newdata ## # A tibble: 17 x 6 ## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 1016. 40.2 50.7 64.8 29.5 ## 2 0.5 1016. 40.2 50.7 64.8 29.5 ## 3 1 1016. 40.2 50.7 64.8 29.5 ## 4 1.5 1016. 40.2 50.7 64.8 29.5 ## 5 2 1016. 40.2 50.7 64.8 29.5 ## 6 2.5 1016. 40.2 50.7 64.8 29.5 ## 7 3 1016. 40.2 50.7 64.8 29.5 ## 8 3.5 1016. 40.2 50.7 64.8 29.5 ## 9 4 1016. 40.2 50.7 64.8 29.5 ## 10 4.5 1016. 40.2 50.7 64.8 29.5 ## 11 5 1016. 40.2 50.7 64.8 29.5 ## 12 5.5 1016. 40.2 50.7 64.8 29.5 ## 13 6 1016. 40.2 50.7 64.8 29.5 ## 14 6.5 1016. 40.2 50.7 64.8 29.5 ## 15 7 1016. 40.2 50.7 64.8 29.5 ## 16 7.5 1016. 40.2 50.7 64.8 29.5 ## 17 8 1016. 40.2 50.7 64.8 29.5 If we feed this big data frame into augment(), we can get the predicted high temperature for each row. We can also use the .se.fit column to calculate the 95% confidence interval for each predicted value. We take the standard error, multiply it by -1.96 and 1.96 (or the quantile of the normal distribution at 2.5% and 97.5%), and add that value to the estimate.\npredicted_values \u0026lt;- augment(model_complex, newdata = newdata) %\u0026gt;% mutate(conf.low = .fitted + (-1.96 * .se.fit), conf.high = .fitted + (1.96 * .se.fit)) predicted_values %\u0026gt;% select(windSpeed, .fitted, .se.fit, conf.low, conf.high) %\u0026gt;% head() ## # A tibble: 6 x 5 ## windSpeed .fitted .se.fit conf.low conf.high ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 95.3 1.63 92.2 98.5 ## 2 0.5 94.5 1.42 91.7 97.2 ## 3 1 93.6 1.22 91.2 96.0 ## 4 1.5 92.7 1.03 90.7 94.7 ## 5 2 91.8 0.836 90.1 93.4 ## 6 2.5 90.9 0.653 89.6 92.2 Cool! Just looking at the data in the table, we can see that predicted temperature drops as windspeed increases. We can plot this to visualize the effect:\nggplot(predicted_values, aes(x = windSpeed, y = .fitted)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \u0026quot;#BF3984\u0026quot;, alpha = 0.5) + geom_line(size = 1, color = \u0026quot;#BF3984\u0026quot;) + labs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) + theme_minimal() We just manipulated one of the model coefficients and held everything else at its mean. We can manipulate multiple variables too and encode them all on the graph. For instance, what is the effect of windspeed and cloud cover on the temperature?\nWe’ll follow the same process, but vary both windSpeed and cloudCover_scaled. Instead of using tibble(), we use exapnd_grid(), which creates every combination of the variables we specify. Instead of varying cloud cover by every possible value (like from 0 to 100), we’ll choose four possible cloud cover types: 0%, 33%, 66%, and 100%. Everything else will be at its mean.\nnewdata_fancy \u0026lt;- expand_grid(windSpeed = seq(0, 8, 0.5), pressure = mean(weather_atl_summer$pressure), precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled), moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled), humidity_scaled = mean(weather_atl_summer$humidity_scaled), cloudCover_scaled = c(0, 33, 66, 100)) newdata_fancy ## # A tibble: 68 x 6 ## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 1016. 40.2 50.7 64.8 0 ## 2 0 1016. 40.2 50.7 64.8 33 ## 3 0 1016. 40.2 50.7 64.8 66 ## 4 0 1016. 40.2 50.7 64.8 100 ## 5 0.5 1016. 40.2 50.7 64.8 0 ## 6 0.5 1016. 40.2 50.7 64.8 33 ## 7 0.5 1016. 40.2 50.7 64.8 66 ## 8 0.5 1016. 40.2 50.7 64.8 100 ## 9 1 1016. 40.2 50.7 64.8 0 ## 10 1 1016. 40.2 50.7 64.8 33 ## # … with 58 more rows Notice now that windSpeed repeats four times (0, 0, 0, 0, 0.5, 0.5, etc.), since there are four possible cloudCover_scaled values (0, 33, 66, 100).\nWe can plot this now, just like before, with wind speed on the x-axis, the predicted temperature on the y-axis, and colored and faceted by cloud cover:\npredicted_values_fancy \u0026lt;- augment(model_complex, newdata = newdata_fancy) %\u0026gt;% mutate(conf.low = .fitted + (-1.96 * .se.fit), conf.high = .fitted + (1.96 * .se.fit)) %\u0026gt;% # Make cloud cover a categorical variable mutate(cloudCover_scaled = factor(cloudCover_scaled)) ggplot(predicted_values_fancy, aes(x = windSpeed, y = .fitted)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled), alpha = 0.5) + geom_line(aes(color = cloudCover_scaled), size = 1) + labs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) + theme_minimal() + guides(fill = FALSE, color = FALSE) + facet_wrap(vars(cloudCover_scaled), nrow = 1) That’s so neat! Temperatures go down slightly as cloud cover increases. If we wanted to improve the model, we’d add an interaction term between cloud cover and windspeed so that each line would have a different slope in addition to a different intercept, but that’s beyond the scope of this class.\n  ","date":1589846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"40eeabbfe9d21a7292000fc4fe538d43","permalink":"/example/07-example/","publishdate":"2020-05-19T00:00:00Z","relpermalink":"/example/07-example/","section":"example","summary":"Live coding example Complete code  Load and clean data Legal dual y-axes Combining plots Scatterplot matrices Correlograms Simple regression Coefficient plots Marginal effects plots    For this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. I downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.","tags":null,"title":"Relationships","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Combining plots Task 3: Visualizing regression Coefficient plot Marginal effects  Bonus task! Correlograms Turning everything in   Getting started For this exercise you’ll use precinct-level data from the 2016 presidential election to visualize relationships between variables. This data comes from the MIT Election Data and Science Lab.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\n  results_2016.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  07-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 07-exercise.Rmd your-project-name.Rproj data\\ results_2016.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  07-exercise.zip  The example for today’s session will be incredibly helpful for this exercise. Reference it.\nAgain, you don’t need to make your plots super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Combining plots Make 2–3 plots of anything you want from the results_2016 data (histogram, density, boxplot, scatterplot, whatever) and combine them with patchwork. Look at the documentation to see fancy ways of combining them, like having two rows inside a column.\n Task 3: Visualizing regression Coefficient plot Use the results_2016 data to create a model that predicts the percent of Democratic votes in a precinct based on age, race, income, rent, and state (hint: the formula will look like this: percent_dem ~ median_age + percent_white + per_capita_income + median_rent + state)\nUse tidy() in the broom package and geom_pointrange() to create a coefficient plot for the model estimates. You’ll have 50 rows for all the states, and that’s excessive for a plot like this, so you’ll want to filter out the state rows. You can do that by adding this:\ntidy(...) %\u0026gt;% filter(!str_detect(term, \u0026quot;state\u0026quot;)) The str_detect() function looks for the characters “state” in the term column. The ! negates it. This is thus saying “only keep rows where the word ‘state’ is not in the term name”.\nYou should also get rid of the intercept (filter(term != \"(Intercept)\")).\n Marginal effects Create a new data frame with tibble() that contains a column for the average value for each variable in your model except for one, which you vary. For state, you’ll need to choose a single state. The new dataset should look something like this (though this is incomplete! You’ll need to include all the variables in your model, and you’ll need to vary one using seq()) (like seq(9000, 60000, by = 100) for per_capita_income). The na.rm argument in mean() here makes it so missing values are removed—without it, R can’t calculate the mean and will return NA instead.\ndata_to_predict \u0026lt;- tibble(median_age = mean(results_2016$median_age, na.rm = TRUE), percent_white = mean(results_2016$percent_white, na.rm = TRUE), state = \u0026quot;Georgia\u0026quot;) # Or whatever Use augment() to generate predictions from this dataset using the model you created before. Plot your varied variable on the x-axis, the fitted values (.fitted) on the y-axis, show the relationship with a line, and add a ribbon to show the 95% confidence interval.\n  Bonus task! Correlograms This is entirely optional but might be fun.\nFor extra fun times, if you feel like it, create a correlogram heatmap, either with geom_tile() or with points sized by the correlation. Use any variables you want from results_2016.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1589846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"d6c8379d745bb544b13d2cc449984b4e","permalink":"/lab/07-exercise/","publishdate":"2020-05-19T00:00:00Z","relpermalink":"/lab/07-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Combining plots Task 3: Visualizing regression Coefficient plot Marginal effects  Bonus task! Correlograms Turning everything in   Getting started For this exercise you’ll use precinct-level data from the 2016 presidential election to visualize relationships between variables. This data comes from the MIT Election Data and Science Lab.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.","tags":null,"title":"Relationships","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load and clean data Small multiples Sparklines Slopegraphs Bump charts    For this example, we’re going to use cross-national data, but instead of using the typical gapminder dataset, we’re going to collect data directly from the World Bank’s Open Data portal\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  wdi_raw.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(WDI) # For getting data from the World Bank library(geofacet) # For map-shaped facets library(scales) # For helpful scale functions like dollar() library(ggrepel) # For non-overlapping labels The World Bank has a ton of country-level data at data.worldbank.org. We can use a package named WDI (world development indicators) to access their servers and download the data directly into R.\nTo do this, we need to find the special World Bank codes for specific variables we want to get. These codes come from the URLs of the World Bank’s website. For instance, if you search for “access to electricity” at the World Bank’s website, you’ll find this page. If you look at the end of the URL, you’ll see a cryptic code: EG.ELC.ACCS.ZS. That’s the World Bank’s ID code for the “Access to electricity (% of population)” indicator.\nWe can feed a list of ID codes to the WDI() function to download data for those specific indicators. We want data from 1995-2015, so we set the start and end years accordingly. The extra=TRUE argument means that it’ll also include other helpful details like region, aid status, etc. Without it, it would only download the indicators we listed.\nindicators \u0026lt;- c(\u0026quot;SP.DYN.LE00.IN\u0026quot;, # Life expectancy \u0026quot;EG.ELC.ACCS.ZS\u0026quot;, # Access to electricity \u0026quot;EN.ATM.CO2E.PC\u0026quot;, # CO2 emissions \u0026quot;NY.GDP.PCAP.KD\u0026quot;) # GDP per capita wdi_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE, start = 1995, end = 2015) head(wdi_raw) Downloading data from the World Bank every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). It’s good practice to save this raw data as a CSV file and then work with that.\nwrite_csv(wdi_raw, \u0026quot;data/wdi_raw.csv\u0026quot;) Since we care about reproducibility, we still want to include the code we used to get data from the World Bank, we just don’t want it to actually run. You can include chunks but not run them by setting eval=FALSE in the chunk options. In this little example, we show the code for downloading the data, but we don’t evaluate the chunk. We then include a chunk that loads the data from a CSV file with read_csv(), but we don’t include it (include=FALSE). That way, in the knitted file we see the WDI() code, but in reality it’s loading the data from CSV. Super tricky.\nI first download data from the World Bank: ```{r get-wdi-data, eval=FALSE} wdi_raw \u0026lt;- WDI(...) write_csv(wdi_raw, \u0026quot;data/wdi_raw.csv\u0026quot;) ``` ```{r load-wdi-data-real, include=FALSE} wdi_raw \u0026lt;- read_csv(\u0026quot;data/wdi_raw.csv\u0026quot;) ``` Then we clean up the data a little, filtering out rows that aren’t actually countries and renaming the ugly World Bank code columns to actual words:\nwdi_clean \u0026lt;- wdi_raw %\u0026gt;% filter(region != \u0026quot;Aggregates\u0026quot;) %\u0026gt;% select(iso2c, country, year, life_expectancy = SP.DYN.LE00.IN, access_to_electricity = EG.ELC.ACCS.ZS, co2_emissions = EN.ATM.CO2E.PC, gdp_per_cap = NY.GDP.PCAP.KD, region, income) head(wdi_clean) ## # A tibble: 6 x 9 ## iso2c country year life_expectancy access_to_electricity co2_emissions gdp_per_cap region income ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AD Andorra 2015 NA 100 NA 41768. Europe \u0026amp; Central Asia High income ## 2 AD Andorra 2004 NA 100 7.36 47033. Europe \u0026amp; Central Asia High income ## 3 AD Andorra 2001 NA 100 7.79 41421. Europe \u0026amp; Central Asia High income ## 4 AD Andorra 2002 NA 100 7.59 42396. Europe \u0026amp; Central Asia High income ## 5 AD Andorra 2014 NA 100 5.83 40790. Europe \u0026amp; Central Asia High income ## 6 AD Andorra 1995 NA 100 6.66 32918. Europe \u0026amp; Central Asia High income  Small multiples First we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country.\nlife_expectancy_small \u0026lt;- wdi_clean %\u0026gt;% filter(country %in% c(\u0026quot;Argentina\u0026quot;, \u0026quot;Bolivia\u0026quot;, \u0026quot;Brazil\u0026quot;, \u0026quot;Belize\u0026quot;, \u0026quot;Canada\u0026quot;, \u0026quot;Chile\u0026quot;)) ggplot(data = life_expectancy_small, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country)) Small multiples! That’s all we need to do.\nWe can do some fancier things, though. We can make this plot hyper minimalist:\nggplot(data = life_expectancy_small, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country), scales = \u0026quot;free_y\u0026quot;) + theme_void() + theme(strip.text = element_text(face = \u0026quot;bold\u0026quot;)) We can do a whole part of a continent (poor Iraq and Syria 😭)\nlife_expectancy_mena \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;Middle East \u0026amp; North Africa\u0026quot;) ggplot(data = life_expectancy_mena, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country), scales = \u0026quot;free_y\u0026quot;, nrow = 3) + theme_void() + theme(strip.text = element_text(face = \u0026quot;bold\u0026quot;)) We can use the geofacet package to arrange these facets by geography:\nlife_expectancy_eu \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;Europe \u0026amp; Central Asia\u0026quot;) ggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_geo(vars(country), grid = \u0026quot;eu_grid1\u0026quot;, scales = \u0026quot;free_y\u0026quot;) + labs(x = NULL, y = NULL, title = \u0026quot;Life expectancy from 1995–2015\u0026quot;, caption = \u0026quot;Source: The World Bank (SP.DYN.LE00.IN)\u0026quot;) + theme_minimal() + theme(strip.text = element_text(face = \u0026quot;bold\u0026quot;), plot.title = element_text(face = \u0026quot;bold\u0026quot;), axis.text.x = element_text(angle = 45, hjust = 1)) Neat!\n Sparklines Sparklines are just line charts (or bar charts) that are really really small.\nindia_co2 \u0026lt;- wdi_clean %\u0026gt;% filter(country == \u0026quot;India\u0026quot;) plot_india \u0026lt;- ggplot(india_co2, aes(x = year, y = co2_emissions)) + geom_line() + theme_void() plot_india ggsave(\u0026quot;india_co2.pdf\u0026quot;, plot_india, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;india_co2.png\u0026quot;, plot_india, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;) china_co2 \u0026lt;- wdi_clean %\u0026gt;% filter(country == \u0026quot;China\u0026quot;) plot_china \u0026lt;- ggplot(china_co2, aes(x = year, y = co2_emissions)) + geom_line() + theme_void() plot_china ggsave(\u0026quot;china_co2.pdf\u0026quot;, plot_china, width = 1, heighlt = 0.15, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;china_co2.png\u0026quot;, plot_china, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;) You can then use those saved tiny plots in your text.\n Both India and China have seen increased CO2 emissions over the past 20 years.\n  Slopegraphs We can make a slopegraph to show changes in GDP per capita between two time periods. We need to first filter our WDI to include only the start and end years (here 1995 and 2015). Then, to make sure that we’re using complete data, we’ll get rid of any country that has missing data for either 1995 or 2015. The group_by(...) %\u0026gt;% filter(...) %\u0026gt;% ungroup() pipeline does this, with the !any(is.na(gdp_per_cap)) test keeping any rows where any of the gdp_per_cap values are not missing for the whole country.\nWe then add a couple special columns for labels. The paste0() function concatenates strings and variables together, so that paste0(\"2 + 2 = \", 2 + 2) would show “2 + 2 = 4”. Here we make labels that say either “Country name: $GDP” or “$GDP” depending on the year.\ngdp_south_asia \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;South Asia\u0026quot;) %\u0026gt;% filter(year %in% c(1995, 2015)) %\u0026gt;% # Look at each country individually group_by(country) %\u0026gt;% # Remove the country if any of its gdp_per_cap values are missing filter(!any(is.na(gdp_per_cap))) %\u0026gt;% ungroup() %\u0026gt;% # Make year a factor mutate(year = factor(year)) %\u0026gt;% # Make some nice label columns # If the year is 1995, format it like \u0026quot;Country name: $GDP\u0026quot;. If the year is # 2015, format it like \u0026quot;$GDP\u0026quot; mutate(label_first = ifelse(year == 1995, paste0(country, \u0026quot;: \u0026quot;, dollar(round(gdp_per_cap))), NA), label_last = ifelse(year == 2015, dollar(round(gdp_per_cap, 0)), NA)) With the data filtered like this, we can plot it by mapping year to the x-axis, GDP per capita to the y-axis, and coloring by country. To make the lines go across the two categorical labels in the x-axis (since we made year a factor/category), we need to also specify the group aesthetic.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) Cool! We’re getting closer. We can definitely see different slopes, but with 7 different colors, it’s hard to see exactly which country is which. Instead, we can directly label each of these lines with geom_text():\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text(aes(label = country)) + guides(color = FALSE) That gets us a little closer, but the country labels are hard to see, and we could include more information, like the actual values. Remember those label_first and label_last columns we made? Let’s use those instead:\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text(aes(label = label_first)) + geom_text(aes(label = label_last)) + guides(color = FALSE) Now we have dollar amounts and country names, but the labels are still overlapping and really hard to read. To fix this, we can make the labels repel away from each other and randomly position in a way that makes them not overlap. The ggrepel package lets us do this with geom_text_repel()\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first)) + geom_text_repel(aes(label = label_last)) + guides(color = FALSE) Now none of the labels are on top of each other, but the labels are still on top of the lines. Also, some of the labels moved inward and outward along the x-axis, but they don’t need to do that—they just need to shift up and down. We can force the labels to only move up and down by setting the direction = \"y\" argument, and we can move all the labels to the left or right with the nudge_x argument. The seed argument makes sure that the random label placement is the same every time we run this. It can be whatever number you want—it just has to be a number.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first), direction = \u0026quot;y\u0026quot;, nudge_x = -1, seed = 1234) + geom_text_repel(aes(label = label_last), direction = \u0026quot;y\u0026quot;, nudge_x = 1, seed = 1234) + guides(color = FALSE) That’s it! Let’s take the theme off completely, change the colors a little, and it should be perfect.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first), direction = \u0026quot;y\u0026quot;, nudge_x = -1, seed = 1234) + geom_text_repel(aes(label = label_last), direction = \u0026quot;y\u0026quot;, nudge_x = 1, seed = 1234) + guides(color = FALSE) + scale_color_viridis_d(option = \u0026quot;magma\u0026quot;, end = 0.9) + theme_void()  Bump charts Finally, we can make a bump chart that shows changes in rankings over time. We’ll look at CO2 emissions in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the rank() function to rank countries by the co2_emissions column.\nsa_co2 \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;South Asia\u0026quot;) %\u0026gt;% filter(year \u0026gt;= 2004, year \u0026lt; 2015) %\u0026gt;% group_by(year) %\u0026gt;% mutate(rank = rank(co2_emissions)) We then plot this with points and lines, reversing the y-axis so 1 is at the top:\nggplot(sa_co2, aes(x = year, y = rank, color = country)) + geom_line() + geom_point() + scale_y_reverse(breaks = 1:8) Afghanistan and Nepal switched around for the number 1 spot, while India dropped from 4 to 6, switching places with Pakistan.\nAs with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use geom_text() again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the data argument in geom_text() though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner.\nggplot(sa_co2, aes(x = year, y = rank, color = country)) + geom_line(size = 2) + geom_point(size = 4) + geom_text(data = filter(sa_co2, year == 2004), aes(label = iso2c, x = 2003.25), fontface = \u0026quot;bold\u0026quot;) + geom_text(data = filter(sa_co2, year == 2014), aes(label = iso2c, x = 2014.75), fontface = \u0026quot;bold\u0026quot;) + guides(color = FALSE) + scale_y_reverse(breaks = 1:8) + scale_x_continuous(breaks = 2004:2014) + scale_color_viridis_d(option = \u0026quot;magma\u0026quot;, begin = 0.2, end = 0.9) + labs(x = NULL, y = \u0026quot;Rank\u0026quot;) + theme_minimal() + theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank()) If you want to be super fancy, you can use flags instead of country codes, but that’s a little more complicated (you need to install the ggflags package. See here for an example.\n  ","date":1589932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"66dae6a89dc933d1691fce47e0612205","permalink":"/example/08-example/","publishdate":"2020-05-20T00:00:00Z","relpermalink":"/example/08-example/","section":"example","summary":"Live coding example Complete code  Load and clean data Small multiples Sparklines Slopegraphs Bump charts    For this example, we’re going to use cross-national data, but instead of using the typical gapminder dataset, we’re going to collect data directly from the World Bank’s Open Data portal\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):","tags":null,"title":"Comparisons","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Small multiples Task 3: Slopegraphs Bonus task! Bump charts Turning everything in Postscript: how I got this unemployment data   Getting started For this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, I describe how I built this dataset down below).\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\n  unemployment.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  08-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 08-exercise.Rmd your-project-name.Rproj data\\ unemployment.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  08-exercise.zip  The example for today’s session will be incredibly helpful for this exercise. Reference it.\nAgain, you don’t need to make your plots super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Small multiples Use data from the US Bureau of Labor Statistics (BLS) to show the trends in employment rate for all 50 states between 2006 and 2016. What stories does this plot tell? Which states struggled to recover from the 2008–09 recession?\nSome hints/tips:\n You won’t need to filter out any missing rows because the data here is complete—there are no state-year combinations with missing unemployment data.\n You’ll be plotting 51 facets. You can filter out DC if you want to have a better grid (like 5 × 10), or you can try using facet_geo() from the geofacet package to lay out the plots like a map of the US (try this!).\n Plot the date column along the x-axis, not the year column. If you plot by year, you’ll get weird looking lines (try it for fun?), since these observations are monthly. If you really want to plot by year only, you’ll need to create a different data frame where you group by year and state and calculate the average unemployment rate for each year/state combination (i.e. group_by(year, state) %\u0026gt;% summarize(avg_unemployment = mean(unemployment)))\n Try mapping other aesthetics onto the graph too. You’ll notice there are columns for region and division—play with those as colors, for instance.\n This plot might be big, so make sure you adjust fig.width and fig.height in the chunk options so that it’s visible when you knit it. You might also want to used ggsave() to save it with extra large dimensions.\n   Task 3: Slopegraphs Use data from the BLS to create a slopegraph that compares the unemployment rate in January 2006 with the unemployment rate in January 2009, either for all 50 states at once (good luck with that!) or for a specific region or division. Make sure the plot doesn’t look too busy or crowded in the end.\nWhat story does this plot tell? Which states in the US (or in the specific region you selected) were the most/least affected the Great Recession?\nSome hints/tips:\n You should use filter() to only select rows where the year is 2006 or 2009 (i.e. filter(year %in% c(2006, 2009)) and to select rows where the month is January (filter(month == 1) or filter(month_name == \"January\"))\n In order for the year to be plotted as separate categories on the x-axis, it needs to be a factor, so use mutate(year = factor(year)) to convert it.\n To make ggplot draw lines between the 2006 and 2009 categories, you need to include group = state in the aesthetics.\n   Bonus task! Bump charts This is entirely optional but might be fun.\nFor extra fun times, if you feel like it, create a bump chart showing something from the unemployment data (perhaps the top 10 states or bottom 10 states in unemployment?) Adapt the code in the example for today’s session.\nIf you do this, plotting 51 lines is going to be a huge mess. But filtering the data is also a bad idea, because states could drop in and out of the top/bottom 10 over time, and we don’t want to get rid of them. Instead, you can zoom in on a specific range of data in your plot with coord_cartesian(ylim = c(1, 10)), for instance.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n Postscript: how I got this unemployment data For the curious, here’s the code I used to download the unemployment data from the BLS.\nAnd to pull the curtain back and show how much googling is involved in data visualization (and data analysis and programming in general), here was my process for getting this data:\nI thought “I want to have students show variation in something domestic over time” and then I googled “us data by state”. Nothing really came up (since it was an exceedingly vague search in the first place), but some results mentioned unemployment rates, so I figured that could be cool. I googled “unemployment statistics by state over time” and found that the BLS keeps statistics on this. I clicked on the “Data Tools” link in their main navigation bar, clicked on “Unemployment”, and then clicked on the “Multi-screen data search” button for the Local Area Unemployment Statistics (LAUS). I walked through the multiple screens and got excited that I’d be able to download all unemployment stats for all states for a ton of years, BUT THEN the final page had links to 51 individual Excel files, which was dumb. So I went back to Google and searched for “download bls data r” and found a few different packages people have written to do this. The first one I clicked on was blscrapeR at GitHub, and it looked like it had been updated recently, so I went with it. I followed the examples in the blscrapeR package and downloaded data for every state.  Another day in the life of doing modern data science. I had no idea people had written R packages to access BLS data, but there are like 3 packages out there! After a few minutes of tinkering, I got it working and it’s super magic.\n ","date":1589932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"16ffee7d9ab8ee672d02b96ff0eec42f","permalink":"/lab/08-exercise/","publishdate":"2020-05-20T00:00:00Z","relpermalink":"/lab/08-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Small multiples Task 3: Slopegraphs Bonus task! Bump charts Turning everything in Postscript: how I got this unemployment data   Getting started For this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, I describe how I built this dataset down below).\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.","tags":null,"title":"Comparisons","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Load data Clean and reshape data Plot the data and annotate    For this example, we’re again going to use cross-national data from the World Bank’s Open Data portal. We’ll download the data with the WDI package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  wdi_co2.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(WDI) # Get data from the World Bank library(ggrepel) # For non-overlapping labels # You need to install ggtext from GitHub. Follow the instructions at # https://github.com/wilkelab/ggtext library(ggtext) # For fancier text handling indicators \u0026lt;- c(\u0026quot;SP.POP.TOTL\u0026quot;, # Population \u0026quot;EN.ATM.CO2E.PC\u0026quot;, # CO2 emissions \u0026quot;NY.GDP.PCAP.KD\u0026quot;) # GDP per capita wdi_co2_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE, start = 1995, end = 2015) Then we clean the data by removing non-country countries and renaming some of the columns.\nwdi_clean \u0026lt;- wdi_co2_raw %\u0026gt;% filter(region != \u0026quot;Aggregates\u0026quot;) %\u0026gt;% select(iso2c, iso3c, country, year, population = SP.POP.TOTL, co2_emissions = EN.ATM.CO2E.PC, gdp_per_cap = NY.GDP.PCAP.KD, region, income)  Clean and reshape data Next we’ll do some substantial filtering and reshaping so that we can end up with the rankings of CO2 emissions in 1995 and 2014. I annotate as much as possible below so you can see what’s happening in each step.\nco2_rankings \u0026lt;- wdi_clean %\u0026gt;% # Get rid of smaller countries filter(population \u0026gt; 200000) %\u0026gt;% # Only look at two years filter(year %in% c(1995, 2014)) %\u0026gt;% # Get rid of all the rows that have missing values in co2_emissions drop_na(co2_emissions) %\u0026gt;% # Look at each year individually and rank countries based on their emissions that year group_by(year) %\u0026gt;% mutate(ranking = rank(co2_emissions)) %\u0026gt;% ungroup() %\u0026gt;% # Only select a handful of columns, mostly just the newly created \u0026quot;ranking\u0026quot; # column and some country identifiers select(iso3c, country, year, region, income, ranking) %\u0026gt;% # Right now the data is tidy and long, but we want to widen it and create # separate columns for emissions in 1995 and in 2014. pivot_wider() will make # new columns based on the existing \u0026quot;year\u0026quot; column (that\u0026#39;s what `names_from` # does), and it will add \u0026quot;rank_\u0026quot; as the prefix, so that the new columns will # be \u0026quot;rank_1995\u0026quot; and \u0026quot;rank_2014\u0026quot;. The values that go in those new columns will # come from the existing \u0026quot;ranking\u0026quot; column pivot_wider(names_from = year, names_prefix = \u0026quot;rank_\u0026quot;, values_from = ranking) %\u0026gt;% # Find the difference in ranking between 2014 and 1995 mutate(rank_diff = rank_2014 - rank_1995) %\u0026gt;% # Remove all rows where there\u0026#39;s a missing value in the rank_diff column drop_na(rank_diff) %\u0026gt;% # Make an indicator variable that is true of the absolute value of the # difference in rankings is greater than 25. 25 is arbitrary here—that just # felt like a big change in rankings mutate(big_change = ifelse(abs(rank_diff) \u0026gt;= 25, TRUE, FALSE)) %\u0026gt;% # Make another indicator variable that indicates if the rank improved by a # lot, worsened by a lot, or didn\u0026#39;t change much. We use the case_when() # function, which is like a fancy version of ifelse() that takes multiple # conditions. This is how it generally works: # # case_when( # some_test ~ value_if_true, # some_other_test ~ value_if_true, # TRUE ~ value_otherwise #) mutate(better_big_change = case_when( rank_diff \u0026lt;= -25 ~ \u0026quot;Rank improved\u0026quot;, rank_diff \u0026gt;= 25 ~ \u0026quot;Rank worsened\u0026quot;, TRUE ~ \u0026quot;Rank changed a little\u0026quot; )) Here’s what that reshaped data looked like before:\nhead(wdi_clean) ## # A tibble: 6 x 9 ## iso2c iso3c country year population co2_emissions gdp_per_cap region income ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AD AND Andorra 2015 78011 NA 41768. Europe \u0026amp; Central Asia High income ## 2 AD AND Andorra 2004 76244 7.36 47033. Europe \u0026amp; Central Asia High income ## 3 AD AND Andorra 2001 67341 7.79 41421. Europe \u0026amp; Central Asia High income ## 4 AD AND Andorra 2002 70049 7.59 42396. Europe \u0026amp; Central Asia High income ## 5 AD AND Andorra 2014 79213 5.83 40790. Europe \u0026amp; Central Asia High income ## 6 AD AND Andorra 1995 63850 6.66 32918. Europe \u0026amp; Central Asia High income And here’s what it looks like now:\nhead(co2_rankings) ## # A tibble: 6 x 9 ## iso3c country region income rank_1995 rank_2014 rank_diff big_change better_big_change ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;lgl\u0026gt; \u0026lt;chr\u0026gt; ## 1 ARE United Arab Emirates Middle East \u0026amp; North Africa High income 167 171 4 FALSE Rank changed a little ## 2 AFG Afghanistan South Asia Low income 8 24 16 FALSE Rank changed a little ## 3 ALB Albania Europe \u0026amp; Central Asia Upper middle income 54 78 24 FALSE Rank changed a little ## 4 ARM Armenia Europe \u0026amp; Central Asia Upper middle income 71 76 5 FALSE Rank changed a little ## 5 AGO Angola Sub-Saharan Africa Lower middle income 59 61 2 FALSE Rank changed a little ## 6 ARG Argentina Latin America \u0026amp; Caribbean High income 103 119 16 FALSE Rank changed a little  Plot the data and annotate I use IBM Plex Sans in this plot. You can download it from Google Fonts.\n# These three functions make it so all geoms that use text, label, and # label_repel will use IBM Plex Sans as the font. Those layers are *not* # influenced by whatever you include in the base_family argument in something # like theme_bw(), so ordinarily you\u0026#39;d need to specify the font in each # individual annotate(geom = \u0026quot;text\u0026quot;) layer or geom_label() layer, and that\u0026#39;s # tedious! This removes that tediousness. update_geom_defaults(\u0026quot;text\u0026quot;, list(family = \u0026quot;IBM Plex Sans\u0026quot;)) update_geom_defaults(\u0026quot;label\u0026quot;, list(family = \u0026quot;IBM Plex Sans\u0026quot;)) update_geom_defaults(\u0026quot;label_repel\u0026quot;, list(family = \u0026quot;IBM Plex Sans\u0026quot;)) ggplot(co2_rankings, aes(x = rank_1995, y = rank_2014)) + # Add a reference line that goes from the bottom corner to the top corner annotate(geom = \u0026quot;segment\u0026quot;, x = 0, xend = 175, y = 0, yend = 175) + # Add points and color them by the type of change in rankings geom_point(aes(color = better_big_change)) + # Add repelled labels. Only use data where big_change is TRUE. Fill them by # the type of change (so they match the color in geom_point() above) and use # white text geom_label_repel(data = filter(co2_rankings, big_change == TRUE), aes(label = country, fill = better_big_change), color = \u0026quot;white\u0026quot;) + # Add notes about what the outliers mean in the bottom left and top right # corners. These are italicized and light grey. The text in the bottom corner # is justified to the right with hjust = 1, and the text in the top corner is # justified to the left with hjust = 0 annotate(geom = \u0026quot;text\u0026quot;, x = 170, y = 6, label = \u0026quot;Outliers improving\u0026quot;, fontface = \u0026quot;italic\u0026quot;, hjust = 1, color = \u0026quot;grey50\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, x = 2, y = 170, label = \u0026quot;Outliers worsening\u0026quot;, fontface = \u0026quot;italic\u0026quot;, hjust = 0, color = \u0026quot;grey50\u0026quot;) + # Add mostly transparent rectangles in the bottom right and top left corners annotate(geom = \u0026quot;rect\u0026quot;, xmin = 0, xmax = 25, ymin = 0, ymax = 25, fill = \u0026quot;#2ECC40\u0026quot;, alpha = 0.25) + annotate(geom = \u0026quot;rect\u0026quot;, xmin = 150, xmax = 175, ymin = 150, ymax = 175, fill = \u0026quot;#FF851B\u0026quot;, alpha = 0.25) + # Add text to define what the rectangles abovee actually mean. The \\n in # \u0026quot;highest\\nemitters\u0026quot; will put a line break in the label annotate(geom = \u0026quot;text\u0026quot;, x = 40, y = 6, label = \u0026quot;Lowest emitters\u0026quot;, hjust = 0, color = \u0026quot;#2ECC40\u0026quot;) + annotate(geom = \u0026quot;text\u0026quot;, x = 162.5, y = 135, label = \u0026quot;Highest\\nemitters\u0026quot;, hjust = 0.5, vjust = 1, lineheight = 1, color = \u0026quot;#FF851B\u0026quot;) + # Add arrows between the text and the rectangles. These use the segment geom, # and the arrows are added with the arrow() function, which lets us define the # angle of the arrowhead and the length of the arrowhead pieces. Here we use # 0.5 lines, which is a unit of measurement that ggplot uses internally (think # of how many lines of text fit in the plot). We could also use unit(1, \u0026quot;cm\u0026quot;) # or unit(0.25, \u0026quot;in\u0026quot;) or anything else annotate(geom = \u0026quot;segment\u0026quot;, x = 38, xend = 20, y = 6, yend = 6, color = \u0026quot;#2ECC40\u0026quot;, arrow = arrow(angle = 15, length = unit(0.5, \u0026quot;lines\u0026quot;))) + annotate(geom = \u0026quot;segment\u0026quot;, x = 162.5, xend = 162.5, y = 140, yend = 155, color = \u0026quot;#FF851B\u0026quot;, arrow = arrow(angle = 15, length = unit(0.5, \u0026quot;lines\u0026quot;))) + # Use three different colors for the points scale_color_manual(values = c(\u0026quot;grey50\u0026quot;, \u0026quot;#0074D9\u0026quot;, \u0026quot;#FF4136\u0026quot;)) + # Use two different colors for the filled labels. There are no grey labels, so # we don\u0026#39;t have to specify that color scale_fill_manual(values = c(\u0026quot;#0074D9\u0026quot;, \u0026quot;#FF4136\u0026quot;)) + # Make the x and y axes expand all the way to the edges of the plot area and # add breaks every 25 units from 0 to 175 scale_x_continuous(expand = c(0, 0), breaks = seq(0, 175, 25)) + scale_y_continuous(expand = c(0, 0), breaks = seq(0, 175, 25)) + # Add labels! There are a couple fancy things here. # 1. In the title we wrap the 2 of CO2 in the HTML \u0026lt;sub\u0026gt;\u0026lt;/sub\u0026gt; tag so that the # number gets subscripted. The only way this will actually get parsed as # HTML is if we tell the plot.title to use element_markdown() in the # theme() function, and element_markdown() comes from the ggtext package. # 2. In the subtitle we bold the two words **improved** and **worsened** using # Markdown asterisks. We also wrap these words with HTML span tags with # inline CSS to specify the color of the text. Like the title, this will # only be processed and parsed as HTML and Markdown if we tell the p # lot.subtitle to use element_markdown() in the theme() function. labs(x = \u0026quot;Rank in 1995\u0026quot;, y = \u0026quot;Rank in 2014\u0026quot;, title = \u0026quot;Changes in CO\u0026lt;sub\u0026gt;2\u0026lt;/sub\u0026gt; emission rankings between 1995 and 2014\u0026quot;, subtitle = \u0026quot;Countries that \u0026lt;span style=\u0026#39;color: #0074D9\u0026#39;\u0026gt;**improved**\u0026lt;/span\u0026gt; or \u0026lt;span style=\u0026#39;color: #FF4136\u0026#39;\u0026gt;**worsened**\u0026lt;/span\u0026gt; more than 25 positions in the rankings highlighted\u0026quot;, caption = \u0026quot;Source: The World Bank.\\nCountries with populations of less than 200,000 excluded.\u0026quot;) + # Turn off the legends for color and fill, since the subtitle includes that guides(color = FALSE, fill = FALSE) + # Use theme_bw() with IBM Plex Sans theme_bw(base_family = \u0026quot;IBM Plex Sans\u0026quot;) + # Tell the title and subtitle to be treated as Markdown/HTML, make the title # 1.6x the size of the base font, and make the subtitle 1.3x the size of the # base font. Also add a little larger margin on the right of the plot so that # the 175 doesn\u0026#39;t get cut off. theme(plot.title = element_markdown(face = \u0026quot;bold\u0026quot;, size = rel(1.6)), plot.subtitle = element_markdown(size = rel(1.3)), plot.margin = unit(c(0.5, 1, 0.5, 0.5), units = \u0026quot;lines\u0026quot;))   ","date":1590019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"4ca8d8f57d9585dcab132a17e9a5a6e7","permalink":"/example/09-example/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/example/09-example/","section":"example","summary":"Live coding example Complete code  Load data Clean and reshape data Plot the data and annotate    For this example, we’re again going to use cross-national data from the World Bank’s Open Data portal. We’ll download the data with the WDI package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):","tags":null,"title":"Annotations","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Annotations Turning everything in   Getting started For this exercise, you’ll use whatever data you want to make a plot and add annotations to it. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the gapminder package, download stuff from the World Bank using the WDI package, or use something from this list of datasets.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  09-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 09-exercise.Rmd your-project-name.Rproj data\\ WHATEVER.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  09-exercise.zip  The documentation for annotate(), geom_text() and geom_label(), and geom_text_repel() and geom_label_repel() will be incredibly helpful for this exercise. The example for today’s session is also helpful for seeing annotations in real life.\nAgain, you don’t need to make your plots super fancy (except for these annotations), but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Annotations Do the following:\nMake a plot. Any kind of plot will do (though it might be easiest to work with geom_point()).\n Label (some or all of) the points using one of geom_text(), geom_label(), geom_text_repel(), or geom_label_repel(). You might need to make a new indicator variable so that you only highlight a few of the points instead of all of them. See this slide for an example, as well as the complete example plot on the example page for today’s session.\n Add *at least two each** the following annotations somewhere on the plot using annotate():\n Text An arrow (look at this page, or search for “arrow” on this page for examples). Make a curved arrow for bonus fun. A rectangle  You can add more if you want, but those three are the minimum. Try to incorporate the annotations into the design of the plot rather than just placing them wherever.\n   Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1590019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"539b7ed4ab6c62f5154f80c9fa8f3e4c","permalink":"/lab/09-exercise/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/lab/09-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Annotations Turning everything in   Getting started For this exercise, you’ll use whatever data you want to make a plot and add annotations to it. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the gapminder package, download stuff from the World Bank using the WDI package, or use something from this list of datasets.","tags":null,"title":"Annotations","type":"docs"},{"authors":null,"categories":null,"content":"      Live coding example Complete code  Get and clean data Creating a basic interactive chart Modifying the tooltip Including more information in the tooltip Making a dashboard with flexdashboard    For this example we’ll use data from the World Bank once again, which we download using the WDI package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  wdi_parl.csv  Live coding example There is no video for this one, since it really only involves feeding a few ggplot plots fed into ggplotly().\n Complete code Get and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(WDI) # Get data from the World Bank library(scales) # For nicer label formatting library(plotly) # For easy interactive plots indicators \u0026lt;- c(\u0026quot;SP.POP.TOTL\u0026quot;, # Population \u0026quot;SG.GEN.PARL.ZS\u0026quot;, # Proportion of seats held by women in national parliaments (%) \u0026quot;NY.GDP.PCAP.KD\u0026quot;) # GDP per capita wdi_parl_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE, start = 2000, end = 2019) Then we clean the data by removing non-country countries and renaming some of the columns.\nwdi_clean \u0026lt;- wdi_parl_raw %\u0026gt;% filter(region != \u0026quot;Aggregates\u0026quot;) %\u0026gt;% select(iso2c, iso3c, country, year, population = SP.POP.TOTL, prop_women_parl = SG.GEN.PARL.ZS, gdp_per_cap = NY.GDP.PCAP.KD, region, income)  Creating a basic interactive chart Let’s make a chart that shows the distribution of the proportion of women in national parliaments in 2019, by continent. We’ll use a strip plot with jittered points.\nFirst we need to make a regular static plot with ggplot:\nwdi_2019 \u0026lt;- wdi_clean %\u0026gt;% filter(year == 2019) %\u0026gt;% drop_na(prop_women_parl) %\u0026gt;% # Scale this down from 0-100 to 0-1 so that scales::percent() can format it as # an actual percent mutate(prop_women_parl = prop_women_parl / 100) static_plot \u0026lt;- ggplot(wdi_2019, aes(y = fct_rev(region), x = prop_women_parl, color = region)) + geom_point(position = position_jitter(width = 0, height = 0.15, seed = 1234)) + guides(color = FALSE) + scale_x_continuous(labels = percent) + # I used https://medialab.github.io/iwanthue/ to generate these colors scale_color_manual(values = c(\u0026quot;#425300\u0026quot;, \u0026quot;#e680ff\u0026quot;, \u0026quot;#01bd71\u0026quot;, \u0026quot;#ff3aad\u0026quot;, \u0026quot;#9f3e00\u0026quot;, \u0026quot;#0146bf\u0026quot;, \u0026quot;#671d56\u0026quot;)) + labs(x = \u0026quot;% women in parliament\u0026quot;, y = NULL, caption = \u0026quot;Source: The World Bank\u0026quot;) + theme_bw() static_plot Great! That looks pretty good.\nTo make it interactive, all we have to do is feed the static_plot object into ggplotly(). That’s it.\nggplotly(static_plot)  {\"x\":{\"url\":\"/example/10-example_files/figure-html//widgets/widget_strip-plot-basic-interactive-real.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} Not everything translates over to JavaScript—the caption is gone now, and the legend is back (which is fine, I guess, since the legend is interactive). But still, this is magic.\n Modifying the tooltip Right now, the default tooltip you see when you hover over the points includes the actual proportion of women in parliament for each point, along with the continent, which is neat, but it’d be great if we could see the country name too. The tooltip picks up the information to include from the variables we use in aes(), and we never map the country column to any aesthetic, so it doesn’t show up.\nTo get around this, we can add a new aesthetic for country to the points. Instead of using one of the real ggplot aesthetics like color or fill, we’ll use a fake one called text (we can call it whatever we want! asdf would also work). ggplot has no idea how to do anything with the text aesthetic, and it’ll give you a warning, but that’s okay. The static plot looks the same:\nstatic_plot_toolip \u0026lt;- ggplot(wdi_2019, aes(y = fct_rev(region), x = prop_women_parl, color = region)) + geom_point(aes(text = country), position = position_jitter(width = 0, height = 0.15, seed = 1234)) + guides(color = FALSE) + scale_x_continuous(labels = percent) + # I used https://medialab.github.io/iwanthue/ to generate these colors scale_color_manual(values = c(\u0026quot;#425300\u0026quot;, \u0026quot;#e680ff\u0026quot;, \u0026quot;#01bd71\u0026quot;, \u0026quot;#ff3aad\u0026quot;, \u0026quot;#9f3e00\u0026quot;, \u0026quot;#0146bf\u0026quot;, \u0026quot;#671d56\u0026quot;)) + labs(x = \u0026quot;% women in parliament\u0026quot;, y = NULL, caption = \u0026quot;Source: The World Bank\u0026quot;) + theme_bw() ## Warning: Ignoring unknown aesthetics: text static_plot_toolip Now we can tell plotly to use this fake text aesthetic for the tooltip:\nggplotly(static_plot_toolip, tooltip = \u0026quot;text\u0026quot;)  {\"x\":{\"url\":\"/example/10-example_files/figure-html//widgets/widget_strip-plot-text-interactive-real.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} Now we should just see the country names in the tooltips!\n Including more information in the tooltip We have country names, but we lost the values in the x-axis. Rwanda has the highest proportion of women in parliament, but what’s the exact number? It’s somewhere above 60%, but that’s all we can see now.\nTo fix this, we can make a new column in the data with all the text we want to include in the tooltip. We’ll use paste0() to combine text and variable values to make the tooltip follow this format:\nName of country X% women in parliament Let’s add a new column with mutate(). A couple things to note here:\n The \u0026lt;br\u0026gt; is HTML code for a line break\n We use the percent() function to format numbers as percents. The accuracy argument tells R how many decimal points to use. If we used 1, it would say 12%; if we used 0.01, it would say 12.08%; etc.\n  wdi_2019 \u0026lt;- wdi_clean %\u0026gt;% filter(year == 2019) %\u0026gt;% drop_na(prop_women_parl) %\u0026gt;% # Scale this down from 0-100 to 0-1 so that scales::percent() can format it as # an actual percent mutate(prop_women_parl = prop_women_parl / 100) %\u0026gt;% mutate(fancy_label = paste0(country, \u0026quot;\u0026lt;br\u0026gt;\u0026quot;, percent(prop_women_parl, accuracy = 0.1), \u0026quot; women in parliament\u0026quot;)) Let’s check to see if it worked:\nwdi_2019 %\u0026gt;% select(country, prop_women_parl, fancy_label) %\u0026gt;% head() ## # A tibble: 6 x 3 ## country prop_women_parl fancy_label ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Andorra 0.5 Andorra\u0026lt;br\u0026gt;50.0% women in parliament ## 2 United Arab Emirates 0.225 United Arab Emirates\u0026lt;br\u0026gt;22.5% women in parliament ## 3 Afghanistan 0.279 Afghanistan\u0026lt;br\u0026gt;27.9% women in parliament ## 4 Antigua and Barbuda 0.111 Antigua and Barbuda\u0026lt;br\u0026gt;11.1% women in parliament ## 5 Albania 0.295 Albania\u0026lt;br\u0026gt;29.5% women in parliament ## 6 Armenia 0.242 Armenia\u0026lt;br\u0026gt;24.2% women in parliament Now instead of using text = country we’ll use text = fancy_label to map that new column onto the plot. Again, this won’t be visible in the static plot (and you’ll get a warning), but it will show up in the interactive plot.\nstatic_plot_toolip_fancy \u0026lt;- ggplot(wdi_2019, aes(y = fct_rev(region), x = prop_women_parl, color = region)) + geom_point(aes(text = fancy_label), position = position_jitter(width = 0, height = 0.15, seed = 1234)) + guides(color = FALSE) + scale_x_continuous(labels = percent) + # I used https://medialab.github.io/iwanthue/ to generate these colors scale_color_manual(values = c(\u0026quot;#425300\u0026quot;, \u0026quot;#e680ff\u0026quot;, \u0026quot;#01bd71\u0026quot;, \u0026quot;#ff3aad\u0026quot;, \u0026quot;#9f3e00\u0026quot;, \u0026quot;#0146bf\u0026quot;, \u0026quot;#671d56\u0026quot;)) + labs(x = \u0026quot;% women in parliament\u0026quot;, y = NULL, caption = \u0026quot;Source: The World Bank\u0026quot;) + theme_bw() ## Warning: Ignoring unknown aesthetics: text ggplotly(static_plot_toolip_fancy, tooltip = \u0026quot;text\u0026quot;)  {\"x\":{\"url\":\"/example/10-example_files/figure-html//widgets/widget_strip-plot-text-interactive-fancy-real.html\",\"options\":{\"xdomain\":\"*\",\"allowfullscreen\":false,\"lazyload\":false}},\"evals\":[],\"jsHooks\":[]} Perfect!\nFinally, if we want to save this plot as a standalone self-contained HTML file, we can use the saveWidget() function from the htmlwidgets package.\n# This is like ggsave, but for interactive HTML plots interactive_plot \u0026lt;- static_plot_toolip_fancy htmlwidgets::saveWidget(interactive_plot, \u0026quot;fancy_plot.html\u0026quot;)  Making a dashboard with flexdashboard The documentation for flexdashboard is so great and complete that I’m not going to include a full example here. There is also a brief overview in chapter 5 of the official R Markdown book. You can also watch this really quick video here. She uses a package called dimple instead of plotly, which doesn’t work with ggplot like ggplotly(), so ignore her code about dimple() and use your ggplotly() skills instead. You can search YouTube for a bunch of other short tutorial videos, too.\nThe quickest and easiest way to get started is to install the flexdashboard package and then in RStudio go to File \u0026gt; New File… \u0026gt; R Markdown… \u0026gt; From Template \u0026gt; Flexdashboard:\nThat will give you an empty dashboard with three chart areas spread across two columns. Put static or dynamic graphs in the different chart areas, knit, and you’ll be good to go!\nIf you’re interested in making the dashboard reactive with Shiny-like elements, check out this tutorial.\n  ","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"0d7091da7131dcaeb0a7a2758ca2db8e","permalink":"/example/10-example/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/example/10-example/","section":"example","summary":"Live coding example Complete code  Get and clean data Creating a basic interactive chart Modifying the tooltip Including more information in the tooltip Making a dashboard with flexdashboard    For this example we’ll use data from the World Bank once again, which we download using the WDI package.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):","tags":null,"title":"Interactivity","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Interactive plots Task 3: Dashboard Turning everything in   This exercise is a little different from past ones because you will not knit to PDF or Word. Pay attention to the instructions below.\nGetting started For this exercise, you’ll use whatever data you want to create an interactive HTML plot and a dashboard. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the gapminder package, download stuff from the World Bank using the WDI package, or use something from this list of datasets.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  10-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 10-exercise.Rmd your-project-name.Rproj data\\ WHATEVER.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  10-exercise.zip  The example for today’s session will be helpful as you tinker with ggplotly(), and the resources listed at the bottom of the example will be helpful for making a dashboard.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Interactive plots In the R Markdown file with your reflection, create an interactive plot with ggplotly() from the plotly package. Modify the tooltip to show something more useful than every single mapped aesthetic.\n Task 3: Dashboard Install the flexdashboard package and create a new R Markdown file in your project by going to File \u0026gt; New File… \u0026gt; R Markdown… \u0026gt; From Template \u0026gt; Flexdashboard.\nUsing the documentation for flexdashboard online, create a super basic dashboard that shows a plot (static or interactive) in at least two chart areas. Play with the layout if you’re feeling brave.\n Turning everything in Here’s where this is all different this time. You will not upload a knitted PDF or Word file to iCollege, since those can’t handle interactivity. Instead, do this:\nKnit the document with Tasks 1 and 2 in it to HTML and publish it to RPubs using the “Publish document” menu in the preview of the knitted file. Take note of the URL.\n Knit the dashboard from Task 3 to HTML and publish it to RPubs using the same menu. Take note of the URL.\n In iCollege, paste the two URLs into the submission form for exercise 10 following this template:\nTask 1 and 2: URL HERE Task 3 dashboard: URL HERE   ","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"0542dcda8caa3b3e296347c401f03433","permalink":"/lab/10-exercise/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/lab/10-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Interactive plots Task 3: Dashboard Turning everything in   This exercise is a little different from past ones because you will not knit to PDF or Word. Pay attention to the instructions below.\nGetting started For this exercise, you’ll use whatever data you want to create an interactive HTML plot and a dashboard. Use a dataset from a past exercise, use one of the built-in datasets like mpg or gapminder from the gapminder package, download stuff from the World Bank using the WDI package, or use something from this list of datasets.","tags":null,"title":"Interactivity","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Get data Look at and clean data Plotting time Improving graphics Decomposition    For this example, we’re going to use economic data from the US Federal Reserve (the Fed). The St. Louis Fed is in charge of publishing Fed economic data, and they host it all at an online portal named FRED. Instead of downloading individual time series data from the FRED website, we’ll do what with did with the World Bank WDI data and download it directly from the internet with the tidyquant package, which includes a function for working with the FRED API/website.\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  fred_raw.csv  Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nGet data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(tidyquant) # For accessing FRED data library(scales) # For nicer labels The US Federal Reserve provides thousands of economic datasets at FRED. We can use the tidyquant R package to access their servers and download the data directly into R.\nLike we did with the WDI indicators in session 8, we need to find the special internal code for the variables we want to get. We need to pay close attention to the details of each variable, since the same measure can be offered with different combinations of real (adjusted for inflation) or nominal (not adjusted for inflation); monthly, quarterly, or annually; and seasonally adjusted or not seasonally adjusted. For instance, if you want to see US GDP, here are some possibilities (all the possible GDP measures are listed here):\n GDPC1: Real (2012 dollars), quarterly, seasonally adjusted ND000334Q: Real (2012 dollars), quarterly, not seasonally adjusted GDPCA: Real (2012 dollars), annual, not seasonally adjusted GDP: Nominal, quarterly, seasonally adjusted GDPA: Nominal, annual, not seasonally adjusted  The code for getting data from FRED works a little differently than WDI(), and the output is a little different too, but it’s hopefully not too complicated. We need to feed the tq_get() function (1) a list of indicators we want, (2) a source for those indicators, and (3) a starting and/or ending date.\ntq_get() can actually get data from a ton of different sources like stocks from Yahoo Finance and general financial data from Bloomberg, Quandl, and Tiingo. Most of those other sources require a subscription and a fancy API key that logs you into their servers when getting data, but FRED is free (yay public goods!).\nWe’ll first make a new dataset named fred_raw that gets a bunch of interesting variables from FRED from January 1, 1990 until today.\nfred_raw \u0026lt;- tq_get(c(\u0026quot;RSXFSN\u0026quot;, # Advance retail sales \u0026quot;GDPC1\u0026quot;, # GDP \u0026quot;ICSA\u0026quot;, # Initial unemployment claims \u0026quot;FPCPITOTLZGUSA\u0026quot;, # Inflation \u0026quot;UNRATE\u0026quot;, # Unemployment rate \u0026quot;USREC\u0026quot;), # Recessions get = \u0026quot;economic.data\u0026quot;, # Use FRED from = \u0026quot;1990-01-01\u0026quot;) Downloading data from FRED every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). As with the World Bank data we used, it’s good practice to save this raw data as a CSV file and then work with that.\nwrite_csv(fred_raw, \u0026quot;data/fred_raw.csv\u0026quot;) Since we care about reproducibility, we still want to include the code we used to get data from FRED, we just don’t want it to actually run. You can include chunks but not run them by setting eval=FALSE in the chunk options. In this little example, we show the code for downloading the data, but we don’t evaluate the chunk. We then include a chunk that loads the data from a CSV file with read_csv(), but we don’t include it (include=FALSE). That way, in the knitted file we see the WDI() code, but in reality it’s loading the data from CSV. Super tricky.\nI first download data from FRED: ```{r get-fred-data, eval=FALSE} fred_raw \u0026lt;- tq_get(...) write_csv(fred_raw, \u0026quot;data/fred_raw.csv\u0026quot;) ``` ```{r load-fred-data-real, include=FALSE} fred_raw \u0026lt;- read_csv(\u0026quot;data/fred_raw.csv\u0026quot;) ```  Look at and clean data The data we get from FRED is in a slightly different format than we’re used to with WDI(), but with good reason. With World Bank data, you get data for every country and every year, so there are rows for Afghanistan 2000, Afghanistan 2001, etc. You then get a column for each of the variables you want (population, life expectancy, GDP/capita, etc.)\nWith FRED data, that kind of format doesn’t work for every possible time series variable because time is spaced differently. If you want to work with annual GDP, you should have a row for each year. If you want quarterly GDP, you should have a row for every quarter. If you put these in the same dataset, you’ll end up with all sorts of missing data issues:\n  time annual_gdp quarterly_gdp    2019, Q1 X X  2019, Q2  X  2019, Q3  X  2019, Q4  X  2020, Q1 X X  2020, Q2  X    To fix this, the tidyquant package gives you data in tidy (or long) form and only provides three columns:\nhead(fred_raw) ## # A tibble: 6 x 3 ## symbol date price ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 RSXFSN 1992-01-01 130683 ## 2 RSXFSN 1992-02-01 131244 ## 3 RSXFSN 1992-03-01 142488 ## 4 RSXFSN 1992-04-01 147175 ## 5 RSXFSN 1992-05-01 152420 ## 6 RSXFSN 1992-06-01 151849 The symbol column is the ID of the variable from FRED , date is… the date, and price is the value. These columns are called symbol and price because the tidyquant package was designed to get and process stock data, so you’d typically see stock symbols (like AAPL or MSFT) and stock prices. When working with FRED data, the price column shows the value of whatever you’re interested in—it’s not technically a price (so unemployment claims, inflation rates, and GDP values are still called price).\nRight now, our fred_raw dataset has only 3 columns, but nearly 3,000 rows since the six indicators we got from the server are all stacked on top of each other. To actually work with these, we need to filter the raw data so that it only includes the indicators we’re interested in. For instance, if we want to plot retail sales, we need to select only the rows where the symbol is RSXFSN. Make a smaller dataset with filter() to do that:\nretail_sales \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;RSXFSN\u0026quot;) retail_sales ## # A tibble: 340 x 3 ## symbol date price ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 RSXFSN 1992-01-01 130683 ## 2 RSXFSN 1992-02-01 131244 ## 3 RSXFSN 1992-03-01 142488 ## 4 RSXFSN 1992-04-01 147175 ## 5 RSXFSN 1992-05-01 152420 ## 6 RSXFSN 1992-06-01 151849 ## 7 RSXFSN 1992-07-01 152586 ## 8 RSXFSN 1992-08-01 152476 ## 9 RSXFSN 1992-09-01 148158 ## 10 RSXFSN 1992-10-01 155987 ## # … with 330 more rows If multiple variables have the same spacing (annual, quarterly, monthly, weekly), you can use filter to select all of them and then the use pivot_wider() or spread() to make separate columns for each. Inflation, unemployment, and retail sales are all monthly, so we can make a dataset for just those:\nfred_monthly_things \u0026lt;- fred_raw %\u0026gt;% filter(symbol %in% c(\u0026quot;FPCPITOTLZGUSA\u0026quot;, \u0026quot;UNRATE\u0026quot;, \u0026quot;RSXFSN\u0026quot;)) %\u0026gt;% # Convert the symbol column into multiple columns, using the \u0026quot;prices\u0026quot; for values pivot_wider(names_from = symbol, values_from = price) fred_monthly_things ## # A tibble: 364 x 4 ## date RSXFSN FPCPITOTLZGUSA UNRATE ## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1992-01-01 130683 3.03 7.3 ## 2 1992-02-01 131244 NA 7.4 ## 3 1992-03-01 142488 NA 7.4 ## 4 1992-04-01 147175 NA 7.4 ## 5 1992-05-01 152420 NA 7.6 ## 6 1992-06-01 151849 NA 7.8 ## 7 1992-07-01 152586 NA 7.7 ## 8 1992-08-01 152476 NA 7.6 ## 9 1992-09-01 148158 NA 7.6 ## 10 1992-10-01 155987 NA 7.3 ## # … with 354 more rows But wait! There’s a problem! The inflation rate we got isn’t actually monthly—it seems to be annual, which explains all the NAs. Let’s fix it by not including it. We’ll also rename the columns so they’re easier to work with\nfred_monthly_things \u0026lt;- fred_raw %\u0026gt;% filter(symbol %in% c(\u0026quot;UNRATE\u0026quot;, \u0026quot;RSXFSN\u0026quot;)) %\u0026gt;% # Convert the symbol column into multiple columns, using the \u0026quot;prices\u0026quot; for values pivot_wider(names_from = symbol, values_from = price) %\u0026gt;% rename(unemployment = UNRATE, retail_sales = RSXFSN) fred_monthly_things ## # A tibble: 364 x 3 ## date retail_sales unemployment ## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1992-01-01 130683 7.3 ## 2 1992-02-01 131244 7.4 ## 3 1992-03-01 142488 7.4 ## 4 1992-04-01 147175 7.4 ## 5 1992-05-01 152420 7.6 ## 6 1992-06-01 151849 7.8 ## 7 1992-07-01 152586 7.7 ## 8 1992-08-01 152476 7.6 ## 9 1992-09-01 148158 7.6 ## 10 1992-10-01 155987 7.3 ## # … with 354 more rows All better.\nWe can make as many subsets of the long, tidy, raw data as we want.\n Plotting time Let’s plot some of these and see what the trends look like. We’ll just use geom_line().\nHere’s GDP:\n# Get just GDP data from the raw FRED data gdp_only \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;GDPC1\u0026quot;) ggplot(gdp_only, aes(x = date, y = price)) + geom_line() Here’s retail sales:\n# Get just GDP data from the raw FRED data retail_sales_only \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;RSXFSN\u0026quot;) ggplot(retail_sales_only, aes(x = date, y = price)) + geom_line() And here’s unemployment claims:\nunemployment_claims_only \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;ICSA\u0026quot;) ggplot(unemployment_claims_only, aes(x = date, y = price)) + geom_line() Yikes COVID-19.\nThere, we visualized time. ✅\n Improving graphics These were simple graphs and they’re kind of helpful, but they’re not incredibly informative. We can clean these up a little. First we can change the labels and themes and colors:\nggplot(gdp_only, aes(x = date, y = price)) + geom_line(color = \u0026quot;#0074D9\u0026quot;, size = 1) + scale_y_continuous(labels = dollar) + labs(y = \u0026quot;Billions of 2012 dollars\u0026quot;, x = NULL, title = \u0026quot;US Gross Domestic Product\u0026quot;, subtitle = \u0026quot;Quarterly data; real 2012 dollars\u0026quot;, caption = \u0026quot;Source: US Bureau of Economic Analysis and FRED\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) That’s great and almost good enough to publish! We can add one additional layer of information onto the plot and highlight when recessions start and end. We included a recessions variable (USREC) when we got data from FRED, so let’s see what it looks like:\nfred_raw %\u0026gt;% filter(symbol == \u0026quot;USREC\u0026quot;) ## # A tibble: 364 x 3 ## symbol date price ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 USREC 1990-01-01 0 ## 2 USREC 1990-02-01 0 ## 3 USREC 1990-03-01 0 ## 4 USREC 1990-04-01 0 ## 5 USREC 1990-05-01 0 ## 6 USREC 1990-06-01 0 ## 7 USREC 1990-07-01 0 ## 8 USREC 1990-08-01 1 ## 9 USREC 1990-09-01 1 ## 10 USREC 1990-10-01 1 ## # … with 354 more rows This is monthly data that shows a 1 if we were in a recession that month and a 0 if we weren’t. The Fed doesn’t decide when recessions happen—the National Bureau of Economic Research (NBER) does, and they have specific guidelines for defining one. We’re probably in one right now, but there’s not enough data for NBER to formally declare it yet.\nThis data is long and tidy, but that makes it harder to work with given our GDP. We want the start and end dates for each recession so that we can shade those areas on the plot. To find those dates, we need to do a little data reshaping. First, we’ll create a temporary variable that marks if there was a switch from 0 to 1 or 1 to 0 in a given row by looking at the previous row\nrecessions_tidy \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;USREC\u0026quot;) %\u0026gt;% mutate(recession_change = price - lag(price)) recessions_tidy ## # A tibble: 364 x 4 ## symbol date price recession_change ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 USREC 1990-01-01 0 NA ## 2 USREC 1990-02-01 0 0 ## 3 USREC 1990-03-01 0 0 ## 4 USREC 1990-04-01 0 0 ## 5 USREC 1990-05-01 0 0 ## 6 USREC 1990-06-01 0 0 ## 7 USREC 1990-07-01 0 0 ## 8 USREC 1990-08-01 1 1 ## 9 USREC 1990-09-01 1 0 ## 10 USREC 1990-10-01 1 0 ## # … with 354 more rows Notice the new column we have that is mostly 0s, but 1 when there’s a switch, like in August 1990. 1 means we went from 0 to 1 (no recession → recession), while -1 means we went from 1 to 0 (recession → no recession).\nWe can see all the start and end dates if we filter:\nrecessions_start_end \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;USREC\u0026quot;) %\u0026gt;% mutate(recession_change = price - lag(price)) %\u0026gt;% filter(recession_change != 0) recessions_start_end ## # A tibble: 6 x 4 ## symbol date price recession_change ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 USREC 1990-08-01 1 1 ## 2 USREC 1991-04-01 0 -1 ## 3 USREC 2001-04-01 1 1 ## 4 USREC 2001-12-01 0 -1 ## 5 USREC 2008-01-01 1 1 ## 6 USREC 2009-07-01 0 -1 Finally, we can use tibble() to create a brand new little dataset that includes columns for the start and end dates:\nrecessions \u0026lt;- tibble(start = filter(recessions_start_end, recession_change == 1)$date, end = filter(recessions_start_end, recession_change == -1)$date) recessions ## # A tibble: 3 x 2 ## start end ## \u0026lt;date\u0026gt; \u0026lt;date\u0026gt; ## 1 1990-08-01 1991-04-01 ## 2 2001-04-01 2001-12-01 ## 3 2008-01-01 2009-07-01 We can now add this tiny dataset to our plot using geom_rect(). Notice how we put geom_rect() before geom_line()—that’s so the recession rectangles go under the line instead of on top of it. Also notice that we have to specify 4 new aesthetics for geom_rect(): min and max values for both x and y. We use the recession start and end dates for xmin and xmax, and then use −∞ and ∞ for ymin and ymax to make the rectangles stretch from the bottom of the plot to the top.\nThe last odd/new thing here is that we also use inherit.aes = FALSE in geom_rect(). That’s because we specified a global x and y aesthetic in ggplot(), which applies to all the other layers we add. geom_rect() doesn’t use x or y, though, and it’ll complain that those columns are missing. The inherit.aes argument tells ggplot that the geom_rect() layer should not get any of the global aesthetics like x or y.\nggplot(gdp_only, aes(x = date, y = price)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line(color = \u0026quot;#0074D9\u0026quot;, size = 1) + scale_y_continuous(labels = dollar) + labs(y = \u0026quot;Billions of 2012 dollars\u0026quot;, x = NULL, title = \u0026quot;US Gross Domestic Product\u0026quot;, subtitle = \u0026quot;Quarterly data; real 2012 dollars\u0026quot;, caption = \u0026quot;Source: US Bureau of Economic Analysis and FRED\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) And that’s it!\nNow that we have the tiny recessions data frame, we can add it to any plot we want. Here’s initial unemployment claims with some extra annotations for fun:\nggplot(unemployment_claims_only, aes(x = date, y = price)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line(color = \u0026quot;#FF4136\u0026quot;, size = 0.5) + annotate(geom = \u0026quot;label\u0026quot;, x = as.Date(\u0026quot;2010-01-01\u0026quot;), y = 1000000, label = \u0026quot;The Great Recession\u0026quot;, size = 3, family = \u0026quot;Roboto Condensed\u0026quot;) + annotate(geom = \u0026quot;label\u0026quot;, x = as.Date(\u0026quot;2020-01-01\u0026quot;), y = 6000000, label = \u0026quot;COVID-19\u0026quot;, size = 3, family = \u0026quot;Roboto Condensed\u0026quot;, hjust = 1) + scale_y_continuous(labels = comma) + labs(y = \u0026quot;Initial unemployment claims\u0026quot;, x = NULL, title = \u0026quot;Initial unemployment claims\u0026quot;, subtitle = \u0026quot;Weekly data\u0026quot;, caption = \u0026quot;Source: US Employment and Training Administration and FRED\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;))  Decomposition The mechanics of decomposing and forecasting time series goes beyond the scope of this class, but there are lots of resources you can use to learn more, including this phenomenal free textbook.\nThere’s a whole ecosystem of time-related packages that make working with time and decomposing trends easy (named tidyverts):\n lubridate: Helpful functions for manipulating dates (you’ve used this before) tsibble: Add fancy support for time variables to data frames feasts: Decompose time series and do other statistical things with time fable: Make forecasts  Here’s a super short example of how these all work.\nThe retail sales data we got from FRED was not seasonally adjusted, so it looks like it has a heartbeat embedded in it:\nretail_sales \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;RSXFSN\u0026quot;) ggplot(retail_sales, aes(x = date, y = price)) + geom_line() We can divide this trend into its main components: the trend, the seasonality, and stuff that’s not explained by either the trend or the seasonality. To do that, we need to first modify our little dataset and tell it to be a time-enabled data frame (a tsibble) that is indexed by the year+month for each row. We’ll create a new column called year_month and then use as_tsibble() to tell R that this is really truly dealing with time:\nlibrary(tsibble) # For embedding time things into data frames retail_sales \u0026lt;- fred_raw %\u0026gt;% filter(symbol == \u0026quot;RSXFSN\u0026quot;) %\u0026gt;% mutate(year_month = yearmonth(date)) %\u0026gt;% as_tsibble(index = year_month) retail_sales ## # A tsibble: 340 x 4 [1M] ## symbol date price year_month ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;mth\u0026gt; ## 1 RSXFSN 1992-01-01 130683 1992 Jan ## 2 RSXFSN 1992-02-01 131244 1992 Feb ## 3 RSXFSN 1992-03-01 142488 1992 Mar ## 4 RSXFSN 1992-04-01 147175 1992 Apr ## 5 RSXFSN 1992-05-01 152420 1992 May ## 6 RSXFSN 1992-06-01 151849 1992 Jun ## 7 RSXFSN 1992-07-01 152586 1992 Jul ## 8 RSXFSN 1992-08-01 152476 1992 Aug ## 9 RSXFSN 1992-09-01 148158 1992 Sep ## 10 RSXFSN 1992-10-01 155987 1992 Oct ## # … with 330 more rows Notice that the year_month column is now just the year+month. Neato.\nNext we need to create a time series model using that data. There are lots of different ways to model time series, and distinguishing between the different types is way beyond the scope of this class. Rob Hyndman’s free books covers them all. We’ll do this with STL decomposition (“Seasonal and Trend decomposition using Loess”) There are other models we can use, like ETS or ARIMA, but again, that’s all beyond this class.\nlibrary(feasts) # For decomposition things like STL() retail_model \u0026lt;- retail_sales %\u0026gt;% model(stl = STL(price)) retail_model ## # A mable: 1 x 1 ## stl ## \u0026lt;model\u0026gt; ## 1 \u0026lt;STL\u0026gt; The decomposition model we create is kind of boring and useless—it’s all stored in a single cell.\nWe can extract the different components of the decomposition with the components() function:\nretail_components \u0026lt;- components(retail_model) retail_components ## # A dable: 340 x 7 [1M] ## # Key: .model [1] ## # STL Decomposition: price = trend + season_year + remainder ## .model year_month price trend season_year remainder season_adjust ## \u0026lt;chr\u0026gt; \u0026lt;mth\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 stl 1992 Jan 130683 148453. -22505. 4735. 153188. ## 2 stl 1992 Feb 131244 148960. -23009. 5292. 154253. ## 3 stl 1992 Mar 142488 149468. -1326. -5654. 143814. ## 4 stl 1992 Apr 147175 149976. -2978. 177. 150153. ## 5 stl 1992 May 152420 150513. 5927. -4020. 146493. ## 6 stl 1992 Jun 151849 151051. 3205. -2407. 148644. ## 7 stl 1992 Jul 152586 151589. 294. 703. 152292. ## 8 stl 1992 Aug 152476 152155. 4343. -4022. 148133. ## 9 stl 1992 Sep 148158 152722. -6162. 1598. 154320. ## 10 stl 1992 Oct 155987 153289. -33.3 2732. 156020. ## # … with 330 more rows And we can use the autoplot() function from the feasts package to quickly plot all the components. The plot that autoplot() creates is made with ggplot, so any normal ggplot layers work with it:\nautoplot(retail_components) + labs(x = NULL) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) We can also plot individual components on their own using the retail_components dataset we made. Here’s seasonality by itself:\nggplot(retail_components, aes(x = year_month, y = season_year)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line() + scale_y_continuous(labels = dollar) + labs(x = NULL, y = \u0026quot;Difference from trend, millions of dollars\u0026quot;, title = \u0026quot;Seasonal trends in retail sales\u0026quot;, subtitle = \u0026quot;Nominal US dollars\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) And here’s the trend by itself:\nggplot(retail_components, aes(x = year_month, y = trend)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line() + scale_y_continuous(labels = dollar) + labs(x = NULL, y = \u0026quot;Trend, millions of dollars\u0026quot;, title = \u0026quot;Seasonally adjusted trends in retail sales\u0026quot;, subtitle = \u0026quot;Nominal US dollars\u0026quot;) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;)) If you want more control over the combined decomposed plot you can either (1) make individual plots for each of the components and then stitch them together with patchwork, or (2) make the components dataset tidy and facet by component. Here’s what that looks like:\nretail_components_tidy \u0026lt;- retail_components %\u0026gt;% # Get rid of this column select(-season_adjust) %\u0026gt;% # Take all these component columns and put them into a long column pivot_longer(cols = c(price, trend, season_year, remainder), names_to = \u0026quot;component\u0026quot;, values_to = \u0026quot;value\u0026quot;) %\u0026gt;% # Recode this values so they\u0026#39;re nicer mutate(component = recode(component, price = \u0026quot;Actual data\u0026quot;, trend = \u0026quot;Trend\u0026quot;, season_year = \u0026quot;Seasonality\u0026quot;, remainder = \u0026quot;Remainder\u0026quot;)) %\u0026gt;% # Make the component categories follow the order they\u0026#39;re in in the data so # that \u0026quot;Actual data\u0026quot; is first, etc. mutate(component = fct_inorder(component)) retail_components_tidy ## # A tibble: 1,360 x 4 ## .model year_month component value ## \u0026lt;chr\u0026gt; \u0026lt;mth\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 stl 1992 Jan Actual data 130683 ## 2 stl 1992 Jan Trend 148453. ## 3 stl 1992 Jan Seasonality -22505. ## 4 stl 1992 Jan Remainder 4735. ## 5 stl 1992 Feb Actual data 131244 ## 6 stl 1992 Feb Trend 148960. ## 7 stl 1992 Feb Seasonality -23009. ## 8 stl 1992 Feb Remainder 5292. ## 9 stl 1992 Mar Actual data 142488 ## 10 stl 1992 Mar Trend 149468. ## # … with 1,350 more rows Now that we have a long dataset, we can facet by component:\nggplot(retail_components_tidy, aes(x = year_month, y = value)) + geom_rect(data = recessions, aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf), inherit.aes = FALSE, fill = \u0026quot;#B10DC9\u0026quot;, alpha = 0.3) + geom_line() + scale_y_continuous(labels = dollar) + labs(x = NULL, y = \u0026quot;Millions of dollars\u0026quot;, title = \u0026quot;Decomposed US Advance Retail Sales\u0026quot;, subtitle = \u0026quot;Nominal US dollars\u0026quot;, caption = \u0026quot;Source: US Census Bureau and FRED (RSXFSN)\u0026quot;) + facet_wrap(vars(component), ncol = 1, scales = \u0026quot;free_y\u0026quot;) + theme_minimal(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(plot.title = element_text(face = \u0026quot;bold\u0026quot;), plot.title.position = \u0026quot;plot\u0026quot;, strip.text = element_text(face = \u0026quot;bold\u0026quot;, hjust = 0)) Beautiful!\n  ","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"04fd6e11389955617e7779fefe3c7a53","permalink":"/example/11-example/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/example/11-example/","section":"example","summary":"Live coding example Complete code  Get data Look at and clean data Plotting time Improving graphics Decomposition    For this example, we’re going to use economic data from the US Federal Reserve (the Fed). The St. Louis Fed is in charge of publishing Fed economic data, and they host it all at an online portal named FRED. Instead of downloading individual time series data from the FRED website, we’ll do what with did with the World Bank WDI data and download it directly from the internet with the tidyquant package, which includes a function for working with the FRED API/website.","tags":null,"title":"Time","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Visualizing time Turning everything in   Getting started For this exercise, you’ll visualize something over time. You can use whatever data you want. Use a dataset from a past exercise, use one of the built-in datasets like gapminder from the gapminder package, download stuff from the World Bank with the WDI package, or download stuff from FRED using the tidyquant package.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  11-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 11-exercise.Rmd your-project-name.Rproj data\\ WHATEVER.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  11-exercise.zip  The example from today’s session shows how to get data from FRED, and the examples from sessions 8 and 9 show. You can also use gapminder, or any other dataset that includes a time-related column (so not mpg).\nThere’s no specific way you should visualize time. Show it as a line, or as bars, or with a heatmap, or with ridgeplots, or with whatever is most appropriate for the story you’re telling. You do not have to recreate the example from today. You’re free to do whatever you want!\nThis can be as simple or as complex as you want. You don’t need to make your plot super fancy, but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Visualizing time Do the following:\nLoad some time-related data\n Make a plot to show how that data changes over time.\n Explain why you chose to visualize the data the way you did.\n   Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"a27e85134b961f58b0b6aebe9e5acf4c","permalink":"/lab/11-exercise/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/lab/11-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Visualizing time Turning everything in   Getting started For this exercise, you’ll visualize something over time. You can use whatever data you want. Use a dataset from a past exercise, use one of the built-in datasets like gapminder from the gapminder package, download stuff from the World Bank with the WDI package, or download stuff from FRED using the tidyquant package.","tags":null,"title":"Time","type":"docs"},{"authors":null,"categories":null,"content":"   Shapefiles Projections and coordinate reference systems Shapefiles to download Live coding example Complete code  Load and look at data Basic plotting World map with different projections US map with different projections Individual states Plotting multiple shapefile layers Plotting multiple shapefile layers when some are bigger than the parent shape Plotting schools in Georgia Making your own geoencoded data Plotting other data on maps    Shapefiles Shapefiles are special types of data that include information about geography, such as points (latitude, longitude), paths (a bunch of connected latitudes and longitudes) and areas (a bunch of connected latitudes and longitudes that form a complete shape). Nowadays, most government agencies provide shapefiles for their jurisdictions. For global mapping data, you can use the Natural Earth project:\n Natural Earth US Census Bureau Georgia GIS Clearinghouse (requires a free account; the interface is incredibly clunky) Atlanta Regional Council Fulton County GIS Portal City of Atlanta, Department of City Planning   Projections and coordinate reference systems As you read in this week’s readings, projections matter a lot for maps. You can convert your geographic data between different coordinate systems (or projections)1 fairly easily with sf. You can use coord_sf(crs = XXXX) to convert coordinate reference systems (CRS) as you plot, or use st_transform() to convert data frames to a different CRS.\nThere are standard indexes of more than 4,000 of these projections (!!!) at spatialreference.org or at epsg.io. Here are some common ones:\n 54002: Equidistant cylindrical projection for the world2 54004: Mercator projection for the world 54008: Sinusoidal projection for the world 54009: Mollweide projection for the world 54030: Robinson projection for the world3 4326: WGS 84: DOD GPS coordinates (standard -180 to 180 system) 4269: NAD 83: Relatively common projection for North America 102003: Albers projection specifically for the contiguous United States  Alternatively, instead of using these index numbers, you can use any of the names listed here, such as:\n \"+proj=merc\": Mercator \"+proj=robin\": Robinson \"+proj=moll\": Mollweide \"+proj=aeqd\": Azimuthal Equidistant \"+proj=cass\": Cassini-Soldner   Shapefiles to download I use a lot of different shapefiles in this example. To save you from having to go find and download each individual one, you can download this zip file:\n  shapefiles.zip  Unzip this and put all the contained folders in a folder named data if you want to follow along. You don’t need to follow along!\nYour project should be structured like this:\nyour-project-name\\ some-name.Rmd your-project-name.Rproj data\\ cb_2018_us_county_5m\\ ... cb_2018_us_county_5m.shp ... cb_2018_us_state_20m\\ ne_10m_admin_1_states_provinces\\ ne_10m_lakes\\ ne_10m_rivers_lake_centerlines\\ ne_10m_rivers_north_america\\ ne_110m_admin_0_countries\\ schools_2009\\ These shapefiles all came from these sources:\n  World map: 110m “Admin 0 - Countries” from Natural Earth  US states: 20m 2018 state boundaries from the US Census Bureau  US counties: 5m 2018 county boundaries from the US Census Bureau  US states high resolution: 10m “Admin 1 – States, Provinces” from Natural Earth  Global rivers: 10m “Rivers + lake centerlines” from Natural Earth  North American rivers: 10m “Rivers + lake centerlines, North America supplement” from Natural Earth  Global lakes: 10m “Lakes + Reservoirs” from Natural Earth  Georgia K–12 schools, 2009: “Georgia K-12 Schools” from the Georgia Department of Education (you must be logged in to access this)   Live coding example    Complete code (This is a slightly cleaned up version of the code from the video.)\nLoad and look at data First we’ll load the libraries we’re going to use:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(sf) # For GIS magic Next we’ll load all the different shapefiles we downloaded using read_sf():\n# Download \u0026quot;Admin 0 – Countries\u0026quot; from # https://www.naturalearthdata.com/downloads/110m-cultural-vectors/ world_map \u0026lt;- read_sf(\u0026quot;data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\u0026quot;) # Download cb_2018_us_state_20m.zip under \u0026quot;States\u0026quot; from # https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html us_states \u0026lt;- read_sf(\u0026quot;data/cb_2018_us_state_20m/cb_2018_us_state_20m.shp\u0026quot;) # Download cb_2018_us_county_5m.zip under \u0026quot;County\u0026quot; from # https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html us_counties \u0026lt;- read_sf(\u0026quot;data/cb_2018_us_county_5m/cb_2018_us_county_5m.shp\u0026quot;) # Download \u0026quot;Admin 1 – States, Provinces\u0026quot; from # https://www.naturalearthdata.com/downloads/10m-cultural-vectors/ us_states_hires \u0026lt;- read_sf(\u0026quot;data/ne_10m_admin_1_states_provinces/ne_10m_admin_1_states_provinces.shp\u0026quot;) # Download \u0026quot;Rivers + lake centerlines\u0026quot; from # https://www.naturalearthdata.com/downloads/10m-physical-vectors/ rivers_global \u0026lt;- read_sf(\u0026quot;data/ne_10m_rivers_lake_centerlines/ne_10m_rivers_lake_centerlines.shp\u0026quot;) # Download \u0026quot;Rivers + lake centerlines, North America supplement\u0026quot; from # https://www.naturalearthdata.com/downloads/10m-physical-vectors/ rivers_na \u0026lt;- read_sf(\u0026quot;data/ne_10m_rivers_north_america/ne_10m_rivers_north_america.shp\u0026quot;) # Download \u0026quot;Lakes + Reservoirs\u0026quot; from # https://www.naturalearthdata.com/downloads/10m-physical-vectors/ lakes \u0026lt;- read_sf(\u0026quot;data/ne_10m_lakes/ne_10m_lakes.shp\u0026quot;) # Download from https://data.georgiaspatial.org/index.asp?body=preview\u0026amp;dataId=41516 # after creating an account and logging in ga_schools \u0026lt;- read_sf(file.path(\u0026quot;data\u0026quot;, \u0026quot;schools_2009\u0026quot;, \u0026quot;DOE Schools 2009.shp\u0026quot;))  Basic plotting If you look at the world_map dataset in RStudio, you’ll see it’s just a standard data frame with 177 rows and 95 columns. The last column is the magical geometry column with the latitude/longitude details for the borders for every country. RStudio only shows you 50 columns at a time in the RStudio viewer, so you’ll need to move to the next page of columns with the » button in the top left corner.\nBecause this is just a data frame, we can do all our normal dplyr things to it. Let’s get rid of Antarctica, since it takes up a big proportion of the southern hemisphere:\nworld_sans_antarctica \u0026lt;- world_map %\u0026gt;% filter(ISO_A3 != \u0026quot;ATA\u0026quot;) Ready to plot a map? Here’s all you need to do:\nggplot() + geom_sf(data = world_sans_antarctica) If you couldn’t tell from the lecture, I’m completely blown away by how amazingly easy this every time I plot a map :)\nBecause this a regular ggplot geom, all our regular aesthetics and themes and everything work:\nggplot() + geom_sf(data = world_sans_antarctica, fill = \u0026quot;#669438\u0026quot;, color = \u0026quot;#32481B\u0026quot;, size = 0.25) + theme_void() The Natural Earth dataset happens to come with some columns with a coloring scheme with 7–13 colors (MAPCOLOR7, MAPCOLOR9, etc.) so that no countries with a shared border share a color. We can fill by that column:\nggplot() + geom_sf(data = world_sans_antarctica, aes(fill = as.factor(MAPCOLOR7)), color = \u0026quot;#401D16\u0026quot;, size = 0.25) + scale_fill_viridis_d(option = \u0026quot;plasma\u0026quot;) + guides(fill = FALSE) + theme_void()  World map with different projections Changing projections is trivial: add a coord_sf() layer where you specify the CRS you want to use.\nHere’s Robinson (yay):\nggplot() + geom_sf(data = world_sans_antarctica, fill = \u0026quot;#669438\u0026quot;, color = \u0026quot;#32481B\u0026quot;, size = 0.25) + coord_sf(crs = 54030) + # Robinson # Or use the name instead of the number # coord_sf(crs = \u0026quot;+proj=robin\u0026quot;) theme_void() Here’s sinusoidal:\nggplot() + geom_sf(data = world_sans_antarctica, fill = \u0026quot;#669438\u0026quot;, color = \u0026quot;#32481B\u0026quot;, size = 0.25) + coord_sf(crs = 54008) + # Sinusoidal theme_void() And here’s Mercator (ewww):\nggplot() + geom_sf(data = world_sans_antarctica, fill = \u0026quot;#669438\u0026quot;, color = \u0026quot;#32481B\u0026quot;, size = 0.25) + coord_sf(crs = 3785) + # Mercator # Or use the name instead of the number # coord_sf(crs = \u0026quot;+proj=merc\u0026quot;) theme_void() Note: Sometimes Windows doesn’t like using the raw number like coord_sf(crs = 54030). If you get an error about a missing or unknown CRS, there are two workarounds: find and look up the name abbreviation like coord_sf(crs = \"+proj=robin\"), or add the prefix “ESRI” like coord_sf(crs = \"ESRI:54030\")\n  US map with different projections This same process works for any shapefile. The map of the US can also be projected differently—two common projections are NAD83 and Albers. We’ll take the us_states dataset, remove Alaska, Hawaii, and Puerto Rico (they’re so far from the rest of the lower 48 states that they make an unusable map—if you want to include them, it’s easiest to plot them as separate plots and use patchwork to stitch them together), and plot it.\nlower_48 \u0026lt;- us_states %\u0026gt;% filter(!(NAME %in% c(\u0026quot;Alaska\u0026quot;, \u0026quot;Hawaii\u0026quot;, \u0026quot;Puerto Rico\u0026quot;))) ggplot() + geom_sf(data = lower_48, fill = \u0026quot;#192DA1\u0026quot;, color = \u0026quot;white\u0026quot;, size = 0.25) + coord_sf(crs = 4269) + # NAD83 theme_void() ggplot() + geom_sf(data = lower_48, fill = \u0026quot;#192DA1\u0026quot;, color = \u0026quot;white\u0026quot;, size = 0.25) + coord_sf(crs = 102003) + # Albers theme_void()  Individual states Again, because these shapefiles are really just fancy data frames, we can filter them with normal dplyr functions. Let’s plot just Georgia:\nonly_georgia \u0026lt;- lower_48 %\u0026gt;% filter(NAME == \u0026quot;Georgia\u0026quot;) ggplot() + geom_sf(data = only_georgia, fill = \u0026quot;#EC8E55\u0026quot;) + theme_void() We can also use a different projection. If we look at spatialreference.org, there’s a version of NAD83 that’s focused specifically on Georgia.\nggplot() + geom_sf(data = only_georgia, fill = \u0026quot;#EC8E55\u0026quot;) + theme_void() + coord_sf(crs = 2239) # NAD83 focused on Georgia There’s one small final issue though: we’re missing all the Atlantic islands in the southeast like Cumberland Island and Amelia Island. That’s because we’re using the Census’s low resolution (20m) data. That’s fine for the map of the whole country, but if we’re looking at a single state, we probably want better detail in the borders. We can use the Census’s high resolution (500k) data, but even then it doesn’t include the islands for whatever reason, but Natural Earth has high resolution US state data that does have the islands, so we can use that:\nonly_georgia_high \u0026lt;- us_states_hires %\u0026gt;% filter(iso_3166_2 == \u0026quot;US-GA\u0026quot;) ggplot() + geom_sf(data = only_georgia_high, fill = \u0026quot;#EC8E55\u0026quot;) + theme_void() + coord_sf(crs = 2239) # NAD83 focused on Georgia Perfect.\n Plotting multiple shapefile layers The state shapefiles from the Census Bureau only include state boundaries. If we want to see counties in Georgia, we need to download and load the Census’s county shapefiles (which we did above). We can then add a second geom_sf() layer for the counties.\nFirst we need to filter the county data to only include Georgia counties. The counties data doesn’t include a column with the state name or state abbreviation, but it does include a column named STATEFP, which is the state FIPS code. Looking at lower_48 we can see that the state FIPS code for Georgia is 13, so we use that to filter.\nga_counties \u0026lt;- us_counties %\u0026gt;% filter(STATEFP == 13) Now we can plot just the counties:\nggplot() + geom_sf(data = ga_counties) + theme_void() Technically we can just draw the county boundaries instead of layer the state boundary + the counties, since the borders of the counties make up the border of the state. But there’s an advantage to including both: we can use different aesthetics on each, like adding a thicker border on the state:\nggplot() + geom_sf(data = only_georgia_high, color = \u0026quot;#EC8E55\u0026quot;, size = 3) + geom_sf(data = ga_counties, fill = \u0026quot;#A5D46A\u0026quot;, color = \u0026quot;white\u0026quot;) + theme_void() It’s also useful if we want to only show some counties, like metropolitan Atlanta:\natl_counties \u0026lt;- ga_counties %\u0026gt;% filter(NAME %in% c(\u0026quot;Cherokee\u0026quot;, \u0026quot;Clayton\u0026quot;, \u0026quot;Cobb\u0026quot;, \u0026quot;DeKalb\u0026quot;, \u0026quot;Douglas\u0026quot;, \u0026quot;Fayette\u0026quot;, \u0026quot;Fulton\u0026quot;, \u0026quot;Gwinnett\u0026quot;, \u0026quot;Henry\u0026quot;, \u0026quot;Rockdale\u0026quot;)) ggplot() + geom_sf(data = only_georgia_high, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = atl_counties, fill = \u0026quot;#A5D46A\u0026quot;, color = \u0026quot;white\u0026quot;) + theme_void()  Plotting multiple shapefile layers when some are bigger than the parent shape So far we’ve been able to filter out states and counties that we don’t want to plot using filter(), which works because the shapefiles have geometry data for each state or county. But what if you’re plotting stuff that doesn’t follow state or county boundaries, like freeways, roads, rivers, or lakes?\nAt the beginning we loaded a shapefile for all large and small rivers in the US. Look at the first few rows of rivers_na:\nhead(rivers_na) ## Simple feature collection with 6 features and 37 fields ## geometry type: MULTILINESTRING ## dimension: XY ## bbox: xmin: -100 ymin: 29 xmax: -86 ymax: 36 ## CRS: 4326 ## # A tibble: 6 x 38 ## featurecla scalerank rivernum dissolve name name_alt note name_full min_zoom strokeweig uident min_label label wikidataid name_ar name_bn name_de ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 River 10 22360 22360Ri… Colo… \u0026lt;NA\u0026gt; ID i… Colorado… 6 0.3 1.99e6 7 Colo… Q847785 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Colora… ## 2 River 10 22572 22572Ri… Cima… \u0026lt;NA\u0026gt; ID i… Cimarron… 6 0.25 2.15e6 7 Cima… Q1092055 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Cimarr… ## 3 River 10 22519 22519Ri… Wash… \u0026lt;NA\u0026gt; ID i… Washita … 6 0.25 1.95e6 7 Wash… Q2993598 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Washita ## 4 River 10 22519 22519Ri… Wash… \u0026lt;NA\u0026gt; ID i… Washita … 6 0.15 1.95e6 7 Wash… Q2993598 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Washita ## 5 River 11 22422 22422Ri… Cone… \u0026lt;NA\u0026gt; ID i… Conecuh … 6.7 0.15 2.17e6 7.7 Cone… Q5159475 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 6 River 10 22421 22421Ri… Pea \u0026lt;NA\u0026gt; ID i… Pea River 6 0.15 1.96e6 7 Pea Q7157190 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## # … with 21 more variables: name_en \u0026lt;chr\u0026gt;, name_es \u0026lt;chr\u0026gt;, name_fr \u0026lt;chr\u0026gt;, name_el \u0026lt;chr\u0026gt;, name_hi \u0026lt;chr\u0026gt;, name_hu \u0026lt;chr\u0026gt;, name_id \u0026lt;chr\u0026gt;, name_it \u0026lt;chr\u0026gt;, ## # name_ja \u0026lt;chr\u0026gt;, name_ko \u0026lt;chr\u0026gt;, name_nl \u0026lt;chr\u0026gt;, name_pl \u0026lt;chr\u0026gt;, name_pt \u0026lt;chr\u0026gt;, name_ru \u0026lt;chr\u0026gt;, name_sv \u0026lt;chr\u0026gt;, name_tr \u0026lt;chr\u0026gt;, name_vi \u0026lt;chr\u0026gt;, ## # name_zh \u0026lt;chr\u0026gt;, wdid_score \u0026lt;int\u0026gt;, ne_id \u0026lt;dbl\u0026gt;, geometry \u0026lt;MULTILINESTRING [°]\u0026gt; The first row is the whole Colorado river, which flows through seven states. We can’t just use filter() to only select some parts of it based on states.\nHere’s what happens if we combine our Georgia map with rivers and lakes:\nggplot() + geom_sf(data = only_georgia, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = rivers_na) + theme_void() It plots Georgia, and it’s filled with orange, but it also plots every single river in North America. Oops.\nWe need to do a little GIS work to basically use only_georgia as a cookie cutter and keep only the rivers that are contained in the only_georgia boundaries. Fortunately, there’s a function in the sf package that does this: st_intersection(). Feed it two shapefile datasets and it will select the parts of the second that fall within the boundaries of the first:\nga_rivers_na \u0026lt;- st_intersection(only_georgia, rivers_na) ## Error in geos_op2_geom(\u0026quot;intersection\u0026quot;, x, y): st_crs(x) == st_crs(y) is not TRUE Oh no! An error! It’s complaining that the reference systems used in these two datasets don’t match. We can check the CRS with st_crs():\nst_crs(only_georgia) ## Coordinate Reference System: ## User input: 4269 ## wkt: ## GEOGCS[\u0026quot;NAD83\u0026quot;, ## DATUM[\u0026quot;North_American_Datum_1983\u0026quot;, ## SPHEROID[\u0026quot;GRS 1980\u0026quot;,6378137,298.257222101, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;7019\u0026quot;]], ## TOWGS84[0,0,0,0,0,0,0], ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;6269\u0026quot;]], ## PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;8901\u0026quot;]], ## UNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;9122\u0026quot;]], ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;4269\u0026quot;]] st_crs(rivers_na) ## Coordinate Reference System: ## User input: 4326 ## wkt: ## GEOGCS[\u0026quot;WGS 84\u0026quot;, ## DATUM[\u0026quot;WGS_1984\u0026quot;, ## SPHEROID[\u0026quot;WGS 84\u0026quot;,6378137,298.257223563, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;7030\u0026quot;]], ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;6326\u0026quot;]], ## PRIMEM[\u0026quot;Greenwich\u0026quot;,0, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;8901\u0026quot;]], ## UNIT[\u0026quot;degree\u0026quot;,0.0174532925199433, ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;9122\u0026quot;]], ## AUTHORITY[\u0026quot;EPSG\u0026quot;,\u0026quot;4326\u0026quot;]] The Georgia map uses 4269 (or NAD83), while the rivers map uses 4326 (or the GPS system of latitude and longitude). We need to convert one of them to make them match. It doesn’t matter which one.\nonly_georgia_4326 \u0026lt;- only_georgia %\u0026gt;% st_transform(crs = 4326) ga_rivers_na \u0026lt;- st_intersection(only_georgia_4326, rivers_na) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant throughout all geometries You’ll get an ominous warning, but you should be okay—it’s just because flattening globes into flat planes is hard, and the cutting might not be 100% accurate, but it’ll be close enough for our mapping purposes.\nNow we can plot our state shape and the truncated rivers:\nggplot() + geom_sf(data = only_georgia, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = ga_rivers_na) + theme_void() Hey! It worked! Let’s put all the rivers and lakes on at once and make it a little more artsy. We’ll use the high resolution Georgia map too, which conveniently already matches the CRS of the rivers and lakes:\nga_rivers_na \u0026lt;- st_intersection(only_georgia_high, rivers_na) ga_rivers_global \u0026lt;- st_intersection(only_georgia_high, rivers_global) ga_lakes \u0026lt;- st_intersection(only_georgia_high, lakes) ggplot() + geom_sf(data = only_georgia_high, color = \u0026quot;black\u0026quot;, size = 0.1, fill = \u0026quot;black\u0026quot;) + geom_sf(data = ga_rivers_global, size = 0.3, color = \u0026quot;grey80\u0026quot;) + geom_sf(data = ga_rivers_na, size = 0.15, color = \u0026quot;grey80\u0026quot;) + geom_sf(data = ga_lakes, size = 0.3, fill = \u0026quot;grey80\u0026quot;, color = NA) + coord_sf(crs = 4269) + # NAD83 theme_void() Heck yeah. That’s a great map. This is basically what Kieran Healy did here, but he used even more detailed shapefiles from the US Geological Survey.\n Plotting schools in Georgia Shapefiles are not limited to just lines and areas—they can also contain points. I made a free account at the Georgia GIS Clearinghouse, searched for “schools” and found a shapefile of all the K–12 schools in 2009. This is the direct link to the page, but it only works if you’re logged in to their system. This is the official metadata for the shapefile, which you can see if you’re not logged in, but you can’t download anything. It’s a dumb system and other states are a lot better at offering their GIS data (like, here’s a shapefile of all of Utah’s schools and libraries as of 2017, publicly accessible without an account).\nWe loaded the shapefile up at the top, but now let’s look at it:\nhead(ga_schools) ## Simple feature collection with 6 features and 16 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2100000 ymin: 320000 xmax: 2200000 ymax: 5e+05 ## proj4string: +proj=tmerc +lat_0=30 +lon_0=-84.16666666666667 +k=0.9999 +x_0=700000.0000001107 +y_0=0 +ellps=GRS80 +units=us-ft +no_defs ## # A tibble: 6 x 17 ## ID DATA COUNTY DISTRICT SCHOOLNAME GRADES ADDRESS CITY STATE ZIP TOTAL SCHOOLID DOE_CONGRE CONGRESS SENATE HOUSE geometry ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;POINT [US_survey_foot]\u0026gt; ## 1 4313 224 Early Early Co… Early Coun… PK,KK,… 283 Ma… Blak… GA 3982… 1175 43549 2 002 011 149 (2052182 494322) ## 2 4321 227 Early Early Co… ETN Eckerd… 06,07,… 313 E … Blak… GA 3982… 30 47559 2 002 011 149 (2053200 5e+05) ## 3 4329 226 Early Early Co… Early Coun… 06,07,… 12053 … Blak… GA 3982… 539 43550 2 002 011 149 (2055712 5e+05) ## 4 4337 225 Early Early Co… Early Coun… 09,10,… 12020 … Blak… GA 3982… 716 43552 2 002 011 149 (2055712 5e+05) ## 5 4345 189 Decatur Decatur … John Johns… PK,KK,… 1947 S… Bain… GA 3981… 555 43279 2 002 011 172 (2168090 321781) ## 6 4353 192 Decatur Decatur … Potter Str… PK,KK,… 725 Po… Bain… GA 3981… 432 43273 2 002 011 172 (2168751 327375) We have a bunch of columns like GRADES that has a list of what grades are included in the school, and TOTAL, which I’m guessing is the number of students. Let’s map it!\nIf we add a geom_sf() layer just for ga_schools, it’ll plot a bunch of points:\nggplot() + geom_sf(data = ga_schools) One of these rows is wildly miscoded and ended up Indonesia! If you sort by the geometry column in RStudio, you’ll see that it’s most likely Allatoona High School in Cobb County (id = 22097). The coordinates are different from all the others, and it has no congressional district information. Let’s remove it.\nga_schools_fixed \u0026lt;- ga_schools %\u0026gt;% filter(ID != 22097) ggplot() + geom_sf(data = ga_schools_fixed) That’s better. However, all we’re plotting now are the points—we’ve lost the state and/or county boundaries. Let’s include those:\nggplot() + geom_sf(data = only_georgia_high) + geom_sf(data = ga_schools_fixed) + theme_void() We’re getting closer. We have some issues with overplotting, so let’s shrink the points down and make them a little transparent:\nggplot() + geom_sf(data = only_georgia_high) + geom_sf(data = ga_schools_fixed, size = 0.5, alpha = 0.5) + theme_void() Neat. One last thing we can do is map the TOTAL column to the color aesthetic and color the points by how many students attend each school:\nggplot() + geom_sf(data = only_georgia_high) + geom_sf(data = ga_schools_fixed, aes(color = TOTAL), size = 0.75, alpha = 0.5) + scale_color_viridis_c() + theme_void() Most schools appear to be under 1,000 students, except for a cluster in Gwinnett County north of Atlanta. Its high schools have nearly 4,000 students each!\nga_schools_fixed %\u0026gt;% select(COUNTY, SCHOOLNAME, TOTAL) %\u0026gt;% arrange(desc(TOTAL)) %\u0026gt;% head() ## Simple feature collection with 6 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2300000 ymin: 1400000 xmax: 2400000 ymax: 1500000 ## proj4string: +proj=tmerc +lat_0=30 +lon_0=-84.16666666666667 +k=0.9999 +x_0=700000.0000001107 +y_0=0 +ellps=GRS80 +units=us-ft +no_defs ## # A tibble: 6 x 4 ## COUNTY SCHOOLNAME TOTAL geometry ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;POINT [US_survey_foot]\u0026gt; ## 1 Gwinnett Mill Creek High School 3997 (2384674 1482772) ## 2 Gwinnett Collins Hill High School 3720 (2341010 1461730) ## 3 Gwinnett Brookwood High School 3455 (2334543 1413396) ## 4 Gwinnett Grayson High School 3230 (2370186 1408579) ## 5 Gwinnett Peachtree Ridge High School 3118 (2319344 1459458) ## 6 Gwinnett Berkmar High School 3095 (2312983 1421933)  Making your own geoencoded data So, plotting shapefiles with geom_sf() is magical because sf deals with all of the projection issues for us automatically and it figures out how to plot all the latitude and longitude data for us automatically. But lots of data doesn’t some as shapefiles. The rats data from mini project 1, for instance, has two columns indicating the latitude and longitude of each rat sighting, but those are stored as just numbers. If we try to use geom_sf() with the rat data, it won’t work. We need that magical geometry column.\nFortunately, if we have latitude and longitude information, we can make our own geometry column.\nLet’s say we want to mark some cities on our map of Georgia. We can make a mini dataset using tribble(). I found these points from Google Maps: right click anywhere in Google Maps, select “What’s here?”, and you’ll see the exact coordinates for that spot.\nga_cities \u0026lt;- tribble( ~city, ~lat, ~long, \u0026quot;Atlanta\u0026quot;, 33.748955, -84.388099, \u0026quot;Athens\u0026quot;, 33.950794, -83.358884, \u0026quot;Savannah\u0026quot;, 32.113192, -81.089350 ) ga_cities ## # A tibble: 3 x 3 ## city lat long ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Atlanta 33.7 -84.4 ## 2 Athens 34.0 -83.4 ## 3 Savannah 32.1 -81.1 This is just a normal dataset, and the lat and long columns are just numbers. R doesn’t know that those are actually geographic coordinates. This is similar to the rats data, or any other data that has columns for latitude and longitude.\nWe can convert those two columns to the magic geometry column with the st_as_sf() function. We have to define two things in the function: which coordinates are the longitude and latitude, and what CRS the coordinates are using. Google Maps uses 4326, or the GPS system, so we specify that:\nga_cities_geometry \u0026lt;- ga_cities %\u0026gt;% st_as_sf(coords = c(\u0026quot;long\u0026quot;, \u0026quot;lat\u0026quot;), crs = 4326) ga_cities_geometry ## Simple feature collection with 3 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: -84 ymin: 32 xmax: -81 ymax: 34 ## CRS: EPSG:4326 ## # A tibble: 3 x 2 ## city geometry ## \u0026lt;chr\u0026gt; \u0026lt;POINT [°]\u0026gt; ## 1 Atlanta (-84 34) ## 2 Athens (-83 34) ## 3 Savannah (-81 32) The longitude and latitude columns are gone now, and we have a single magical geometry column. That means we can plot it with geom_sf():\nggplot() + geom_sf(data = only_georgia_high, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = ga_cities_geometry, size = 3) + theme_void() We can use geom_sf_label() (or geom_sf_text()) to add labels in the correct locations too. It will give you a warning, but you can ignore it—again, it’s complaining that the positioning might not be 100% accurate because of issues related to taking a globe and flattening it. It’s fine.\nggplot() + geom_sf(data = only_georgia_high, fill = \u0026quot;#EC8E55\u0026quot;) + geom_sf(data = ga_cities_geometry, size = 3) + geom_sf_label(data = ga_cities_geometry, aes(label = city), nudge_y = 0.2) + theme_void()  Plotting other data on maps So far we’ve just plotted whatever data the shapefile creators decided to include and publish in their data. But what if you want to visualize some other variable on a map? We can do this by combining our shapefile data with any other kind of data, as long as the two have a shared column. For instance, we can make a choropleth map of life expectancy with data from the World Bank.\nFirst, let’s grab some data from the World Bank for just 2015:\nlibrary(WDI) # For getting data from the World Bank indicators \u0026lt;- c(\u0026quot;SP.DYN.LE00.IN\u0026quot;) # Life expectancy wdi_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE, start = 2015, end = 2015) Let’s see what we got:\nhead(wdi_raw) ## # A tibble: 6 x 11 ## iso2c country SP.DYN.LE00.IN year iso3c region capital longitude latitude income lending ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1A Arab World 71.2 2015 ARB Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 2 1W World 71.9 2015 WLD Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 3 4E East Asia \u0026amp; Pacific (excluding high i… 74.5 2015 EAP Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 4 7E Europe \u0026amp; Central Asia (excluding high… 72.7 2015 ECA Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 5 8S South Asia 68.6 2015 SAS Aggregates \u0026lt;NA\u0026gt; NA NA Aggregat… Aggregates ## 6 AD Andorra NA 2015 AND Europe \u0026amp; Central … Andorra la Vel… 1.52 42.5 High inc… Not classif… We have a bunch of columns here, but we care about two in particular: life expectancy, and the ISO3 code. This three-letter code is a standard system for identifying countries (see the full list here), and that column will let us combine this World Bank data with the global shapefile, which also has a column for the ISO3 code.\n(We also have columns for the latitude and longitude for each capital, so we could theoretically convert those to a geometry column with st_as_sf() and plot world capitals, which would be neat, but we won’t do that now.)\nLet’s clean up the WDI data and shrink it down substantially:\nwdi_clean_small \u0026lt;- wdi_raw %\u0026gt;% select(life_expectancy = SP.DYN.LE00.IN, iso3c) wdi_clean_small ## # A tibble: 264 x 2 ## life_expectancy iso3c ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 71.2 ARB ## 2 71.9 WLD ## 3 74.5 EAP ## 4 72.7 ECA ## 5 68.6 SAS ## 6 NA AND ## 7 77.3 ARE ## 8 63.4 AFG ## 9 76.5 ATG ## 10 78.0 ALB ## # … with 254 more rows Next we need to merge this tiny dataset into the world_map_sans_antarctica shapefile data we were using earlier. To do this we’ll use a function named left_join(). We feed two data frames into left_join(), and R will keep all the rows from the first and include all the columns from both the first and the second wherever the two datasets match with one specific column. That’s wordy and weird—stare at this animation here for a few seconds to see what’s really going to happen. We’re essentially going to append the World Bank data to the end of the world shapefiles and line up rows that have matching ISO3 codes. The ISO3 column is named ISO_A3 in the shapefile data, and it’s named iso3c in the WDI data, so we tell left_join() that those are the same column:\nworld_map_with_life_expectancy \u0026lt;- world_sans_antarctica %\u0026gt;% left_join(wdi_clean_small, by = c(\u0026quot;ISO_A3\u0026quot; = \u0026quot;iso3c\u0026quot;)) If you look at this dataset in RStudio now and look at the last column, you’ll see the WDI life expectancy right next to the magic geometry column.\nWe technically didn’t need to shrink the WDI data down to just two columns—had we left everything else, all the WDI columns would have come over to the world_sans_antarctica, including columns for region and income level, etc. But I generally find it easier and cleaner to only merge in the columns I care about instead of making massive datasets with a billion extra columns.\nNow that we have a column for life expectancy, we can map it to the fill aesthetic and fill each country by 2015 life expectancy:\nggplot() + geom_sf(data = world_map_with_life_expectancy, aes(fill = life_expectancy), size = 0.25) + coord_sf(crs = 54030) + # Robinson scale_fill_viridis_c(option = \u0026quot;viridis\u0026quot;) + labs(fill = \u0026quot;Life expectancy\u0026quot;) + theme_void() + theme(legend.position = \u0026quot;bottom\u0026quot;) Voila! Global life expectancy in 2015!\n(Sharp-eyed readers will notice that France and Norway are grayed out because they’re missing data. That’s because the ISO_A3 code in the Natural Earth data is missing for both France and Norway for whatever reason, so the WDI data didn’t merge with those rows. To fix that, we can do some manual recoding before joining in the WDI data)\nworld_sans_antarctica_fixed \u0026lt;- world_sans_antarctica %\u0026gt;% mutate(ISO_A3 = case_when( # If the country name is Norway or France, redo the ISO3 code ADMIN == \u0026quot;Norway\u0026quot; ~ \u0026quot;NOR\u0026quot;, ADMIN == \u0026quot;France\u0026quot; ~ \u0026quot;FRA\u0026quot;, # Otherwise use the existing ISO3 code TRUE ~ ISO_A3 )) %\u0026gt;% left_join(wdi_clean_small, by = c(\u0026quot;ISO_A3\u0026quot; = \u0026quot;iso3c\u0026quot;)) ggplot() + geom_sf(data = world_sans_antarctica_fixed, aes(fill = life_expectancy), size = 0.25) + coord_sf(crs = 54030) + # Robinson scale_fill_viridis_c(option = \u0026quot;viridis\u0026quot;) + labs(fill = \u0026quot;Life expectancy\u0026quot;) + theme_void() + theme(legend.position = \u0026quot;bottom\u0026quot;)    TECHNICALLY coordinate systems and projection systems are different things, but I’m not a geographer and I don’t care that much about the nuance.↩︎\n This is essentially the Gall-Peters projection from the West Wing clip.↩︎\n This is my favorite world projection.↩︎\n   ","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"8f294a1c92be1a918ba3fa24cc427a78","permalink":"/example/12-example/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/example/12-example/","section":"example","summary":"Shapefiles Projections and coordinate reference systems Shapefiles to download Live coding example Complete code  Load and look at data Basic plotting World map with different projections US map with different projections Individual states Plotting multiple shapefile layers Plotting multiple shapefile layers when some are bigger than the parent shape Plotting schools in Georgia Making your own geoencoded data Plotting other data on maps    Shapefiles Shapefiles are special types of data that include information about geography, such as points (latitude, longitude), paths (a bunch of connected latitudes and longitudes) and areas (a bunch of connected latitudes and longitudes that form a complete shape).","tags":null,"title":"Space","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: World map Bonus (optional) task!: Personal map Turning everything in   Getting started For this exercise, you’ll visualize the proportion of the world that uses the interent. You’ll use data from Max Roser’s Our World in Data project, which collects all sorts of interesting cross-national data. You’ll also use national shapefiles from Natural Earth.\nDownload these two data files:\n  share-of-individuals-using-the-internet-1990-2015.csv  ne_110m_admin_0_countries.zip. This is the “110m Admin 0—Countries” shapefile from Natural Earth. It will download as a .zip file. Unzip the file and move the entire ne_110m_admin_0_countries directory into your data folder.  You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and join the two datasets. Download that here and include it in your project:\n  12-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 12-exercise.Rmd your-project-name.Rproj data\\ share-of-individuals-using-the-internet-1990-2015.csv ne_110m_admin_0_countries/ ... ne_110m_admin_0_countries.shp ... To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  12-exercise.zip  The example from today’s session shows how to load and plot shapefiles and will be incredibly helpful as you do this exercise.\nThis can be as simple or as complex as you want. You don’t need to make your plot super fancy, but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: World map Make a map showing the proportion of individuals in each country that had access to the internet in 2015.\n I’ve provided some starter code in the R Markdown file. You’ll want to fill each country by the users column. Make sure you choose a good projection. See the “Projections and coordinate reference systems” section from the example.  Bonus optional extra fun: Use your comparison/time skills to show the change in internet access between 2000 and 2015, perhaps with facetting some years, or calculating ratios or proportions or percent changes\n Bonus (optional) task!: Personal map Draw your own map with your own points. This could be a map of places you’ve lived, or a map of places you’ve visited, or a map of places you want to visit. Anything!\nThe only requirement is that you find an appropriate shapefile (states, counties, world, etc.), collect latitude and longitude data from Google Maps, and plot the points (with or without labels) on a map. Use multiple shapefiles if you want—add roads, rivers, lakes, whatever.\nHint: Basically follow the code from the example in the section named “Making your own geoencoded data”\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"92e9e2afff0fd74881fceaee758714a5","permalink":"/lab/12-exercise/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/lab/12-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: World map Bonus (optional) task!: Personal map Turning everything in   Getting started For this exercise, you’ll visualize the proportion of the world that uses the interent. You’ll use data from Max Roser’s Our World in Data project, which collects all sorts of interesting cross-national data. You’ll also use national shapefiles from Natural Earth.\nDownload these two data files:\n  share-of-individuals-using-the-internet-1990-2015.","tags":null,"title":"Space","type":"docs"},{"authors":null,"categories":null,"content":"   Live coding example Complete code  Get data Clean data Tokens and word counts  Single words Bigrams  Bigrams and probability Term frequency-inverse document frequency (tf-idf) Sentiment analysis Neat extra stuff  Part of speech tagging Topic modeling and fingerprinting Text features     For this example, we’re going to use the text of Little Women by Louisa May Alcott and four Shakespearean tragedies (Romeo and Juliet, King Lear, Macbeth, and Hamlet) to explore how to do some basic text visualization.\nYou can follow along if you want, but don’t feel like you have too. This is mostly just to give you a taste of different methods for visualizing text. It’s by no means comprehensive, but it is well annotated and commented and should (hopefully) be easy to follow.\nIf you want to play with part-of-speech tagging, you can download an already-tagged version of Little Women here (you’ll likely need to right click and choose “Save Link As…”):\n  little_women_tagged.csv  If you want to see other examples of text visualizations with the tidytext package, check out some of these:\n  Harry Potter Sentiment Analysis for Beginners (this uses the harrypotter package, which you can install from GitHub (not from CRAN))  Peer Christensen “Fair is foul, and foul is fair: a tidytext sentiment analysis of Shakespeare’s tragedies”  “Tidy text, parts of speech, and unique words in the Bible”  “Tidy text, parts of speech, and unique words in the Qur’an”  Live coding example    Complete code (This is a highly cleaned up version of the code from the video.)\nGet data First, as always, we’ll load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, etc. library(tidytext) # For neat text things library(gutenbergr) # For downloading books from Project Gutenberg We’re going to use the gutenbergr package to download some books directly from Project Gutenberg. The IDs for these books come from the URLs at their website. For instance, Little Women is book #514. We’ll store these books as `*_raw* and then clean them up later.\n# 514 Little Women little_women_raw \u0026lt;- gutenberg_download(514, meta_fields = \u0026quot;title\u0026quot;) # 1524 - Hamlet # 1532 - King Lear # 1533 - Macbeth # 1513 - Romeo and Juliet tragedies_raw \u0026lt;- gutenberg_download(c(1524, 1532, 1533, 1513), meta_fields = \u0026quot;title\u0026quot;) If you won’t want to redownload the books every time you knit (you don’t), you can do the same trick we’ve used for WDI and FRED data. Put the actual code for getting the books in a chunk with eval=FALSE on it and run it manually in RStudio when you want to get the data. Then you can write the downloaded data as a CSV file, and then load it invisibly from the CSV file when you knit:\nI first download data from Project Gutenberg: ```{r get-book, eval=FALSE} books_raw \u0026lt;- gutenberg_download(...) write_csv(books_raw, \u0026quot;data/books_raw.csv\u0026quot;) ``` ```{r load-book-data-real, include=FALSE} books_raw \u0026lt;- read_csv(\u0026quot;data/books_raw.csv\u0026quot;) ```  Clean data The data you get from Project Gutenberg comes in a tidy format, with a column for the book id, a column for the title, and a column for text. Sometimes this text column will be divided by lines in the book; sometimes it might be an entire page or paragraph or chapter. It all depends on how the book is formatted at Project Gutenberg.\nHere’s what the start of our little_women_raw data looks like:\nhead(little_women_raw) ## # A tibble: 6 x 3 ## gutenberg_id text title ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 514 LITTLE WOMEN Little Women ## 2 514 \u0026lt;NA\u0026gt; Little Women ## 3 514 \u0026lt;NA\u0026gt; Little Women ## 4 514 by Little Women ## 5 514 \u0026lt;NA\u0026gt; Little Women ## 6 514 Louisa May Alcott Little Women If we look at the data in RStudio, we can see that the actual book doesn’t start until row 70 (the first 69 rows are the table of contents and other parts of the front matter).\nIt would be nice if we had a column that indicated what chapter each line is in, since we could then group by chapter and look at patterns within chapters. Since the data doesn’t come with a chapter column, we have to make one ourselves using a fun little trick. Each chapter in the book starts with “CHAPTER ONE” or “CHAPTER TWO”, with “chapter” in ALL CAPS. We can make a variable named chapter_start that will be true if a line starts with “CHAPTER” and false if not. Then we can use the cumsum() function to take the cumulative sum of this column, which will increment up one number ever time there’s a new chapter, thus creating a helpful chapter column.\n# Clean up Little Women little_women \u0026lt;- little_women_raw %\u0026gt;% # The actual book doesn\u0026#39;t start until line 70 slice(70:n()) %\u0026gt;% # Get rid of rows where text is missing drop_na(text) %\u0026gt;% # Chapters start with CHAPTER X, so mark if each row is a chapter start # cumsum() calculates the cumulative sum, so it\u0026#39;ll increase every time there\u0026#39;s # a new chapter and automatically make chapter numbers mutate(chapter_start = str_detect(text, \u0026quot;^CHAPTER\u0026quot;), chapter_number = cumsum(chapter_start)) %\u0026gt;% # Get rid of these columns select(-gutenberg_id, -title, -chapter_start) head(little_women) ## # A tibble: 6 x 2 ## text chapter_number ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 \u0026quot;CHAPTER ONE\u0026quot; 1 ## 2 \u0026quot;PLAYING PILGRIMS\u0026quot; 1 ## 3 \u0026quot;\\\u0026quot;Christmas won\u0026#39;t be Christmas without any presents,\\\u0026quot; grumbled Jo, lying\u0026quot; 1 ## 4 \u0026quot;on the rug.\u0026quot; 1 ## 5 \u0026quot;\\\u0026quot;It\u0026#39;s so dreadful to be poor!\\\u0026quot; sighed Meg, looking down at her old\u0026quot; 1 ## 6 \u0026quot;dress.\u0026quot; 1 The data from Shakespeare is similarly messy, with just three columns:\nhead(tragedies_raw) ## # A tibble: 6 x 3 ## gutenberg_id text title ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1513 ROMEO AND JULIET Romeo and Juliet ## 2 1513 \u0026lt;NA\u0026gt; Romeo and Juliet ## 3 1513 by William Shakespeare Romeo and Juliet ## 4 1513 \u0026lt;NA\u0026gt; Romeo and Juliet ## 5 1513 \u0026lt;NA\u0026gt; Romeo and Juliet ## 6 1513 \u0026lt;NA\u0026gt; Romeo and Juliet The initial text sometimes isn’t the actual text of the book. If you look at the beginning of Hamlet, for instance, there’s a bunch of introductory stuff from editors and transcribers. In real life, we’d want to find a systematic way to get rid of that (perhaps by looking at how many introductory rows there are in each of the four plays and removing those rows), but for now, we’ll just live with it and pretend Shakespeare wrote these notes. 🤷\nWe could also figure out a systematic way to indicate acts and scenes, but that’s tricky, so we won’t for this example. (This guy did though!)\nNow that we have tidy text data, let’s do stuff with it!\n Tokens and word counts Single words One way we can visualize text is to look at word frequencies and find the most common words. This is even more important when looking across documents.\nRight now the text we have is tidy, but it is based on lines of text, not words. In order to count words correctly, we need each token (or text element, whether it be a word or bigram or paragraph or whatever) to be in its own row. The unnest_tokens() functions from tidytext does this for us. The first argument is the name of the column we want to create; the second argument is the name of the column we want to split into tokens.\nLet’s just work with the Shakespeare tragedies:\ntragedies_words \u0026lt;- tragedies_raw %\u0026gt;% drop_na(text) %\u0026gt;% unnest_tokens(word, text) head(tragedies_words) ## # A tibble: 6 x 3 ## gutenberg_id title word ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1513 Romeo and Juliet romeo ## 2 1513 Romeo and Juliet and ## 3 1513 Romeo and Juliet juliet ## 4 1513 Romeo and Juliet by ## 5 1513 Romeo and Juliet william ## 6 1513 Romeo and Juliet shakespeare Now that we have words, we can filter and count the words. Here’s what’s happening in this next chunk:\n We use anti_join() to remove all common stop words like “a” and “the” that are listed in the stop_words dataset that is loaded when you load tidytext We count how many times each word appears in each title/play We only keep the top 15 words  top_words_tragedies \u0026lt;- tragedies_words %\u0026gt;% # Remove stop words anti_join(stop_words) %\u0026gt;% # Get rid of old timey words and stage directions filter(!(word %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;haue\u0026quot;, \u0026quot;thee\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;))) %\u0026gt;% # Count all the words in each play count(title, word, sort = TRUE) %\u0026gt;% # Keep top 15 in each play group_by(title) %\u0026gt;% top_n(15) %\u0026gt;% ungroup() %\u0026gt;% # Make the words an ordered factor so they plot in order mutate(word = fct_inorder(word)) top_words_tragedies ## # A tibble: 63 x 3 ## title word n ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Hamlet, Prince of Denmark ham 358 ## 2 Romeo and Juliet romeo 296 ## 3 Macbeth macbeth 282 ## 4 The Tragedy of King Lear lear 230 ## 5 Hamlet, Prince of Denmark lord 223 ## 6 Hamlet, Prince of Denmark king 197 ## 7 Romeo and Juliet juliet 178 ## 8 The Tragedy of King Lear kent 174 ## 9 Romeo and Juliet nurse 149 ## 10 Romeo and Juliet capulet 145 ## # … with 53 more rows Now we can plot these results, facetting and filling by title:\nggplot(top_words_tragedies, aes(y = fct_rev(word), x = n, fill = title)) + geom_col() + guides(fill = FALSE) + labs(y = \u0026quot;Count\u0026quot;, x = NULL, title = \u0026quot;15 most frequent words in four Shakespearean tragedies\u0026quot;) + facet_wrap(vars(title), scales = \u0026quot;free_y\u0026quot;) + theme_bw() These results aren’t terribly surprising. “lear” is the most common word in King Lear, “macbeth” is the most common word in Macbeth, and so on. But the results are still really neat! This is a wordcloud for grownups!\n(Sharp-eyed readers will notice that the words aren’t actually in perfect order! That’s because some common words are repeated across the plays, like “lord” and “sir”. However, each category in a factor can only have one possible position in the orer, so because “lord” is the second most common word in Hamlet it also appears as #2 in Macbeth and King Lear. You can fix this with the reorder_within() function in tidytext—see Julia Silge’s tutorial here for how to use it.)\n Bigrams We can also look at pairs of words instead of single words. To do this, we need to change a couple arguments in unnest_tokens(), but otherwise everything else stays the same. In order to remove stopwords, we need to split the bigram column into two columns (word1 and word2) with separate(), filter each of those columns, and then combine the word columns back together as bigram with unite()\ntragedies_bigrams \u0026lt;- tragedies_raw %\u0026gt;% drop_na(text) %\u0026gt;% # n = 2 here means bigrams. We could also make trigrams (n = 3) or any type of n-gram unnest_tokens(bigram, text, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;% # Split the bigrams into two words so we can remove stopwords separate(bigram, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %\u0026gt;% filter(!word1 %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;), !word2 %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;)) %\u0026gt;% # Put the two word columns back together unite(bigram, word1, word2, sep = \u0026quot; \u0026quot;) tragedies_bigrams ## # A tibble: 14,237 x 3 ## gutenberg_id title bigram ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1513 Romeo and Juliet william shakespeare ## 2 1513 Romeo and Juliet shakespeare persons ## 3 1513 Romeo and Juliet persons represented ## 4 1513 Romeo and Juliet represented escalus ## 5 1513 Romeo and Juliet escalus prince ## 6 1513 Romeo and Juliet verona paris ## 7 1513 Romeo and Juliet nobleman kinsman ## 8 1513 Romeo and Juliet prince montague ## 9 1513 Romeo and Juliet montague heads ## 10 1513 Romeo and Juliet capulet romeo ## # … with 14,227 more rows top_bigrams \u0026lt;- tragedies_bigrams %\u0026gt;% # Count all the bigrams in each play count(title, bigram, sort = TRUE) %\u0026gt;% # Keep top 15 in each play group_by(title) %\u0026gt;% top_n(15) %\u0026gt;% ungroup() %\u0026gt;% # Make the bigrams an ordered factor so they plot in order mutate(bigram = fct_inorder(bigram)) ## Selecting by n ggplot(top_bigrams, aes(y = fct_rev(bigram), x = n, fill = title)) + geom_col() + guides(fill = FALSE) + labs(y = \u0026quot;Count\u0026quot;, x = NULL, title = \u0026quot;15 most frequent bigrams in four Shakespearean tragedies\u0026quot;) + facet_wrap(vars(title), scales = \u0026quot;free\u0026quot;) + theme_bw() There are some neat trends here. “Lord Hamlet” is the most common pair of words in Hamlet (not surprisingly), but in Macbeth the repeated “knock knock” (the first non-name repeated pair) is a well-known plot point and reoccurring symbolic theme throughout the play.\n  Bigrams and probability We can replicate the “She Giggles, He Gallops” idea by counting the bigrams that match “he X” and “she X”.\nThe log ratio idea shows how much more likely a word is compared to its counterpart (so “he that” is about 5 more likely to appear than “she that”. In this graph, I replaced the x-axis labels with “2x” and “4x”, but without those, you get numbers like 1, 2, and 3 (or -1, -2, -3)). To convert those logged ratio numbers into the multiplicative version (i.e. 2x instead of 1), raise 2 to the power of the log ratio. If the log ratio is 3, the human-readable version is \\(2^3\\), or 8 times.\n# Take the log of 8: log2(8) ## [1] 3 # Reverse log of 3: 2^3 ## [1] 8 The only text wizardry here is tokenizing the words. Pretty much the rest of all this code is just dplyr mutating, filtering, and counting:\npronouns \u0026lt;- c(\u0026quot;he\u0026quot;, \u0026quot;she\u0026quot;) bigram_he_she_counts \u0026lt;- tragedies_raw %\u0026gt;% drop_na(text) %\u0026gt;% # Split into bigrams unnest_tokens(bigram, text, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;% # Find counts of bigrams count(bigram, sort = TRUE) %\u0026gt;% # Split the bigram column into two columns separate(bigram, c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;) %\u0026gt;% # Only choose rows where the first word is he or she filter(word1 %in% pronouns) %\u0026gt;% count(word1, word2, wt = n, sort = TRUE) %\u0026gt;% rename(total = n) word_ratios \u0026lt;- bigram_he_she_counts %\u0026gt;% # Look at each of the second words group_by(word2) %\u0026gt;% # Only choose rows where the second word appears more than 10 times filter(sum(total) \u0026gt; 10) %\u0026gt;% ungroup() %\u0026gt;% # Spread out the word1 column so that there\u0026#39;s a column named \u0026quot;he\u0026quot; and one named \u0026quot;she\u0026quot; spread(word1, total, fill = 0) %\u0026gt;% # Add 1 to each number so that logs work (just in case any are zero) mutate_if(is.numeric, ~(. + 1) / sum(. + 1)) %\u0026gt;% # Create a new column that is the logged ratio of the she counts to he counts mutate(logratio = log2(she / he)) %\u0026gt;% # Sort by that ratio arrange(desc(logratio)) # Rearrange this data so it\u0026#39;s plottable plot_word_ratios \u0026lt;- word_ratios %\u0026gt;% # This gets the words in the right order---we take the absolute value, select # only rows where the log ratio is bigger than 0, and then take the top 15 words mutate(abslogratio = abs(logratio)) %\u0026gt;% group_by(logratio \u0026lt; 0) %\u0026gt;% top_n(15, abslogratio) %\u0026gt;% ungroup() %\u0026gt;% mutate(word = reorder(word2, logratio)) # Finally we plot this ggplot(plot_word_ratios, aes(y = word, x = logratio, color = logratio \u0026lt; 0)) + geom_segment(aes(y = word, yend = word, x = 0, xend = logratio), size = 1.1, alpha = 0.6) + geom_point(size = 3.5) + labs(x = \u0026quot;How much more/less likely\u0026quot;, y = NULL) + scale_color_discrete(name = \u0026quot;\u0026quot;, labels = c(\u0026quot;More \u0026#39;she\u0026#39;\u0026quot;, \u0026quot;More \u0026#39;he\u0026#39;\u0026quot;)) + scale_x_continuous(breaks = seq(-3, 3), labels = c(\u0026quot;8x\u0026quot;, \u0026quot;4x\u0026quot;, \u0026quot;2x\u0026quot;, \u0026quot;Same\u0026quot;, \u0026quot;2x\u0026quot;, \u0026quot;4x\u0026quot;, \u0026quot;8x\u0026quot;)) + theme_bw() + theme(legend.position = \u0026quot;bottom\u0026quot;) Shakespeare doesn’t use a lot of fancy verbs in his plays, so we’re left with incredibly common verbs like “should” and “comes” and “was”. Oh well.\n Term frequency-inverse document frequency (tf-idf) We can determine which words are the most unique for each book/document in our corpus using by calculating the tf-idf (term frequency-inverse document frequency) score for each term. The tf-idf is the product of the term frequency and the inverse document frequency:\n\\[ \\begin{aligned} tf(\\text{term}) \u0026amp;= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\ idf(\\text{term}) \u0026amp;= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\ tf\\text{-}idf(\\text{term}) \u0026amp;= tf(\\text{term}) \\times idf(\\text{term}) \\end{aligned} \\]\nFortunately you don’t need to remember that formula. The bind_tf_idf() function will calculate this for you. Remember, the higher the tf-idf number, the more unique the term is in the document, but these numbers are meaningless and unitless—you can’t convert them to a percentage or anything.\nHere are the most unique words in these four tragedies, compared to all the tragedies:\ntragedy_words \u0026lt;- tragedies_raw %\u0026gt;% drop_na() %\u0026gt;% # Split into word tokens unnest_tokens(word, text) %\u0026gt;% # Remove stop words and old timey words anti_join(stop_words) %\u0026gt;% filter(!word %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;haue\u0026quot;, \u0026quot;thee\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;)) %\u0026gt;% count(title, word, sort = TRUE) # Add the tf-idf values to the counts tragedy_tf_idf \u0026lt;- tragedy_words %\u0026gt;% bind_tf_idf(word, title, n) # Get the top 10 uniquest words tragedy_tf_idf_plot \u0026lt;- tragedy_tf_idf %\u0026gt;% arrange(desc(tf_idf)) %\u0026gt;% group_by(title) %\u0026gt;% top_n(10) %\u0026gt;% ungroup() %\u0026gt;% mutate(word = fct_inorder(word)) ggplot(tragedy_tf_idf_plot, aes(y = fct_rev(word), x = tf_idf, fill = title)) + geom_col() + guides(fill = FALSE) + labs(x = \u0026quot;tf-idf\u0026quot;, y = NULL) + facet_wrap(~ title, scales = \u0026quot;free\u0026quot;) + theme_bw() Not surprisingly, the most unique words for each play happen to be the names of the characters in those plays.\n Sentiment analysis In the video, I plotted the sentiment of Little Women across the book, but it wasn’t a very interesting plot. We’ll try with Shakespeare here instead.\nAt its core, sentiment analysis involves looking at a big list of words for how negative or positive they are. Some sentiment dictionaries mark if a word is “negative” or “positive”; some give words a score from -3 to 3; some give different emotions like “sadness” or “anger”. You can see what the different dictionaries look like with get_sentiments()\nget_sentiments(\u0026quot;afinn\u0026quot;) # Scoring system ## # A tibble: 2,477 x 2 ## word value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 abandon -2 ## 2 abandoned -2 ## 3 abandons -2 ## 4 abducted -2 ## 5 abduction -2 ## 6 abductions -2 ## 7 abhor -3 ## 8 abhorred -3 ## 9 abhorrent -3 ## 10 abhors -3 ## # … with 2,467 more rows # get_sentiments(\u0026quot;bing\u0026quot;) # Negative/positive # get_sentiments(\u0026quot;nrc\u0026quot;) # Specific emotions # get_sentiments(\u0026quot;loughran\u0026quot;) # Designed for financial statements; positive/negative Here we split the Shakespearean tragedies into words, join a sentiment dictionary to it, and use dplyr data wrangling to calculate the net number positive words in each chapter. Had we used the AFINN library, we could calculate the average sentiment per chapter, since AFINN uses a scoring system instead of negative/positive labels. Or we could’ve used the NRC library, which has specific emotions like trust and fear.\ntragedy_words \u0026lt;- tragedies_raw %\u0026gt;% drop_na() %\u0026gt;% # Split into word tokens unnest_tokens(word, text) %\u0026gt;% # Remove stop words and old timey words anti_join(stop_words) %\u0026gt;% filter(!word %in% c(\u0026quot;thou\u0026quot;, \u0026quot;thy\u0026quot;, \u0026quot;haue\u0026quot;, \u0026quot;thee\u0026quot;, \u0026quot;thine\u0026quot;, \u0026quot;enter\u0026quot;, \u0026quot;exeunt\u0026quot;, \u0026quot;exit\u0026quot;)) # Join the sentiment dictionary tragedy_sentiment \u0026lt;- tragedy_words %\u0026gt;% inner_join(get_sentiments(\u0026quot;bing\u0026quot;)) tragedy_sentiment ## # A tibble: 7,736 x 4 ## gutenberg_id title word sentiment ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1513 Romeo and Juliet dignity positive ## 2 1513 Romeo and Juliet fair positive ## 3 1513 Romeo and Juliet grudge negative ## 4 1513 Romeo and Juliet break negative ## 5 1513 Romeo and Juliet unclean negative ## 6 1513 Romeo and Juliet fatal negative ## 7 1513 Romeo and Juliet overthrows negative ## 8 1513 Romeo and Juliet death negative ## 9 1513 Romeo and Juliet strife negative ## 10 1513 Romeo and Juliet fearful negative ## # … with 7,726 more rows We can look at these sentiments a few different ways. First we can get a count of total positive and negative words in the four books. We can see that in all four, there are more negative words than positive ones (they’re tragdies, after all):\ntragedy_sentiment_plot \u0026lt;- tragedy_sentiment %\u0026gt;% count(title, sentiment) ggplot(tragedy_sentiment_plot, aes(x = sentiment, y = n, fill = title, alpha = sentiment)) + geom_col(position = position_dodge()) + scale_alpha_manual(values = c(0.5, 1)) + facet_wrap(vars(title)) + theme_bw() Perhaps more usefully, we can divide each of the plays into groups of 100 lines, and then get the net sentiment of each group (number of positive words − number of negative words). By splitting the data into groups of lines, we can show a more granular view of the progression of the plot. To do this we make a column that indicates the row number, and then we use the special %/% operator to perform integer division, which essentially lops off the decimal point when dividing numbers: 150/100 normally is 1.5, but in integer divison, it is 1. This is a helpful trick for putting rows 1-99 in one group, then rows 100-199 in another group, etc.\ntragedies_split_into_lines \u0026lt;- tragedy_sentiment %\u0026gt;% # Divide lines into groups of 100 mutate(line = row_number(), line_chunk = line %/% 100) %\u0026gt;% # Get a count of postiive and negative words in each 100-line chunk in each play count(title, line_chunk, sentiment) %\u0026gt;% # Convert the sentiment column into two columns named \u0026quot;positive\u0026quot; and \u0026quot;negative\u0026quot; pivot_wider(names_from = sentiment, values_from = n) %\u0026gt;% # Calculate net sentiment mutate(sentiment = positive - negative) ggplot(tragedies_split_into_lines, aes(x = line_chunk, y = sentiment, fill = sentiment)) + geom_col() + scale_fill_viridis_c(option = \u0026quot;magma\u0026quot;, end = 0.9) + facet_wrap(vars(title), scales = \u0026quot;free_x\u0026quot;) + theme_bw() Neat. They’re all really sad and negative, except for the beginning of Romeo and Juliet where the two lovers meet and fall in love. Then everyone dies later.\n Neat extra stuff None of this stuff was in the video, but it’s useful to know and see how to do it. It all generally comes from the Tidy Text Mining book by Julia Silge and David Robinson\nPart of speech tagging R has no way of knowing if words are nouns, verbs, or adjectives. You can algorithmically predict what part of speech each word is using a part-of-speech tagger, like spaCy or Stanford’s Natural Langauge Processing (NLP) library.\nThese are external programs that are not written in R and don’t naturally communicate with R (spaCy is written in Python; Stanford’s CoreNLP is written in Java). There is a helpful R package named cleanNLP that helps you interact with these programs from within R, whis is super helpful. cleanNLP also comes with its own R-only tagger so you don’t need to install anything with Python or Java (however, it’s not as powerful as either spaCy, which is faster, and doesn’t deal with foreign languages like Arabic and Chinese like Stanford’s NLP library).\nYou can see other examples of part-of-speech tagging (along with instructions for how to install spaCy and coreNLP) here:\n  “Tidy text, parts of speech, and unique words in the Bible”  “Tidy text, parts of speech, and unique words in the Qur’an”  Here’s the general process for tagging (or “annotating”) text with the cleanNLP package:\nMake a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\n Initialize the NLP tagger. You can use any of these:\n cnlp_init_udpipe(): Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!) cnlp_init_spacy(): Use spaCy (if you’ve installed it on your computer with Python) cnlp_init_corenlp(): Use Stanford’s NLP library (if you’ve installed it on your computer with Java)  Feed the data frame from step 1 into the cnlp_annotate() function and wait.\n Save the tagged data on your computer so you don’t have to re-tag it every time.\n  Here’s an example using the Little Women data:\n# For the tagger to work, each row needs to be unique, which means we need to # combine all the text into individual chapter-based rows. This takes a little # bit of text-wrangling with dplyr: little_women_to_tag \u0026lt;- little_women %\u0026gt;% # Group by chapter number group_by(chapter_number) %\u0026gt;% # Take all the rows in each chapter and collapse them into a single cell nest(data = c(text)) %\u0026gt;% ungroup() %\u0026gt;% # Look at each individual cell full of text lines and paste them together into # one really long string of text per chapter mutate(text = map_chr(data, ~paste(.$text, collapse = \u0026quot; \u0026quot;))) %\u0026gt;% # Get rid of this column select(-data) little_women_to_tag ## # A tibble: 47 x 2 ## chapter_number text ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 \u0026quot;CHAPTER ONE PLAYING PILGRIMS \\\u0026quot;Christmas won\u0026#39;t be Christmas without any presents,\\\u0026quot; grumbled Jo, lying on the rug. \\\u0026quot;It\u0026#39;s so dread… ## 2 2 \u0026quot;CHAPTER TWO A MERRY CHRISTMAS Jo was the first to wake in the gray dawn of Christmas morning. No stockings hung at the fireplace, … ## 3 3 \u0026quot;CHAPTER THREE THE LAURENCE BOY \\\u0026quot;Jo! Jo! Where are you?\\\u0026quot; cried Meg at the foot of the garret stairs. \\\u0026quot;Here!\\\u0026quot; answered a husky… ## 4 4 \u0026quot;CHAPTER FOUR BURDENS \\\u0026quot;Oh, dear, how hard it does seem to take up our packs and go on,\\\u0026quot; sighed Meg the morning after the party, f… ## 5 5 \u0026quot;CHAPTER FIVE BEING NEIGHBORLY \\\u0026quot;What in the world are you going to do now, Jo?\\\u0026quot; asked Meg one snowy afternoon, as her sister came… ## 6 6 \u0026quot;CHAPTER SIX BETH FINDS THE PALACE BEAUTIFUL The big house did prove a Palace Beautiful, though it took some time for all to get in… ## 7 7 \u0026quot;CHAPTER SEVEN AMY\u0026#39;S VALLEY OF HUMILIATION \\\u0026quot;That boy is a perfect cyclops, isn\u0026#39;t he?\\\u0026quot; said Amy one day, as Laurie clattered by on… ## 8 8 \u0026quot;CHAPTER EIGHT JO MEETS APOLLYON \\\u0026quot;Girls, where are you going?\\\u0026quot; asked Amy, coming into their room one Saturday afternoon, and find… ## 9 9 \u0026quot;CHAPTER NINE MEG GOES TO VANITY FAIR \\\u0026quot;I do think it was the most fortunate thing in the world that those children should have the… ## 10 10 \u0026quot;CHAPTER TEN THE P.C. AND P.O. As spring came on, a new set of amusements became the fashion, and the lengthening days gave long af… ## # … with 37 more rows Notice how there’s now a row for each chapter, and the whole chapter is contained in the text column. With the data in this format, we can annotate it. It takes about 3 minutes to run this on my 2016 MacBook Pro with the R-only udpipe tagger (and only 30 seconds if I use the spaCy tagger). Notice how I immediately save the tagged tokens as a CSV file after so I don’t have to do it again.\nlibrary(cleanNLP) # Use the built-in R-based tagger cnlp_init_udpipe() little_women_tagged \u0026lt;- cnlp_annotate(little_women_to_tag, text_name = \u0026quot;text\u0026quot;, doc_name = \u0026quot;chapter_number\u0026quot;) write_csv(little_women_tagged$token, \u0026quot;little_women_tagged.csv\u0026quot;) Here’s what the tagged text looks like:\nlittle_women_tagged ## # A tibble: 232,093 x 10 ## doc_id sid tid token token_with_ws lemma upos xpos tid_source relation ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 1 1 \u0026quot;CHAPTER\u0026quot; \u0026quot;CHAPTER\u0026quot; \u0026quot;chapter\u0026quot; NOUN NN 4 nmod ## 2 1 1 2 \u0026quot;ONE\u0026quot; \u0026quot;ONE\u0026quot; \u0026quot;one\u0026quot; NUM CD 1 nummod ## 3 1 1 3 \u0026quot;PLAYING\u0026quot; \u0026quot;PLAYING\u0026quot; \u0026quot;playing\u0026quot; NOUN NN 4 compound ## 4 1 1 4 \u0026quot;PILGRIMS\u0026quot; \u0026quot;PILGRIMS\u0026quot; \u0026quot;pilgrims\u0026quot; NOUN NN 0 root ## 5 1 1 5 \u0026quot;\\\u0026quot;\u0026quot; \u0026quot;\\\u0026quot;\u0026quot; \u0026quot;\\\u0026quot;\u0026quot; PUNCT `` 4 punct ## 6 1 2 1 \u0026quot;Christmas\u0026quot; \u0026quot;Christmas\u0026quot; \u0026quot;Christmas\u0026quot; PROPN NNP 4 nsubj ## 7 1 2 2 \u0026quot;wo\u0026quot; \u0026quot;wo\u0026quot; \u0026quot;will\u0026quot; VERB MD 4 aux ## 8 1 2 3 \u0026quot;n\u0026#39;t\u0026quot; \u0026quot;n\u0026#39;t\u0026quot; \u0026quot;not\u0026quot; PART RB 4 neg ## 9 1 2 4 \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; \u0026quot;be\u0026quot; AUX VB 0 root ## 10 1 2 5 \u0026quot;Christmas\u0026quot; \u0026quot;Christmas\u0026quot; \u0026quot;Christmas\u0026quot; PROPN NNP 4 attr ## # … with 232,083 more rows There are a bunch of new columns like lemma (or the base stemmed word), and upos and pos for the different parts of speech. These use the Penn Treebank codes.\nNow that everything is tagged, we can do any grouping and summarizing and filtering we want. We could find the most common verbs, or the most common nouns or proper names, for instance. Here’s a fun plot that shows the proportion of mentions of the four main characters (Meg, Jo, Beth, and Amy) in each chapter.\n# Find all proper nouns proper_nouns \u0026lt;- little_women_tagged %\u0026gt;% filter(upos == \u0026quot;PROPN\u0026quot;) main_characters_by_chapter \u0026lt;- proper_nouns %\u0026gt;% # Find only Meg, Jo, Beth, and Amy filter(lemma %in% c(\u0026quot;Meg\u0026quot;, \u0026quot;Jo\u0026quot;, \u0026quot;Beth\u0026quot;, \u0026quot;Amy\u0026quot;)) %\u0026gt;% # Group by chapter and character name group_by(doc_id, lemma) %\u0026gt;% # Get the count of mentions summarize(n = n()) %\u0026gt;% # Make a new column named \u0026quot;name\u0026quot; that is an ordered factor of the girls\u0026#39; names mutate(name = factor(lemma, levels = c(\u0026quot;Meg\u0026quot;, \u0026quot;Jo\u0026quot;, \u0026quot;Beth\u0026quot;, \u0026quot;Amy\u0026quot;), ordered = TRUE)) %\u0026gt;% # Rename this so it\u0026#39;s called chapter rename(chapter = doc_id) %\u0026gt;% # Group by chapter group_by(chapter) %\u0026gt;% # Calculate the proportion of each girl\u0026#39;s mentions in each chapter mutate(prop = n / sum(n)) %\u0026gt;% ungroup() %\u0026gt;% # Make a cleaner chapter name column mutate(chapter_name = paste(\u0026quot;Chapter\u0026quot;, chapter)) %\u0026gt;% mutate(chapter_name = fct_inorder(chapter_name)) main_characters_by_chapter ## # A tibble: 177 x 6 ## chapter lemma n name prop chapter_name ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;ord\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 Amy 23 Amy 0.195 Chapter 1 ## 2 1 Beth 26 Beth 0.220 Chapter 1 ## 3 1 Jo 43 Jo 0.364 Chapter 1 ## 4 1 Meg 26 Meg 0.220 Chapter 1 ## 5 2 Amy 13 Amy 0.197 Chapter 2 ## 6 2 Beth 12 Beth 0.182 Chapter 2 ## 7 2 Jo 21 Jo 0.318 Chapter 2 ## 8 2 Meg 20 Meg 0.303 Chapter 2 ## 9 3 Amy 2 Amy 0.0202 Chapter 3 ## 10 3 Beth 2 Beth 0.0202 Chapter 3 ## # … with 167 more rows And here’s the polished plot:\nggplot(main_characters_by_chapter, aes(x = prop, y = 1, fill = fct_rev(name))) + geom_col(position = position_stack()) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + scale_fill_viridis_d(option = \u0026quot;plasma\u0026quot;, end = 0.9, name = NULL) + guides(fill = guide_legend(reverse = TRUE)) + labs(x = NULL, y = NULL, title = \u0026quot;Proportion of mentions of each\\nLittle Woman per chapter\u0026quot;, subtitle = \u0026quot;Jo basically dominates the last third of the book\u0026quot;) + facet_wrap(vars(chapter_name), nrow = 6) + theme_bw(base_family = \u0026quot;Roboto Condensed\u0026quot;) + theme(legend.position = \u0026quot;top\u0026quot;, axis.text = element_blank(), axis.ticks = element_blank(), strip.background = element_rect(fill = \u0026quot;white\u0026quot;), legend.text = element_text(face = \u0026quot;bold\u0026quot;, size = rel(1)), plot.title = element_text(face = \u0026quot;bold\u0026quot;, hjust = 0.5, size = rel(1.7)), plot.subtitle = element_text(hjust = 0.5, size = rel(1.1)))  Topic modeling and fingerprinting If you want to see some examples of topic modeling with Latent Dirichlet Allocation (LDA) or text fingerprinting based on sentence length and counts of hapax legomena (based on this article), see these examples from a previous version of this class: topic modeling and fingerprinting\n Text features Finally, you can use the textfeatures package to find all sorts of interesting numeric statistics about text, like the number of exclamation points, commas, digits, characters per word, uppercase letters, lowercase letters, and more!\n   ","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"9b49beb720db92e02ef300134b02a9dc","permalink":"/example/13-example/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/example/13-example/","section":"example","summary":"Live coding example Complete code  Get data Clean data Tokens and word counts  Single words Bigrams  Bigrams and probability Term frequency-inverse document frequency (tf-idf) Sentiment analysis Neat extra stuff  Part of speech tagging Topic modeling and fingerprinting Text features     For this example, we’re going to use the text of Little Women by Louisa May Alcott and four Shakespearean tragedies (Romeo and Juliet, King Lear, Macbeth, and Hamlet) to explore how to do some basic text visualization.","tags":null,"title":"Text","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Word frequencies 100% optional bonus fun tasks Turning everything in   Getting started For this exercise, you’ll download some books from Project Gutenberg and visualize patterns in the words.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some helpful starter code. Download that here and include it in your project:\n  13-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 13-exercise.Rmd your-project-name.Rproj To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  13-exercise.zip  The example from today’s session will be incredibly helpful for this exercise.\nThis can be as simple or as complex as you want. You don’t need to make your plots super fancy, but if you’re feeling brave, experiment with changing colors or modifying themes and theme elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\n Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Word frequencies Download 4+ books by some author on Project Gutenberg. Jane Austen, Victor Hugo, Emily Brontë, Lucy Maud Montgomery, Arthur Conan Doyle, Mark Twain, Henry David Thoreau, Fyodor Dostoyevsky, Leo Tolstoy. Anyone. Just make sure it’s all from the same author.\nMake these two plots and describe what each tell about your author’s books:\nTop 10 most frequent words in each book Top 10 most unique words in each book (i.e. tf-idf)   100% optional bonus fun tasks If you want, do some other things with the text you’ve downloaded. Make a “he verbs vs. she verbs” plot. Tag the parts of speech and find the most common verbs or nouns. Try some sentiment analysis.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create an HTML or Word version (or PDF if you’ve installed tinytex) of your document. Upload that file to iCollege.\n ","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"179541800a3cec6ec93b7e2160b2ee00","permalink":"/lab/13-exercise/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/lab/13-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Word frequencies 100% optional bonus fun tasks Turning everything in   Getting started For this exercise, you’ll download some books from Project Gutenberg and visualize patterns in the words.\nYou should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.","tags":null,"title":"Text","type":"docs"},{"authors":null,"categories":null,"content":"   Why enhance graphics? Enhancing graphics in 2020 Abbreviated example   Why enhance graphics? The content from today isn’t really code-based at all. Instead, you’re learning about how to take a plot from R and make it fancy in a vector editing program like Illustrator, Gravit Designer, or Inkscape.\nThis concept comes from a common workflow in the real world, where organizations like news outlets, think tanks, research centers, or nonprofits will publish highly polished plots in annual reports, magazines, and other types of publications. These graphics often have to follow specific in-house style guidelines and use specific colors and fonts and other design elements. Even if you don’t work for a place with in-house style guides, you’ll often want to make some edits to your plots by hand after you create them.\nThe general workflow goes like this:\nCreate a plot in R and ggplot Export that plot as a vector image (either a PDF or an SVG) Edit and enhance the vector image in a vector editor, like Adobe Illustrator Export the polished version from Illustrator as either a PDF or PNG (or both)  Big data-focused organizations have been using a process like this for years. Nathan Yau describes this whole process in his 2011 book Visualize This and the book contains a bunch of tutorials to help you learn how create something in R, export it, and edit it in Illustrator.\nFor instance, in his first chapter, he guides you through the process of creating the skeleton of this chart in R, exporting it as a PDF, and adding all the titles and annotations and arrows and extra lines in Illustrator (original post from 2008):\n Enhancing graphics in 2020 In 2011, that was the best possible workflow because ggplot couldn’t deal with subtitles, captions, repelled labels, embedded fonts, and differently-styled text (like bold in the middle of a title). Illustrator was the only way to do this stuff.\nNowadays in 2020, though, you can do nearly all of this annotating and enhancing with packages like ggtext and patchwork and ggrepel. You can almost perfectly replicate in-house style guides with the theme() function and put text and arrows and labels and text boxes wherever you want with annotate(). It’s a brave exciting new world.\nYou still can’t do everything with R. ggplot can’t create fancy font ligatures like “ﬁ” in words that have an “f” followed by an “i”, and it can’t handle automatic hyphenation and full text justification, among other limitations. But these are the minorest of graphic design issues (and the ggplot team is working on them!).\nThat all said, it’s still often faster and easier to make edits to your graphs in Illustrator rather than fight with a reluctant annotate() layer that just won’t put an arrow exactly where you want. And ggtext is so new (it’s not on CRAN yet) that lots of people haven’t heard of it yet. This is all cutting edge stuff.\nSo it’s still a good idea to understand how to follow the standard workflow of exporting from R and enhancing in Illustrator.\n Abbreviated example In this video I use the code for the hot dog plot that I provide in today’s assignment to create a plot, export it, and make edits to it both in Illustrator and Gravit Designer. It’s not a complete example at all, but I show you the general process for adding text and lines and editing plot elements.\n   ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"e5f768bcbe278964a1cfab6c6674ea2f","permalink":"/example/14-example/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/example/14-example/","section":"example","summary":"Why enhance graphics? Enhancing graphics in 2020 Abbreviated example   Why enhance graphics? The content from today isn’t really code-based at all. Instead, you’re learning about how to take a plot from R and make it fancy in a vector editing program like Illustrator, Gravit Designer, or Inkscape.\nThis concept comes from a common workflow in the real world, where organizations like news outlets, think tanks, research centers, or nonprofits will publish highly polished plots in annual reports, magazines, and other types of publications.","tags":null,"title":"Enhancing graphics","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Task 1: Reflection Task 2: Hot dog eating contest winners Turning everything in   Getting started For this exercise, you’ll export a PDF and/or an SVG from R, open it in Adobe Illustrator (free for GSU students) or Gravit Designer (free for the basic version), add annotations and make minor edits, and then export a final polished version.\nI have given you 100% of the R code you need to use. All you have to do is run it. You need to download one CSV file:\n  hot-dog-contest-winners.csv  You should use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud). Either create a new project for this exercise only, or make a project for all your work in this class.\nTo help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with all the code you’ll need. Download that here and include it in your project:\n  14-exercise.Rmd  In the end, the structure of your project directory should look something like this:\nyour-project-name\\ 14-exercise.Rmd your-project-name.Rproj data\\ hot-dog-contest-winners.csv To check that you put everything in the right places, you can download and unzip this file, which contains everything in the correct structure:\n  14-exercise.zip   Task 1: Reflection Write your reflection for the day’s readings.\n Task 2: Hot dog eating contest winners Recreate this plot (or something like it):\nCreate and save a basic bar chart of hot dog eating contest winners using the code provided. Open the resulting file in Illustrator or Gravit Designer. Open the PDF in Illustrator; open the SVG in Gravit Designer.\nBe sure that you save your file in Illustrator or Gravit Designer with a different name. You don’t want to accidentally overwrite all your enhancements and updates when you knit this document. That would be so sad.\nYou don’t have data prior to 1980, so don’t worry about recreating that half of the graph. You don’t have to put all the text boxes in exactly the same locations—you can even do a completely different design and add different annotations if you want.\nThe point of this assignment is to help you get familiar with vector editing software, so don’t stress out about R issues or graphic design issues (though try to follow CRAP where possible).\nTo save you some typing, here’s all the text from the original plot. Copy and paste it into your enhanced version (or change the text if you want—again, do whatever you want):\n Winners from Nathan’s Hot Dog Eating Contest It’s that time of year again. Since 1916, the annual eating competition has grown substantially attracting competitors from around the world Frank Dellarosa eats 21 and a half HDBs over 12 minutes, breaking the previous record of 19 and a half Through 2001-2005, Takeru Kobayashi wins by no less than 12 HDBs. In 2006 he only wins by 1.75. After winning 6 years in a row and setting the world record 4 times, Kobayashi places second in 2007. For the first time since 1999, an American reclaims the title when Joey Chestnut consumes 66 HDBs, a new world record. Chestnut repeats in 2008. Source: Wikipedia and Nathan’s Famous   Turning everything in When you’re all done, knit your R Markdown file and use Illustrator or Gravit Designer to export a PDF or PNG version (or both) of your enhanced plot. Upload these files to iCollege.\n ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"363f61198514a40ca4874ba58a622709","permalink":"/lab/14-exercise/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/lab/14-exercise/","section":"lab","summary":"Getting started Task 1: Reflection Task 2: Hot dog eating contest winners Turning everything in   Getting started For this exercise, you’ll export a PDF and/or an SVG from R, open it in Adobe Illustrator (free for GSU students) or Gravit Designer (free for the basic version), add annotations and make minor edits, and then export a final polished version.\nI have given you 100% of the R code you need to use.","tags":null,"title":"Enhancing graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Publishing your stuff online  Quickest and easiest way: RPubs Great for standalone projects: R Markdown websites More complex blogs and websites: blogdown Books, dissertations, and theses: bookdown Slides: xaringan Code: GitHub and GitHub gists  Telling stories with data   Publishing your stuff online Quickest and easiest way: RPubs The easiest way to get a knitted R Markdown onto the internet is to use RPubs. We talked about this in session 10, and you used it to turn in exercise 10. After knitting an HTML document in RStudio, click on the “Publish” button in the top right corner to upload the document to the RPubs server and get a URL that you can share with others:\nYou don’t have to set up a web server or anything—it’s all pretty automatic and seamless.\n Great for standalone projects: R Markdown websites If you have something slightly more complex, like a collection of R Markdown files that do related things, it’s easy to stitch them all together in an R Markdown website. RStudio supports these automatically—after telling RStudio to consider an RStudio project to be a website, it will knit all the .Rmd files in the root of your project directory every time you click on the “Build Website” button.\nRStudio generates a standalone folder named public with static HTML pages of all your knitted documents. You then have to put that folder on the internet somewhere, either on a web server you have access to, or a free service like Netlify.\nSee this page for complete documentation, or follow these tutorials by Lucy D’Agostino McGowan and Emily Zabor.\nThese websites are especially helpful for standalone projects like research papers and reports. I’ve had students do their master’s capstone projects with these, with specific pages for their introduction, literature review, data cleaning, exploratory data analysis, modeling, and results.\nI typically make a website for each of my research projects and will include pages with IRB details, copies of survey experiments, data cleaning, results, and so on. Here are some examples:\n NGO Crackdowns and Philanthropy Are Donors Really Responding? The Power of Ranking Constraint Closure  You can also make really neat small websites like Desirée De Leon’s Teacup Giraffes for teaching basic statistics.\n More complex blogs and websites: blogdown If you want more control (i.e. total control) over the HTML output and the structure of a website, you can use a package named blogdown to convert R Markdown files into an entire website. This course website is built with blogdown: you can see all the underlying R Markdown files at GitHub.\nLike R Markdown websites, blogdown generates a complete static version of the knitted website and puts it in a folder named public. You’re then responsible for putting that somewhere on the internet, either on your own server or by using a free hosting service like Netlify.\nBlogdown is incredibly well documented, and there are lots of tutorials for how to get started. Alison Hill’s tutorial here is the best place to get started—follow it and you’ll have a basic blog completely free.\n Books, dissertations, and theses: bookdown If you don’t want to create a website, you can use a package named bookdown to stitch a collection of R Markdown files into a PDF, Word, or HTML book. (You could even put all your exercises from this class into a single book!). bookdown is incredibly well documented too (as a bookdown book), and you can get familiar with it fairly quickly.\nDozens of real-world books, dissertations, and theses have been written with bookdown, including both Claus Wilke’s and Kieran Healy’s books from this course. Because of the magic of Markdown, you can create parallel HTML and PDF versions of your book and post one type of output on the internet and print and bind the other one.\n Slides: xaringan R Markdown isn’t just for PDF, Word, and HTML documents. You can also make slides! All the slides for this course were made in R Markdown with a package named xaringan. You can see the documentation here, and see the main example presentation here. You can also see all the R Markdown files I wrote to create the slides for this class here.\n Code: GitHub and GitHub gists And finally, if you want to share code (and keep track of versions of your code), GitHub is one of the best places for that. Posting your code at places like GitHub lets other people see and borrow and adapt and make suggestions to your code. You can see all my different repositories and projects here, for example.\nJenny Bryan has a useful bookdown website explaining how to get started, and GitHub itself has excellent materials for learning how to use git.\nIf you don’t want to go through the process of creating a full-blown git repository, GitHub also lets you make “gists”, which are single shareable files of code. (See all mine here for examples). Gists are excellent ways to share reproducible examples (or reprexes), and the reprex package in R generates output that you can paste directly into a new gist for sharing (see this one, for instance, which I used to show someone how to run and plot logistic regression with R).\n  Telling stories with data If you’re interested in learning more about data storytelling and science communication, check out these resources:\n  BUSM 491R: Telling Stories with Data (BYU, Fall 2017)  Cole Nussbaumer Knaflic, Storytelling with Data: A Data Visualization Guide for Business Professionals (Hoboken, New Jersey: John Wiley \u0026amp; Sons, Inc., 2015).  Alan Alda, If I Understood You, Would I Have This Look on My Face? My Adventures in the Art and Science of Relating and Communicating (New York: Random House, 2017).  Nancy Duarte, Resonate: Present Visual Stories That Transform Audiences (Hoboken, New Jersey: John Wiley \u0026amp; Sons, Inc., 2010).  “Understanding the way scientists speak,” MSNBC Morning Joe, 2013-04-24  “Improvisation for Scientists: Workshops by Alan Alda and the Center for Communicating Science,” Stony Brook Journalism, 2010-03-23   ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"b7eba0e8bc4f8880d263e93aced4e1c5","permalink":"/example/15-example/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/example/15-example/","section":"example","summary":"Publishing your stuff online  Quickest and easiest way: RPubs Great for standalone projects: R Markdown websites More complex blogs and websites: blogdown Books, dissertations, and theses: bookdown Slides: xaringan Code: GitHub and GitHub gists  Telling stories with data   Publishing your stuff online Quickest and easiest way: RPubs The easiest way to get a knitted R Markdown onto the internet is to use RPubs. We talked about this in session 10, and you used it to turn in exercise 10.","tags":null,"title":"Sharing R output online","type":"docs"},{"authors":null,"categories":null,"content":"  Task 1: Storytelling reflection Task 2: Summary reflection Turning everything in   For your final exercise, you won’t do anything with R. You’ll instead have two writing tasks. You can write these in R Markdown if you want, or you can do it in Word or Google Docs or wherever else.\nTask 1: Storytelling reflection Write your standard reflection about the storytelling readings and videos.\n Task 2: Summary reflection Write a longer (400ish words) reflection on what you learned in the course in general. What was new? What was exciting? What will you remember? How has this class changed the way you look at data and graphics?\nYou might explore a few of these summative questions (but definitely don’t just go through and answer each of these!):\n What is truth? How do we find truth? Are facts truth? What’s the difference between content and form? Does beauty matter when describing truth? How does any of this philosophical humanities stuff relate to data visualization?   Turning everything in When you’re all done, upload the document with both reflection to iCollege.\nCongratulations! You did it!\n ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"03bd9f26eabfcea12e95e588578a138b","permalink":"/lab/15-exercise/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/lab/15-exercise/","section":"lab","summary":"Task 1: Storytelling reflection Task 2: Summary reflection Turning everything in   For your final exercise, you won’t do anything with R. You’ll instead have two writing tasks. You can write these in R Markdown if you want, or you can do it in Word or Google Docs or wherever else.\nTask 1: Storytelling reflection Write your standard reflection about the storytelling readings and videos.\n Task 2: Summary reflection Write a longer (400ish words) reflection on what you learned in the course in general.","tags":null,"title":"Truth, beauty, and data revisited","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Questions to reflect on  Slides R basics Case study: US Gun Murders The very basics Objects The workspace Functions Other prebuilt objects Variable names Saving your workspace Motivating scripts Commenting your code  Exercises Data types Data frames Examining an object The accessor: $ Vectors: numerics, characters, and logical Factors Lists Matrices  Exercises Vectors Creating vectors Names Sequences Subsetting  Coercion Not availables (NA)  Exercises Sorting sort order max and which.max rank Beware of recycling  Exercises Vector arithmetics Rescaling a vector Two vectors  Exercises Indexing Subsetting with logicals Logical operators which match %in%  Exercises Basic plots plot hist boxplot image  Exercises Videos    Readings As noted in the syllabus, your readings will be assigned each week in this area.\n The syllabus, content, examples, and labs pages for this class  Questions to reflect on  Do I remember anything about R?    Slides There are no slides for this lesson; the content below is intended to serve as a refresher for those who may have forgotten basics of R.\nALERT: The course content below should be considered a prerequisite for success. If you struggle to follow the content, please contact the professor or TA.\n  R basics In this book, we will be using the R software environment for all our analysis. You will learn R and data analysis techniques simultaneously. To follow along you will therefore need access to R. We also recommend the use of an integrated development environment (IDE), such as RStudio, to save your work. Note that it is common for a course or workshop to offer access to an R environment and an IDE through your web browser, as done by RStudio cloud1. If you have access to such a resource, you don’t need to install R and RStudio. However, if you intend on becoming an advanced data analyst, we highly recommend installing these tools on your computer2. Both R and RStudio are free and available online.\nCase study: US Gun Murders Imagine you live in Europe and are offered a job in a US company with many locations across all states. It is a great job, but news with headlines such as US Gun Homicide Rate Higher Than Other Developed Countries3 have you worried. Charts like this may concern you even more:\nOr even worse, this version from everytown.org: But then you remember that the US is a large and diverse country with 50 very different states as well as the District of Columbia (DC).\nCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial. You have options of where to live and want to determine the safety of each particular state. We will gain some insights by examining data related to gun homicides in the US during 2010 using R.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Be aware that the usefulness of some of these building blocks may not be immediately obvious, but later in the book you will appreciate having mastered these skills.\n The very basics Before we get started with the motivating dataset, we need to cover the very basics of R.\nObjects Suppose a high school student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). The quadratic formula gives us the solutions:\n\\[ \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} \\] which of course change depending on the values of \\(a\\), \\(b\\), and \\(c\\). One advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define:\na \u0026lt;- 1 b \u0026lt;- 1 c \u0026lt;- -1 which stores the values for later use. We use \u0026lt;- to assign values to the variables.\nWe can also assign values using = instead of \u0026lt;-, but we recommend against using = to avoid confusion.\nCopy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message.\nTo see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\na ## [1] 1 A more explicit way to ask R to show us the value stored in a is using print like this:\nprint(a) ## [1] 1 We use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\n The workspace As we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\nls() ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;dat\u0026quot; \u0026quot;img_path\u0026quot; \u0026quot;murders\u0026quot; In RStudio, the Environment tab shows the values:\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a ) ## [1] 0.618034 (-b - sqrt(b^2 - 4*a*c) ) / ( 2*a ) ## [1] -1.618034  Functions Once you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several predefined functions and most of the analysis pipelines we construct make extensive use of these.\nWe already used the install.packages, library, and ls functions. We also used the function sqrt to solve the quadratic equation above. There are many more prebuilt functions and even more can be added through packages. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\nlog(8) ## [1] 2.079442 log(a) ## [1] 0 You can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\nhelp(\u0026quot;log\u0026quot;) For most functions, we can also use this shorthand:\n?log The help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional. For example, the base of the function log defaults to base = exp(1) making log the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\nargs(log) ## function (x, base = exp(1)) ## NULL You can change the default values by simply assigning another object:\nlog(8, base = 2) ## [1] 3 Note that we have not been specifying the argument x as such:\nlog(x = 8, base = 2) ## [1] 3 The above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\nlog(8,2) ## [1] 3 If using the arguments’ names, then we can include them in whatever order we want:\nlog(base = 2, x = 8) ## [1] 3 To specify arguments, we must use =, and cannot use \u0026lt;-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n2 ^ 3 ## [1] 8 You can see the arithmetic operators by typing:\nhelp(\u0026quot;+\u0026quot;) or\n?\u0026quot;+\u0026quot; and the relational operators by typing:\nhelp(\u0026quot;\u0026gt;\u0026quot;) or\n?\u0026quot;\u0026gt;\u0026quot;  Other prebuilt objects There are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\ndata() This shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\nco2 R will show you Mauna Loa atmospheric CO2 concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant \\(\\pi\\) and \\(\\infty\\):\npi ## [1] 3.141593 Inf+1 ## [1] Inf  Variable names We have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in R. For example, don’t name one of your variables install.packages by typing something like install.packages \u0026lt;- 2.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\nsolution_1 \u0026lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a) solution_2 \u0026lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a) For more advice, we highly recommend studying Hadley Wickham’s style guide4.\n Saving your workspace Values remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image, and load to learn more.\n Motivating scripts To solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\na \u0026lt;- 3 b \u0026lt;- 2 c \u0026lt;- -1 (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a) By creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\n Commenting your code If a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n## Code to compute solution to quadratic equation of the form ax^2 + bx + c ## define the variables a \u0026lt;- 3 b \u0026lt;- 2 c \u0026lt;- -1 ## now compute the solution (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a)   Exercises 1. What is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\n2. Now use the same formula to compute the sum of the integers from 1 through 1,000.\n3. Look at the result of typing the following code into R:\nn \u0026lt;- 1000 x \u0026lt;- seq(1, n) sum(x) Based on the result, what do you think the functions seq and sum do? You can use help.\nsum creates a list of numbers and seq adds them up. seq creates a list of numbers and sum adds them up. seq creates a random list and sum computes the sum of 1 through 1,000. sum always returns the same number.  4. In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\n5. Which of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\nlog(10^x) log10(x^10) log(exp(x)) exp(log(x, base = 2))   Data types Variables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\na \u0026lt;- 2 class(a) ## [1] \u0026quot;numeric\u0026quot; To work efficiently in R, it is important to learn the different types of variables and what we can do with these.\nData frames Up to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\nlibrary(dslabs) data(murders) To see that this is in fact a data frame, we type:\nclass(murders) ## [1] \u0026quot;data.frame\u0026quot;  Examining an object The function str is useful for finding out more about the structure of an object:\nstr(murders) ## \u0026#39;data.frame\u0026#39;: 51 obs. of 5 variables: ## $ state : chr \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; ... ## $ abb : chr \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; ... ## $ region : Factor w/ 4 levels \u0026quot;Northeast\u0026quot;,\u0026quot;South\u0026quot;,..: 2 4 4 2 4 4 1 2 2 2 ... ## $ population: num 4779736 710231 6392017 2915918 37253956 ... ## $ total : num 135 19 232 93 1257 ... This tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\nhead(murders) ## state abb region population total ## 1 Alabama AL South 4779736 135 ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232 ## 4 Arkansas AR South 2915918 93 ## 5 California CA West 37253956 1257 ## 6 Colorado CO West 5029196 65 In this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let’s learn more about the components of this object.\n The accessor: $ For our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\nmurders$population ## [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 897934 ## [9] 601723 19687653 9920000 1360301 1567582 12830632 6483802 3046355 ## [17] 2853118 4339367 4533372 1328361 5773552 6547629 9883640 5303925 ## [25] 2967297 5988927 989415 1826341 2700551 1316470 8791894 2059179 ## [33] 19378102 9535483 672591 11536504 3751351 3831074 12702379 1052567 ## [41] 4625364 814180 6346105 25145561 2763885 625741 8001024 6724540 ## [49] 1852994 5686986 563626 But how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\nnames(murders) ## [1] \u0026quot;state\u0026quot; \u0026quot;abb\u0026quot; \u0026quot;region\u0026quot; \u0026quot;population\u0026quot; \u0026quot;total\u0026quot; It is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\nTip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\n Vectors: numerics, characters, and logical The object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\npop \u0026lt;- murders$population length(pop) ## [1] 51 This particular vector is numeric since population sizes are numbers:\nclass(pop) ## [1] \u0026quot;numeric\u0026quot; In a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\nclass(murders$state) ## [1] \u0026quot;character\u0026quot; As with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\nz \u0026lt;- 3 == 2 z ## [1] FALSE class(z) ## [1] \u0026quot;logical\u0026quot; Here the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality.\nYou can see the other relational operators by typing:\n?Comparison In future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\nAdvanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\n Factors In the murders dataset, we might expect the region to also be a character vector. However, it is not:\nclass(murders$region) ## [1] \u0026quot;factor\u0026quot; It is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\nlevels(murders$region) ## [1] \u0026quot;Northeast\u0026quot; \u0026quot;South\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot; In the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\nregion \u0026lt;- murders$region value \u0026lt;- murders$total region \u0026lt;- reorder(region, value, FUN = sum) levels(region) ## [1] \u0026quot;Northeast\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot; \u0026quot;South\u0026quot; The new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\nWarning: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\n Lists Data frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:\nrecord ## $name ## [1] \u0026quot;John Doe\u0026quot; ## ## $student_id ## [1] 1234 ## ## $grades ## [1] 95 82 91 97 93 ## ## $final_grade ## [1] \u0026quot;A\u0026quot; class(record) ## [1] \u0026quot;list\u0026quot; As with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list.\nrecord$student_id ## [1] 1234 We can also use double square brackets ([[) like this:\nrecord[[\u0026quot;student_id\u0026quot;]] ## [1] 1234 You should get used to the fact that in R, there are often several ways to do the same thing, such as accessing entries.\nYou might also encounter lists without variable names.\nrecord2 ## [[1]] ## [1] \u0026quot;John Doe\u0026quot; ## ## [[2]] ## [1] 1234 If a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\nrecord2[[1]] ## [1] \u0026quot;John Doe\u0026quot; We won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\n Matrices Matrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this book, but much of what happens in the background when you perform a data analysis involves matrices. We cover matrices in more detail in Chapter ?? but describe them briefly here since some of the functions we will learn return matrices.\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\nmat \u0026lt;- matrix(1:12, 4, 3) mat ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 You can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\nmat[2, 3] ## [1] 10 If you want the entire second row, you leave the column spot empty:\nmat[2, ] ## [1] 2 6 10 Notice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\nmat[, 3] ## [1] 9 10 11 12 This is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\nmat[, 2:3] ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10 ## [3,] 7 11 ## [4,] 8 12 You can subset both rows and columns:\nmat[1:2, 2:3] ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10 We can convert matrices into data frames using the function as.data.frame:\nas.data.frame(mat) ## V1 V2 V3 ## 1 1 5 9 ## 2 2 6 10 ## 3 3 7 11 ## 4 4 8 12 You can also use single square brackets ([) to access rows and columns of a data frame:\ndata(\u0026quot;murders\u0026quot;) murders[25, 1] ## [1] \u0026quot;Mississippi\u0026quot; murders[2:3, ] ## state abb region population total ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232   Exercises 1. Load the US murders dataset.\nlibrary(dslabs) data(murders) Use the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\nThe 51 states. The murder rates for all 50 states and DC. The state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010. str shows no relevant information.  2. What are the column names used by the data frame for these five variables?\n3. Use the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\n4. Now use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\n5. We saw that the region column stores a factor. You can corroborate this by typing:\nclass(murders$region) With one line of code, use the function levels and length to determine the number of regions defined by this dataset.\n6. The function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.\n Vectors In R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\nCreating vectors We can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\ncodes \u0026lt;- c(380, 124, 818) codes ## [1] 380 124 818 We can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\ncountry \u0026lt;- c(\u0026quot;italy\u0026quot;, \u0026quot;canada\u0026quot;, \u0026quot;egypt\u0026quot;) In R you can also use single quotes:\ncountry \u0026lt;- c(\u0026#39;italy\u0026#39;, \u0026#39;canada\u0026#39;, \u0026#39;egypt\u0026#39;) But be careful not to confuse the single quote ’ with the back quote `.\nBy now you should know that if you type:\ncountry \u0026lt;- c(italy, canada, egypt) you receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\n Names Sometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\ncodes \u0026lt;- c(italy = 380, canada = 124, egypt = 818) codes ## italy canada egypt ## 380 124 818 The object codes continues to be a numeric vector:\nclass(codes) ## [1] \u0026quot;numeric\u0026quot; but with names:\nnames(codes) ## [1] \u0026quot;italy\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;egypt\u0026quot; If the use of strings without quotes looks confusing, know that you can use the quotes as well:\ncodes \u0026lt;- c(\u0026quot;italy\u0026quot; = 380, \u0026quot;canada\u0026quot; = 124, \u0026quot;egypt\u0026quot; = 818) codes ## italy canada egypt ## 380 124 818 There is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\nWe can also assign names using the names functions:\ncodes \u0026lt;- c(380, 124, 818) country \u0026lt;- c(\u0026quot;italy\u0026quot;,\u0026quot;canada\u0026quot;,\u0026quot;egypt\u0026quot;) names(codes) \u0026lt;- country codes ## italy canada egypt ## 380 124 818  Sequences Another useful function for creating vectors generates sequences:\nseq(1, 10) ## [1] 1 2 3 4 5 6 7 8 9 10 The first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\nseq(1, 10, 2) ## [1] 1 3 5 7 9 If we want consecutive integers, we can use the following shorthand:\n1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 When we use these functions, R produces integers, not numerics, because they are typically used to index something:\nclass(1:10) ## [1] \u0026quot;integer\u0026quot; However, if we create a sequence including non-integers, the class changes:\nclass(seq(1, 10, 0.5)) ## [1] \u0026quot;numeric\u0026quot;  Subsetting We use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\ncodes[2] ## canada ## 124 You can get more than one entry by using a multi-entry vector as an index:\ncodes[c(1,3)] ## italy egypt ## 380 818 The sequences defined above are particularly useful if we want to access, say, the first two elements:\ncodes[1:2] ## italy canada ## 380 124 If the elements have names, we can also access the entries using these names. Below are two examples.\ncodes[\u0026quot;canada\u0026quot;] ## canada ## 124 codes[c(\u0026quot;egypt\u0026quot;,\u0026quot;italy\u0026quot;)] ## egypt italy ## 818 380   Coercion In general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\nx \u0026lt;- c(1, \u0026quot;canada\u0026quot;, 3) But we don’t get one, not even a warning! What happened? Look at x and its class:\nx ## [1] \u0026quot;1\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;3\u0026quot; class(x) ## [1] \u0026quot;character\u0026quot; R coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \"1\" and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\nx \u0026lt;- 1:5 y \u0026lt;- as.character(x) y ## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5\u0026quot; You can turn it back with as.numeric:\nas.numeric(y) ## [1] 1 2 3 4 5 This function is actually quite useful since datasets that include numbers as character strings are common.\nNot availables (NA) When a function tries to coerce one type to another and encounters an impossible case, it usually gives us a warning and turns the entry into a special value called an NA for “not available”. For example:\nx \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;3\u0026quot;) as.numeric(x) ## Warning: NAs introduced by coercion ## [1] 1 NA 3 R does not have any guesses for what number you want when you type b, so it does not try.\nAs a data scientist you will encounter the NAs often as they are generally used for missing data, a common problem in real-world datasets.\n  Exercises 1. Use the function c to create a vector with the average high temperatures in January for Beijing, Lagos, Paris, Rio de Janeiro, San Juan, and Toronto, which are 35, 88, 42, 84, 81, and 30 degrees Fahrenheit. Call the object temp.\n2. Now create a vector with the city names and call the object city.\n3. Use the names function and the objects defined in the previous exercises to associate the temperature data with its corresponding city.\n4. Use the [ and : operators to access the temperature of the first three cities on the list.\n5. Use the [ operator to access the temperature of Paris and San Juan.\n6. Use the : operator to create a sequence of numbers \\(12,13,14,\\dots,73\\).\n7. Create a vector containing all the positive odd numbers smaller than 100.\n8. Create a vector of numbers that starts at 6, does not pass 55, and adds numbers in increments of 4/7: 6, 6 + 4/7, 6 + 8/7, and so on. How many numbers does the list have? Hint: use seq and length.\n9. What is the class of the following object a \u0026lt;- seq(1, 10, 0.5)?\n10. What is the class of the following object a \u0026lt;- seq(1, 10)?\n11. The class of class(a\u0026lt;-1) is numeric, not integer. R defaults to numeric and to force an integer, you need to add the letter L. Confirm that the class of 1L is integer.\n12. Define the following vector:\nx \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;5\u0026quot;) and coerce it to get integers.\n Sorting Now that we have mastered some basic R knowledge, let’s try to gain some insights into the safety of different states in the context of gun murders.\nsort Say we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\nlibrary(dslabs) data(murders) sort(murders$total) ## [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32 ## [16] 36 38 53 63 65 67 84 93 93 97 97 99 111 116 118 ## [31] 120 135 142 207 219 232 246 250 286 293 310 321 351 364 376 ## [46] 413 457 517 669 805 1257 However, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.\n order The function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:\nx \u0026lt;- c(31, 4, 15, 92, 65) sort(x) ## [1] 4 15 31 65 92 Rather than sort the input vector, the function order returns the index that sorts input vector:\nindex \u0026lt;- order(x) x[index] ## [1] 4 15 31 65 92 This is the same output as that returned by sort(x). If we look at this index, we see why it works:\nx ## [1] 31 4 15 92 65 order(x) ## [1] 2 3 1 5 4 The second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\nmurders$state[1:6] ## [1] \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; \u0026quot;California\u0026quot; ## [6] \u0026quot;Colorado\u0026quot; murders$abb[1:6] ## [1] \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;CO\u0026quot; This means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\nind \u0026lt;- order(murders$total) murders$abb[ind] ## [1] \u0026quot;VT\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;WV\u0026quot; \u0026quot;NE\u0026quot; ## [16] \u0026quot;OR\u0026quot; \u0026quot;DE\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;WA\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;DC\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;MA\u0026quot; ## [31] \u0026quot;MS\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;IL\u0026quot; \u0026quot;GA\u0026quot; ## [46] \u0026quot;MI\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;CA\u0026quot; According to the above, California had the most murders.\n max and which.max If we are only interested in the entry with the largest value, we can use max for the value:\nmax(murders$total) ## [1] 1257 and which.max for the index of the largest value:\ni_max \u0026lt;- which.max(murders$total) murders$state[i_max] ## [1] \u0026quot;California\u0026quot; For the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\n rank Although not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\nx \u0026lt;- c(31, 4, 15, 92, 65) rank(x) ## [1] 3 1 2 5 4 To summarize, let’s look at the results of the three functions we have introduced:\n  original  sort  order  rank      31  4  2  3    4  15  3  1    15  31  1  2    92  65  5  5    65  92  4  4      Beware of recycling Another common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:\nx \u0026lt;- c(1,2,3) y \u0026lt;- c(10, 20, 30, 40, 50, 60, 70) x+y ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 11 22 33 41 52 63 71 We do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\n  Exercises For these exercises we will use the US murders dataset. Make sure you load it prior to starting.\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;) 1. Use the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\n2. Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\n3. We can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\n4. Now we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\n5. You can create a data frame using the data.frame function. Here is a quick example:\ntemp \u0026lt;- c(35, 88, 42, 84, 81, 30) city \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;, \u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;) city_temps \u0026lt;- data.frame(name = city, temperature = temp) Use the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\n6. Repeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\n7. The na_example vector represents a series of counts. You can quickly examine the object using:\ndata(\u0026quot;na_example\u0026quot;) str(na_example) ## int [1:1000] 2 1 3 2 1 3 1 4 3 2 ... However, when we compute the average with the function mean, we obtain an NA:\nmean(na_example) ## [1] NA The is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have.\n8. Now compute the average again, but only for the entries that are not NA. Hint: remember the ! operator.\n Vector arithmetics California had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;) murders$state[which.max(murders$population)] ## [1] \u0026quot;California\u0026quot; with over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\nRescaling a vector In R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\ninches \u0026lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70) and want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\ninches * 2.54 ## [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80 In the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\ninches - 69 ## [1] 0 -7 -3 1 1 4 -2 4 -2 1  Two vectors If we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n\\[ \\begin{pmatrix} a\\\\ b\\\\ c\\\\ d \\end{pmatrix} + \\begin{pmatrix} e\\\\ f\\\\ g\\\\ h \\end{pmatrix} = \\begin{pmatrix} a +e\\\\ b + f\\\\ c + g\\\\ d + h \\end{pmatrix} \\]\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000 Once we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\nmurders$abb[order(murder_rate)] ## [1] \u0026quot;VT\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;OR\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;WA\u0026quot; ## [16] \u0026quot;WV\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;NE\u0026quot; \u0026quot;MA\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IL\u0026quot; ## [31] \u0026quot;OK\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;GA\u0026quot; \u0026quot;MS\u0026quot; \u0026quot;MI\u0026quot; ## [46] \u0026quot;DE\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;DC\u0026quot;   Exercises 1. Previously we created this data frame:\ntemp \u0026lt;- c(35, 88, 42, 84, 81, 30) city \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;, \u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;) city_temps \u0026lt;- data.frame(name = city, temperature = temp) Remake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is \\(C = \\frac{5}{9} \\times (F - 32)\\).\n2. What is the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler, we know it should be close to \\(\\pi^2/6\\).\n3. Compute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?\n Indexing R provides a powerful and convenient way of indexing vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;) Subsetting with logicals We have now calculated the murder rate using:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000 Imagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\nind \u0026lt;- murder_rate \u0026lt; 0.71 If we instead want to know if a value is less or equal, we can use:\nind \u0026lt;- murder_rate \u0026lt;= 0.71 Note that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\nmurders$state[ind] ## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Iowa\u0026quot; \u0026quot;New Hampshire\u0026quot; \u0026quot;North Dakota\u0026quot; ## [5] \u0026quot;Vermont\u0026quot; In order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\nsum(ind) ## [1] 5  Logical operators Suppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with \u0026amp;. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\nTRUE \u0026amp; TRUE ## [1] TRUE TRUE \u0026amp; FALSE ## [1] FALSE FALSE \u0026amp; FALSE ## [1] FALSE For our example, we can form two logicals:\nwest \u0026lt;- murders$region == \u0026quot;West\u0026quot; safe \u0026lt;- murder_rate \u0026lt;= 1 and we can use the \u0026amp; to get a vector of logicals that tells us which states satisfy both conditions:\nind \u0026lt;- safe \u0026amp; west murders$state[ind] ## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Idaho\u0026quot; \u0026quot;Oregon\u0026quot; \u0026quot;Utah\u0026quot; \u0026quot;Wyoming\u0026quot;  which Suppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\nind \u0026lt;- which(murders$state == \u0026quot;California\u0026quot;) murder_rate[ind] ## [1] 3.374138  match If instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function match. This function tells us which indexes of a second vector match each of the entries of a first vector:\nind \u0026lt;- match(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state) ind ## [1] 33 10 44 Now we can look at the murder rates:\nmurder_rate[ind] ## [1] 2.667960 3.398069 3.201360  %in% If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\nc(\u0026quot;Boston\u0026quot;, \u0026quot;Dakota\u0026quot;, \u0026quot;Washington\u0026quot;) %in% murders$state ## [1] FALSE FALSE TRUE Note that we will be using %in% often throughout the book.\nAdvanced: There is a connection between match and %in% through which. To see this, notice that the following two lines produce the same index (although in different order):\nmatch(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state) ## [1] 33 10 44 which(murders$state%in%c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;)) ## [1] 10 33 44   Exercises Start by loading the library and data.\nlibrary(dslabs) data(murders) 1. Compute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\n2. Now use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\n3. Use the results from the previous exercise to report the names of the states with murder rates lower than 1.\n4. Now extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator \u0026amp;.\n5. In a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\n6. Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\n7. Use the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\n8. Extend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index.\n Basic plots In Chapter ?? we describe an add-on package that provides a powerful approach to producing plots in R. We then have an entire part on Data Visualization in which we provide many examples. Here we briefly describe some of the functions that are available in a basic R installation.\nplot The plot function can be used to make scatterplots. Here is a plot of total murders versus population.\nx \u0026lt;- murders$population / 10^6 y \u0026lt;- murders$total plot(x, y) For a quick plot that avoids accessing variables twice, we can use the with function:\nwith(murders, plot(population, total)) The function with lets us use the murders column names in the plot function. It also works with any data frames and any function.\n hist We will describe histograms as they relate to distributions in the Data Visualization part of the book. Here we will simply note that histograms are a powerful graphical summary of a list of numbers that gives you a general overview of the types of values you have. We can make a histogram of our murder rates by simply typing:\nx \u0026lt;- with(murders, total / population * 100000) hist(x) We can see that there is a wide range of values with most of them between 2 and 3 and one very extreme case with a murder rate of more than 15:\nmurders$state[which.max(x)] ## [1] \u0026quot;District of Columbia\u0026quot;  boxplot Boxplots will also be described in the Data Visualization part of the book. They provide a more terse summary than histograms, but they are easier to stack with other boxplots. For example, here we can use them to compare the different regions:\nmurders$rate \u0026lt;- with(murders, total / population * 100000) boxplot(rate~region, data = murders) We can see that the South has higher murder rates than the other three regions.\n image The image function displays the values in a matrix using color. Here is a quick example:\nx \u0026lt;- matrix(1:120, 12, 10) image(x)   Exercises 1. We made a plot of total murders versus population and noted a strong relationship. Not surprisingly, states with larger populations had more murders.\nlibrary(dslabs) data(murders) population_in_millions \u0026lt;- murders$population/10^6 total_gun_murders \u0026lt;- murders$total plot(population_in_millions, total_gun_murders) Keep in mind that many states have populations below 5 million and are bunched up. We may gain further insights from making this plot in the log scale. Transform the variables using the log10 transformation and then plot them.\n2. Create a histogram of the state populations.\n3. Generate boxplots of the state populations by region.\n Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction     https://rstudio.cloud↩︎\n https://rafalab.github.io/dsbook/installing-r-rstudio.html↩︎\n http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/↩︎\n http://adv-r.had.co.nz/Style.html↩︎\n   ","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"9be773bd8dbb1773f9326846c039d666","permalink":"/content/00-content/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/content/00-content/","section":"content","summary":"Readings Questions to reflect on  Slides R basics Case study: US Gun Murders The very basics Objects The workspace Functions Other prebuilt objects Variable names Saving your workspace Motivating scripts Commenting your code  Exercises Data types Data frames Examining an object The accessor: $ Vectors: numerics, characters, and logical Factors Lists Matrices  Exercises Vectors Creating vectors Names Sequences Subsetting  Coercion Not availables (NA)  Exercises Sorting sort order max and which.","tags":null,"title":"Welcome to `R`","type":"docs"},{"authors":null,"categories":null,"content":"Readings As noted in the syllabus, your readings will be assigned each week in this area.\n The syllabus, content, examples, and labs pages for this class  Questions to reflect on  Do I remember anything about R?  Slides There are no slides for this lesson; the content below is intended to serve as a refresher for those who may have forgotten basics of R.\n:::fyi ALERT: The course content below should be considered a prerequisite for success. If you struggle to follow the content, please contact the professor or TA. :::\nR basics In this book, we will be using the R software environment for all our analysis. You will learn R and data analysis techniques simultaneously. To follow along you will therefore need access to R. We also recommend the use of an integrated development environment (IDE), such as RStudio, to save your work. Note that it is common for a course or workshop to offer access to an R environment and an IDE through your web browser, as done by RStudio cloud^[https://rstudio.cloud]. If you have access to such a resource, you don\u0026rsquo;t need to install R and RStudio. However, if you intend on becoming an advanced data analyst, we highly recommend installing these tools on your computer^[https://rafalab.github.io/dsbook/installing-r-rstudio.html]. Both R and RStudio are free and available online.\nCase study: US Gun Murders Imagine you live in Europe and are offered a job in a US company with many locations across all states. It is a great job, but news with headlines such as US Gun Homicide Rate Higher Than Other Developed Countries^[http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/] have you worried. Charts like this may concern you even more:\nOr even worse, this version from everytown.org: But then you remember that the US is a large and diverse country with 50 very different states as well as the District of Columbia (DC).\nCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial. You have options of where to live and want to determine the safety of each particular state. We will gain some insights by examining data related to gun homicides in the US during 2010 using R.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Be aware that the usefulness of some of these building blocks may not be immediately obvious, but later in the book you will appreciate having mastered these skills.\nThe very basics Before we get started with the motivating dataset, we need to cover the very basics of R.\nObjects Suppose a high school student asks us for help solving several quadratic equations of the form $ax^2+bx+c = 0$. The quadratic formula gives us the solutions:\n$$ \\frac{-b - \\sqrt{b^2 - 4ac}}{2a},, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} $$ which of course change depending on the values of $a$, $b$, and $c$. One advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve $x^2 + x -1 = 0$, then we define:\na \u0026lt;- 1 b \u0026lt;- 1 c \u0026lt;- -1  which stores the values for later use. We use \u0026lt;- to assign values to the variables.\nWe can also assign values using = instead of \u0026lt;-, but we recommend against using = to avoid confusion.\nCopy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message.\nTo see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\na  ## [1] 1  A more explicit way to ask R to show us the value stored in a is using print like this:\nprint(a)  ## [1] 1  We use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\nThe workspace As we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\nls()  ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;dat\u0026quot; \u0026quot;img_path\u0026quot; \u0026quot;murders\u0026quot;  In RStudio, the Environment tab shows the values:\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )  ## [1] 0.618034  (-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )  ## [1] -1.618034  Functions Once you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several predefined functions and most of the analysis pipelines we construct make extensive use of these.\nWe already used the install.packages, library, and ls functions. We also used the function sqrt to solve the quadratic equation above. There are many more prebuilt functions and even more can be added through packages. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\nlog(8)  ## [1] 2.079442  log(a)  ## [1] 0  You can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\nhelp(\u0026quot;log\u0026quot;)  For most functions, we can also use this shorthand:\n?log  The help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional. For example, the base of the function log defaults to base = exp(1) making log the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\nargs(log)  ## function (x, base = exp(1)) ## NULL  You can change the default values by simply assigning another object:\nlog(8, base = 2)  ## [1] 3  Note that we have not been specifying the argument x as such:\nlog(x = 8, base = 2)  ## [1] 3  The above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\nlog(8,2)  ## [1] 3  If using the arguments\u0026rsquo; names, then we can include them in whatever order we want:\nlog(base = 2, x = 8)  ## [1] 3  To specify arguments, we must use =, and cannot use \u0026lt;-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n2 ^ 3  ## [1] 8  You can see the arithmetic operators by typing:\nhelp(\u0026quot;+\u0026quot;)  or\n?\u0026quot;+\u0026quot;  and the relational operators by typing:\nhelp(\u0026quot;\u0026gt;\u0026quot;)  or\n?\u0026quot;\u0026gt;\u0026quot;  Other prebuilt objects There are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\ndata()  This shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\nco2  R will show you Mauna Loa atmospheric CO2 concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant $\\pi$ and $\\infty$:\npi  ## [1] 3.141593  Inf+1  ## [1] Inf  Variable names We have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can\u0026rsquo;t contain spaces, and should not be variables that are predefined in R. For example, don\u0026rsquo;t name one of your variables install.packages by typing something like install.packages \u0026lt;- 2.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\nsolution_1 \u0026lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a) solution_2 \u0026lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a)  For more advice, we highly recommend studying Hadley Wickham\u0026rsquo;s style guide^[http://adv-r.had.co.nz/Style.html].\nSaving your workspace Values remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image, and load to learn more.\nMotivating scripts To solve another equation such as $3x^2 + 2x -1$, we can copy and paste the code above and then redefine the variables and recompute the solution:\na \u0026lt;- 3 b \u0026lt;- 2 c \u0026lt;- -1 (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a)  By creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\nCommenting your code If a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n## Code to compute solution to quadratic equation of the form ax^2 + bx + c ## define the variables a \u0026lt;- 3 b \u0026lt;- 2 c \u0026lt;- -1 ## now compute the solution (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a)  Exercises 1. What is the sum of the first 100 positive integers? The formula for the sum of integers $1$ through $n$ is $n(n+1)/2$. Define $n=100$ and then use R to compute the sum of $1$ through $100$ using the formula. What is the sum?\n2. Now use the same formula to compute the sum of the integers from 1 through 1,000.\n3. Look at the result of typing the following code into R:\nn \u0026lt;- 1000 x \u0026lt;- seq(1, n) sum(x)  Based on the result, what do you think the functions seq and sum do? You can use help.\na. sum creates a list of numbers and seq adds them up. b. seq creates a list of numbers and sum adds them up. c. seq creates a random list and sum computes the sum of 1 through 1,000. d. sum always returns the same number.\n4. In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\n5. Which of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\na. log(10^x) b. log10(x^10) c. log(exp(x)) d. exp(log(x, base = 2))\nData types Variables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\na \u0026lt;- 2 class(a)  ## [1] \u0026quot;numeric\u0026quot;  To work efficiently in R, it is important to learn the different types of variables and what we can do with these.\nData frames Up to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\nlibrary(dslabs) data(murders)  To see that this is in fact a data frame, we type:\nclass(murders)  ## [1] \u0026quot;data.frame\u0026quot;  Examining an object The function str is useful for finding out more about the structure of an object:\nstr(murders)  ## 'data.frame':\t51 obs. of 5 variables: ## $ state : chr \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; ... ## $ abb : chr \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; ... ## $ region : Factor w/ 4 levels \u0026quot;Northeast\u0026quot;,\u0026quot;South\u0026quot;,..: 2 4 4 2 4 4 1 2 2 2 ... ## $ population: num 4779736 710231 6392017 2915918 37253956 ... ## $ total : num 135 19 232 93 1257 ...  This tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\nhead(murders)  ## state abb region population total ## 1 Alabama AL South 4779736 135 ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232 ## 4 Arkansas AR South 2915918 93 ## 5 California CA West 37253956 1257 ## 6 Colorado CO West 5029196 65  In this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let\u0026rsquo;s learn more about the components of this object.\nThe accessor: $ For our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\nmurders$population  ## [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 897934 ## [9] 601723 19687653 9920000 1360301 1567582 12830632 6483802 3046355 ## [17] 2853118 4339367 4533372 1328361 5773552 6547629 9883640 5303925 ## [25] 2967297 5988927 989415 1826341 2700551 1316470 8791894 2059179 ## [33] 19378102 9535483 672591 11536504 3751351 3831074 12702379 1052567 ## [41] 4625364 814180 6346105 25145561 2763885 625741 8001024 6724540 ## [49] 1852994 5686986 563626  But how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\nnames(murders)  ## [1] \u0026quot;state\u0026quot; \u0026quot;abb\u0026quot; \u0026quot;region\u0026quot; \u0026quot;population\u0026quot; \u0026quot;total\u0026quot;  It is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\nTip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\nVectors: numerics, characters, and logical The object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\npop \u0026lt;- murders$population length(pop)  ## [1] 51  This particular vector is numeric since population sizes are numbers:\nclass(pop)  ## [1] \u0026quot;numeric\u0026quot;  In a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\nclass(murders$state)  ## [1] \u0026quot;character\u0026quot;  As with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\nz \u0026lt;- 3 == 2 z  ## [1] FALSE  class(z)  ## [1] \u0026quot;logical\u0026quot;  Here the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality.\nYou can see the other relational operators by typing:\n?Comparison  In future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\nAdvanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\nFactors In the murders dataset, we might expect the region to also be a character vector. However, it is not:\nclass(murders$region)  ## [1] \u0026quot;factor\u0026quot;  It is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\nlevels(murders$region)  ## [1] \u0026quot;Northeast\u0026quot; \u0026quot;South\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot;  In the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\nregion \u0026lt;- murders$region value \u0026lt;- murders$total region \u0026lt;- reorder(region, value, FUN = sum) levels(region)  ## [1] \u0026quot;Northeast\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot; \u0026quot;South\u0026quot;  The new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\nWarning: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\nLists Data frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:\nrecord  ## $name ## [1] \u0026quot;John Doe\u0026quot; ## ## $student_id ## [1] 1234 ## ## $grades ## [1] 95 82 91 97 93 ## ## $final_grade ## [1] \u0026quot;A\u0026quot;  class(record)  ## [1] \u0026quot;list\u0026quot;  As with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list.\nrecord$student_id  ## [1] 1234  We can also use double square brackets ([[) like this:\nrecord[[\u0026quot;student_id\u0026quot;]]  ## [1] 1234  You should get used to the fact that in R, there are often several ways to do the same thing, such as accessing entries.\nYou might also encounter lists without variable names.\nrecord2  ## [[1]] ## [1] \u0026quot;John Doe\u0026quot; ## ## [[2]] ## [1] 1234  If a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\nrecord2[[1]]  ## [1] \u0026quot;John Doe\u0026quot;  We won\u0026rsquo;t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\nMatrices Matrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this book, but much of what happens in the background when you perform a data analysis involves matrices. We cover matrices in more detail in Chapter @ref(matrix-algebra) but describe them briefly here since some of the functions we will learn return matrices.\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\nmat \u0026lt;- matrix(1:12, 4, 3) mat  ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12  You can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\nmat[2, 3]  ## [1] 10  If you want the entire second row, you leave the column spot empty:\nmat[2, ]  ## [1] 2 6 10  Notice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\nmat[, 3]  ## [1] 9 10 11 12  This is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\nmat[, 2:3]  ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10 ## [3,] 7 11 ## [4,] 8 12  You can subset both rows and columns:\nmat[1:2, 2:3]  ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10  We can convert matrices into data frames using the function as.data.frame:\nas.data.frame(mat)  ## V1 V2 V3 ## 1 1 5 9 ## 2 2 6 10 ## 3 3 7 11 ## 4 4 8 12  You can also use single square brackets ([) to access rows and columns of a data frame:\ndata(\u0026quot;murders\u0026quot;) murders[25, 1]  ## [1] \u0026quot;Mississippi\u0026quot;  murders[2:3, ]  ## state abb region population total ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232  Exercises 1. Load the US murders dataset.\nlibrary(dslabs) data(murders)  Use the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\na. The 51 states. b. The murder rates for all 50 states and DC. c. The state name, the abbreviation of the state name, the state\u0026rsquo;s region, and the state\u0026rsquo;s population and total number of murders for 2010. d. str shows no relevant information.\n2. What are the column names used by the data frame for these five variables?\n3. Use the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\n4. Now use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\n5. We saw that the region column stores a factor. You can corroborate this by typing:\nclass(murders$region)  With one line of code, use the function levels and length to determine the number of regions defined by this dataset.\n6. The function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.\nVectors In R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\nCreating vectors We can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\ncodes \u0026lt;- c(380, 124, 818) codes  ## [1] 380 124 818  We can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\ncountry \u0026lt;- c(\u0026quot;italy\u0026quot;, \u0026quot;canada\u0026quot;, \u0026quot;egypt\u0026quot;)  In R you can also use single quotes:\ncountry \u0026lt;- c('italy', 'canada', 'egypt')  But be careful not to confuse the single quote ' with the back quote `.\nBy now you should know that if you type:\ncountry \u0026lt;- c(italy, canada, egypt)  you receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\nNames Sometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\ncodes \u0026lt;- c(italy = 380, canada = 124, egypt = 818) codes  ## italy canada egypt ## 380 124 818  The object codes continues to be a numeric vector:\nclass(codes)  ## [1] \u0026quot;numeric\u0026quot;  but with names:\nnames(codes)  ## [1] \u0026quot;italy\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;egypt\u0026quot;  If the use of strings without quotes looks confusing, know that you can use the quotes as well:\ncodes \u0026lt;- c(\u0026quot;italy\u0026quot; = 380, \u0026quot;canada\u0026quot; = 124, \u0026quot;egypt\u0026quot; = 818) codes  ## italy canada egypt ## 380 124 818  There is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\nWe can also assign names using the names functions:\ncodes \u0026lt;- c(380, 124, 818) country \u0026lt;- c(\u0026quot;italy\u0026quot;,\u0026quot;canada\u0026quot;,\u0026quot;egypt\u0026quot;) names(codes) \u0026lt;- country codes  ## italy canada egypt ## 380 124 818  Sequences Another useful function for creating vectors generates sequences:\nseq(1, 10)  ## [1] 1 2 3 4 5 6 7 8 9 10  The first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\nseq(1, 10, 2)  ## [1] 1 3 5 7 9  If we want consecutive integers, we can use the following shorthand:\n1:10  ## [1] 1 2 3 4 5 6 7 8 9 10  When we use these functions, R produces integers, not numerics, because they are typically used to index something:\nclass(1:10)  ## [1] \u0026quot;integer\u0026quot;  However, if we create a sequence including non-integers, the class changes:\nclass(seq(1, 10, 0.5))  ## [1] \u0026quot;numeric\u0026quot;  Subsetting We use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\ncodes[2]  ## canada ## 124  You can get more than one entry by using a multi-entry vector as an index:\ncodes[c(1,3)]  ## italy egypt ## 380 818  The sequences defined above are particularly useful if we want to access, say, the first two elements:\ncodes[1:2]  ## italy canada ## 380 124  If the elements have names, we can also access the entries using these names. Below are two examples.\ncodes[\u0026quot;canada\u0026quot;]  ## canada ## 124  codes[c(\u0026quot;egypt\u0026quot;,\u0026quot;italy\u0026quot;)]  ## egypt italy ## 818 380  Coercion In general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let\u0026rsquo;s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\nx \u0026lt;- c(1, \u0026quot;canada\u0026quot;, 3)  But we don\u0026rsquo;t get one, not even a warning! What happened? Look at x and its class:\nx  ## [1] \u0026quot;1\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;3\u0026quot;  class(x)  ## [1] \u0026quot;character\u0026quot;  R coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \u0026quot;1\u0026quot; and \u0026ldquo;3\u0026rdquo;. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\nx \u0026lt;- 1:5 y \u0026lt;- as.character(x) y  ## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5\u0026quot;  You can turn it back with as.numeric:\nas.numeric(y)  ## [1] 1 2 3 4 5  This function is actually quite useful since datasets that include numbers as character strings are common.\nNot availables (NA) When a function tries to coerce one type to another and encounters an impossible case, it usually gives us a warning and turns the entry into a special value called an NA for \u0026ldquo;not available\u0026rdquo;. For example:\nx \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;3\u0026quot;) as.numeric(x)  ## Warning: NAs introduced by coercion  ## [1] 1 NA 3  R does not have any guesses for what number you want when you type b, so it does not try.\nAs a data scientist you will encounter the NAs often as they are generally used for missing data, a common problem in real-world datasets.\nExercises 1. Use the function c to create a vector with the average high temperatures in January for Beijing, Lagos, Paris, Rio de Janeiro, San Juan, and Toronto, which are 35, 88, 42, 84, 81, and 30 degrees Fahrenheit. Call the object temp.\n2. Now create a vector with the city names and call the object city.\n3. Use the names function and the objects defined in the previous exercises to associate the temperature data with its corresponding city.\n4. Use the [ and : operators to access the temperature of the first three cities on the list.\n5. Use the [ operator to access the temperature of Paris and San Juan.\n6. Use the : operator to create a sequence of numbers $12,13,14,\\dots,73$.\n7. Create a vector containing all the positive odd numbers smaller than 100.\n8. Create a vector of numbers that starts at 6, does not pass 55, and adds numbers in increments of 4/7: 6, 6 + 4/7, 6 + 8/7, and so on. How many numbers does the list have? Hint: use seq and length.\n9. What is the class of the following object a \u0026lt;- seq(1, 10, 0.5)?\n10. What is the class of the following object a \u0026lt;- seq(1, 10)?\n11. The class of class(a\u0026lt;-1) is numeric, not integer. R defaults to numeric and to force an integer, you need to add the letter L. Confirm that the class of 1L is integer.\n12. Define the following vector:\nx \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;5\u0026quot;)  and coerce it to get integers.\nSorting Now that we have mastered some basic R knowledge, let\u0026rsquo;s try to gain some insights into the safety of different states in the context of gun murders.\nsort Say we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\nlibrary(dslabs) data(murders) sort(murders$total)  ## [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32 ## [16] 36 38 53 63 65 67 84 93 93 97 97 99 111 116 118 ## [31] 120 135 142 207 219 232 246 250 286 293 310 321 351 364 376 ## [46] 413 457 517 669 805 1257  However, this does not give us information about which states have which murder totals. For example, we don\u0026rsquo;t know which state had 1257.\norder The function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let\u0026rsquo;s look at a simple example. We can create a vector and sort it:\nx \u0026lt;- c(31, 4, 15, 92, 65) sort(x)  ## [1] 4 15 31 65 92  Rather than sort the input vector, the function order returns the index that sorts input vector:\nindex \u0026lt;- order(x) x[index]  ## [1] 4 15 31 65 92  This is the same output as that returned by sort(x). If we look at this index, we see why it works:\nx  ## [1] 31 4 15 92 65  order(x)  ## [1] 2 3 1 5 4  The second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\nmurders$state[1:6]  ## [1] \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; \u0026quot;California\u0026quot; ## [6] \u0026quot;Colorado\u0026quot;  murders$abb[1:6]  ## [1] \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;CO\u0026quot;  This means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\nind \u0026lt;- order(murders$total) murders$abb[ind]  ## [1] \u0026quot;VT\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;WV\u0026quot; \u0026quot;NE\u0026quot; ## [16] \u0026quot;OR\u0026quot; \u0026quot;DE\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;WA\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;DC\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;MA\u0026quot; ## [31] \u0026quot;MS\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;IL\u0026quot; \u0026quot;GA\u0026quot; ## [46] \u0026quot;MI\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;CA\u0026quot;  According to the above, California had the most murders.\nmax and which.max If we are only interested in the entry with the largest value, we can use max for the value:\nmax(murders$total)  ## [1] 1257  and which.max for the index of the largest value:\ni_max \u0026lt;- which.max(murders$total) murders$state[i_max]  ## [1] \u0026quot;California\u0026quot;  For the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\nrank Although not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\nx \u0026lt;- c(31, 4, 15, 92, 65) rank(x)  ## [1] 3 1 2 5 4  To summarize, let\u0026rsquo;s look at the results of the three functions we have introduced:\n  original  sort  order  rank      31  4  2  3    4  15  3  1    15  31  1  2    92  65  5  5    65  92  4  4     Beware of recycling Another common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don\u0026rsquo;t match in length, it is natural to assume that we should get an error. But we don\u0026rsquo;t. Notice what happens:\nx \u0026lt;- c(1,2,3) y \u0026lt;- c(10, 20, 30, 40, 50, 60, 70) x+y  ## Warning in x + y: longer object length is not a multiple of shorter object ## length  ## [1] 11 22 33 41 52 63 71  We do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\nExercises For these exercises we will use the US murders dataset. Make sure you load it prior to starting.\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;)  1. Use the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\n2. Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\n3. We can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\n4. Now we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\n5. You can create a data frame using the data.frame function. Here is a quick example:\ntemp \u0026lt;- c(35, 88, 42, 84, 81, 30) city \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;, \u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;) city_temps \u0026lt;- data.frame(name = city, temperature = temp)  Use the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\n6. Repeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\n7. The na_example vector represents a series of counts. You can quickly examine the object using:\ndata(\u0026quot;na_example\u0026quot;) str(na_example)  ## int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...  However, when we compute the average with the function mean, we obtain an NA:\nmean(na_example)  ## [1] NA  The is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have.\n8. Now compute the average again, but only for the entries that are not NA. Hint: remember the ! operator.\nVector arithmetics California had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;) murders$state[which.max(murders$population)]  ## [1] \u0026quot;California\u0026quot;  with over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\nRescaling a vector In R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\ninches \u0026lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)  and want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\ninches * 2.54  ## [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80  In the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\ninches - 69  ## [1] 0 -7 -3 1 1 4 -2 4 -2 1  Two vectors If we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n$$ \\begin{pmatrix} a\\\nb\\\nc\\\nd \\end{pmatrix} + \\begin{pmatrix} e\\\nf\\\ng\\\nh \\end{pmatrix} \\begin{pmatrix} a +e\\\nb + f\\\nc + g\\\nd + h \\end{pmatrix} $$\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000  Once we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\nmurders$abb[order(murder_rate)]  ## [1] \u0026quot;VT\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;OR\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;WA\u0026quot; ## [16] \u0026quot;WV\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;NE\u0026quot; \u0026quot;MA\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IL\u0026quot; ## [31] \u0026quot;OK\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;GA\u0026quot; \u0026quot;MS\u0026quot; \u0026quot;MI\u0026quot; ## [46] \u0026quot;DE\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;DC\u0026quot;  Exercises 1. Previously we created this data frame:\ntemp \u0026lt;- c(35, 88, 42, 84, 81, 30) city \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;, \u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;) city_temps \u0026lt;- data.frame(name = city, temperature = temp)  Remake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is $C = \\frac{5}{9} \\times (F - 32)$.\n2. What is the following sum $1+1/2^2 + 1/3^2 + \\dots 1/100^2$? Hint: thanks to Euler, we know it should be close to $\\pi^2/6$.\n3. Compute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?\nIndexing R provides a powerful and convenient way of indexing vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;)  Subsetting with logicals We have now calculated the murder rate using:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000  Imagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\nind \u0026lt;- murder_rate \u0026lt; 0.71  If we instead want to know if a value is less or equal, we can use:\nind \u0026lt;- murder_rate \u0026lt;= 0.71  Note that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\nmurders$state[ind]  ## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Iowa\u0026quot; \u0026quot;New Hampshire\u0026quot; \u0026quot;North Dakota\u0026quot; ## [5] \u0026quot;Vermont\u0026quot;  In order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\nsum(ind)  ## [1] 5  Logical operators Suppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with \u0026amp;. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\nTRUE \u0026amp; TRUE  ## [1] TRUE  TRUE \u0026amp; FALSE  ## [1] FALSE  FALSE \u0026amp; FALSE  ## [1] FALSE  For our example, we can form two logicals:\nwest \u0026lt;- murders$region == \u0026quot;West\u0026quot; safe \u0026lt;- murder_rate \u0026lt;= 1  and we can use the \u0026amp; to get a vector of logicals that tells us which states satisfy both conditions:\nind \u0026lt;- safe \u0026amp; west murders$state[ind]  ## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Idaho\u0026quot; \u0026quot;Oregon\u0026quot; \u0026quot;Utah\u0026quot; \u0026quot;Wyoming\u0026quot;  which Suppose we want to look up California\u0026rsquo;s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\nind \u0026lt;- which(murders$state == \u0026quot;California\u0026quot;) murder_rate[ind]  ## [1] 3.374138  match If instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function match. This function tells us which indexes of a second vector match each of the entries of a first vector:\nind \u0026lt;- match(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state) ind  ## [1] 33 10 44  Now we can look at the murder rates:\nmurder_rate[ind]  ## [1] 2.667960 3.398069 3.201360  %in% If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let\u0026rsquo;s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\nc(\u0026quot;Boston\u0026quot;, \u0026quot;Dakota\u0026quot;, \u0026quot;Washington\u0026quot;) %in% murders$state  ## [1] FALSE FALSE TRUE  Note that we will be using %in% often throughout the book.\nAdvanced: There is a connection between match and %in% through which. To see this, notice that the following two lines produce the same index (although in different order):\nmatch(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state)  ## [1] 33 10 44  which(murders$state%in%c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;))  ## [1] 10 33 44  Exercises Start by loading the library and data.\nlibrary(dslabs) data(murders)  1. Compute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\n2. Now use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\n3. Use the results from the previous exercise to report the names of the states with murder rates lower than 1.\n4. Now extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator \u0026amp;.\n5. In a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\n6. Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\n7. Use the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\n8. Extend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index.\nBasic plots In Chapter @ref(ggplot2) we describe an add-on package that provides a powerful approach to producing plots in R. We then have an entire part on Data Visualization in which we provide many examples. Here we briefly describe some of the functions that are available in a basic R installation.\nplot The plot function can be used to make scatterplots. Here is a plot of total murders versus population.\nx \u0026lt;- murders$population / 10^6 y \u0026lt;- murders$total plot(x, y)  For a quick plot that avoids accessing variables twice, we can use the with function:\nwith(murders, plot(population, total))  The function with lets us use the murders column names in the plot function. It also works with any data frames and any function.\nhist We will describe histograms as they relate to distributions in the Data Visualization part of the book. Here we will simply note that histograms are a powerful graphical summary of a list of numbers that gives you a general overview of the types of values you have. We can make a histogram of our murder rates by simply typing:\nx \u0026lt;- with(murders, total / population * 100000) hist(x)  We can see that there is a wide range of values with most of them between 2 and 3 and one very extreme case with a murder rate of more than 15:\nmurders$state[which.max(x)]  ## [1] \u0026quot;District of Columbia\u0026quot;  boxplot Boxplots will also be described in the Data Visualization part of the book. They provide a more terse summary than histograms, but they are easier to stack with other boxplots. For example, here we can use them to compare the different regions:\nmurders$rate \u0026lt;- with(murders, total / population * 100000) boxplot(rate~region, data = murders)  We can see that the South has higher murder rates than the other three regions.\nimage The image function displays the values in a matrix using color. Here is a quick example:\nx \u0026lt;- matrix(1:120, 12, 10) image(x)  Exercises 1. We made a plot of total murders versus population and noted a strong relationship. Not surprisingly, states with larger populations had more murders.\nlibrary(dslabs) data(murders) population_in_millions \u0026lt;- murders$population/10^6 total_gun_murders \u0026lt;- murders$total plot(population_in_millions, total_gun_murders)  Keep in mind that many states have populations below 5 million and are bunched up. We may gain further insights from making this plot in the log scale. Transform the variables using the log10 transformation and then plot them.\n2. Create a histogram of the state populations.\n3. Generate boxplots of the state populations by region.\nVideos Videos for each section of the lecture are available at this YouTube playlist.\n  Introduction  ","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"271956a1cff1432ecb7a806df208530e","permalink":"/content/00-content.knit/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/content/00-content.knit/","section":"content","summary":"Readings As noted in the syllabus, your readings will be assigned each week in this area.\n The syllabus, content, examples, and labs pages for this class  Questions to reflect on  Do I remember anything about R?  Slides There are no slides for this lesson; the content below is intended to serve as a refresher for those who may have forgotten basics of R.\n:::fyi ALERT: The course content below should be considered a prerequisite for success.","tags":null,"title":"Welcome to `R`","type":"docs"},{"authors":null,"categories":null,"content":"Readings As noted in the syllabus, your readings will be assigned each week in this area.\n The syllabus, content, examples, and labs pages for this class  Questions to reflect on  Do I remember anything about R?  Slides There are no slides for this lesson; the content below is intended to serve as a refresher for those who may have forgotten basics of R.\n:::fyi ALERT: The course content below should be considered a prerequisite for success. If you struggle to follow the content, please contact the professor or TA. :::\nR basics In this book, we will be using the R software environment for all our analysis. You will learn R and data analysis techniques simultaneously. To follow along you will therefore need access to R. We also recommend the use of an integrated development environment (IDE), such as RStudio, to save your work. Note that it is common for a course or workshop to offer access to an R environment and an IDE through your web browser, as done by RStudio cloud^[https://rstudio.cloud]. If you have access to such a resource, you don\u0026rsquo;t need to install R and RStudio. However, if you intend on becoming an advanced data analyst, we highly recommend installing these tools on your computer^[https://rafalab.github.io/dsbook/installing-r-rstudio.html]. Both R and RStudio are free and available online.\nCase study: US Gun Murders Imagine you live in Europe and are offered a job in a US company with many locations across all states. It is a great job, but news with headlines such as US Gun Homicide Rate Higher Than Other Developed Countries^[http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/] have you worried. Charts like this may concern you even more:\nOr even worse, this version from everytown.org: But then you remember that the US is a large and diverse country with 50 very different states as well as the District of Columbia (DC).\nCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial. You have options of where to live and want to determine the safety of each particular state. We will gain some insights by examining data related to gun homicides in the US during 2010 using R.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Be aware that the usefulness of some of these building blocks may not be immediately obvious, but later in the book you will appreciate having mastered these skills.\nThe very basics Before we get started with the motivating dataset, we need to cover the very basics of R.\nObjects Suppose a high school student asks us for help solving several quadratic equations of the form $ax^2+bx+c = 0$. The quadratic formula gives us the solutions:\n$$ \\frac{-b - \\sqrt{b^2 - 4ac}}{2a},, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} $$ which of course change depending on the values of $a$, $b$, and $c$. One advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve $x^2 + x -1 = 0$, then we define:\na \u0026lt;- 1 b \u0026lt;- 1 c \u0026lt;- -1  which stores the values for later use. We use \u0026lt;- to assign values to the variables.\nWe can also assign values using = instead of \u0026lt;-, but we recommend against using = to avoid confusion.\nCopy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message.\nTo see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\na  ## [1] 1  A more explicit way to ask R to show us the value stored in a is using print like this:\nprint(a)  ## [1] 1  We use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\nThe workspace As we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\nls()  ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;dat\u0026quot; \u0026quot;img_path\u0026quot; \u0026quot;murders\u0026quot;  In RStudio, the Environment tab shows the values:\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )  ## [1] 0.618034  (-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )  ## [1] -1.618034  Functions Once you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several predefined functions and most of the analysis pipelines we construct make extensive use of these.\nWe already used the install.packages, library, and ls functions. We also used the function sqrt to solve the quadratic equation above. There are many more prebuilt functions and even more can be added through packages. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\nlog(8)  ## [1] 2.079442  log(a)  ## [1] 0  You can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\nhelp(\u0026quot;log\u0026quot;)  For most functions, we can also use this shorthand:\n?log  The help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional. For example, the base of the function log defaults to base = exp(1) making log the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\nargs(log)  ## function (x, base = exp(1)) ## NULL  You can change the default values by simply assigning another object:\nlog(8, base = 2)  ## [1] 3  Note that we have not been specifying the argument x as such:\nlog(x = 8, base = 2)  ## [1] 3  The above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\nlog(8,2)  ## [1] 3  If using the arguments\u0026rsquo; names, then we can include them in whatever order we want:\nlog(base = 2, x = 8)  ## [1] 3  To specify arguments, we must use =, and cannot use \u0026lt;-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n2 ^ 3  ## [1] 8  You can see the arithmetic operators by typing:\nhelp(\u0026quot;+\u0026quot;)  or\n?\u0026quot;+\u0026quot;  and the relational operators by typing:\nhelp(\u0026quot;\u0026gt;\u0026quot;)  or\n?\u0026quot;\u0026gt;\u0026quot;  Other prebuilt objects There are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\ndata()  This shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\nco2  R will show you Mauna Loa atmospheric CO2 concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant $\\pi$ and $\\infty$:\npi  ## [1] 3.141593  Inf+1  ## [1] Inf  Variable names We have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can\u0026rsquo;t contain spaces, and should not be variables that are predefined in R. For example, don\u0026rsquo;t name one of your variables install.packages by typing something like install.packages \u0026lt;- 2.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\nsolution_1 \u0026lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a) solution_2 \u0026lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a)  For more advice, we highly recommend studying Hadley Wickham\u0026rsquo;s style guide^[http://adv-r.had.co.nz/Style.html].\nSaving your workspace Values remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image, and load to learn more.\nMotivating scripts To solve another equation such as $3x^2 + 2x -1$, we can copy and paste the code above and then redefine the variables and recompute the solution:\na \u0026lt;- 3 b \u0026lt;- 2 c \u0026lt;- -1 (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a)  By creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\nCommenting your code If a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n## Code to compute solution to quadratic equation of the form ax^2 + bx + c ## define the variables a \u0026lt;- 3 b \u0026lt;- 2 c \u0026lt;- -1 ## now compute the solution (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a)  Exercises 1. What is the sum of the first 100 positive integers? The formula for the sum of integers $1$ through $n$ is $n(n+1)/2$. Define $n=100$ and then use R to compute the sum of $1$ through $100$ using the formula. What is the sum?\n2. Now use the same formula to compute the sum of the integers from 1 through 1,000.\n3. Look at the result of typing the following code into R:\nn \u0026lt;- 1000 x \u0026lt;- seq(1, n) sum(x)  Based on the result, what do you think the functions seq and sum do? You can use help.\na. sum creates a list of numbers and seq adds them up. b. seq creates a list of numbers and sum adds them up. c. seq creates a random list and sum computes the sum of 1 through 1,000. d. sum always returns the same number.\n4. In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\n5. Which of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\na. log(10^x) b. log10(x^10) c. log(exp(x)) d. exp(log(x, base = 2))\nData types Variables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\na \u0026lt;- 2 class(a)  ## [1] \u0026quot;numeric\u0026quot;  To work efficiently in R, it is important to learn the different types of variables and what we can do with these.\nData frames Up to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\nlibrary(dslabs) data(murders)  To see that this is in fact a data frame, we type:\nclass(murders)  ## [1] \u0026quot;data.frame\u0026quot;  Examining an object The function str is useful for finding out more about the structure of an object:\nstr(murders)  ## 'data.frame':\t51 obs. of 5 variables: ## $ state : chr \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; ... ## $ abb : chr \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; ... ## $ region : Factor w/ 4 levels \u0026quot;Northeast\u0026quot;,\u0026quot;South\u0026quot;,..: 2 4 4 2 4 4 1 2 2 2 ... ## $ population: num 4779736 710231 6392017 2915918 37253956 ... ## $ total : num 135 19 232 93 1257 ...  This tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\nhead(murders)  ## state abb region population total ## 1 Alabama AL South 4779736 135 ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232 ## 4 Arkansas AR South 2915918 93 ## 5 California CA West 37253956 1257 ## 6 Colorado CO West 5029196 65  In this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let\u0026rsquo;s learn more about the components of this object.\nThe accessor: $ For our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\nmurders$population  ## [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 897934 ## [9] 601723 19687653 9920000 1360301 1567582 12830632 6483802 3046355 ## [17] 2853118 4339367 4533372 1328361 5773552 6547629 9883640 5303925 ## [25] 2967297 5988927 989415 1826341 2700551 1316470 8791894 2059179 ## [33] 19378102 9535483 672591 11536504 3751351 3831074 12702379 1052567 ## [41] 4625364 814180 6346105 25145561 2763885 625741 8001024 6724540 ## [49] 1852994 5686986 563626  But how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\nnames(murders)  ## [1] \u0026quot;state\u0026quot; \u0026quot;abb\u0026quot; \u0026quot;region\u0026quot; \u0026quot;population\u0026quot; \u0026quot;total\u0026quot;  It is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\nTip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\nVectors: numerics, characters, and logical The object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\npop \u0026lt;- murders$population length(pop)  ## [1] 51  This particular vector is numeric since population sizes are numbers:\nclass(pop)  ## [1] \u0026quot;numeric\u0026quot;  In a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\nclass(murders$state)  ## [1] \u0026quot;character\u0026quot;  As with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\nz \u0026lt;- 3 == 2 z  ## [1] FALSE  class(z)  ## [1] \u0026quot;logical\u0026quot;  Here the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality.\nYou can see the other relational operators by typing:\n?Comparison  In future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\nAdvanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\nFactors In the murders dataset, we might expect the region to also be a character vector. However, it is not:\nclass(murders$region)  ## [1] \u0026quot;factor\u0026quot;  It is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\nlevels(murders$region)  ## [1] \u0026quot;Northeast\u0026quot; \u0026quot;South\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot;  In the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\nregion \u0026lt;- murders$region value \u0026lt;- murders$total region \u0026lt;- reorder(region, value, FUN = sum) levels(region)  ## [1] \u0026quot;Northeast\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot; \u0026quot;South\u0026quot;  The new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\nWarning: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\nLists Data frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:\nrecord  ## $name ## [1] \u0026quot;John Doe\u0026quot; ## ## $student_id ## [1] 1234 ## ## $grades ## [1] 95 82 91 97 93 ## ## $final_grade ## [1] \u0026quot;A\u0026quot;  class(record)  ## [1] \u0026quot;list\u0026quot;  As with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list.\nrecord$student_id  ## [1] 1234  We can also use double square brackets ([[) like this:\nrecord[[\u0026quot;student_id\u0026quot;]]  ## [1] 1234  You should get used to the fact that in R, there are often several ways to do the same thing, such as accessing entries.\nYou might also encounter lists without variable names.\nrecord2  ## [[1]] ## [1] \u0026quot;John Doe\u0026quot; ## ## [[2]] ## [1] 1234  If a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\nrecord2[[1]]  ## [1] \u0026quot;John Doe\u0026quot;  We won\u0026rsquo;t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\nMatrices Matrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this book, but much of what happens in the background when you perform a data analysis involves matrices. We cover matrices in more detail in Chapter @ref(matrix-algebra) but describe them briefly here since some of the functions we will learn return matrices.\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\nmat \u0026lt;- matrix(1:12, 4, 3) mat  ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12  You can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\nmat[2, 3]  ## [1] 10  If you want the entire second row, you leave the column spot empty:\nmat[2, ]  ## [1] 2 6 10  Notice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\nmat[, 3]  ## [1] 9 10 11 12  This is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\nmat[, 2:3]  ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10 ## [3,] 7 11 ## [4,] 8 12  You can subset both rows and columns:\nmat[1:2, 2:3]  ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10  We can convert matrices into data frames using the function as.data.frame:\nas.data.frame(mat)  ## V1 V2 V3 ## 1 1 5 9 ## 2 2 6 10 ## 3 3 7 11 ## 4 4 8 12  You can also use single square brackets ([) to access rows and columns of a data frame:\ndata(\u0026quot;murders\u0026quot;) murders[25, 1]  ## [1] \u0026quot;Mississippi\u0026quot;  murders[2:3, ]  ## state abb region population total ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232  Exercises 1. Load the US murders dataset.\nlibrary(dslabs) data(murders)  Use the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\na. The 51 states. b. The murder rates for all 50 states and DC. c. The state name, the abbreviation of the state name, the state\u0026rsquo;s region, and the state\u0026rsquo;s population and total number of murders for 2010. d. str shows no relevant information.\n2. What are the column names used by the data frame for these five variables?\n3. Use the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\n4. Now use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\n5. We saw that the region column stores a factor. You can corroborate this by typing:\nclass(murders$region)  With one line of code, use the function levels and length to determine the number of regions defined by this dataset.\n6. The function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.\nVectors In R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\nCreating vectors We can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\ncodes \u0026lt;- c(380, 124, 818) codes  ## [1] 380 124 818  We can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\ncountry \u0026lt;- c(\u0026quot;italy\u0026quot;, \u0026quot;canada\u0026quot;, \u0026quot;egypt\u0026quot;)  In R you can also use single quotes:\ncountry \u0026lt;- c('italy', 'canada', 'egypt')  But be careful not to confuse the single quote ' with the back quote `.\nBy now you should know that if you type:\ncountry \u0026lt;- c(italy, canada, egypt)  you receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\nNames Sometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\ncodes \u0026lt;- c(italy = 380, canada = 124, egypt = 818) codes  ## italy canada egypt ## 380 124 818  The object codes continues to be a numeric vector:\nclass(codes)  ## [1] \u0026quot;numeric\u0026quot;  but with names:\nnames(codes)  ## [1] \u0026quot;italy\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;egypt\u0026quot;  If the use of strings without quotes looks confusing, know that you can use the quotes as well:\ncodes \u0026lt;- c(\u0026quot;italy\u0026quot; = 380, \u0026quot;canada\u0026quot; = 124, \u0026quot;egypt\u0026quot; = 818) codes  ## italy canada egypt ## 380 124 818  There is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\nWe can also assign names using the names functions:\ncodes \u0026lt;- c(380, 124, 818) country \u0026lt;- c(\u0026quot;italy\u0026quot;,\u0026quot;canada\u0026quot;,\u0026quot;egypt\u0026quot;) names(codes) \u0026lt;- country codes  ## italy canada egypt ## 380 124 818  Sequences Another useful function for creating vectors generates sequences:\nseq(1, 10)  ## [1] 1 2 3 4 5 6 7 8 9 10  The first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\nseq(1, 10, 2)  ## [1] 1 3 5 7 9  If we want consecutive integers, we can use the following shorthand:\n1:10  ## [1] 1 2 3 4 5 6 7 8 9 10  When we use these functions, R produces integers, not numerics, because they are typically used to index something:\nclass(1:10)  ## [1] \u0026quot;integer\u0026quot;  However, if we create a sequence including non-integers, the class changes:\nclass(seq(1, 10, 0.5))  ## [1] \u0026quot;numeric\u0026quot;  Subsetting We use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\ncodes[2]  ## canada ## 124  You can get more than one entry by using a multi-entry vector as an index:\ncodes[c(1,3)]  ## italy egypt ## 380 818  The sequences defined above are particularly useful if we want to access, say, the first two elements:\ncodes[1:2]  ## italy canada ## 380 124  If the elements have names, we can also access the entries using these names. Below are two examples.\ncodes[\u0026quot;canada\u0026quot;]  ## canada ## 124  codes[c(\u0026quot;egypt\u0026quot;,\u0026quot;italy\u0026quot;)]  ## egypt italy ## 818 380  Coercion In general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let\u0026rsquo;s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\nx \u0026lt;- c(1, \u0026quot;canada\u0026quot;, 3)  But we don\u0026rsquo;t get one, not even a warning! What happened? Look at x and its class:\nx  ## [1] \u0026quot;1\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;3\u0026quot;  class(x)  ## [1] \u0026quot;character\u0026quot;  R coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \u0026quot;1\u0026quot; and \u0026ldquo;3\u0026rdquo;. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\nx \u0026lt;- 1:5 y \u0026lt;- as.character(x) y  ## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5\u0026quot;  You can turn it back with as.numeric:\nas.numeric(y)  ## [1] 1 2 3 4 5  This function is actually quite useful since datasets that include numbers as character strings are common.\nNot availables (NA) When a function tries to coerce one type to another and encounters an impossible case, it usually gives us a warning and turns the entry into a special value called an NA for \u0026ldquo;not available\u0026rdquo;. For example:\nx \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;3\u0026quot;) as.numeric(x)  ## Warning: NAs introduced by coercion  ## [1] 1 NA 3  R does not have any guesses for what number you want when you type b, so it does not try.\nAs a data scientist you will encounter the NAs often as they are generally used for missing data, a common problem in real-world datasets.\nExercises 1. Use the function c to create a vector with the average high temperatures in January for Beijing, Lagos, Paris, Rio de Janeiro, San Juan, and Toronto, which are 35, 88, 42, 84, 81, and 30 degrees Fahrenheit. Call the object temp.\n2. Now create a vector with the city names and call the object city.\n3. Use the names function and the objects defined in the previous exercises to associate the temperature data with its corresponding city.\n4. Use the [ and : operators to access the temperature of the first three cities on the list.\n5. Use the [ operator to access the temperature of Paris and San Juan.\n6. Use the : operator to create a sequence of numbers $12,13,14,\\dots,73$.\n7. Create a vector containing all the positive odd numbers smaller than 100.\n8. Create a vector of numbers that starts at 6, does not pass 55, and adds numbers in increments of 4/7: 6, 6 + 4/7, 6 + 8/7, and so on. How many numbers does the list have? Hint: use seq and length.\n9. What is the class of the following object a \u0026lt;- seq(1, 10, 0.5)?\n10. What is the class of the following object a \u0026lt;- seq(1, 10)?\n11. The class of class(a\u0026lt;-1) is numeric, not integer. R defaults to numeric and to force an integer, you need to add the letter L. Confirm that the class of 1L is integer.\n12. Define the following vector:\nx \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;5\u0026quot;)  and coerce it to get integers.\nSorting Now that we have mastered some basic R knowledge, let\u0026rsquo;s try to gain some insights into the safety of different states in the context of gun murders.\nsort Say we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\nlibrary(dslabs) data(murders) sort(murders$total)  ## [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32 ## [16] 36 38 53 63 65 67 84 93 93 97 97 99 111 116 118 ## [31] 120 135 142 207 219 232 246 250 286 293 310 321 351 364 376 ## [46] 413 457 517 669 805 1257  However, this does not give us information about which states have which murder totals. For example, we don\u0026rsquo;t know which state had 1257.\norder The function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let\u0026rsquo;s look at a simple example. We can create a vector and sort it:\nx \u0026lt;- c(31, 4, 15, 92, 65) sort(x)  ## [1] 4 15 31 65 92  Rather than sort the input vector, the function order returns the index that sorts input vector:\nindex \u0026lt;- order(x) x[index]  ## [1] 4 15 31 65 92  This is the same output as that returned by sort(x). If we look at this index, we see why it works:\nx  ## [1] 31 4 15 92 65  order(x)  ## [1] 2 3 1 5 4  The second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\nmurders$state[1:6]  ## [1] \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; \u0026quot;California\u0026quot; ## [6] \u0026quot;Colorado\u0026quot;  murders$abb[1:6]  ## [1] \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;CO\u0026quot;  This means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\nind \u0026lt;- order(murders$total) murders$abb[ind]  ## [1] \u0026quot;VT\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;WV\u0026quot; \u0026quot;NE\u0026quot; ## [16] \u0026quot;OR\u0026quot; \u0026quot;DE\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;WA\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;DC\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;MA\u0026quot; ## [31] \u0026quot;MS\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;IL\u0026quot; \u0026quot;GA\u0026quot; ## [46] \u0026quot;MI\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;CA\u0026quot;  According to the above, California had the most murders.\nmax and which.max If we are only interested in the entry with the largest value, we can use max for the value:\nmax(murders$total)  ## [1] 1257  and which.max for the index of the largest value:\ni_max \u0026lt;- which.max(murders$total) murders$state[i_max]  ## [1] \u0026quot;California\u0026quot;  For the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\nrank Although not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\nx \u0026lt;- c(31, 4, 15, 92, 65) rank(x)  ## [1] 3 1 2 5 4  To summarize, let\u0026rsquo;s look at the results of the three functions we have introduced:\n  original  sort  order  rank      31  4  2  3    4  15  3  1    15  31  1  2    92  65  5  5    65  92  4  4     Beware of recycling Another common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don\u0026rsquo;t match in length, it is natural to assume that we should get an error. But we don\u0026rsquo;t. Notice what happens:\nx \u0026lt;- c(1,2,3) y \u0026lt;- c(10, 20, 30, 40, 50, 60, 70) x+y  ## Warning in x + y: longer object length is not a multiple of shorter object ## length  ## [1] 11 22 33 41 52 63 71  We do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\nExercises For these exercises we will use the US murders dataset. Make sure you load it prior to starting.\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;)  1. Use the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\n2. Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\n3. We can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\n4. Now we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\n5. You can create a data frame using the data.frame function. Here is a quick example:\ntemp \u0026lt;- c(35, 88, 42, 84, 81, 30) city \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;, \u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;) city_temps \u0026lt;- data.frame(name = city, temperature = temp)  Use the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\n6. Repeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\n7. The na_example vector represents a series of counts. You can quickly examine the object using:\ndata(\u0026quot;na_example\u0026quot;) str(na_example)  ## int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...  However, when we compute the average with the function mean, we obtain an NA:\nmean(na_example)  ## [1] NA  The is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have.\n8. Now compute the average again, but only for the entries that are not NA. Hint: remember the ! operator.\nVector arithmetics California had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;) murders$state[which.max(murders$population)]  ## [1] \u0026quot;California\u0026quot;  with over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\nRescaling a vector In R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\ninches \u0026lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)  and want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\ninches * 2.54  ## [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80  In the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\ninches - 69  ## [1] 0 -7 -3 1 1 4 -2 4 -2 1  Two vectors If we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n$$ \\begin{pmatrix} a\\\nb\\\nc\\\nd \\end{pmatrix} + \\begin{pmatrix} e\\\nf\\\ng\\\nh \\end{pmatrix} \\begin{pmatrix} a +e\\\nb + f\\\nc + g\\\nd + h \\end{pmatrix} $$\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000  Once we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\nmurders$abb[order(murder_rate)]  ## [1] \u0026quot;VT\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;OR\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;WA\u0026quot; ## [16] \u0026quot;WV\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;NE\u0026quot; \u0026quot;MA\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IL\u0026quot; ## [31] \u0026quot;OK\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;GA\u0026quot; \u0026quot;MS\u0026quot; \u0026quot;MI\u0026quot; ## [46] \u0026quot;DE\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;DC\u0026quot;  Exercises 1. Previously we created this data frame:\ntemp \u0026lt;- c(35, 88, 42, 84, 81, 30) city \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;, \u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;) city_temps \u0026lt;- data.frame(name = city, temperature = temp)  Remake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is $C = \\frac{5}{9} \\times (F - 32)$.\n2. What is the following sum $1+1/2^2 + 1/3^2 + \\dots 1/100^2$? Hint: thanks to Euler, we know it should be close to $\\pi^2/6$.\n3. Compute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?\nIndexing R provides a powerful and convenient way of indexing vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;)  Subsetting with logicals We have now calculated the murder rate using:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000  Imagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\nind \u0026lt;- murder_rate \u0026lt; 0.71  If we instead want to know if a value is less or equal, we can use:\nind \u0026lt;- murder_rate \u0026lt;= 0.71  Note that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\nmurders$state[ind]  ## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Iowa\u0026quot; \u0026quot;New Hampshire\u0026quot; \u0026quot;North Dakota\u0026quot; ## [5] \u0026quot;Vermont\u0026quot;  In order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\nsum(ind)  ## [1] 5  Logical operators Suppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with \u0026amp;. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\nTRUE \u0026amp; TRUE  ## [1] TRUE  TRUE \u0026amp; FALSE  ## [1] FALSE  FALSE \u0026amp; FALSE  ## [1] FALSE  For our example, we can form two logicals:\nwest \u0026lt;- murders$region == \u0026quot;West\u0026quot; safe \u0026lt;- murder_rate \u0026lt;= 1  and we can use the \u0026amp; to get a vector of logicals that tells us which states satisfy both conditions:\nind \u0026lt;- safe \u0026amp; west murders$state[ind]  ## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Idaho\u0026quot; \u0026quot;Oregon\u0026quot; \u0026quot;Utah\u0026quot; \u0026quot;Wyoming\u0026quot;  which Suppose we want to look up California\u0026rsquo;s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\nind \u0026lt;- which(murders$state == \u0026quot;California\u0026quot;) murder_rate[ind]  ## [1] 3.374138  match If instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function match. This function tells us which indexes of a second vector match each of the entries of a first vector:\nind \u0026lt;- match(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state) ind  ## [1] 33 10 44  Now we can look at the murder rates:\nmurder_rate[ind]  ## [1] 2.667960 3.398069 3.201360  %in% If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let\u0026rsquo;s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\nc(\u0026quot;Boston\u0026quot;, \u0026quot;Dakota\u0026quot;, \u0026quot;Washington\u0026quot;) %in% murders$state  ## [1] FALSE FALSE TRUE  Note that we will be using %in% often throughout the book.\nAdvanced: There is a connection between match and %in% through which. To see this, notice that the following two lines produce the same index (although in different order):\nmatch(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state)  ## [1] 33 10 44  which(murders$state%in%c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;))  ## [1] 10 33 44  Exercises Start by loading the library and data.\nlibrary(dslabs) data(murders)  1. Compute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\n2. Now use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\n3. Use the results from the previous exercise to report the names of the states with murder rates lower than 1.\n4. Now extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator \u0026amp;.\n5. In a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\n6. Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\n7. Use the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\n8. Extend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index.\nBasic plots In Chapter @ref(ggplot2) we describe an add-on package that provides a powerful approach to producing plots in R. We then have an entire part on Data Visualization in which we provide many examples. Here we briefly describe some of the functions that are available in a basic R installation.\nplot The plot function can be used to make scatterplots. Here is a plot of total murders versus population.\nx \u0026lt;- murders$population / 10^6 y \u0026lt;- murders$total plot(x, y)  For a quick plot that avoids accessing variables twice, we can use the with function:\nwith(murders, plot(population, total))  The function with lets us use the murders column names in the plot function. It also works with any data frames and any function.\nhist We will describe histograms as they relate to distributions in the Data Visualization part of the book. Here we will simply note that histograms are a powerful graphical summary of a list of numbers that gives you a general overview of the types of values you have. We can make a histogram of our murder rates by simply typing:\nx \u0026lt;- with(murders, total / population * 100000) hist(x)  We can see that there is a wide range of values with most of them between 2 and 3 and one very extreme case with a murder rate of more than 15:\nmurders$state[which.max(x)]  ## [1] \u0026quot;District of Columbia\u0026quot;  boxplot Boxplots will also be described in the Data Visualization part of the book. They provide a more terse summary than histograms, but they are easier to stack with other boxplots. For example, here we can use them to compare the different regions:\nmurders$rate \u0026lt;- with(murders, total / population * 100000) boxplot(rate~region, data = murders)  We can see that the South has higher murder rates than the other three regions.\nimage The image function displays the values in a matrix using color. Here is a quick example:\nx \u0026lt;- matrix(1:120, 12, 10) image(x)  Exercises 1. We made a plot of total murders versus population and noted a strong relationship. Not surprisingly, states with larger populations had more murders.\nlibrary(dslabs) data(murders) population_in_millions \u0026lt;- murders$population/10^6 total_gun_murders \u0026lt;- murders$total plot(population_in_millions, total_gun_murders)  Keep in mind that many states have populations below 5 million and are bunched up. We may gain further insights from making this plot in the log scale. Transform the variables using the log10 transformation and then plot them.\n2. Create a histogram of the state populations.\n3. Generate boxplots of the state populations by region.\nVideos Videos for each section of the lecture are available at this YouTube playlist.\n  Introduction  ","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"ba825ef6ae5c761d5849097eda8c00c3","permalink":"/content/00-content.utf8/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/content/00-content.utf8/","section":"content","summary":"Readings As noted in the syllabus, your readings will be assigned each week in this area.\n The syllabus, content, examples, and labs pages for this class  Questions to reflect on  Do I remember anything about R?  Slides There are no slides for this lesson; the content below is intended to serve as a refresher for those who may have forgotten basics of R.\n:::fyi ALERT: The course content below should be considered a prerequisite for success.","tags":null,"title":"Welcome to `R`","type":"docs"},{"authors":null,"categories":null,"content":"  Readings Questions to reflect on  Slides Videos   Readings As noted in the syllabus, your readings will be assigned each week in this area.\n The syllabus, content, examples, and labs pages for this class  Chapter 1 in Kieran Healy, Data Visualization1  Chapters 2 and 3 in Alberto Cairo, The Truthful Art2 (skim the introduction and chapter 1)  Study: Charts change hearts and minds better than words do  Questions to reflect on  How do we know what is true? (Whoa!) Are facts truth? Why do we visualize data? What makes a great visualization? How do you choose which kind of visualization to use?    Slides The slides for the week’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nTruth, beauty, and data  Facts, truth, and beauty  Data, truth, and beauty  Beautiful visualizations  Class details                   Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Facts, truth, and beauty Data, truth, and beauty Beautiful visualizations Class details  You can also watch the playlist (and skip around to different sections) here:\n    Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n   ","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592869554,"objectID":"fbbae951c935dd3d35f82710943d5efd","permalink":"/content/01-content/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/content/01-content/","section":"content","summary":"Readings Questions to reflect on  Slides Videos   Readings As noted in the syllabus, your readings will be assigned each week in this area.\n The syllabus, content, examples, and labs pages for this class  Chapter 1 in Kieran Healy, Data Visualization1  Chapters 2 and 3 in Alberto Cairo, The Truthful Art2 (skim the introduction and chapter 1)  Study: Charts change hearts and minds better than words do  Questions to reflect on  How do we know what is true?","tags":null,"title":"Truth, beauty, and data + R and tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 12 in Alberto Cairo, The Truthful Art1  Chapter 26 in Claus Wilke, Fundamentals of Data Visualization2  Martin Krzywinski and Alberto Cairo, “Storytelling”  Ben Wellington, “Making data mean more through storytelling”  Jonathan Schwabish, “Better Data Communication,” National Bureau of Economic Research  Why People Make Bad Charts (and What to Do When it Happens)  Questions to reflect on  Why are stories so powerful? How are stories related to truth? Is it ethical to emphasize certain aspects of the facts in data more than others? How do you decide which facts to use to convince audiences? When you’re telling a story about data, you’re inherently manipulating audience emotions. Is that okay?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nTruth, beauty, and data revisited  Telling stories with data  Curiosity             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Telling stories with data Curiosity  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n   ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"6328480b5032d9a31a9d7d7d2cb4a78f","permalink":"/content/15-content/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/content/15-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 12 in Alberto Cairo, The Truthful Art1  Chapter 26 in Claus Wilke, Fundamentals of Data Visualization2  Martin Krzywinski and Alberto Cairo, “Storytelling”  Ben Wellington, “Making data mean more through storytelling”  Jonathan Schwabish, “Better Data Communication,” National Bureau of Economic Research  Why People Make Bad Charts (and What to Do When it Happens)  Questions to reflect on  Why are stories so powerful?","tags":null,"title":"Truth, beauty, and data revisited","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos   Readings   Chapter 14, Chapter 15, Chapter 16, Chapter 22, and Chapter 23 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 8 in Kieran Healy, Data Visualization2 Browse through recent visualizations at The Pudding (like this one!), FiveThirtyEight, Vox, Christopher Ingraham’s articles at the Washington Post, WSJ Graphics, and the New York Times’s TheUpshot. Most (if not all) of these graphics were made in R (or something similar) and exported for enhancement in Illustrator or D3.   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nEnhancing graphics       Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Enhancing graphics  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n   ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"6c1d7292f14fb9f873bbf517670d292d","permalink":"/content/14-content/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/content/14-content/","section":"content","summary":"Readings Slides Videos   Readings   Chapter 14, Chapter 15, Chapter 16, Chapter 22, and Chapter 23 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 8 in Kieran Healy, Data Visualization2 Browse through recent visualizations at The Pudding (like this one!), FiveThirtyEight, Vox, Christopher Ingraham’s articles at the Washington Post, WSJ Graphics, and the New York Times’s TheUpshot. Most (if not all) of these graphics were made in R (or something similar) and exported for enhancement in Illustrator or D3.","tags":null,"title":"Enhancing graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Look through some of the chapters in Julia Silge and David Robinson, Tidy Text Mining1 (but definitely don’t read them all! You just need to get a taste of what modern text analysis looks like.) A bunch of (really) short things:   Evangeline Reynolds, “Federalist Papers”  Julia Silge, “She Giggles, He Gallops”  Abby Ohlheiser, “These are the words most associated with men and women, according to Facebook status updates”  Six Ideas for Displaying Qualitative Data  Word clouds considered harmful  Word clouds cause death… or something  When It’s Ok to Use Word Clouds  The Class of 2011  Every time Ford and Kavanaugh dodged a question, in one chart  Tweet by @s_soroka   Questions to reflect on  Why is qualitative data difficult to visualize? Why are word clouds so problematic? When is (not) okay to use them?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nText  Qualitative text-based data  Crash course in computational linguistics             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Qualitative text-based data Crash course in computational linguistics  You can also watch the playlist (and skip around to different sections) here:\n    Julia Silge and David Robinson, Text Mining with R (Sebastopol, California: O’Reilly Media, 2017), https://www.tidytextmining.com/.↩︎\n   ","date":1590624000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"5b7a67b9a0df14e8aac6e2750a796e6e","permalink":"/content/13-content/","publishdate":"2020-05-28T00:00:00Z","relpermalink":"/content/13-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Look through some of the chapters in Julia Silge and David Robinson, Tidy Text Mining1 (but definitely don’t read them all! You just need to get a taste of what modern text analysis looks like.) A bunch of (really) short things:   Evangeline Reynolds, “Federalist Papers”  Julia Silge, “She Giggles, He Gallops”  Abby Ohlheiser, “These are the words most associated with men and women, according to Facebook status updates”  Six Ideas for Displaying Qualitative Data  Word clouds considered harmful  Word clouds cause death… or something  When It’s Ok to Use Word Clouds  The Class of 2011  Every time Ford and Kavanaugh dodged a question, in one chart  Tweet by @s_soroka   Questions to reflect on  Why is qualitative data difficult to visualize?","tags":null,"title":"Text","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 10 in Alberto Cairo, The Truthful Art1  Chapter 7 in Kieran Healy, Data Visualization2  It looks like this is a lot of reading, but lots of these are short videos or tweets or interactive websites, so don’t worry!\n  Why all world maps are wrong  The True Size Of…  Map projections  Gall-Peters Projection  “When Maps Lie”  Animated Mercator distortion  “These Twisted Maps Prove That America Isn’t a Red Country”  “The next great fake news threat? Bot-designed maps”  “New World Map That Accurately Shows Earth in 2D Created by Scientists”  Questions to reflect on  How can you know if a map projection is truthful or misleading? What’s wrong (or not wrong) with using points on maps? Choropleths? Lines?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nSpace  Maps and truth  Putting data on maps  GIS in R with sf                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Maps and truth Putting data on maps GIS in R with sf  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n   ","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"9eee4da015dc05af8a4bee2b0e8794b9","permalink":"/content/12-content/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/content/12-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 10 in Alberto Cairo, The Truthful Art1  Chapter 7 in Kieran Healy, Data Visualization2  It looks like this is a lot of reading, but lots of these are short videos or tweets or interactive websites, so don’t worry!\n  Why all world maps are wrong  The True Size Of…  Map projections  Gall-Peters Projection  “When Maps Lie”  Animated Mercator distortion  “These Twisted Maps Prove That America Isn’t a Red Country”  “The next great fake news threat?","tags":null,"title":"Space","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 8 in Alberto Cairo, The Truthful Art1  The Nuclear Threat—The Shadow Peace, part 1  11 Ways to Visualize Changes Over Time – A Guide A bunch of (really) short blog posts:   What a Hundred Million Calls to 311 Reveal About New York (just look at the picture; you don’t need to read this unless you’re really curious about trends in 311 calls)  A century of ocean shipping animated  What is seasonal adjustment and why is it used?  The start-at-zero rule  Keeping one’s appetite after touring the sausage factory  How Common is Your Birthday? This Visualization Might Surprise You   Recommended   The Fallen of World War II  Visualizing Statistical Mix Effects and Simpson’s Paradox2  How To Fix a Toilet (And Other Things We Couldn’t Do Without Search)   Questions to reflect on  When is it okay (or not) to truncate the y-axis? It is remarkably easy to mislead people with many of these chart types. Why? How can you avoid the same mistakes? All these types of charts are good at communicating change over time, but some are more appropriate in different situations. When is it best to use these different types (e.g. line graphs vs. area graphs vs. horizon charts vs. heatmaps, etc.)?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nTime  Axis issues  Visualizing time  Starting, ending, and decomposing time                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Axis issues Visualizing time Starting, ending, and decomposing time  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Zan Armstrong and Martin Wattenberg, “Visualizing Statistical Mix Effects and Simpson’s Paradox,” in Proceedings of IEEE InfoVis 2014, 2014, https://research.google.com/pubs/pub42901.html.↩︎\n   ","date":1590451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"d9c34011e152a6099f13882fb144b965","permalink":"/content/11-content/","publishdate":"2020-05-26T00:00:00Z","relpermalink":"/content/11-content/","section":"content","summary":"Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 8 in Alberto Cairo, The Truthful Art1  The Nuclear Threat—The Shadow Peace, part 1  11 Ways to Visualize Changes Over Time – A Guide A bunch of (really) short blog posts:   What a Hundred Million Calls to 311 Reveal About New York (just look at the picture; you don’t need to read this unless you’re really curious about trends in 311 calls)  A century of ocean shipping animated  What is seasonal adjustment and why is it used?","tags":null,"title":"Time","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Explorable explanations Dashboards Questions to reflect on  Slides Videos   Readings Explorable explanations   Marcel Salathé and Nicky Case, “What Happens Next: COVID-19 Futures, Explained with Playable Situations”  Brett Victor, “Explorable Explanations”  Look at some of the explorable explorations here  Dragicevic, Jansen, Sarma, Kay, and Chevalier, “Explorable Multiverse Analyses”. Use Chrome, open Example 1, scroll to page 2, and click on some of the blue text to change the results of the paper within the paper itself. This is magical. R Markdown can’t quite get this interactive in real-time, but you can knit different versions of a document with slightly different parameters and options.   Dashboards   Look at some of these examples of Shiny apps  Stephanie Evergreen, “How a Dashboard Changes the Conversation”  Stephanie Evergreen, “The Problem with Dashboards (and a Solution)”  Stephen Few, “2012 Perceptual Edge Dashboard Design Competition: A Solution of My Own”  Skim through Stephen Few’s presentation on Information Dashboard Design  Google “dashboard design” and skim through some of the thousands of articles about what makes a good (and bad) dashboard   Questions to reflect on  How helpful (or unhelpful) are explorable explanations? Have you seen examples of good dashboards before this class? Bad dashboards? What makes them good or bad?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nInteractivity  Making interactive graphs  Sharing content             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Making interactive graphics Sharing content  You can also watch the playlist (and skip around to different sections) here:\n   ","date":1590105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"71d614e4debdb64f2a26fc68ad217ddd","permalink":"/content/10-content/","publishdate":"2020-05-22T00:00:00Z","relpermalink":"/content/10-content/","section":"content","summary":"Readings  Explorable explanations Dashboards Questions to reflect on  Slides Videos   Readings Explorable explanations   Marcel Salathé and Nicky Case, “What Happens Next: COVID-19 Futures, Explained with Playable Situations”  Brett Victor, “Explorable Explanations”  Look at some of the explorable explorations here  Dragicevic, Jansen, Sarma, Kay, and Chevalier, “Explorable Multiverse Analyses”. Use Chrome, open Example 1, scroll to page 2, and click on some of the blue text to change the results of the paper within the paper itself.","tags":null,"title":"Interactivity","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Slides Videos   Readings   Chapter 17, Chapter 18, Chapter 19, and Chapter 21 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 5 in Kieran Healy, Data Visualization2   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nAnnotations  Fretting the little things  Text in plots  Seeds                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Fretting the little things Text in plots Seeds  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n   ","date":1590019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"35bbbb8e03581e65c2b2e10271df2fce","permalink":"/content/09-content/","publishdate":"2020-05-21T00:00:00Z","relpermalink":"/content/09-content/","section":"content","summary":"Readings Slides Videos   Readings   Chapter 17, Chapter 18, Chapter 19, and Chapter 21 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 5 in Kieran Healy, Data Visualization2   Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later).","tags":null,"title":"Annotations","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Reproducible examples Questions to reflect on  Slides Videos   Readings   Chapter 9 in Claus Wilke, Fundamentals of Data Visualization1  Mike Bostock, “Methods of Comparison, Compared”. Explanation of the differences between showing relative differences, absolute differences, and log ratios.  Sparkline theory and practice. This is a collection of posts by Edward Tufte about sparklines—scroll down a ways and check out his examples, analysis, and critiques.  Time series sparklines  Comparisons with lollipop charts. If you’re feeling adventurous (and you should!), do this tutorial as you read it.  Reproducible examples Reprexes (or reproducible examples) are the best way to (1) get help online and (2) fix issues on your own.\nMaking a good reprex is tricky, but it’s a very valuable skill to know (regardless of programming language!). Here are some helpful resources for making them:\n  What’s a reproducible example (reprex) and how do I do one?  So you’ve been asked to make a reprex  The reprex package   Questions to reflect on  These readings all show a ton of new ways to present comparisons. Which ones are your favorite? Which ones didn’t quite click with you? In what situations are some more appropriate than others?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nComparisons  Visualizing comparisons  Reproducible examples             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Visualizing comparisons Reproducible examples  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n   ","date":1589932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"d3f7fc26f58a06eab9a68e8656b03674","permalink":"/content/08-content/","publishdate":"2020-05-20T00:00:00Z","relpermalink":"/content/08-content/","section":"content","summary":"Readings  Reproducible examples Questions to reflect on  Slides Videos   Readings   Chapter 9 in Claus Wilke, Fundamentals of Data Visualization1  Mike Bostock, “Methods of Comparison, Compared”. Explanation of the differences between showing relative differences, absolute differences, and log ratios.  Sparkline theory and practice. This is a collection of posts by Edward Tufte about sparklines—scroll down a ways and check out his examples, analysis, and critiques.","tags":null,"title":"Comparisons","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 9 in Alberto Cairo, The Truthful Art1  Chapter 12 in Claus Wilke, Fundamentals of Data Visualization2  Kieran Healy, “Two y-axes”  Two Alternatives to Using a Second Y-Axis \u0026amp; Illusion of success \u0026amp; Dissecting two axes  Dual-Scaled Axes in Graphs: Are They Ever the Best Solution?3  Recommended   “A Study on Dual-Scale Data Charts”4   Questions to reflect on  How can you correctly and honestly communicate relationships between variables? How can you communicate the uncertainty in those relationships? What are the dangers of visualizing two variables? When is it appropriate to use two y-axes?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nRelationships  The dangers of dual y-axes  Visualizing correlations  Visualizing regressions                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction The dangers of dual y-axes Visualizing correlations Visualizing regressions  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Stephen Few, “Dual-Scaled Axes in Graphs: Are They Ever the Best Solution?” Visual Business Intelligence Newsletter, March 2008, http://www.perceptualedge.com/articles/visual_business_intelligence/dual-scaled_axes.pdf.↩︎\n Petra Isenberg et al., “A Study on Dual-Scale Data Charts,” IEEE Transactions on Visualization and Computer Graphics 17, no. 12 (2011): 2469–78, doi:10.1109/tvcg.2011.160.↩︎\n   ","date":1589846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"79f283e309555def72f172af8fa16106","permalink":"/content/07-content/","publishdate":"2020-05-19T00:00:00Z","relpermalink":"/content/07-content/","section":"content","summary":"Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 9 in Alberto Cairo, The Truthful Art1  Chapter 12 in Claus Wilke, Fundamentals of Data Visualization2  Kieran Healy, “Two y-axes”  Two Alternatives to Using a Second Y-Axis \u0026amp; Illusion of success \u0026amp; Dissecting two axes  Dual-Scaled Axes in Graphs: Are They Ever the Best Solution?3  Recommended   “A Study on Dual-Scale Data Charts”4   Questions to reflect on  How can you correctly and honestly communicate relationships between variables?","tags":null,"title":"Relationships","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 7 in Claus Wilke, Fundamentals of Data Visualization1  Chapters 4, 7, and 11 in Alberto Cairo, The Truthful Art2  Why It’s So Hard for Us to Visualize Uncertainty  Amanda Cox’s keynote address at the 2017 OpenVis Conf  Communicating Uncertainty When Lives Are on the Line  Showing uncertainty during the live election forecast \u0026amp; Trolling the uncertainty dial  Questions to reflect on  Why is it important to deal with uncertainty in data? What was good or bad about the New York Times’ 2016 live election guage? Why is it so hard to visualize uncertainty? Why is it so hard to communicate uncertainty to others?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nUncertainty  Communicating uncertainty  Visualizing uncertainty             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Communicating uncertainty Visualizing uncertainty  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n   ","date":1589760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"4fb51266c58f677bf1e1454e5cd9c527","permalink":"/content/06-content/","publishdate":"2020-05-18T00:00:00Z","relpermalink":"/content/06-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 7 in Claus Wilke, Fundamentals of Data Visualization1  Chapters 4, 7, and 11 in Alberto Cairo, The Truthful Art2  Why It’s So Hard for Us to Visualize Uncertainty  Amanda Cox’s keynote address at the 2017 OpenVis Conf  Communicating Uncertainty When Lives Are on the Line  Showing uncertainty during the live election forecast \u0026amp; Trolling the uncertainty dial  Questions to reflect on  Why is it important to deal with uncertainty in data?","tags":null,"title":"Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 22 in Claus Wilke, Fundamentals of Data Visualization1  Naomi Robbins, “Are Grid Lines Useful or Chartjunk?”  Stephen Few, “Grid Lines in Graphs are Rarely Useful”  Henry Wang, “ggplot2 Theme Elements Demonstration”  Glance through the documentation for ggplot’s complete themes and theme(), especially the examples near the bottom  Questions to reflect on  How do the principles of CRAP apply to graph design and other theme elements? Should plots use gridlines? Naomi Robbins says yes; Stephen Few says no—what do you say?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nThemes  CRAP and ggplot  The anatomy of a ggplot theme             Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction CRAP and ggplot The anatomy of a ggplot theme  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n   ","date":1589500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"744a9e81e6eae570052294b144f8d401","permalink":"/content/05-content/","publishdate":"2020-05-15T00:00:00Z","relpermalink":"/content/05-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 22 in Claus Wilke, Fundamentals of Data Visualization1  Naomi Robbins, “Are Grid Lines Useful or Chartjunk?”  Stephen Few, “Grid Lines in Graphs are Rarely Useful”  Henry Wang, “ggplot2 Theme Elements Demonstration”  Glance through the documentation for ggplot’s complete themes and theme(), especially the examples near the bottom  Questions to reflect on  How do the principles of CRAP apply to graph design and other theme elements?","tags":null,"title":"Themes","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 6 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 6 in Alberto Cairo, The Truthful Art2  Chapter 10 in Claus Wilke, Fundamentals of Data Visualization3  Engaging Readers with Square Pie/Waffle Charts  Understanding Pie Charts  Square pie chart beats out the rest in perception study  Twitter thread from John Burn-Murdoch on why the Financial Times uses log scales in their COVID-19 tracking charts  Tweet and Twitter thread from John Burn-Murdoch on why the Financial Times doesn’t use population-adjusted numbers in their COVID-19 tracking charts  Video from the Financial Times about the design decisions behind their COVID-19 tracking charts  Recommended   See how to create your own COVID-19 tracking chart with R   Questions to reflect on  How do these types of visualizations help or hinder our search for truth in data? What do you think of the Financial Times explanations of their use of absolute numbers (not per capita numbers) and log scales (not regular scales)? How have these decisions affected your perception of the pandemic? How have they affected others’ perceptions?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nAmounts and proportions  Reproducibility  Amounts  Proportions                Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Reproducibility Amounts Proportions  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Wilke, Fundamentals of Data Visualization.↩︎\n   ","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"570f902af3e232a725f0155a46b1910b","permalink":"/content/04-content/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/content/04-content/","section":"content","summary":"Readings  Recommended Questions to reflect on  Slides Videos   Readings   Chapter 6 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 6 in Alberto Cairo, The Truthful Art2  Chapter 10 in Claus Wilke, Fundamentals of Data Visualization3  Engaging Readers with Square Pie/Waffle Charts  Understanding Pie Charts  Square pie chart beats out the rest in perception study  Twitter thread from John Burn-Murdoch on why the Financial Times uses log scales in their COVID-19 tracking charts  Tweet and Twitter thread from John Burn-Murdoch on why the Financial Times doesn’t use population-adjusted numbers in their COVID-19 tracking charts  Video from the Financial Times about the design decisions behind their COVID-19 tracking charts  Recommended   See how to create your own COVID-19 tracking chart with R   Questions to reflect on  How do these types of visualizations help or hinder our search for truth in data?","tags":null,"title":"Amounts and proportions","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Hans Rosling, “200 Countries, 200 Years, 4 Minutes”  Chapter 2 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 3 in Kieran Healy, Data Visualization2  Questions to reflect on  Why is it important to visualize variables and data? What does it mean to map data to graph aesthetics? What data was mapped to which aesthetics in Rosling’s video?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nMapping data to graphics  Data, aesthetics, \u0026amp; the grammar of graphics  Grammatical layers  Aesthetics in extra dimensions  Tidy data                   Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Data, aesthetics, and the grammar of graphics Grammatical layers Aesthetics in extra dimensions Tidy data  You can also watch the playlist (and skip around to different sections) here:\n    Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Kieran Healy, Data Visualization: A Practical Introduction (Princeton: Princeton University Press, 2018), http://socviz.co/.↩︎\n   ","date":1589328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"267a7446c3b4fe31bb41b58ba3b49b11","permalink":"/content/03-content/","publishdate":"2020-05-13T00:00:00Z","relpermalink":"/content/03-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Hans Rosling, “200 Countries, 200 Years, 4 Minutes”  Chapter 2 in Claus Wilke, Fundamentals of Data Visualization1  Chapter 3 in Kieran Healy, Data Visualization2  Questions to reflect on  Why is it important to visualize variables and data? What does it mean to map data to graph aesthetics? What data was mapped to which aesthetics in Rosling’s video?","tags":null,"title":"Mapping data to graphics","type":"docs"},{"authors":null,"categories":null,"content":"   Readings  Questions to reflect on  Slides Videos   Readings   Chapter 5 in Alberto Cairo, The Truthful Art1  Chapter 4 and Chapter 22 in Claus Wilke, Fundamentals of Data Visualization2  Summary of CRAP graphic design principles from Garr Reynolds, Presentation Zen.3 These principles are from Robin Williams’ The Non-Designer’s Design \u0026amp; Type Books,4 which you should really get if you’re interested in doing anything design-related ever. Her stuff is life-changing.  Typography in ten minutes. The rest of the Practical Typography book is phenomenal and you’d be remiss if you didn’t read the whole thing and bookmark it for life, but for now just read this quick summary.  “What’s the Difference Between JPG, PNG, and GIF?”  “File formats explained”  Questions to reflect on  Why does graphic design matter when conveying truth? What makes something well designed (vs. poorly designed)?    Slides The slides for today’s lesson are available online as an HTML file. Use the buttons below to open the slides either as an interactive website or as a static PDF (for printing or storing for later). You can also click in the slides below and navigate through them with your left and right arrow keys.\n View all slides in new window  Download PDF of all slides\nGraphic design  Truth, beauty, stories, design  Graphic design and CRAP  Contrast  Repetition  Alignment  Proximity  Image types                            Fun fact: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.\n  Videos Videos for each section of the lecture are available at this YouTube playlist.\n Introduction Truth, beauty, stories, design Graphic design and CRAP Image types  You can also watch the playlist (and skip around to different sections) here:\n    Alberto Cairo, The Truthful Art: Data, Charts, and Maps for Communication (Berkeley, California: New Riders, 2016).↩︎\n Claus E. Wilke, Fundamentals of Data Visualization (Sebastopol, California: O’Reilly Media, 2018), https://serialmentor.com/dataviz/.↩︎\n Garr Reynolds, Presentation Zen: Simple Ideas on Presentation Design and Delivery, 1st ed. (Berkeley, California: New Riders, 2008).↩︎\n Robin Williams, The Non-Designer’s Design \u0026amp; Type Books: Design and Typographic Principles for the Visual Novice, Deluxe Edition (Berkeley, California: Peachpit Press, 2008).↩︎\n   ","date":1589241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"f4445f225dbc16ad343f16f0677c881d","permalink":"/content/02-content/","publishdate":"2020-05-12T00:00:00Z","relpermalink":"/content/02-content/","section":"content","summary":"Readings  Questions to reflect on  Slides Videos   Readings   Chapter 5 in Alberto Cairo, The Truthful Art1  Chapter 4 and Chapter 22 in Claus Wilke, Fundamentals of Data Visualization2  Summary of CRAP graphic design principles from Garr Reynolds, Presentation Zen.3 These principles are from Robin Williams’ The Non-Designer’s Design \u0026amp; Type Books,4 which you should really get if you’re interested in doing anything design-related ever.","tags":null,"title":"Graphic design","type":"docs"},{"authors":null,"categories":null,"content":" Below is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon.\nAs mentioned in the syllabus, the course is structured by each week. Every week has three important sections.\nA/B Structure The class is structured by dividing the student roster into two subsets, forming a “Tuesday” group and a “Thursday” group. Your assigned group dictates when you come to class. In class, we will have a short, practical lecture and an activity which is designed to give you hands-on experience and give you a greater understanding of the broader material. Out of class, you will complete readings and will watch (short) recorded lectures on the week’s topic. Each week is intended to be self contained — the fact you are doing something on Tuesday rather than Thursday or vice versa will not hurt you.\n Content (): This page contains the readings and recorded lectures for the topic.\n Example (): This page contains R code that we will discuss in-class. These examples are intended as a useful reference to various functions that you will need on weekly labs and in your group project.\n Labs (): This page contains the instructions for either the Lab (1–3 brief tasks), or for the two mini projects and final project. Assignments are due by 11:59 PM (Eastern) on the day they’re listed.\n  tl;dr: You should follow this general process each week:\n Come to class on your assigned day Do everything on the content () page Glance through the example () page Complete the lab ().      Foundations Content Example Assignment   Week 1 Truth, beauty, and data + R and tidyverse      Week 2 Graphic design      Week 3 Mapping data to graphics       Core types of graphics Content Example Assignment   Week 4 Amounts and proportions      End of Week 4  Mini project 1 due        Week 5 Themes      Week 6 Uncertainty      Week 7 Relationships      Week 8 Comparisons      End of Week 8  Mini project 2 due        Week 9 Annotations       Special applications Content Example Assignment   Week 10 Interactivity      Week 11 Time      Week 12 Space      Week 13 Text      Week 14 Enhancing graphics       Conclusions Content Example Assignment   June 1 Truth, beauty, and data revisited      Final Exam  Final project due          ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592865877,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"Below is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon.\nAs mentioned in the syllabus, the course is structured by each week. Every week has three important sections.\nA/B Structure The class is structured by dividing the student roster into two subsets, forming a “Tuesday” group and a “Thursday” group.","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"  What is This Course and Can / Should You Take It? What This Course is Not Success in this Course Course materials R and RStudio Online help  Evaluations and Grades Academic honesty  COVID-19 Miscellanea Contacting Me    Instructor  Prof. Ben Bushong  25A Marshall-Adams Hall  bbushong@msu.edu  @benbushong  Schedule an appointment   Course details  Tuesday and Thursday  September – December, 2020  10:20 - 11:40 AM  Slack   Contacting me Please consider whether your question is short and concrete; if so, feel free to email me. If your question is deep, vague, interesting, or otherwise complex, please come to office hours or we can discuss in class. See syllabus for details.\n  What is This Course and Can / Should You Take It? Innovations in statistical learning have created many engineering breakthroughs. From real time voice recognition to automatic categorization (and in some cases production) of news stories, machine learning is transforming the way we live our lives. These techniques are, at their heart, novel ways to work with data, and therefore they should have implications for social science. This course explores the intersection of statistical learning (or machine learning) and social science and aims to answer two primary questions about these new techniques:\nHow does statistical learning work and what kinds of statistical guarantees can be made about the performance of statistical-learning algorithms?\n How can statistical learning be used to answer questions that interest social science researchers, such as testing theories or improving social policy?\n  In order to address these questions, we will cover so-called “standard” techniques such as supervised and unsupervised learning, statistical learning theory and nonparametric and Bayesian approaches. If it were up to me, this course would be titled “Statistical Learning for Social Scientists”—I believe this provides a more appropriate guide to the content of this course. And while this class will cover these novel statistical methodologies in some detail, it is not a substitute for the appropriate class in Computer Science or Statistics. Nor is this a class that teaches specific skills for the job market. Rather, this class will teach you to think about data analytics broadly. We will spend a great deal of time learning how to interpret the output of statistical learning algorithms and approaches, and will also spend a great deal of time on better understanding the basic ideas in statistical learning. This, of course, comes at some cost in terms of time spent on learning computational and/or programming skills.\nEnrollment for credit in this course is simply not suitable for those unprepared in or uninterested in elementary statistical theory no matter the intensity of interest in machine learning or “Big Data”. Really.\nYou will be required to understand elementary mathematics in this course and should have at least some exposure to statistical theory. The class is front-loaded technically: early lectures are more mathematically oriented, while later lectures are more applied.\nThe topics covered in this course are listed later in this document. I will assign readings sparingly from Introduction to Statistical Learning, henceforth referred to as ISL. This text is available for free online and, for those who like physical books, can be purchased for about $25. Importantly, the lectures deviate a fair bit from the reading, and thus you will rely on your course notes much more than you might in other classes.\nIf—after you have read this document and preferably after attending the first lecture—you have any questions about whether this course is appropriate for you, please come talk to me. Anybody is permitted to attend the lectures and I am delighted if people can benefit.\n What This Course is Not The focus of this course is conceptual. The goal is to create a working understanding of when and how tools from computer science and statistics can be profitably applied to problems in social science. Though students will be required to apply some of these techniques themselves, this course is not…\n…a replacement for EC420 or a course in causal inference.\nAs social scientists, we are most often concerned with causal inference in order to analyze and write policies. Statistical learning and the other methods we will discuss in this course are generally not well-suited to these problems, and while I’ll give a short overview of standard methods, this is only to build intuitions. Ultimately, this course has a different focus and you should still pursue standard methodological insights from your home departments.\n…a course on the computational aspects of the underlying methods.\nThere are many important innovations that have made machine learning techniques computationally feasible. We will not discuss these, as there are computer science courses better equipped to cover them. When appropriate, we will discuss whether something is computable, and we will even give rough approximations of the amount of time required (e.g. P vs NP). But we will not discuss how optimizers work or best practices in programming.\n…a primer on the nitty-gritty of how to use these tools or a way to pad your resume.\nThe mechanics of implementation, whether it be programming languages or learning to use APIs, will not be covered in any satisfying level of depth. Students will be expected to learn most of the programming skills on their own. Specifically, while there will be some material to remind you of basic R commands, this is not a good course for people who are simply looking to learn the mechanics of programming. This course is designed to get you to use both traditional analytics and, eventually, machine learning tools. We will do some review of basic programming, and you will have the opportunity to explore topics that interest you through a final project, but ultimately this is a course that largely focuses on the theoretical aspects of statistical learning as applied to social science and not a class on programming.\nPerhaps most importantly, this course is an attempt to push toward the frontiers in social science. Thus, please allow some messiness. Some topics may be underdeveloped for a given person’s passions, but given the wide variety of technical skills and overall interests, this is a required sacrifice. Both the challenge and opportunity of this area comes from the fact that there is no fully developed, wholly unifying framework.\n Success in this Course I promise, you are equipped to succeed in this course.\nLearning R can be difficult at first. Like learning a new language—Spanish, French, or Chinese—it takes dedication and perseverance. Hadley Wickham (the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like) ggplot2—made this wise observation:\n It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n Even experienced programmers (like me) find themselves bashing their heads against seemingly intractable errors. If you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, e-mail me, etc.\n\n Course materials All of the readings and software in this class are free. There are free online version of all the textbooks and R / RStudio are free. (Don’t pay for RStudio.)\nR and RStudio You will do all of your analysis with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R! We will have a shared class workspace in RStudio.cloud that will let you quickly copy templates for examples, exercises, and mini projects.\nRStudio.cloud is convenient, but it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis and graphics. You also can’t use your own custom fonts with RStudio.cloud. Over the course of the semester, you’ll probably want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud. This isn’t 100% necessary, but it’s helpful.\nYou can find instructions for installing R, RStudio, and all the tidyverse packages here.\n Online help Data science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Also, since most of your R work will deal with ggplot2, it’s often easier to just search for that instead of the letter “r” (e.g. “ggplot scatterplot”).\nAdditionally, we have a class chatroom at Slack where anyone in the class can ask questions and anyone can answer. I will monitor Slack regularly and will respond quickly. (It’s one of the rare Slack workspaces where I actually have notifications enabled!) Ask questions about the readings, exercises, and mini projects. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too.\n  Evaluations and Grades Your grade in this course will be based on attendance, labs, and a final project.\nThe general breakdown will be 40% for both labs and the final project, and 20% for attendance and participation. The final project requires two “mini-projects” to ensure you’re making satisfactory progress. Assignment of numeric grades will follow the standard, where ties (e.g., 91.5%) are rounded to favor the student. Evaluations (read: grades) are designed not to deter anyone from taking this course who might otherwise be interested, but will be taken seriously.\nLabs will be short homework assignments that require you to do something practical using a basic statistical language. Support will be provided for the R language only, although I may present some examples in Python from time to time. You must have access to computing resources and the ability to program basic statistical analyses.\nAs mentioned above, this course will not teach you how to program or how to write code in a specific language. If you are unprepared to do implement basic statistical coding, please take (or retake) PLS202. I highly encourage seeking coding advice from those who instruct computer science courses – it’s their job and they are better at it than I am. I’ll try to provide a good service, but I’m really not an expert in instruction as it relates to programming.\nAs the course progresses, you will be able to find descriptions for all the assignments on the assignments page.\nTo Recap:\n   Assignment Points Percent    In-Class Mini-Exercises (10 × 10) 100 20%  Labs (10 × 10) 200 40%  Mini project 1 50 10%  Mini project 2 50 10%  Final project 100 20%  Total 500 —        Grade Range Grade Range    4.0 92–100% 2.0 72–76%  3.5 87–91% 1.5 67–72%  3.0 82-87% 1.0 62–67%  2.5 77–81% 0.0 bad–66%     Academic honesty Violation of MSU’s Spartan Code of Honor will result in a grade of 0.0 in the course. Moreover, I am required by MSU policy to report suspected cases of academic dishonesty for possible disciplinary action.1\n  COVID-19 Things are hard right now. You most likely know people who have lost their jobs, have tested positive for COVID-19, have been hospitalized, or perhaps have even died. You all have increased (or possibly decreased) work responsibilities and increased family care responsibilities. You might be caring for extra people right now, and you are likely facing uncertain job prospects.\nI’m fully committed to making sure that you learn everything you were hoping to learn from this class! I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Under pandemic conditions, that flexibility and leniency is intensified.\nIf you feel like you’re behind or not understanding everything, do not suffer in silence! Please contact me. I’m available through e-mail and Slack.\n Miscellanea All class notes will be posted on https://msudataanalytics.github.io/SSC442.\nOffice Hours are Tues \u0026amp; Thur, 4:30 - 5:45 PM in 25A Marshall Adams Hall Please use my office hours. It would be remarkable if you didn’t need some assistance with the material, and I am here to help. One of the benefits of open office hours is to accommodate many students at once; if fellow students are in my office, please join in and feel very free to show up in groups. Office hours will move around a little bit throughout the semester to attempt to meet the needs of all students.\nIn addition to drop-in office hours, I always have sign-up office hours for advising and other purposes. They are online, linked from my web page. As a general rule, please first seek course-related help from the drop-in office hours. However, if my scheduled office hours are always infeasible for you, let me know, and then I may encourage you to make appointments with me. I ask that you schedule your studying so that you are prepared to ask questions during office hours – office hours are not a lecture and if you’re not prepared with questions we will end up awkwardly staring at each other for an hour until you leave.\nSome gentle requests regarding office hours and on contacting me. First, my office hours end sharply at the end, so don’t arrive 10 minutes before the scheduled end and expect a full session. Please arrive early if you have lengthy questions, or if you don’t want to risk not having time due to others’ questions. You are free to ask me some stuff by e-mail, (e.g. a typo or something on a handout), but please know e-mail sucks for answering many types of questions. “How do I do this lab?” or “What’s up with Python?” are short questions with long answers. Come to office hours.\n Contacting Me Email is a blessing and a curse. Instant communication is wonderful, but often email is the wrong medium to have a productive conversation about course material. Moreover, I get a lot of emails. This means that I am frequently triaging emails into two piles: “my house is burning down” and “everything else”. Your email is unlikely to make the former pile. So… asking questions about course material is always best done in-class or in office hours. Students always roll their eyes when professors say things like that, but it’s true that if you have a question, it’s very likely someone else has the same question. (An alternative here would be via Slack but again, in-class is still the best option.)\nThat said, email is still useful. If you’re going to use it, you should at least use if effectively. There’s a running joke in academia that professors only read an email until they find a question. They then respond to that question and ignore the rest of the email. I won’t do this, but I do think it is helpful to assume that the person on the receiving end of an email will operate this way. By keeping this in mind, you will write a much more concise and easy to understand email.\nSome general tips:\n Use your University supplied email for University business. This helps me know who you are. Use a short but informative subject line. For example: [SSC442] Final Project Grading One topic, one email. If you have multiple things to discuss, and you anticipate followup replies, it is best to split them into two emails so that the threads do not get cluttered. Ask direct questions. If you’re asking multiple questions in one email, use a bulleted list. Don’t ask questions that are answered by reading the syllabus! This drives me nuts. I’ve also found that students are overly polite in emails. I suppose it may be intimidating to email a professor, and you should try to match the style that the professor prefers, but I view email for a course as a casual form of communication. Said another way: get to the point. Students often send an entire paragraph introducing themselves, but if you use your University email address, and add the course name in the subject, I will already know who you are. Here’s an example of a perfectly reasonable email:   Subject: [SSC442] Lab, Question 2, Typo\nHi Prof. Bushong,\nThere seems to be a typo in the Lab on Question 2. The problem says to use a column of data that doesn’t seem to exist. Can you correct this or which should we use?\nThanks, Student McStudentFace\n    So just don’t cheat or plagiarize. This is an easy problem to avoid.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592865877,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"What is This Course and Can / Should You Take It? What This Course is Not Success in this Course Course materials R and RStudio Online help  Evaluations and Grades Academic honesty  COVID-19 Miscellanea Contacting Me    Instructor  Prof. Ben Bushong  25A Marshall-Adams Hall  bbushong@msu.edu  @benbushong  Schedule an appointment   Course details  Tuesday and Thursday  September – December, 2020  10:20 - 11:40 AM  Slack   Contacting me Please consider whether your question is short and concrete; if so, feel free to email me.","tags":null,"title":"Syllabus","type":"page"}]