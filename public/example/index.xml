<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Examples | Data Analytics</title>
    <link>https://ssc442.netlify.app/example/</link>
      <atom:link href="https://ssc442.netlify.app/example/index.xml" rel="self" type="application/rss+xml" />
    <description>Examples</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <image>
      <url>https://ssc442.netlify.app/img/social-image.png</url>
      <title>Examples</title>
      <link>https://ssc442.netlify.app/example/</link>
    </image>
    
    <item>
      <title>Visualizing Uncertainty</title>
      <link>https://ssc442.netlify.app/example/04-example/</link>
      <pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/04-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-exercise&#34; id=&#34;toc-in-class-exercise&#34;&gt;In-Class Exercise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regularly-scheduled-content&#34; id=&#34;toc-regularly-scheduled-content&#34;&gt;Regularly Scheduled Content&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-1-statistical-inference-and-polls&#34; id=&#34;toc-part-1-statistical-inference-and-polls&#34;&gt;Part 1: Statistical Inference and Polls&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#polls&#34; id=&#34;toc-polls&#34;&gt;Polls&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-sampling-model-for-polls&#34; id=&#34;toc-the-sampling-model-for-polls&#34;&gt;The sampling model for polls&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#populations-samples-parameters-and-estimates&#34; id=&#34;toc-populations-samples-parameters-and-estimates&#34;&gt;Populations, samples, parameters, and estimates&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-sample-average&#34; id=&#34;toc-the-sample-average&#34;&gt;The sample average&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameters&#34; id=&#34;toc-parameters&#34;&gt;Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#polling-versus-forecasting&#34; id=&#34;toc-polling-versus-forecasting&#34;&gt;Polling versus forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-our-estimate-expected-value-and-standard-error&#34; id=&#34;toc-properties-of-our-estimate-expected-value-and-standard-error&#34;&gt;Properties of our estimate: expected value and standard error&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clt&#34; id=&#34;toc-clt&#34;&gt;Central Limit Theorem&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#margin-of-error&#34; id=&#34;toc-margin-of-error&#34;&gt;Margin of Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-monte-carlo-simulation&#34; id=&#34;toc-a-monte-carlo-simulation&#34;&gt;A Monte Carlo simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-spread&#34; id=&#34;toc-the-spread&#34;&gt;The spread&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-why-not-run-a-very-large-poll&#34; id=&#34;toc-bias-why-not-run-a-very-large-poll&#34;&gt;Bias: why not run a very large poll?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-2-supplemental-additional-visualization-techniques&#34; id=&#34;toc-part-2-supplemental-additional-visualization-techniques&#34;&gt;Part 2: (Supplemental) Additional Visualization Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34; id=&#34;toc-code&#34;&gt;Code&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-and-clean-data&#34; id=&#34;toc-load-and-clean-data&#34;&gt;Load and clean data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34; id=&#34;toc-histograms&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plots&#34; id=&#34;toc-density-plots&#34;&gt;Density plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#box-violin-and-rain-cloud-plots&#34; id=&#34;toc-box-violin-and-rain-cloud-plots&#34;&gt;Box, violin, and rain cloud plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;in-class-exercise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In-Class Exercise&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You’ll need to download one CSV files and put them somewhere on your computer (or upload it to RStudio.cloud if you’ve gone that direction)—preferably in a folder named &lt;code&gt;data&lt;/code&gt; in your project folder. You can download the data from the link below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/biden_score_2023.xlsx&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/fivethirtyeight/data/tree/master/congress-biden-score&#34;&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;code&gt;About this Dataset&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Task:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Explore the relationship between the vote margin in a given district and whether the congressperson voted with or against President Biden’s position.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regularly-scheduled-content&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Regularly Scheduled Content&lt;/h1&gt;
&lt;p&gt;Probabilistic thinking is central in the human experience. How we describe that thinking is mixed, but most of the time we use (rather imprecise) language. With only a few moments of searching, one can find thousands of articles that use probabilistic words to describe events. Here are some examples:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“‘Highly unlikely’ State of the Union will happen amid shutdown” – The Hill&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Tiger Woods makes Masters 15th and most improbable major” – Fox&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Trump predicts ‘very good chance’ of China trade deal” – CNN&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet people don’t have a good sense of what these things mean. Uncertainty is key to data analytics: if we were certain of things, A study in the 1960s explored the perception of probabilistic words like these among NATO officers. A more modern replication of this found the following basic pattern:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../../../../../../../../../img/words.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A deep, basic fact about humans is that we struggle to understand probabilities. But visualizing things can help. The graphic above shows the uncertainty about uncertainty (very meta). We can convey all manner of uncertainty with clever graphics. Today’s practical example works through some of the myriad of ways to visualize variable data. We’ll cover some territory that we’ve already hit, in hopes of locking in some key concepts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-1-statistical-inference-and-polls&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part 1: Statistical Inference and Polls&lt;/h1&gt;
&lt;p&gt;In this Example we will describe, in some detail, how poll aggregators such as FiveThirtyEight use data to predict election outcomes. To understand how they do this, we first need to learn the basics of &lt;em&gt;Statistical Inference&lt;/em&gt;, the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;div id=&#34;polls&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Polls&lt;/h2&gt;
&lt;p&gt;Opinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as &lt;em&gt;inference&lt;/em&gt; and it is the main topic of this chapter.&lt;/p&gt;
&lt;p&gt;Perhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.&lt;/p&gt;
&lt;p&gt;Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.&lt;/p&gt;
&lt;p&gt;Although typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.&lt;/p&gt;
&lt;p&gt;Real Clear Politics&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;!-- (Source: [Real Clear Politics](https://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html)) --&gt;
&lt;p&gt;Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we will go into some detail on this later.&lt;/p&gt;
&lt;p&gt;Let’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different &lt;em&gt;spread&lt;/em&gt;: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled &lt;strong&gt;MoE&lt;/strong&gt; which stands for &lt;em&gt;margin of error&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this example, we will show how the probability concepts we learned in the previous content can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define &lt;em&gt;estimates&lt;/em&gt; and &lt;em&gt;margins of errors&lt;/em&gt;, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: &lt;em&gt;confidence intervals&lt;/em&gt; and &lt;em&gt;p-values&lt;/em&gt;. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.&lt;/p&gt;
&lt;p&gt;We start by connecting probability theory to the task of using polls to learn about a population.&lt;/p&gt;
&lt;div id=&#34;the-sampling-model-for-polls&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The sampling model for polls&lt;/h3&gt;
&lt;p&gt;To help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this hypothetical urn.&lt;/p&gt;
&lt;p&gt;Before making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you 10 cents per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay \$25 to collect your \$25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;dslabs&lt;/strong&gt; package includes a function that shows a random draw from this urn:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
take_poll(25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/first-simulated-poll-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Think about how you would construct your interval based on the data shown above.&lt;/p&gt;
&lt;p&gt;We have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;populations-samples-parameters-and-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Populations, samples, parameters, and estimates&lt;/h2&gt;
&lt;p&gt;We want to predict the proportion of blue beads in the urn. Let’s call this quantity &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which then tells us the proportion of red beads &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt;, and the spread &lt;span class=&#34;math inline&#34;&gt;\(p - (1-p)\)&lt;/span&gt;, which simplifies to &lt;span class=&#34;math inline&#34;&gt;\(2p - 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In statistical textbooks, the beads in the urn are called the &lt;em&gt;population&lt;/em&gt;. The proportion of blue beads in the population &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is called a &lt;em&gt;parameter&lt;/em&gt;. The 25 beads we see in the previous plot are called a &lt;em&gt;sample&lt;/em&gt;. The task of statistical inference is to predict the parameter &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; using the observed data in the sample.&lt;/p&gt;
&lt;p&gt;Can we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; &amp;gt; .9 or &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; &amp;lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?&lt;/p&gt;
&lt;p&gt;We want to construct an estimate of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample &lt;span class=&#34;math inline&#34;&gt;\(0.48\)&lt;/span&gt; must be at least related to the actual proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. But do we simply predict &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to be 0.48? First, remember that the sample proportion is a random variable. If we run the command &lt;code&gt;take_poll(25)&lt;/code&gt; four times, we get a different answer each time, since the sample proportion is a random variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/four-simulated-polls-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.&lt;/p&gt;
&lt;div id=&#34;the-sample-average&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The sample average&lt;/h3&gt;
&lt;p&gt;Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an &lt;em&gt;estimate&lt;/em&gt; of the parameter &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Once we have this estimate, we can easily report an estimate for the spread &lt;span class=&#34;math inline&#34;&gt;\(2p-1\)&lt;/span&gt;, but for simplicity we will illustrate the concepts for estimating &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We start by defining the random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as: &lt;span class=&#34;math inline&#34;&gt;\(X=1\)&lt;/span&gt; if we pick a blue bead at random and &lt;span class=&#34;math inline&#34;&gt;\(X=0\)&lt;/span&gt; if it is red. This implies that the population is a list of 0s and 1s. If we sample &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; beads, then the average of the draws &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_N\)&lt;/span&gt; is equivalent to the proportion of blue beads in our sample. This is because adding the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s is equivalent to counting the blue beads and dividing this count by the total &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is equivalent to computing a proportion. We use the symbol &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant &lt;span class=&#34;math inline&#34;&gt;\(1/N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\bar{X} = 1/N \times \sum_{i=1}^N X_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, the proportion of blue beads.&lt;/p&gt;
&lt;p&gt;Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to &lt;strong&gt;estimate&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parameters&lt;/h3&gt;
&lt;p&gt;Just like we use variables to define unknowns in systems of equations, in statistical inference we define &lt;em&gt;parameters&lt;/em&gt; to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, &lt;strong&gt;we do not know the proportion of blue beads in the urn&lt;/strong&gt;. We define the parameters &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to represent this quantity. &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we are going to &lt;em&gt;estimate this parameter&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-versus-forecasting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling versus forecasting&lt;/h3&gt;
&lt;p&gt;Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for that moment and not for election day. The &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;properties-of-our-estimate-expected-value-and-standard-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Properties of our estimate: expected value and standard error&lt;/h3&gt;
&lt;p&gt;To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt;. Remember that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is the sum of independent draws so the rules we covered in the probability chapter apply.&lt;/p&gt;
&lt;p&gt;Using what we have learned, the expected value of the sum &lt;span class=&#34;math inline&#34;&gt;\(N\bar{X}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(N \times\)&lt;/span&gt; the average of the urn, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. So dividing by the non-random constant &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; gives us that the expected value of the average &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We can write it using our mathematical notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}(\bar{X}) = p
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can also use what we learned to figure out the standard error: the standard error of the sum is &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{N} \times\)&lt;/span&gt; the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is &lt;span class=&#34;math inline&#34;&gt;\((1-0) \sqrt{p (1-p)}\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p (1-p)}\)&lt;/span&gt;. Because we are dividing the sum by &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, we arrive at the following formula for the standard error of the average:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This result reveals the power of polls. The expected value of the sample proportion &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is the parameter of interest &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and we can make the standard error as small as we want by increasing &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. The law of large numbers tells us that with a large enough poll, our estimate converges to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?&lt;/p&gt;
&lt;p&gt;One problem is that we do not know &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, so we can’t compute the standard error. However, for illustrative purposes, let’s assume that &lt;span class=&#34;math inline&#34;&gt;\(p=0.51\)&lt;/span&gt; and make a plot of the standard error versus the sample size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/standard-error-versus-sample-size-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and &lt;span class=&#34;math inline&#34;&gt;\(p=0.51\)&lt;/span&gt;, the standard error is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(p*(1-p))/sqrt(1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01580823&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or 1.5 percentage points. So even with large polls, for close elections, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;clt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Central Limit Theorem&lt;/h2&gt;
&lt;p&gt;The Central Limit Theorem (CLT) tells us that the distribution function for a sum of draws is approximately normal. You also may recall that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is approximately normal.&lt;/p&gt;
&lt;p&gt;In summary, we have that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; has an approximately normal distribution with expected value &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and standard error &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p(1-p)/N}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now how does this help us? Suppose we want to know what is the probability that we are within 1% from &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We are basically asking what is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(| \bar{X} - p| \leq .01)
\]&lt;/span&gt;
which is the same as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(\bar{X}\leq p + .01) - \mbox{Pr}(\bar{X} \leq p - .01)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Can we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, on the left. Since &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the expected value and &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}\)&lt;/span&gt; is the standard error we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq \frac{ \,.01} {\mbox{SE}(\bar{X})} \right) -
\mbox{Pr}\left(Z \leq - \frac{ \,.01} {\mbox{SE}(\bar{X})} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One problem we have is that since we don’t know &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we don’t know &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SE}(\bar{X})\)&lt;/span&gt;. But it turns out that the CLT still works if we estimate the standard error by using &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; in place of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We say that we &lt;em&gt;plug-in&lt;/em&gt; the estimate. Our estimate of the standard error is therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N}
\]&lt;/span&gt;
In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now we continue with our calculation, but dividing by &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N})\)&lt;/span&gt; instead. In our first sample we had 12 blue and 13 red so &lt;span class=&#34;math inline&#34;&gt;\(\bar{X} = 0.48\)&lt;/span&gt; and our estimate of standard error is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_hat &amp;lt;- 0.48
se &amp;lt;- sqrt(x_hat*(1-x_hat)/25)
se&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09991997&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we can answer the question of the probability of being close to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. The answer is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(0.01/se) - pnorm(-0.01/se)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.07971926&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, there is a small chance that we will be close. A poll of only &lt;span class=&#34;math inline&#34;&gt;\(N=25\)&lt;/span&gt; people is not really very useful, at least not for a close election.&lt;/p&gt;
&lt;div id=&#34;margin-of-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Margin of Error&lt;/h3&gt;
&lt;p&gt;Earlier we mentioned the &lt;em&gt;margin of error&lt;/em&gt;. Now we can define it because it is simply 1.96 times the standard error, which we can now estimate. In our case it is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.96*se&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1958431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq \, 1.96\,\mbox{SE}(\bar{X})  / \mbox{SE}(\bar{X}) \right) -
\mbox{Pr}\left(Z \leq - 1.96\, \mbox{SE}(\bar{X}) / \mbox{SE}(\bar{X}) \right)
\]&lt;/span&gt;
which is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq 1.96 \right) -
\mbox{Pr}\left(Z \leq - 1.96\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which we know is about 95%:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(1.96)-pnorm(-1.96)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9500042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hence, there is a 95% probability that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; will be within &lt;span class=&#34;math inline&#34;&gt;\(1.96\times \hat{SE}(\bar{X})\)&lt;/span&gt;, in our case within about 0.2, of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.&lt;/p&gt;
&lt;p&gt;In summary, the CLT tells us that our poll based on a sample size of &lt;span class=&#34;math inline&#34;&gt;\(25\)&lt;/span&gt; is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.&lt;/p&gt;
&lt;p&gt;From the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt;=0.48 with a sample size of 2,000, our standard error &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mbox{SE}}(\bar{X})\)&lt;/span&gt; would have been 0.0111714. So our result is an estimate of &lt;code&gt;48&lt;/code&gt;% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-monte-carlo-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Monte Carlo simulation&lt;/h3&gt;
&lt;p&gt;(Optional) Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
N &amp;lt;- 1000
x_hat &amp;lt;- replicate(B, {
  x &amp;lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem is, of course, we don’t know &lt;code&gt;p&lt;/code&gt;. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function &lt;code&gt;take_poll(n=1000)&lt;/code&gt; instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.&lt;/p&gt;
&lt;p&gt;One thing we therefore do to corroborate theoretical results is to pick one or several values of &lt;code&gt;p&lt;/code&gt; and run the simulations. Let’s set &lt;code&gt;p=0.45&lt;/code&gt;. We can then simulate a poll:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- 0.45
N &amp;lt;- 1000

x &amp;lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat &amp;lt;- mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this particular sample, our estimate is &lt;code&gt;x_hat&lt;/code&gt;. We can use that code to do a Monte Carlo simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
x_hat &amp;lt;- replicate(B, {
  x &amp;lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To review, the theory tells us that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is approximately normally distributed, has expected value &lt;span class=&#34;math inline&#34;&gt;\(p=\)&lt;/span&gt; 0.45 and standard error &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p(1-p)/N}\)&lt;/span&gt; = 0.0157321. The simulation confirms this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4500761&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(x_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01579523&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A histogram and qq-plot confirm that the normal approximation is accurate as well:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/normal-approximation-for-polls-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course, in real life we would never be able to run such an experiment because we don’t know &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. But we could run it for various values of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;N&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-spread&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The spread&lt;/h3&gt;
&lt;p&gt;The competition is to predict the spread, not the proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. However, because we are assuming there are only two parties, we know that the spread is &lt;span class=&#34;math inline&#34;&gt;\(p - (1-p) = 2p - 1\)&lt;/span&gt;. As a result, everything we have done can easily be adapted to an estimate of &lt;span class=&#34;math inline&#34;&gt;\(2p - 1\)&lt;/span&gt;. Once we have our estimate &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mbox{SE}}(\bar{X})\)&lt;/span&gt;, we estimate the spread with &lt;span class=&#34;math inline&#34;&gt;\(2\bar{X} - 1\)&lt;/span&gt; and, since we are multiplying by 2, the standard error is &lt;span class=&#34;math inline&#34;&gt;\(2\hat{\mbox{SE}}(\bar{X})\)&lt;/span&gt;. Note that subtracting 1 does not add any variability so it does not affect the standard error.&lt;/p&gt;
&lt;p&gt;For our 25 item sample above, our estimate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is &lt;code&gt;.48&lt;/code&gt; with margin of error &lt;code&gt;.20&lt;/code&gt; and our estimate of the spread is &lt;code&gt;0.04&lt;/code&gt; with margin of error &lt;code&gt;.40&lt;/code&gt;. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we have it for the spread &lt;span class=&#34;math inline&#34;&gt;\(2p-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-why-not-run-a-very-large-poll&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bias: why not run a very large poll?&lt;/h3&gt;
&lt;p&gt;For realistic values of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `qplot()` was deprecated in ggplot2 3.4.0.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/standard-error-versus-p-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this in our Assignment for this week.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-supplemental-additional-visualization-techniques&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part 2: (Supplemental) Additional Visualization Techniques&lt;/h1&gt;
&lt;p&gt;For this second part of the example, we’re going to use historical weather data from &lt;a href=&#34;https://darksky.net/forecast/33.7546,-84.39/us12/en&#34;&gt;Dark Sky&lt;/a&gt; about wind speed and temperature trends for downtown Atlanta (&lt;a href=&#34;https://www.google.com/maps/place/33°45&amp;#39;16.4%22N+84°23&amp;#39;24.0%22W/@33.754557,-84.3921977,17z/&#34;&gt;specifically &lt;code&gt;33.754557, -84.390009&lt;/code&gt;&lt;/a&gt;) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the &lt;a href=&#34;https://github.com/hrbrmstr/darksky&#34;&gt;&lt;strong&gt;darksky&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/atl-weather-2019.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;atl-weather-2019.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;div id=&#34;load-and-clean-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load and clean data&lt;/h3&gt;
&lt;p&gt;First, we load the libraries we’ll be using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(ggridges)
library(gghalves)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we load the data with &lt;code&gt;read_csv()&lt;/code&gt;. Here we assume that the CSV file lives in a subfolder in my project named &lt;code&gt;data&lt;/code&gt;. Naturally, you’ll need to point this to wherever you stashed the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_raw &amp;lt;- read_csv(&amp;quot;data/atl-weather-2019.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll add a couple columns that we can use for faceting and filling using the &lt;code&gt;month()&lt;/code&gt; and &lt;code&gt;wday()&lt;/code&gt; functions from &lt;strong&gt;lubridate&lt;/strong&gt; for extracting parts of the date:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl &amp;lt;- weather_atl_raw %&amp;gt;%
  mutate(Month = month(time, label = TRUE, abbr = FALSE),
         Day = wday(time, label = TRUE, abbr = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to go!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;histograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histograms&lt;/h3&gt;
&lt;p&gt;We can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/basic-histogram-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the &lt;code&gt;boundary&lt;/code&gt; argument for that. We also add &lt;code&gt;scale_x_continuous()&lt;/code&gt; to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;, boundary = 1) +
  scale_x_continuous(breaks = seq(0, 12, by = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/basic-histogram-better-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can show the distribution of wind speed by month if we map the &lt;code&gt;Month&lt;/code&gt; column we made onto the fill aesthetic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;, boundary = 1) +
  scale_x_continuous(breaks = seq(0, 12, by = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/histogram-by-month-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;, boundary = 1) +
  scale_x_continuous(breaks = seq(0, 12, by = 1)) +
  guides(fill = FALSE) +
  facet_wrap(vars(Month))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The `&amp;lt;scale&amp;gt;` argument of `guides()` cannot be `FALSE`. Use &amp;quot;none&amp;quot; instead as
## of ggplot2 3.3.4.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/histogram-by-month-facet-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Neat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density plots&lt;/h3&gt;
&lt;p&gt;The code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the &lt;code&gt;geom&lt;/code&gt; layer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_density(color = &amp;quot;grey20&amp;quot;, fill = &amp;quot;grey50&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/basic-density-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we want, we can mess with some of the calculus options like the kernel and bandwidth:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_density(color = &amp;quot;grey20&amp;quot;, fill = &amp;quot;grey50&amp;quot;,
               bw = 0.1, kernel = &amp;quot;epanechnikov&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/density-kernel-bw-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_density(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/density-fill-by-month-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_density(alpha = 0.5) +
  guides(fill = FALSE) +
  facet_wrap(vars(Month))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/density-facet-by-month-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or we can stack the density plots behind each other with &lt;a href=&#34;https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html&#34;&gt;&lt;strong&gt;ggridges&lt;/strong&gt;&lt;/a&gt;. For that to work, we also need to map &lt;code&gt;Month&lt;/code&gt; to the y-axis. We can reverse the y-axis so that January is at the top if we use the &lt;code&gt;fct_rev()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +
  geom_density_ridges() +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/ggridges-basic-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can add some extra information to &lt;code&gt;geom_density_ridges()&lt;/code&gt; with some other arguments like &lt;code&gt;quantile_lines&lt;/code&gt;. We can use the &lt;code&gt;quantiles&lt;/code&gt; argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +
  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/ggridges-quantile-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have good working code, we can easily substitute in other variables by changing the x mapping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) +
  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/ggridges-quantile-temp-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use &lt;code&gt;geom_density_ridges_gradient()&lt;/code&gt;, and we need to change the &lt;code&gt;fill&lt;/code&gt; mapping to the strange looking &lt;code&gt;..x..&lt;/code&gt;, which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, &lt;code&gt;fill = temperatureHigh&lt;/code&gt; doesn’t work:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) +
  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;) +
  labs(x = &amp;quot;High temperature&amp;quot;, y = NULL, color = &amp;quot;Temp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The dot-dot notation (`..x..`) was deprecated in ggplot2 3.4.0.
## ℹ Please use `after_stat(x)` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/ggridges-gradient-temp-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And finally, we can get &lt;em&gt;extra&lt;/em&gt; fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: &lt;code&gt;temperatureLow&lt;/code&gt; and &lt;code&gt;temperatureHigh&lt;/code&gt;. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like &lt;code&gt;linetype&lt;/code&gt;), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using &lt;code&gt;pivot_longer()&lt;/code&gt; from &lt;strong&gt;tidyr&lt;/strong&gt;, which was already loaded with &lt;code&gt;library(tidyverse)&lt;/code&gt;. In the RStudio primers, you did this same thing with &lt;code&gt;gather()&lt;/code&gt;—&lt;code&gt;pivot_longer()&lt;/code&gt; is the newer version of &lt;code&gt;gather()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_long &amp;lt;- weather_atl %&amp;gt;%
  pivot_longer(cols = c(temperatureLow, temperatureHigh),
               names_to = &amp;quot;temp_type&amp;quot;,
               values_to = &amp;quot;temp&amp;quot;) %&amp;gt;%
  # Clean up the new temp_type column so that &amp;quot;temperatureHigh&amp;quot; becomes &amp;quot;High&amp;quot;, etc.
  mutate(temp_type = recode(temp_type,
                            temperatureHigh = &amp;quot;High&amp;quot;,
                            temperatureLow = &amp;quot;Low&amp;quot;)) %&amp;gt;%
  # This is optional—just select a handful of columns
  select(time, temp_type, temp, Month)

# Show the first few rows
head(weather_atl_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 4
##   time                temp_type  temp Month  
##   &amp;lt;dttm&amp;gt;              &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;ord&amp;gt;  
## 1 2019-01-01 05:00:00 Low        50.6 January
## 2 2019-01-01 05:00:00 High       63.9 January
## 3 2019-01-02 05:00:00 Low        49.0 January
## 4 2019-01-02 05:00:00 High       57.4 January
## 5 2019-01-03 05:00:00 Low        53.1 January
## 6 2019-01-03 05:00:00 High       55.3 January&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a column for the temperature (&lt;code&gt;temp&lt;/code&gt;) and a column indicating if it is high or low (&lt;code&gt;temp_type&lt;/code&gt;). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the &lt;code&gt;linetype&lt;/code&gt; aesthetic to show high/low in the border of the plots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month),
                             fill = ..x.., linetype = temp_type)) +
  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;) +
  labs(x = &amp;quot;High temperature&amp;quot;, y = NULL, color = &amp;quot;Temp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/ggridges-gradient-temp-high-low-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;box-violin-and-rain-cloud-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Box, violin, and rain cloud plots&lt;/h3&gt;
&lt;p&gt;Finally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the &lt;code&gt;Day&lt;/code&gt; variable we made indicating weekday:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, fill = Day)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/basic-boxplot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can switch this to a violin plot by just changing the &lt;code&gt;geom&lt;/code&gt; layer and mapping &lt;code&gt;Day&lt;/code&gt; to the x-axis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/basic-violin-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin() +
  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/violin-strip-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also add larger points for the daily averages. We’ll use a special layer for this: &lt;code&gt;stat_summary()&lt;/code&gt;. It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here &lt;code&gt;&#34;mean&#34;&lt;/code&gt;) and then plotting that result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin() +
  stat_summary(geom = &amp;quot;point&amp;quot;, fun = &amp;quot;mean&amp;quot;, size = 5, color = &amp;quot;white&amp;quot;) +
  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/violin-strip-mean-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also show the mean and confidence interval at the same time by changing the summary function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin() +
  stat_summary(geom = &amp;quot;pointrange&amp;quot;, fun.data = &amp;quot;mean_se&amp;quot;, size = 1, color = &amp;quot;white&amp;quot;) +
  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/violin-strip-mean-ci-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Overlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use &lt;a href=&#34;https://github.com/erocoar/gghalves&#34;&gt;the &lt;strong&gt;gghalves&lt;/strong&gt; package&lt;/a&gt;, we can use special halved versions of some of these geoms like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(x = fct_rev(Day), y = temperatureHigh)) +
  geom_half_point(aes(color = Day), side = &amp;quot;l&amp;quot;, size = 0.5) +
  geom_half_boxplot(aes(fill = Day), side = &amp;quot;r&amp;quot;) +
  guides(color = FALSE, fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/gghalves-point-boxplot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note the &lt;code&gt;side&lt;/code&gt; argument for specifying which half of the column the geom goes. We can also use &lt;code&gt;geom_half_violin()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(x = fct_rev(Day), y = temperatureHigh)) +
  geom_half_point(aes(color = Day), side = &amp;quot;l&amp;quot;, size = 0.5) +
  geom_half_violin(aes(fill = Day), side = &amp;quot;r&amp;quot;) +
  guides(color = FALSE, fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/gghalves-point-violon-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we flip the plot, we can make a &lt;a href=&#34;https://micahallen.org/2018/03/15/introducing-raincloud-plots/&#34;&gt;rain cloud plot&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(x = fct_rev(Day), y = temperatureHigh)) +
  geom_half_boxplot(aes(fill = Day), side = &amp;quot;l&amp;quot;, width = 0.5, nudge = 0.1) +
  geom_half_point(aes(color = Day), side = &amp;quot;l&amp;quot;, size = 0.5) +
  geom_half_violin(aes(fill = Day), side = &amp;quot;r&amp;quot;) +
  guides(color = FALSE, fill = FALSE) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/gghalves-rain-cloud-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;!-- ## Lecture Video --&gt;
&lt;!-- The lecture video for Tuesday (this material, but we only made it about 2/3rds of the way) is [available here &lt;i class=&#34;fas fa-film&#34;&gt;&lt;/i&gt;](https://mediaspace.msu.edu/media/SSC442+-+Example+04+-+Feb+10th/1_tyohp7mc) --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.realclearpolitics.com&#34; class=&#34;uri&#34;&gt;http://www.realclearpolitics.com&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html&#34; class=&#34;uri&#34;&gt;http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ggplot2: Everything you ever wanted to know</title>
      <link>https://ssc442.netlify.app/example/02-example/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/02-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#distributions&#34; id=&#34;toc-distributions&#34;&gt;Visualizing data distributions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#variable-types&#34; id=&#34;toc-variable-types&#34;&gt;Variable types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-describing-student-heights&#34; id=&#34;toc-case-study-describing-student-heights&#34;&gt;Case study: describing student heights&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#distribution-function&#34; id=&#34;toc-distribution-function&#34;&gt;Distribution function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cdf-intro&#34; id=&#34;toc-cdf-intro&#34;&gt;Cumulative distribution functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geometries-for-describing-distributions&#34; id=&#34;toc-geometries-for-describing-distributions&#34;&gt;Geometries for describing distributions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34; id=&#34;toc-histograms&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#smoothed-density&#34; id=&#34;toc-smoothed-density&#34;&gt;Smoothed density&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpreting-the-y-axis&#34; id=&#34;toc-interpreting-the-y-axis&#34;&gt;Interpreting the y-axis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#densities-permit-stratification&#34; id=&#34;toc-densities-permit-stratification&#34;&gt;Densities permit stratification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normal-distribution&#34; id=&#34;toc-normal-distribution&#34;&gt;The normal distribution&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#standard-units&#34; id=&#34;toc-standard-units&#34;&gt;Standard units&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantile-quantile-plots&#34; id=&#34;toc-quantile-quantile-plots&#34;&gt;Quantile-quantile plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#percentiles&#34; id=&#34;toc-percentiles&#34;&gt;Percentiles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-geometries&#34; id=&#34;toc-other-geometries&#34;&gt;ggplot2 geometries&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#barplots&#34; id=&#34;toc-barplots&#34;&gt;Barplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histograms-1&#34; id=&#34;toc-histograms-1&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plots&#34; id=&#34;toc-density-plots&#34;&gt;Density plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boxplots&#34; id=&#34;toc-boxplots&#34;&gt;Boxplots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#try-it&#34; id=&#34;toc-try-it&#34;&gt;Try it!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;distributions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualizing data distributions&lt;/h1&gt;
&lt;p&gt;Throughout your education, you may have noticed that numerical data is often summarized with the &lt;em&gt;average&lt;/em&gt; value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the &lt;em&gt;standard deviation&lt;/em&gt;. For example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list?&lt;/p&gt;
&lt;p&gt;Our first data visualization building block is learning to summarize lists of factors or numeric vectors—the two primary data types that we encounter in data analytics. More often than not, the best way to share or explore this summary is through data visualization. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as a distribution, there are several data visualization techniques to effectively relay this information.&lt;/p&gt;
&lt;p&gt;In this section, we first discuss properties of a variety of distributions and how to visualize distributions using a motivating example of student heights. We then discuss some principles of data visualizations more broadly, and introduce new ggplot geometries to help us along the way.&lt;/p&gt;
&lt;div id=&#34;variable-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variable types&lt;/h2&gt;
&lt;p&gt;We will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.&lt;/p&gt;
&lt;p&gt;When each entry in a vector comes from one of a small number of groups, we refer to the data as &lt;em&gt;categorical data&lt;/em&gt;. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as &lt;em&gt;ordinal&lt;/em&gt; data. In psychology, a number of different terms are used for this same idea.&lt;/p&gt;
&lt;p&gt;Examples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as population sizes, are discrete because they have to be integers—that’s how we count.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-describing-student-heights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: describing student heights&lt;/h2&gt;
&lt;p&gt;Here we consider an artificial problem to help us illustrate the underlying concepts.&lt;/p&gt;
&lt;p&gt;Pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the &lt;code&gt;heights&lt;/code&gt; data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
data(heights)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights.&lt;/p&gt;
&lt;div id=&#34;distribution-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Distribution function&lt;/h3&gt;
&lt;p&gt;It turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.&lt;/p&gt;
&lt;p&gt;The most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##    Female      Male 
## 0.2266667 0.7733333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This two-category &lt;em&gt;frequency table&lt;/em&gt; is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/state-region-distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cdf-intro&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cumulative distribution functions&lt;/h3&gt;
&lt;p&gt;Numerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of &lt;code&gt;68.503937007874&lt;/code&gt; inches and only one student reported a height &lt;code&gt;68.8976377952756&lt;/code&gt; inches. We assume that they converted from 174 and 175 centimeters, respectively.&lt;/p&gt;
&lt;p&gt;Statistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; for all possible values of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;. This function is called the cumulative distribution function (CDF). In statistics, the following notation is used:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ F(a) = \mbox{Pr}(x \leq a) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here is a plot of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; for the male height data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/ecdf-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar to what the frequency table does for categorical data, the CDF
defines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since &lt;span class=&#34;math inline&#34;&gt;\(F(66)=\)&lt;/span&gt; 0.1637931, or that 84% of the values are below 72, since &lt;span class=&#34;math inline&#34;&gt;\(F(72)=\)&lt;/span&gt; 0.841133,
and so on. In fact, we can report the proportion of values between any two heights, say &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, by computing &lt;span class=&#34;math inline&#34;&gt;\(F(b) - F(a)\)&lt;/span&gt;. This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.&lt;/p&gt;
&lt;p&gt;A final note: because CDFs can be defined mathematically—and absent any data—the word &lt;em&gt;empirical&lt;/em&gt; is added to make the distinction when data is used. We therefore use the term empirical CDF (eCDF).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;geometries-for-describing-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geometries for describing distributions&lt;/h2&gt;
&lt;p&gt;Now, we’ll introduce ggplot geometries useful for describing distributions (or for many other things).&lt;/p&gt;
&lt;div id=&#34;histograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histograms&lt;/h3&gt;
&lt;p&gt;Although the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? I doubt you can figure these out from glancing at the plot above. Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret.&lt;/p&gt;
&lt;p&gt;The simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: &lt;span class=&#34;math inline&#34;&gt;\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_histogram(binwidth = 1, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/height-histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we send this &lt;strong&gt;histogram&lt;/strong&gt; plot to some uninformed reader, she will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, this reader could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.&lt;/p&gt;
&lt;p&gt;What information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;geom_histogram&lt;/code&gt; layer only requires one aesthetic mapping - the x-axis. This is because the y-axis is computed from counts of the x-axis. Giving an aesthetic mapping to an additional variable for y will result in an error. Using an aesthetic mapping like &lt;code&gt;fill&lt;/code&gt; will work - it’ll give you two histograms on top of each other. Try it! Try setting the &lt;code&gt;alpha&lt;/code&gt; aesthetic to .5 (not an aesthetic mapping) so you can see both layers when they overlap.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;smoothed-density&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Smoothed density&lt;/h3&gt;
&lt;p&gt;Smooth density plots are aesthetically more appealing than histograms. &lt;code&gt;geom_density&lt;/code&gt; is the geometry that gives a smoothed density. Here is what a smooth density plot looks like for our heights data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_density(alpha = .2, fill= &amp;quot;#00BFC4&amp;quot;, color = &amp;#39;gray50&amp;#39;)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/example-of-smoothed-density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to &lt;em&gt;density&lt;/em&gt;. That is, the area under the curve will add up to 1, so we can read it like a probability density.&lt;/p&gt;
&lt;p&gt;To understand the smooth densities, we have to understand &lt;em&gt;estimates&lt;/em&gt;, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool.&lt;/p&gt;
&lt;p&gt;The main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.&lt;/p&gt;
&lt;p&gt;However, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/simulated-data-histogram-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/simulated-data-histogram-2-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts. We do this by using the double-dot object &lt;code&gt;..density..&lt;/code&gt;. Objects surrounded by &lt;code&gt;..&lt;/code&gt; are objects that are &lt;em&gt;calculated by ggplot&lt;/em&gt;. If we look at &lt;code&gt;?geom_histogram&lt;/code&gt;, and go down to “Computed variables”, we see that we could use &lt;code&gt;..count..&lt;/code&gt; to get “number of points in a bin”; &lt;code&gt;..ncount..&lt;/code&gt; for the count scaled to a max of 1; or &lt;code&gt;..ndensity..&lt;/code&gt; which scales the density to a max of 1 (which is a strange one). We can manually set the y aesthetic mapping, which defaults to &lt;code&gt;..count..&lt;/code&gt;, to &lt;code&gt;..density..&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x %&amp;gt;% ggplot(aes(x = height)) +
  geom_histogram(aes(y=..density..), binwidth = 0.1, color = &amp;quot;black&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.
## ℹ Please use `after_stat(density)` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/simulated-density-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, back to reality. We don’t have millions of measurements. In this concrete example, we have 812 and we can’t make a histogram with very small bins.&lt;/p&gt;
&lt;p&gt;We therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots (loosely) demonstrate the steps that the computer goes through to ultimately create a smooth density:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/smooth-density-2-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpreting-the-y-axis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpreting the y-axis&lt;/h3&gt;
&lt;p&gt;Note that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/area-under-curve-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The proportion of this area is about
0.3,
meaning that about
30%
of male heights are between 65 and 68 inches.&lt;/p&gt;
&lt;p&gt;By understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_density(alpha=.2, fill= &amp;quot;#00BFC4&amp;quot;, color = &amp;#39;black&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/example-of-smoothed-density-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the only aesthetic mapping is &lt;code&gt;x = height&lt;/code&gt;, while the fill and color are set as un-mapped aesthetics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;densities-permit-stratification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Densities permit stratification&lt;/h3&gt;
&lt;p&gt;As a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities make it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(height, fill=sex)) +
  geom_density(alpha = 0.2, color = &amp;#39;black&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/two-densities-one-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the right argument, &lt;code&gt;ggplot&lt;/code&gt; automatically shades the intersecting region with a different color.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The normal distribution&lt;/h2&gt;
&lt;p&gt;Histograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.&lt;/p&gt;
&lt;p&gt;The normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.&lt;/p&gt;
&lt;p&gt;Rather than using data, the normal distribution is defined with a mathematical formula. For any interval &lt;span class=&#34;math inline&#34;&gt;\((a,b)\)&lt;/span&gt;, the proportion of values in that interval can be computed using this formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{Pr}(a &amp;lt; x &amp;lt; b) = \int_a^b \frac{1}{\sqrt{2\pi}s} e^{-\frac{1}{2}\left( \frac{x-m}{s} \right)^2} \, dx\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. The rest of the symbols in the formula represent the interval ends that we determine, &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and known mathematical constants &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. These two parameters, &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, are referred to as the &lt;em&gt;average&lt;/em&gt; (also called the &lt;em&gt;mean&lt;/em&gt;) and the &lt;em&gt;standard deviation&lt;/em&gt; (SD) of the distribution, respectively.&lt;/p&gt;
&lt;p&gt;The distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/normal-distribution-density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.&lt;/p&gt;
&lt;p&gt;For a list of numbers contained in a vector &lt;code&gt;x&lt;/code&gt;, the average is defined as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- sum(x) / length(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the SD is defined as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- sqrt(sum((x-mu)^2) / length(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which can be interpreted as the average distance between values and their average.&lt;/p&gt;
&lt;p&gt;Let’s compute the values for the height for males which we will store in the object &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- heights$sex == &amp;quot;Male&amp;quot;
x &amp;lt;- heights$height[index]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pre-built functions &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sd&lt;/code&gt; (note that for reasons explained later, &lt;code&gt;sd&lt;/code&gt; divides by &lt;code&gt;length(x)-1&lt;/code&gt; rather than &lt;code&gt;length(x)&lt;/code&gt;) can be used here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- mean(x)
s &amp;lt;- sd(x)
c(average = m, sd = s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   average        sd 
## 69.314755  3.611024&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/data-and-normal-densities-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, we can ask the question “is our height data approximately normally distributed?”. The normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.&lt;/p&gt;
&lt;div id=&#34;standard-units&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standard units&lt;/h3&gt;
&lt;p&gt;For data that is approximately normally distributed, it is convenient to think in terms of &lt;em&gt;standard units&lt;/em&gt;. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value &lt;code&gt;x&lt;/code&gt; from a vector &lt;code&gt;X&lt;/code&gt;, we define the value of &lt;code&gt;x&lt;/code&gt; in standard units as &lt;code&gt;z = (x - m)/s&lt;/code&gt; with &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;s&lt;/code&gt; the average and standard deviation of &lt;code&gt;X&lt;/code&gt;, respectively. Why is this convenient?&lt;/p&gt;
&lt;p&gt;First look back at the formula for the normal distribution and note that what is being exponentiated is &lt;span class=&#34;math inline&#34;&gt;\(-z^2/2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; equivalent to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in standard units. Because the maximum of &lt;span class=&#34;math inline&#34;&gt;\(e^{-z^2/2}\)&lt;/span&gt; is when &lt;span class=&#34;math inline&#34;&gt;\(z=0\)&lt;/span&gt;, this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since &lt;span class=&#34;math inline&#34;&gt;\(- z^2/2\)&lt;/span&gt; is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (&lt;span class=&#34;math inline&#34;&gt;\(z=0\)&lt;/span&gt;), one of the largest (&lt;span class=&#34;math inline&#34;&gt;\(z \approx 2\)&lt;/span&gt;), one of the smallest (&lt;span class=&#34;math inline&#34;&gt;\(z \approx -2\)&lt;/span&gt;), or an extremely rare occurrence (&lt;span class=&#34;math inline&#34;&gt;\(z &amp;gt; 3\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(z &amp;lt; -3\)&lt;/span&gt;). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.&lt;/p&gt;
&lt;p&gt;In R, we can obtain standard units using the function &lt;code&gt;scale&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- scale(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to see how many men are within 2 SDs from the average, we simply type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(abs(z) &amp;lt; 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9495074&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-quantile-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quantile-quantile plots&lt;/h3&gt;
&lt;p&gt;A systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot). If our heights distribution is really normal, then the 10th percentile of our heights data should be the same as the 10th percentile of a theoretical normal, as should the 20th, 30th, 33rd, 37.5th, etc. percentiles.&lt;/p&gt;
&lt;p&gt;First let’s define the theoretical quantiles (percentiles) for the normal distribution. In statistics books we use the symbol &lt;span class=&#34;math inline&#34;&gt;\(\Phi(x)\)&lt;/span&gt; to define the function that gives us the probability of a standard normal distribution being smaller than &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. So, for example, &lt;span class=&#34;math inline&#34;&gt;\(\Phi(-1.96) = 0.025\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Phi(1.96) = 0.975\)&lt;/span&gt;. In R, we can evaluate &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; using the &lt;code&gt;pnorm&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(-1.96)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0249979&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The inverse function &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1}(x)\)&lt;/span&gt; gives us the &lt;em&gt;theoretical quantiles&lt;/em&gt; for the normal distribution. So, for example, &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1}(0.975) = 1.96\)&lt;/span&gt;. In R, we can evaluate the inverse of &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; using the &lt;code&gt;qnorm&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qnorm(0.975)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.959964&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution. We can do this using the &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sd&lt;/code&gt; arguments in the &lt;code&gt;pnorm&lt;/code&gt; and &lt;code&gt;qnorm&lt;/code&gt; function. For example, we can use &lt;code&gt;qnorm&lt;/code&gt; to determine quantiles of a distribution with a specific average and standard deviation&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qnorm(0.975, mean = 5, sd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8.919928&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the normal distribution, all the calculations related to quantiles are done without data, thus the name &lt;em&gt;theoretical quantiles&lt;/em&gt;. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we can define the quantile associated with any proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; as the &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; for which the proportion of values below &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Using R code, we can define &lt;code&gt;q&lt;/code&gt; as the value for which &lt;code&gt;mean(x &amp;lt;= q) = p&lt;/code&gt;. Notice that not all &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; have a &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; for which the proportion is exactly &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. There are several ways of defining the best &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; as discussed in the help for the &lt;code&gt;quantile&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;To give a quick example, for the male heights data, we have that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 69.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5147783&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So about 50% are shorter or equal to 69 inches. This implies that if &lt;span class=&#34;math inline&#34;&gt;\(p=0.50\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(q=69.5\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Define a vector of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; proportions &lt;span class=&#34;math inline&#34;&gt;\(p_1, p_2, \dots, p_m\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Define a vector of quantiles &lt;span class=&#34;math inline&#34;&gt;\(q_1, \dots, q_m\)&lt;/span&gt; for your data for the proportions &lt;span class=&#34;math inline&#34;&gt;\(p_1, \dots, p_m\)&lt;/span&gt;. We refer to these as the &lt;em&gt;sample quantiles&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Define a vector of theoretical quantiles for the proportions &lt;span class=&#34;math inline&#34;&gt;\(p_1, \dots, p_m\)&lt;/span&gt; for a normal distribution with the same average and standard deviation as the data.&lt;/li&gt;
&lt;li&gt;Plot the sample quantiles versus the theoretical quantiles.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s construct a QQ-plot using R code. Start by defining the vector of proportions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- seq(0.005, 0.995, 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain the quantiles from the data, we can use the &lt;code&gt;quantile&lt;/code&gt; function like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_quantiles &amp;lt;- quantile(x, p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the &lt;code&gt;qnorm&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theoretical_quantiles &amp;lt;- qnorm(p, mean = mean(x), sd = sd(x))

df = data.frame(sample_quantiles, theoretical_quantiles)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see if they match or not, we plot them against each other and draw the identity line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df, aes(x = theoretical_quantiles, y = sample_quantiles)) + 
  geom_point() + 
  geom_abline() # a 45-degree line &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/qqplot-original-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that this code becomes much cleaner if we use standard units:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_quantiles &amp;lt;- quantile(z, p)
theoretical_quantiles &amp;lt;- qnorm(p)
df2 =  data.frame(sample_quantiles, theoretical_quantiles)

ggplot(data = df2, aes(x = theoretical_quantiles, y = sample_quantiles)) + 
  geom_point() + 
  geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/qqplot-standardized-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above code is included to help describe QQ-plots. However, in practice it is easier to use the &lt;code&gt;ggplot&lt;/code&gt; geometry &lt;code&gt;geom_qq&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% filter(sex == &amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(sample = scale(height))) +
  geom_qq() +
  geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While for the illustration above we used 100 quantiles, the default from the &lt;code&gt;geom_qq&lt;/code&gt; function is to use as many quantiles as data points.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;percentiles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Percentiles&lt;/h3&gt;
&lt;p&gt;Before we move on, let’s define some terms that are commonly used in exploratory data analysis.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Percentiles&lt;/em&gt; are special cases of &lt;em&gt;quantiles&lt;/em&gt; that are commonly used. The percentiles are the quantiles you obtain when setting the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(0.01, 0.02, ..., 0.99\)&lt;/span&gt;. We call, for example, the case of &lt;span class=&#34;math inline&#34;&gt;\(p=0.25\)&lt;/span&gt; the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the &lt;em&gt;median&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For the normal distribution the &lt;em&gt;median&lt;/em&gt; and average are the same, but this is generally not the case.&lt;/p&gt;
&lt;p&gt;Another special case that receives a name are the &lt;em&gt;quartiles&lt;/em&gt;, which are obtained when setting &lt;span class=&#34;math inline&#34;&gt;\(p=0.25,0.50\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(0.75\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;other-geometries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ggplot2 geometries&lt;/h2&gt;
&lt;p&gt;Alhough we haven’t gone into detain about the &lt;strong&gt;ggplot2&lt;/strong&gt; package for data visualization, we now will briefly discuss some of the geometries involved in the plots above. We will discuss &lt;strong&gt;ggplot2&lt;/strong&gt; in (excruciating) detail later this week. For now, we will briefly demonstrate how to generate plots related to distributions.&lt;/p&gt;
&lt;div id=&#34;barplots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Barplots&lt;/h3&gt;
&lt;p&gt;To generate a barplot we can use the &lt;code&gt;geom_bar&lt;/code&gt; geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot(aes(region)) + geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/barplot-geom-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(murders)
tab &amp;lt;- murders %&amp;gt;%
  count(region) %&amp;gt;%
  mutate(proportion = n/sum(n))
tab&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          region  n proportion
## 1     Northeast  9  0.1764706
## 2         South 17  0.3333333
## 3 North Central 12  0.2352941
## 4          West 13  0.2549020&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We no longer want &lt;code&gt;geom_bar&lt;/code&gt; to count, but rather just plot a bar to the height provided by the &lt;code&gt;proportion&lt;/code&gt; variable. For this we need to provide &lt;code&gt;x&lt;/code&gt; (the categories) and &lt;code&gt;y&lt;/code&gt; (the values) and use the &lt;code&gt;stat=&#34;identity&#34;&lt;/code&gt; option. This tells R to just use the actual value in &lt;code&gt;proportion&lt;/code&gt; for the y aesthetic. This is only necessary when you’re telling R that you have your own field (&lt;code&gt;proportion&lt;/code&gt;) that you want to use instead of just the count.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab %&amp;gt;% ggplot(aes(x = region, y = proportion)) + geom_bar(stat = &amp;quot;identity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/region-freq-barplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;histograms-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histograms&lt;/h3&gt;
&lt;p&gt;To generate histograms we use &lt;code&gt;geom_histogram&lt;/code&gt;. By looking at the help file for this function, we learn that the only required argument is &lt;code&gt;x&lt;/code&gt;, the variable for which we will construct a histogram. We dropped the &lt;code&gt;x&lt;/code&gt; because we know it is the first argument.
The code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we run the code above, it gives us a message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;stat_bin()&lt;/code&gt; using &lt;code&gt;bins = 30&lt;/code&gt;. Pick better value with
&lt;code&gt;binwidth&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We previously used a bin size of 1 inch (of observed height), so the code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1, fill = &amp;quot;blue&amp;quot;, col = &amp;quot;black&amp;quot;) +
  labs(x = &amp;quot;Male heights in inches&amp;quot;, title = &amp;quot;Histogram&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/height-histogram-geom-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density plots&lt;/h3&gt;
&lt;p&gt;To create a smooth density, we use the &lt;code&gt;geom_density&lt;/code&gt;. To make a smooth density plot with the data previously shown as a histogram we can use this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To fill in with color, we can use the &lt;code&gt;fill&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_density(fill=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/ggplot-density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To change the smoothness of the density, we use the &lt;code&gt;adjust&lt;/code&gt; argument to multiply the default value by that &lt;code&gt;adjust&lt;/code&gt;. For example, if we want the bandwidth to be twice as big we use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) + 
  geom_density(fill=&amp;quot;blue&amp;quot;, adjust = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Boxplots&lt;/h3&gt;
&lt;p&gt;The geometry for boxplot is &lt;code&gt;geom_boxplot&lt;/code&gt;. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments &lt;code&gt;x&lt;/code&gt; as the categories, and &lt;code&gt;y&lt;/code&gt; as the values.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/female-male-boxplots-geom-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that our x-axis is a categorical variable. The order is determined by either the factor variable levels in &lt;code&gt;heights&lt;/code&gt; or, if no levels are set, in the order in which the &lt;code&gt;sex&lt;/code&gt; variable first encounters them. Later on, we’ll learn how to change the ordering.&lt;/p&gt;
&lt;p&gt;We can do much more with boxplots when we have more data. Right now, our &lt;code&gt;heights&lt;/code&gt; data has only two variables - &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt;. Let’s say we took the measurements over two different years - 2010 and 2020. That’s not in our data, so &lt;strong&gt;purely for exposition&lt;/strong&gt;, we’ll add it by randomly drawing a year for each observation. We’ll do this with &lt;code&gt;sample&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights = heights %&amp;gt;%
  dplyr::mutate(year = sample(x = c(2010, 2020), size = n(), replace = TRUE, prob = c(.5, .5)))

head(heights)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      sex height year
## 1   Male     75 2020
## 2   Male     70 2010
## 3   Male     68 2020
## 4   Male     74 2010
## 5   Male     61 2020
## 6 Female     65 2010&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s look at the boxplot of heights by sex, but broken out by year. We can do this by adding &lt;code&gt;year&lt;/code&gt; as an aesthetic mapping. Because our &lt;code&gt;year&lt;/code&gt; variable is an integer, R will start by thinking it’s a continuous numeric, but we want to treat it as a discrete variable. So, we wrap it in &lt;code&gt;as.factor()&lt;/code&gt; to force R to recognize it as a discrete variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% ggplot(aes(x = sex, y = height, fill = as.factor(year))) +
  geom_boxplot() +
  labs(fill = &amp;#39;Year&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/addYear-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we have each sex broken out by year! Since we randomly assigned year to our data (and didn’t actually take samples in two different decades), the distribution between years and within sex is nearly identical.&lt;/p&gt;
&lt;p&gt;What if we wanted to have &lt;code&gt;year&lt;/code&gt; on the x-axis, but then put the sex boxplots next to each other. This would let us compare the difference in heights by sex over the two sample years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% ggplot(aes(x = year, y = height, fill = sex)) + 
  geom_boxplot() +
  labs(fill = &amp;#39;Sex&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/addYear2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Woah. Wait. What? Remember, in our data, &lt;code&gt;class(heights$year)&lt;/code&gt; is numeric, so when we ask R to put &lt;code&gt;year&lt;/code&gt; on the x-axis, it thinks it’s plotting a number. It gives us a nonsense x-axis. How do we fix this? We force &lt;code&gt;as.factor(year)&lt;/code&gt; to tell R that yes, &lt;code&gt;year&lt;/code&gt; is a categorical variable. Note that we &lt;strong&gt;didn’t&lt;/strong&gt; have to use &lt;code&gt;as.factor(sex)&lt;/code&gt; - that’s because sex is already a categorical variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% ggplot(aes(x = as.factor(year), y = height, fill = sex)) + 
  geom_boxplot() +
  labs(fill = &amp;#39;Sex&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can see the height difference by sex, by year.&lt;/p&gt;
&lt;p&gt;We will explore more with boxplots and colors in our next lecture.&lt;/p&gt;
&lt;!-- ## Lecture Videos --&gt;
&lt;!-- The video from this lecture will be [available here around 3-6 days after class &lt;i class=&#34;fas fa-film&#34;&gt;&lt;/i&gt;](https://mediaspace.msu.edu/media/SSC442+-+Example+02+-+Jan+27th/1_m4no05nj?st=675) --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;try-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Try it!&lt;/h2&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Try it!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start by loading the &lt;strong&gt;dplyr&lt;/strong&gt; and &lt;strong&gt;ggplot2&lt;/strong&gt; library as well as the &lt;code&gt;murders&lt;/code&gt; and &lt;code&gt;heights&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(dslabs)
data(heights)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First, create a new variable in &lt;code&gt;murders&lt;/code&gt; that has &lt;code&gt;murders_per_capita&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders = murders %&amp;gt;%
  mutate(........)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Make a histogram of murders per capita. Use the default values for color and fill, but make sure you label the x-axis with a meaningful label.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make the same histogram, but set the fill aesthetic to MSU Green and the color to black.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do the same, but make it a smooth density plot&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, plot the smooth density but use a &lt;code&gt;fill&lt;/code&gt; aesthetic mapping so that each &lt;code&gt;region&lt;/code&gt;’s density is shown. Set a meaningful title on the legend, and make sure you make the density transparent so we can see all of the region’s densities (see &lt;code&gt;alpha&lt;/code&gt; aesthetic).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now, try making a boxplot to show the same data - the distribution across states of murders per capita by region. What is the average Northeastern state’s murder rate? What about the average Southern state?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Keep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Working with R and RStudio</title>
      <link>https://ssc442.netlify.app/example/00-example/</link>
      <pubDate>Thu, 11 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/00-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#frontmatter-spring-2024&#34; id=&#34;toc-frontmatter-spring-2024&#34;&gt;Frontmatter (Spring 2024)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#participation-extra-credit&#34; id=&#34;toc-participation-extra-credit&#34;&gt;Participation extra credit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assignment-exercises&#34; id=&#34;toc-assignment-exercises&#34;&gt;Assignment / Exercises&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dont-slide-into-my-dms&#34; id=&#34;toc-dont-slide-into-my-dms&#34;&gt;Don’t Slide into My DMs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-examples&#34; id=&#34;toc-introduction-to-examples&#34;&gt;Introduction to Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-basics&#34; id=&#34;toc-r-basics&#34;&gt;&lt;code&gt;R&lt;/code&gt; basics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#installing-r-and-r-studio-posit-and-review-resources&#34; id=&#34;toc-installing-r-and-r-studio-posit-and-review-resources&#34;&gt;Installing R and &lt;del&gt;R Studio&lt;/del&gt; Posit and Review Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-very-basics-of-r&#34; id=&#34;toc-the-very-basics-of-r&#34;&gt;The (very) basics of &lt;code&gt;R&lt;/code&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objects&#34; id=&#34;toc-objects&#34;&gt;Objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-workspace&#34; id=&#34;toc-the-workspace&#34;&gt;The workspace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functions&#34; id=&#34;toc-functions&#34;&gt;Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-prebuilt-objects&#34; id=&#34;toc-other-prebuilt-objects&#34;&gt;Other prebuilt objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variable-names&#34; id=&#34;toc-variable-names&#34;&gt;Variable names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#saving-your-workspace&#34; id=&#34;toc-saving-your-workspace&#34;&gt;Saving your workspace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivating-scripts&#34; id=&#34;toc-motivating-scripts&#34;&gt;Motivating scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#commenting-your-code&#34; id=&#34;toc-commenting-your-code&#34;&gt;Commenting your code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-frames&#34; id=&#34;toc-data-frames&#34;&gt;Data frames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examining-an-object&#34; id=&#34;toc-examining-an-object&#34;&gt;Examining an object&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-accessor&#34; id=&#34;toc-the-accessor&#34;&gt;The accessor: &lt;code&gt;$&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors-numerics-characters-and-logical&#34; id=&#34;toc-vectors-numerics-characters-and-logical&#34;&gt;Vectors: numerics, characters, and logical&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#factors&#34; id=&#34;toc-factors&#34;&gt;Factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lists&#34; id=&#34;toc-lists&#34;&gt;Lists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrices&#34; id=&#34;toc-matrices&#34;&gt;Matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors&#34; id=&#34;toc-vectors&#34;&gt;Vectors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#creating-vectors&#34; id=&#34;toc-creating-vectors&#34;&gt;Creating vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#names&#34; id=&#34;toc-names&#34;&gt;Names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sequences&#34; id=&#34;toc-sequences&#34;&gt;Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subsetting&#34; id=&#34;toc-subsetting&#34;&gt;Subsetting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coercion&#34; id=&#34;toc-coercion&#34;&gt;Coercion&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#not-availables-na&#34; id=&#34;toc-not-availables-na&#34;&gt;Not availables (NA)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sorting&#34; id=&#34;toc-sorting&#34;&gt;Sorting&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sort&#34; id=&#34;toc-sort&#34;&gt;&lt;code&gt;sort&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#order&#34; id=&#34;toc-order&#34;&gt;&lt;code&gt;order&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#max-and-which.max&#34; id=&#34;toc-max-and-which.max&#34;&gt;&lt;code&gt;max&lt;/code&gt; and &lt;code&gt;which.max&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rank&#34; id=&#34;toc-rank&#34;&gt;&lt;code&gt;rank&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#beware-of-recycling&#34; id=&#34;toc-beware-of-recycling&#34;&gt;Beware of recycling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vector-arithmetics&#34; id=&#34;toc-vector-arithmetics&#34;&gt;Vector arithmetics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rescaling-a-vector&#34; id=&#34;toc-rescaling-a-vector&#34;&gt;Rescaling a vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-vectors&#34; id=&#34;toc-two-vectors&#34;&gt;Two vectors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#indexing&#34; id=&#34;toc-indexing&#34;&gt;Indexing&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#subsetting-with-logicals&#34; id=&#34;toc-subsetting-with-logicals&#34;&gt;Subsetting with logicals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logical-operators&#34; id=&#34;toc-logical-operators&#34;&gt;Logical operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#which&#34; id=&#34;toc-which&#34;&gt;&lt;code&gt;which&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#in&#34; id=&#34;toc-in&#34;&gt;&lt;code&gt;%in%&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#further-help-with-r&#34; id=&#34;toc-further-help-with-r&#34;&gt;Further help with &lt;code&gt;R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;frontmatter-spring-2024&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Frontmatter (Spring 2024)&lt;/h1&gt;
&lt;p&gt;I like to use this spot to publish course announcements. Not so much for y’all, but more so I remember. If you see any announcements that don’t say “Spring 2024” there’s a good change it’s leftover from earlier course offerings.&lt;/p&gt;
&lt;div id=&#34;participation-extra-credit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Participation extra credit&lt;/h3&gt;
&lt;p&gt;A careful read of our syllabus under “class participation” will show that I do give extra credit for answering questions and (mainly) sharing completed R coding tasks. That is, we’ll walk through some examples, and when we hit a “try it”, I’ll ask you to give it a go. Then, after a few minutes, I’ll ask if anyone wants to share their answer. You get one point of participation extra credit. Five points is worth 1% of a grade boost, so these aren’t negligible points.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assignment-exercises&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assignment / Exercises&lt;/h3&gt;
&lt;p&gt;The Assignments page has all of our weekly lab assignments (including Week 1, due on Monday at 11:59pm). The assignments often have a preamble and some code that has to be used to set you up for the questions. &lt;strong&gt;The questions to be completed and turned in are under “Exercises” at the very end&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dont-slide-into-my-dms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Don’t Slide into My DMs&lt;/h3&gt;
&lt;p&gt;I don’t like to use the DM feature of Slack. Not because I don’t like getting DMs, but because the point of Slack is so we can all learn together. If you have a question, even if you’re worried it’s a silly question, then &lt;em&gt;others&lt;/em&gt; will, I promise, have the same question. Asking in the public channel will answer everyone’s question at once. Related to this: you should check the Slack to see if your question has already been asked.&lt;/p&gt;
&lt;p&gt;Another reason is that the TA will often be faster at responding than I will be, depending on day of week and time of day (I start my day before some of you even go to bed). If you DM me, the TA can’t see it and can’t reply.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction-to-examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to Examples&lt;/h1&gt;
&lt;p&gt;Examples in this class are designed to be presented in-class. Accordingly, the notes here are &lt;em&gt;not&lt;/em&gt; comprehensive. Instead, they are intended to guide students through&lt;/p&gt;
&lt;p&gt;I’m also aware that my writing is dry and lifeless. If you’re reading this online without the advantage of seeing it in person, don’t worry—I’ll be “funnier” in class.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;code&gt;R&lt;/code&gt; basics&lt;/h1&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;ALERT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The course content below should be considered a prerequisite for success. For those concerned about basics of &lt;code&gt;R&lt;/code&gt;, you absolutely must read this content and attempt the coding exercises. If you struggle to follow the content, please contact the professor or TA.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this class, we will be using &lt;code&gt;R&lt;/code&gt; software environment for all our analyses. You will learn &lt;code&gt;R&lt;/code&gt; and data analysis techniques simultaneously. To follow along you will therefore need access to &lt;code&gt;R&lt;/code&gt;. We also recommend the use of an &lt;em&gt;integrated development environment&lt;/em&gt; (IDE), such as RStudio, to save your work.&lt;/p&gt;
&lt;p&gt;Note that it is common for a course or workshop to offer access to an &lt;code&gt;R&lt;/code&gt; environment and an IDE through your web browser, as done by RStudio/Posit cloud&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. If you have access to such a resource, you don’t need to install &lt;code&gt;R&lt;/code&gt; and RStudio. However, if you intend on becoming a practicing data analyst, we highly recommend installing these tools on your computer&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. This is not hard.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;ALERT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RStudio, the separate &lt;em&gt;integrated development environment&lt;/em&gt; (IDE) commonly used to run &lt;code&gt;R&lt;/code&gt; has re-branded. I haven’t the foggiest as to why. The name is now &lt;strong&gt;Posit&lt;/strong&gt;, so if you clicked on a link that says &lt;strong&gt;RStudio&lt;/strong&gt; and you got &lt;strong&gt;Posit&lt;/strong&gt;, that’s fine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-r-and-r-studio-posit-and-review-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing R and &lt;del&gt;R Studio&lt;/del&gt; Posit and Review Resources&lt;/h2&gt;
&lt;p&gt;Both &lt;code&gt;R&lt;/code&gt; and &lt;del&gt;RStudio&lt;/del&gt; Posit are free and available online. If you have not yet done so, you’ll need to install both R and &lt;del&gt;RStudio&lt;/del&gt; Posit. See the &lt;a href=&#34;https://ssc442.netlify.app/resource/install/&#34;&gt;Installing page of our course resources&lt;/a&gt; for instructions. This will be part of your assignment for this week.&lt;/p&gt;
&lt;p&gt;Professor Kirkpatrick (the other instructor for this course) has created a video walkthrough for the basics of using &lt;code&gt;R&lt;/code&gt; for another course, but it is useful here. You can &lt;a href=&#34;https://mediaspace.msu.edu/media/R+Part+2a/1_kmszjw36&#34;&gt;see part A here (labeled “Part 2a”) here &lt;i class=&#34;fas fa-film&#34;&gt;&lt;/i&gt;&lt;/a&gt;] and &lt;a href=&#34;https://mediaspace.msu.edu/media/R+Part+2b/1_c0vf0i63&#34;&gt;part B here (labeled “Part 2b”) &lt;i class=&#34;fas fa-film&#34;&gt;&lt;/i&gt;&lt;/a&gt;. You should already be at this level of familiarity with R, but if you need a review, this is a good place to start.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-very-basics-of-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The (very) basics of &lt;code&gt;R&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Before we get started with the motivating dataset, we need to cover the very basics of &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;objects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Objects&lt;/h3&gt;
&lt;p&gt;Suppose a relatively math unsavvy student asks us for help solving several quadratic equations of the form &lt;span class=&#34;math inline&#34;&gt;\(ax^2+bx+c = 0\)&lt;/span&gt;. You—a savvy student—recall that the quadratic formula gives us the solutions:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \frac{-b + \sqrt{b^2 - 4ac}}{2a}\,\, \mbox{ and } \frac{-b - \sqrt{b^2 - 4ac}}{2a}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which of course depend on the values of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. That is, the quadratic equation represents a &lt;em&gt;function&lt;/em&gt; with three &lt;em&gt;arguments&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;One advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve &lt;span class=&#34;math inline&#34;&gt;\(x^2 + x -1 = 0\)&lt;/span&gt;, then we define:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 1
b &amp;lt;- 1
c &amp;lt;- -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which stores the values for later use. We use &lt;code&gt;&amp;lt;-&lt;/code&gt; to assign values to the variables.&lt;/p&gt;
&lt;p&gt;We can also assign values using &lt;code&gt;=&lt;/code&gt; instead of &lt;code&gt;&amp;lt;-&lt;/code&gt;, but some recommend against using &lt;code&gt;=&lt;/code&gt; to avoid confusion.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Copy and paste the code above into your console to define the three variables. Note that &lt;code&gt;R&lt;/code&gt; does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Throughout these written notes, you’ll have the most success if you continue to copy code into your own console.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To see the value stored in a variable, we simply ask &lt;code&gt;R&lt;/code&gt; to evaluate &lt;code&gt;a&lt;/code&gt; and it shows the stored value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A more explicit way to ask &lt;code&gt;R&lt;/code&gt; to show us the value stored in &lt;code&gt;a&lt;/code&gt; is using &lt;code&gt;print&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the term &lt;em&gt;object&lt;/em&gt; to describe stuff that is stored in &lt;code&gt;R&lt;/code&gt;. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-workspace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The workspace&lt;/h3&gt;
&lt;p&gt;As we define objects in the console, we are actually changing the &lt;em&gt;workspace&lt;/em&gt;. You can see all the variables saved in your workspace by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ls()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot;      &amp;quot;b&amp;quot;      &amp;quot;c&amp;quot;      &amp;quot;filter&amp;quot; &amp;quot;select&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that one of &lt;em&gt;my&lt;/em&gt; variables listed above comes from generating the graphs above). In &lt;del&gt;RStudio&lt;/del&gt; Posit, the &lt;em&gt;Environment&lt;/em&gt; tab shows the values:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/rstudio-environment.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We should see &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt;. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type &lt;code&gt;x&lt;/code&gt; you will receive the following message: &lt;code&gt;Error: object &#39;x&#39; not found&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.618034&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1.618034&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;p&gt;Once you define variables, the data analysis process can usually be described as a series of &lt;em&gt;functions&lt;/em&gt; applied to the data. &lt;code&gt;R&lt;/code&gt; includes several zillion predefined functions and most of the analysis pipelines we construct make extensive use of the built-in functions. But &lt;code&gt;R&lt;/code&gt;’s power comes from its scalability. We have access to (nearly) infinite functions via &lt;code&gt;install.packages&lt;/code&gt; and &lt;code&gt;library&lt;/code&gt;. As we go through the course, we will carefully note new functions we bring to each problem. For now, though, we will stick to the basics.&lt;/p&gt;
&lt;p&gt;Note that you’ve used a function already: you used the function &lt;code&gt;sqrt&lt;/code&gt; to solve the quadratic equation above. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.&lt;/p&gt;
&lt;p&gt;In general, we need to use parentheses to evaluate a function. If you type &lt;code&gt;ls&lt;/code&gt;, the function is not evaluated and instead &lt;code&gt;R&lt;/code&gt; shows you the code that defines the function. If you type &lt;code&gt;ls()&lt;/code&gt; the function is evaluated and, as seen above, we see objects in the workspace.&lt;/p&gt;
&lt;p&gt;Unlike &lt;code&gt;ls&lt;/code&gt;, most functions require one or more &lt;em&gt;arguments&lt;/em&gt;. Below is an example of how we assign an object to the argument of the function &lt;code&gt;log&lt;/code&gt;. Remember that we earlier defined &lt;code&gt;a&lt;/code&gt; to be 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.079442&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find out what the function expects and what it does by reviewing the very useful manuals included in &lt;code&gt;R&lt;/code&gt;. You can get help by using the &lt;code&gt;help&lt;/code&gt; function like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(&amp;quot;log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For most functions, we can also use this shorthand:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?log&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The help page will show you what arguments the function is expecting. For example, &lt;code&gt;log&lt;/code&gt; needs &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;base&lt;/code&gt; to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with &lt;code&gt;=&lt;/code&gt;. Defining these is optional.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; For example, the base of the function &lt;code&gt;log&lt;/code&gt; defaults to &lt;code&gt;base = exp(1)&lt;/code&gt;—that is, &lt;code&gt;log&lt;/code&gt; evaluates the natural log by default.&lt;/p&gt;
&lt;p&gt;If you want a quick look at the arguments without opening the help system, you can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;args(log)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (x, base = exp(1)) 
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can change the default values by simply assigning another object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(8, base = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we have not been specifying the argument &lt;code&gt;x&lt;/code&gt; as such:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(x = 8, base = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code works, but we can save ourselves some typing: if no argument name is used, &lt;code&gt;R&lt;/code&gt; assumes you are entering arguments in the order shown in the help file or by &lt;code&gt;args&lt;/code&gt;. So by not using the names, it assumes the arguments are &lt;code&gt;x&lt;/code&gt; followed by &lt;code&gt;base&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(8,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If using the arguments’ names, then we can include them in whatever order we want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(base = 2, x = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To specify arguments, we must use &lt;code&gt;=&lt;/code&gt;, and cannot use &lt;code&gt;&amp;lt;-&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 ^ 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the arithmetic operators by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(&amp;quot;+&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?&amp;quot;+&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the relational operators by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(&amp;quot;&amp;gt;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?&amp;quot;&amp;gt;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-prebuilt-objects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other prebuilt objects&lt;/h3&gt;
&lt;p&gt;There are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; will show you Mauna Loa atmospheric &lt;span class=&#34;math inline&#34;&gt;\(CO^2\)&lt;/span&gt; concentration data.&lt;/p&gt;
&lt;p&gt;Other prebuilt objects are mathematical quantities, such as the constant &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.141593&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Inf+1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Inf&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;variable-names&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variable names&lt;/h3&gt;
&lt;p&gt;We have used the letters &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt; as variable names, but variable names can be almost anything. Some basic rules in &lt;code&gt;R&lt;/code&gt; are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in &lt;code&gt;R&lt;/code&gt;. For example, don’t name one of your variables &lt;code&gt;install.packages&lt;/code&gt; by typing something like &lt;code&gt;install.packages &amp;lt;- 2&lt;/code&gt;. Usually, &lt;code&gt;R&lt;/code&gt; is smart enough to prevent you from doing such nonsense, but it’s important to develop good habits.&lt;/p&gt;
&lt;p&gt;A nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solution_1 &amp;lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a)
solution_2 &amp;lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more advice, we highly recommend studying (Hadley Wickham’s style guide)[&lt;a href=&#34;http://adv-r.had.co.nz/Style.html&#34; class=&#34;uri&#34;&gt;http://adv-r.had.co.nz/Style.html&lt;/a&gt;].&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;saving-your-workspace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Saving your workspace&lt;/h3&gt;
&lt;p&gt;Values remain in the workspace until you end your session or erase them with the function &lt;code&gt;rm&lt;/code&gt;. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.&lt;/p&gt;
&lt;p&gt;We actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function &lt;code&gt;save&lt;/code&gt; or &lt;code&gt;save.image&lt;/code&gt;. To load, use the function &lt;code&gt;load&lt;/code&gt;. When saving a workspace, we recommend the suffix &lt;code&gt;rda&lt;/code&gt; or &lt;code&gt;RData&lt;/code&gt;. In RStudio, you can also do this by navigating to the &lt;em&gt;Session&lt;/em&gt; tab and choosing &lt;em&gt;Save Workspace as&lt;/em&gt;. You can later load it using the &lt;em&gt;Load Workspace&lt;/em&gt; options in the same tab.
You can read the help pages on &lt;code&gt;save&lt;/code&gt;, &lt;code&gt;save.image&lt;/code&gt;, and &lt;code&gt;load&lt;/code&gt; to learn more.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivating-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Motivating scripts&lt;/h3&gt;
&lt;p&gt;To solve another equation such as &lt;span class=&#34;math inline&#34;&gt;\(3x^2 + 2x -1\)&lt;/span&gt;, we can copy and paste the code above and then redefine the variables and recompute the solution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 3
b &amp;lt;- 2
c &amp;lt;- -1
(-b + sqrt(b^2 - 4*a*c)) / (2*a)
(-b - sqrt(b^2 - 4*a*c)) / (2*a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.&lt;/p&gt;
&lt;p&gt;The answer you get from the 4th and 5th lines will depend on the values of &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt;. &lt;strong&gt;If&lt;/strong&gt; you were to type new numbers directly into your console: &lt;code&gt;c = 5.33&lt;/code&gt; then re-run the last two lines, you will get a different answer. Your R “environment” is affected by what is run from a script &lt;strong&gt;and&lt;/strong&gt; by what you type in the console. It is good (and necessary) practice to write all your code in a script (or in your Rmarkdown document), and run from the script. Always. Periodically running a script fresh from the start (clearing everything out of the environment first) is a good idea as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;commenting-your-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Commenting your code&lt;/h3&gt;
&lt;p&gt;If a line of &lt;code&gt;R&lt;/code&gt; code starts with the symbol &lt;code&gt;#&lt;/code&gt;, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Code to compute solution to quadratic equation of the form ax^2 + bx + c
## define the variables
a &amp;lt;- 3
b &amp;lt;- 2
c &amp;lt;- -1

## now compute the solution
(-b + sqrt(b^2 - 4*a*c)) / (2*a)
(-b - sqrt(b^2 - 4*a*c)) / (2*a)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What is the sum of the first 100 positive integers? The formula for the sum of integers &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n(n+1)/2\)&lt;/span&gt;. Define &lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt; and then use &lt;code&gt;R&lt;/code&gt; to compute the sum of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt; using the formula. What is the sum?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the same formula to compute the sum of the integers from 1 through 1,000.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Look at the result of typing the following code into R:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
x &amp;lt;- seq(1, n)
sum(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on the result, what do you think the functions &lt;code&gt;seq&lt;/code&gt; and &lt;code&gt;sum&lt;/code&gt; do? You can use &lt;code&gt;help&lt;/code&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;sum&lt;/code&gt; creates a list of numbers and &lt;code&gt;seq&lt;/code&gt; adds them up.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;seq&lt;/code&gt; creates a list of numbers and &lt;code&gt;sum&lt;/code&gt; adds them up.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;seq&lt;/code&gt; creates a random list and &lt;code&gt;sum&lt;/code&gt; computes the sum of 1 through 1,000.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sum&lt;/code&gt; always returns the same number.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type &lt;code&gt;sqrt(4)&lt;/code&gt;, we evaluate the &lt;code&gt;sqrt&lt;/code&gt; function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which of the following will always return the numeric value stored in &lt;code&gt;x&lt;/code&gt;? You can try out examples and use the help system if you want.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;log(10^x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;log10(x^10)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;log(exp(x))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;exp(log(x, base = 2))&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;## Data types&lt;/p&gt;
&lt;p&gt;Variables in &lt;code&gt;R&lt;/code&gt; can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function &lt;code&gt;class&lt;/code&gt; helps us determine what type of object we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 2
class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To work efficiently in R, it is important to learn the different types of variables and what we can do with these.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-frames&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data frames&lt;/h3&gt;
&lt;p&gt;Up to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in &lt;code&gt;R&lt;/code&gt; is in a &lt;em&gt;data frame&lt;/em&gt;. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.&lt;/p&gt;
&lt;p&gt;A large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the &lt;strong&gt;dslabs&lt;/strong&gt; library and loading the &lt;code&gt;murders&lt;/code&gt; dataset using the &lt;code&gt;data&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dslabs&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:gapminder&amp;#39;:
## 
##     gapminder&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see that this is in fact a data frame, we type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;examining-an-object&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Examining an object&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;str&lt;/code&gt; is useful for finding out more about the structure of an object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:	51 obs. of  5 variables:
## $ state : chr &amp;quot;Alabama&amp;quot; &amp;quot;Alaska&amp;quot; &amp;quot;Arizona&amp;quot; &amp;quot;Arkansas&amp;quot; ...
## $ abb : chr &amp;quot;AL&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;AR&amp;quot; ...
## $ region : Factor w/ 4 levels &amp;quot;Northeast&amp;quot;,&amp;quot;South&amp;quot;,..: 2 4 4 2 4 4 1 2 2 2 ...
## $ population: num 4779736 710231 6392017 2915918 37253956 ...
## $ total : num 135 19 232 93 1257 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function &lt;code&gt;head&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total
## 1    Alabama  AL  South    4779736   135
## 2     Alaska  AK   West     710231    19
## 3    Arizona  AZ   West    6392017   232
## 4   Arkansas  AR  South    2915918    93
## 5 California  CA   West   37253956  1257
## 6   Colorado  CO   West    5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this dataset, each state is considered an observation and five variables are reported for each state.&lt;/p&gt;
&lt;p&gt;Before we go any further in answering our original question about different states, let’s learn more about the components of this object.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-accessor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The accessor: &lt;code&gt;$&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;For our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator &lt;code&gt;$&lt;/code&gt; in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$population&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097   897934
##  [9]   601723 19687653  9920000  1360301  1567582 12830632  6483802  3046355
## [17]  2853118  4339367  4533372  1328361  5773552  6547629  9883640  5303925
## [25]  2967297  5988927   989415  1826341  2700551  1316470  8791894  2059179
## [33] 19378102  9535483   672591 11536504  3751351  3831074 12702379  1052567
## [41]  4625364   814180  6346105 25145561  2763885   625741  8001024  6724540
## [49]  1852994  5686986   563626&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But how did we know to use &lt;code&gt;population&lt;/code&gt;? Previously, by applying the function &lt;code&gt;str&lt;/code&gt; to the object &lt;code&gt;murders&lt;/code&gt;, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;state&amp;quot;      &amp;quot;abb&amp;quot;        &amp;quot;region&amp;quot;     &amp;quot;population&amp;quot; &amp;quot;total&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to know that the order of the entries in &lt;code&gt;murders$population&lt;/code&gt; preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: &lt;code&gt;R&lt;/code&gt; comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing &lt;code&gt;murders$p&lt;/code&gt; then hitting the &lt;kbd&gt;tab&lt;/kbd&gt; key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors-numerics-characters-and-logical&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vectors: numerics, characters, and logical&lt;/h3&gt;
&lt;p&gt;The object &lt;code&gt;murders$population&lt;/code&gt; is not one number but several. We call these types of objects &lt;em&gt;vectors&lt;/em&gt;. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function &lt;code&gt;length&lt;/code&gt; tells you how many entries are in the vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pop &amp;lt;- murders$population
length(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This particular vector is &lt;em&gt;numeric&lt;/em&gt; since population sizes are numbers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a numeric vector, every entry must be a number.&lt;/p&gt;
&lt;p&gt;To store character strings, vectors can also be of class &lt;em&gt;character&lt;/em&gt;. For example, the state names are characters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders$state)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with numeric vectors, all entries in a character vector need to be a character.&lt;/p&gt;
&lt;p&gt;Another important type of vectors are &lt;em&gt;logical vectors&lt;/em&gt;. These must be either &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- 3 == 2
z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;logical&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the &lt;code&gt;==&lt;/code&gt; is a relational operator asking if 3 is equal to 2. In &lt;code&gt;R&lt;/code&gt;, if you just use one &lt;code&gt;=&lt;/code&gt;, you actually assign a variable, but if you use two &lt;code&gt;==&lt;/code&gt; you test for equality. Yet another reason to avoid assigning via &lt;code&gt;=&lt;/code&gt;… it can get confusing and typos can really mess things up.&lt;/p&gt;
&lt;p&gt;You can see the other &lt;em&gt;relational operators&lt;/em&gt; by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?Comparison&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In future sections, you will see how useful relational operators can be.&lt;/p&gt;
&lt;p&gt;We discuss more important features of vectors after the next set of exercises.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Mathematically, the values in &lt;code&gt;pop&lt;/code&gt; are integers and there is an integer class in &lt;code&gt;R&lt;/code&gt;. However, by default, numbers are assigned class numeric even when they are round integers. For example, &lt;code&gt;class(1)&lt;/code&gt; returns numeric. You can turn them into class integer with the &lt;code&gt;as.integer()&lt;/code&gt; function or by adding an &lt;code&gt;L&lt;/code&gt; like this: &lt;code&gt;1L&lt;/code&gt;. Note the class by typing: &lt;code&gt;class(1L)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factors&lt;/h3&gt;
&lt;p&gt;In the &lt;code&gt;murders&lt;/code&gt; dataset, we might expect the region to also be a character vector. However, it is not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders$region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a &lt;em&gt;factor&lt;/em&gt;. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the &lt;code&gt;levels&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(murders$region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Northeast&amp;quot;     &amp;quot;South&amp;quot;         &amp;quot;North Central&amp;quot; &amp;quot;West&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the background, &lt;code&gt;R&lt;/code&gt; stores these &lt;em&gt;levels&lt;/em&gt; as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. It is also useful for computational reasons we’ll explore later.&lt;/p&gt;
&lt;p&gt;Note that the levels have an order that is different from the order of appearance in the factor object. The default in &lt;code&gt;R&lt;/code&gt; is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the &lt;code&gt;levels&lt;/code&gt; argument when creating the factor with the &lt;code&gt;factor&lt;/code&gt; function. For example, in the murders dataset regions are ordered from east to west. The function &lt;code&gt;reorder&lt;/code&gt; lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.&lt;/p&gt;
&lt;p&gt;Suppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the &lt;code&gt;reorder&lt;/code&gt; and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;region &amp;lt;- murders$region
value &amp;lt;- murders$total
region &amp;lt;- reorder(region, value, FUN = sum)
levels(region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Northeast&amp;quot;     &amp;quot;North Central&amp;quot; &amp;quot;West&amp;quot;          &amp;quot;South&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new order is in agreement with the fact that the Northeast has the least murders and the South has the most.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lists&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lists&lt;/h3&gt;
&lt;p&gt;Data frames are a special case of &lt;em&gt;lists&lt;/em&gt;. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $name
## [1] &amp;quot;John Doe&amp;quot;
## 
## $student_id
## [1] 1234
## 
## $grades
## [1] 95 82 91 97 93
## 
## $final_grade
## [1] &amp;quot;A&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(record)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with data frames, you can extract the components of a list with the accessor &lt;code&gt;$&lt;/code&gt;. In fact, data frames are a type of list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record$student_id&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use double square brackets (&lt;code&gt;[[&lt;/code&gt;) like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record[[&amp;quot;student_id&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should get used to the fact that in &lt;code&gt;R&lt;/code&gt; there are often several ways to do the same thing. such as accessing entries.&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You might also encounter lists without variable names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;John Doe&amp;quot;
## 
## [[2]]
## [1] 1234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If a list does not have names, you cannot extract the elements with &lt;code&gt;$&lt;/code&gt;, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record2[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;John Doe&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We won’t be using lists until later, but you might encounter one in your own exploration of &lt;code&gt;R&lt;/code&gt;. For this reason, we show you some basics here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrices&lt;/h3&gt;
&lt;p&gt;Matrices are another type of object that are common in &lt;code&gt;R&lt;/code&gt;. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.&lt;/p&gt;
&lt;p&gt;Yet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this class, but much of what happens in the background when you perform a data analysis involves matrices. We describe them briefly here since some of the functions we will learn return matrices.&lt;/p&gt;
&lt;p&gt;We can define a matrix using the &lt;code&gt;matrix&lt;/code&gt; function. We need to specify the number of rows and columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat &amp;lt;- matrix(1:12, 4, 3)
mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    5    9
## [2,]    2    6   10
## [3,]    3    7   11
## [4,]    4    8   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can access specific entries in a matrix using square brackets (&lt;code&gt;[&lt;/code&gt;). If you want the second row, third column, you use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[2, 3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want the entire second row, you leave the column spot empty:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[2, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  2  6 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that this returns a vector, not a matrix.&lt;/p&gt;
&lt;p&gt;Similarly, if you want the entire third column, you leave the row spot empty:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[, 3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  9 10 11 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is also a vector, not a matrix.&lt;/p&gt;
&lt;p&gt;You can access more than one column or more than one row if you like. This will give you a new matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[, 2:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    5    9
## [2,]    6   10
## [3,]    7   11
## [4,]    8   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can subset both rows and columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[1:2, 2:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    5    9
## [2,]    6   10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can convert matrices into data frames using the function &lt;code&gt;as.data.frame&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.data.frame(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   V1 V2 V3
## 1  1  5  9
## 2  2  6 10
## 3  3  7 11
## 4  4  8 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use single square brackets (&lt;code&gt;[&lt;/code&gt;) to access rows and columns of a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;murders&amp;quot;)
murders[25, 1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Mississippi&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders[2:3, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     state abb region population total
## 2  Alaska  AK   West     710231    19
## 3 Arizona  AZ   West    6392017   232&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the US murders dataset.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the function &lt;code&gt;str&lt;/code&gt; to examine the structure of the &lt;code&gt;murders&lt;/code&gt; object. Which of the following best describes the variables represented in this data frame?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The 51 states.&lt;/li&gt;
&lt;li&gt;The murder rates for all 50 states and DC.&lt;/li&gt;
&lt;li&gt;The state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;str&lt;/code&gt; shows no relevant information.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What are the column names used by the data frame for these five variables?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the accessor &lt;code&gt;$&lt;/code&gt; to extract the state abbreviations and assign them to the object &lt;code&gt;a&lt;/code&gt;. What is the class of this object?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the square brackets to extract the state abbreviations and assign them to the object &lt;code&gt;b&lt;/code&gt;. Use the &lt;code&gt;identical&lt;/code&gt; function to determine if &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are the same.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We saw that the &lt;code&gt;region&lt;/code&gt; column stores a factor. You can corroborate this by typing:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders$region)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With one line of code, use the function &lt;code&gt;levels&lt;/code&gt; and &lt;code&gt;length&lt;/code&gt; to determine the number of regions defined by this dataset.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The function &lt;code&gt;table&lt;/code&gt; takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vectors&lt;/h2&gt;
&lt;p&gt;In R, the most basic objects available to store data are &lt;em&gt;vectors&lt;/em&gt;. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.&lt;/p&gt;
&lt;div id=&#34;creating-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating vectors&lt;/h3&gt;
&lt;p&gt;We can create vectors using the function &lt;code&gt;c&lt;/code&gt;, which stands for &lt;em&gt;concatenate&lt;/em&gt;. We use &lt;code&gt;c&lt;/code&gt; to concatenate entries in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(380, 124, 818)
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 380 124 818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country &amp;lt;- c(&amp;quot;italy&amp;quot;, &amp;quot;canada&amp;quot;, &amp;quot;egypt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt; you can also use single quotes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country &amp;lt;- c(&amp;#39;italy&amp;#39;, &amp;#39;canada&amp;#39;, &amp;#39;egypt&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But be careful not to confuse the single quote ’ with the &lt;em&gt;back quote&lt;/em&gt;, which shares a keyboard key with &lt;kbd&gt;~&lt;/kbd&gt;.&lt;/p&gt;
&lt;p&gt;By now you should know that if you type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country &amp;lt;- c(italy, canada, egypt)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;you receive an error because the variables &lt;code&gt;italy&lt;/code&gt;, &lt;code&gt;canada&lt;/code&gt;, and &lt;code&gt;egypt&lt;/code&gt; are not defined. If we do not use the quotes, &lt;code&gt;R&lt;/code&gt; looks for variables with those names and returns an error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;names&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Names&lt;/h3&gt;
&lt;p&gt;Sometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(italy = 380, canada = 124, egypt = 818)
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada  egypt 
##    380    124    818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The object &lt;code&gt;codes&lt;/code&gt; continues to be a numeric vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(codes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but with names:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(codes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;italy&amp;quot;  &amp;quot;canada&amp;quot; &amp;quot;egypt&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the use of strings without quotes looks confusing, know that you can use the quotes as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(&amp;quot;italy&amp;quot; = 380, &amp;quot;canada&amp;quot; = 124, &amp;quot;egypt&amp;quot; = 818)
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada  egypt 
##    380    124    818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no difference between this function call and the previous one. This is one of the many ways in which &lt;code&gt;R&lt;/code&gt; is quirky compared to other languages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sequences&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sequences&lt;/h3&gt;
&lt;p&gt;Another useful function for creating vectors generates sequences:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 10, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 3 5 7 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want consecutive integers, we can use the following shorthand:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we use these functions, &lt;code&gt;R&lt;/code&gt; produces integers, not numerics, because they are typically used to index something:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, if we create a sequence including non-integers, the class changes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(seq(1, 10, 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;subsetting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting&lt;/h3&gt;
&lt;p&gt;We use square brackets to access specific elements of a vector. For the vector &lt;code&gt;codes&lt;/code&gt; we defined above, we can access the second element using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## canada 
##    124&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can get more than one entry by using a multi-entry vector as an index:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[c(1,3)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## italy egypt 
##   380   818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sequences defined above are particularly useful if we want to access, say, the first two elements:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada 
##    380    124&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the elements have names, we can also access the entries using these names. Below are two examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[&amp;quot;canada&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## canada 
##    124&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[c(&amp;quot;egypt&amp;quot;,&amp;quot;italy&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## egypt italy 
##   818   380&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;coercion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coercion&lt;/h2&gt;
&lt;p&gt;In general, &lt;em&gt;coercion&lt;/em&gt; is an attempt by &lt;code&gt;R&lt;/code&gt; to be flexible with data types. When an entry does not match the expected, some of the prebuilt &lt;code&gt;R&lt;/code&gt; functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand &lt;em&gt;coercion&lt;/em&gt; can drive programmers crazy when attempting to code in &lt;code&gt;R&lt;/code&gt; since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.&lt;/p&gt;
&lt;p&gt;We said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(1, &amp;quot;canada&amp;quot;, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we don’t get one, not even a warning! What happened? Look at &lt;code&gt;x&lt;/code&gt; and its class:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1&amp;quot;      &amp;quot;canada&amp;quot; &amp;quot;3&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R &lt;em&gt;coerced&lt;/em&gt; the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings &lt;code&gt;&#34;1&#34;&lt;/code&gt; and “&lt;code&gt;3&lt;/code&gt;”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;R also offers functions to change from one type to another. For example, you can turn numbers into characters with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:5
y &amp;lt;- as.character(x)
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; &amp;quot;5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can turn it back with &lt;code&gt;as.numeric&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is actually quite useful since datasets that include numbers as character strings are common.&lt;/p&gt;
&lt;div id=&#34;not-availables-na&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Not availables (NA)&lt;/h3&gt;
&lt;p&gt;This “topic” seems to be wholly unappreciated and it has been our experience that students often panic when encountering an &lt;code&gt;NA&lt;/code&gt;. This often happens when a function tries to coerce one type to another and encounters an impossible case. In such circumstances, &lt;code&gt;R&lt;/code&gt; usually gives us a warning and turns the entry into a special value called an &lt;code&gt;NA&lt;/code&gt; (for “not available”). For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(&amp;quot;1&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;3&amp;quot;)
as.numeric(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1 NA  3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R does not have any guesses for what number you want when you type &lt;code&gt;b&lt;/code&gt;, so it does not try.&lt;/p&gt;
&lt;p&gt;While coercion is a common case leading to &lt;code&gt;NA&lt;/code&gt;s, you’ll see them in nearly every real-world dataset. Most often, you will encounter the &lt;code&gt;NA&lt;/code&gt;s as a stand-in for missing data. Again, this a common problem in real-world datasets and you need to be aware that it will come up.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sorting&lt;/h2&gt;
&lt;p&gt;Now that we have mastered some basic &lt;code&gt;R&lt;/code&gt; knowledge (ha!), let’s try to gain some insights into the safety of different states in the context of gun murders.&lt;/p&gt;
&lt;div id=&#34;sort&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;sort&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Say we want to rank the states from least to most gun murders. The function &lt;code&gt;sort&lt;/code&gt; sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
sort(murders$total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]    2    4    5    5    7    8   11   12   12   16   19   21   22   27   32
## [16]   36   38   53   63   65   67   84   93   93   97   97   99  111  116  118
## [31]  120  135  142  207  219  232  246  250  286  293  310  321  351  364  376
## [46]  413  457  517  669  805 1257&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;order&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;order&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;order&lt;/code&gt; is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(31, 4, 15, 92, 65)
sort(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  4 15 31 65 92&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rather than sort the input vector, the function &lt;code&gt;order&lt;/code&gt; returns the index that sorts input vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- order(x)
x[index]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  4 15 31 65 92&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the same output as that returned by &lt;code&gt;sort(x)&lt;/code&gt;. If we look at this index, we see why it works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31  4 15 92 65&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;order(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2 3 1 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second entry of &lt;code&gt;x&lt;/code&gt; is the smallest, so &lt;code&gt;order(x)&lt;/code&gt; starts with &lt;code&gt;2&lt;/code&gt;. The next smallest is the third entry, so the second entry is &lt;code&gt;3&lt;/code&gt; and so on.&lt;/p&gt;
&lt;p&gt;How does this help us order the states by murders? First, remember that the entries of vectors you access with &lt;code&gt;$&lt;/code&gt; follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$state[1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Alabama&amp;quot;    &amp;quot;Alaska&amp;quot;     &amp;quot;Arizona&amp;quot;    &amp;quot;Arkansas&amp;quot;   &amp;quot;California&amp;quot;
## [6] &amp;quot;Colorado&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$abb[1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;AL&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;AR&amp;quot; &amp;quot;CA&amp;quot; &amp;quot;CO&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- order(murders$total)
murders$abb[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;VT&amp;quot; &amp;quot;ND&amp;quot; &amp;quot;NH&amp;quot; &amp;quot;WY&amp;quot; &amp;quot;HI&amp;quot; &amp;quot;SD&amp;quot; &amp;quot;ME&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;MT&amp;quot; &amp;quot;RI&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;IA&amp;quot; &amp;quot;UT&amp;quot; &amp;quot;WV&amp;quot; &amp;quot;NE&amp;quot;
## [16] &amp;quot;OR&amp;quot; &amp;quot;DE&amp;quot; &amp;quot;MN&amp;quot; &amp;quot;KS&amp;quot; &amp;quot;CO&amp;quot; &amp;quot;NM&amp;quot; &amp;quot;NV&amp;quot; &amp;quot;AR&amp;quot; &amp;quot;WA&amp;quot; &amp;quot;CT&amp;quot; &amp;quot;WI&amp;quot; &amp;quot;DC&amp;quot; &amp;quot;OK&amp;quot; &amp;quot;KY&amp;quot; &amp;quot;MA&amp;quot;
## [31] &amp;quot;MS&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;SC&amp;quot; &amp;quot;TN&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;NJ&amp;quot; &amp;quot;VA&amp;quot; &amp;quot;NC&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;OH&amp;quot; &amp;quot;MO&amp;quot; &amp;quot;LA&amp;quot; &amp;quot;IL&amp;quot; &amp;quot;GA&amp;quot;
## [46] &amp;quot;MI&amp;quot; &amp;quot;PA&amp;quot; &amp;quot;NY&amp;quot; &amp;quot;FL&amp;quot; &amp;quot;TX&amp;quot; &amp;quot;CA&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the above, California had the most murders.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;max-and-which.max&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;max&lt;/code&gt; and &lt;code&gt;which.max&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If we are only interested in the entry with the largest value, we can use &lt;code&gt;max&lt;/code&gt; for the value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(murders$total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1257&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and &lt;code&gt;which.max&lt;/code&gt; for the index of the largest value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;i_max &amp;lt;- which.max(murders$total)
murders$state[i_max]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;California&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the minimum, we can use &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;which.min&lt;/code&gt; in the same way.&lt;/p&gt;
&lt;p&gt;Does this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: &lt;code&gt;rank&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rank&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;rank&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Although not as frequently used as &lt;code&gt;order&lt;/code&gt; and &lt;code&gt;sort&lt;/code&gt;, the function &lt;code&gt;rank&lt;/code&gt; is also related to order and can be useful.
For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(31, 4, 15, 92, 65)
rank(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 1 2 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To summarize, let’s look at the results of the three functions we have introduced:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
original
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sort
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rank
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;beware-of-recycling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Beware of recycling&lt;/h3&gt;
&lt;p&gt;Another common source of unnoticed errors in &lt;code&gt;R&lt;/code&gt; is the use of &lt;em&gt;recycling&lt;/em&gt;. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(1,2,3)
y &amp;lt;- c(10, 20, 30, 40, 50, 60, 70)
x+y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in x + y: longer object length is not a multiple of shorter object
## length&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11 22 33 41 52 63 71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do get a warning, but no error. For the output, &lt;code&gt;R&lt;/code&gt; has recycled the numbers in &lt;code&gt;x&lt;/code&gt;. Notice the last digit of numbers in the output.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For these exercises we will use the US murders dataset. Make sure you load it prior to starting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;$&lt;/code&gt; operator to access the population size data and store it as the object &lt;code&gt;pop&lt;/code&gt;. Then use the &lt;code&gt;sort&lt;/code&gt; function to redefine &lt;code&gt;pop&lt;/code&gt; so that it is sorted. Finally, use the &lt;code&gt;[&lt;/code&gt; operator to report the smallest population size.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use &lt;code&gt;order&lt;/code&gt; instead of &lt;code&gt;sort&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can actually perform the same operation as in the previous exercise using the function &lt;code&gt;which.min&lt;/code&gt;. Write one line of code that does this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable &lt;code&gt;states&lt;/code&gt; to be the state names from the &lt;code&gt;murders&lt;/code&gt; data frame. Report the name of the state with the smallest population.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can create a data frame using the &lt;code&gt;data.frame&lt;/code&gt; function. Here is a quick example:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp &amp;lt;- c(35, 88, 42, 84, 81, 30)
city &amp;lt;- c(&amp;quot;Beijing&amp;quot;, &amp;quot;Lagos&amp;quot;, &amp;quot;Paris&amp;quot;, &amp;quot;Rio de Janeiro&amp;quot;,
          &amp;quot;San Juan&amp;quot;, &amp;quot;Toronto&amp;quot;)
city_temps &amp;lt;- data.frame(name = city, temperature = temp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the &lt;code&gt;rank&lt;/code&gt; function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called &lt;code&gt;ranks&lt;/code&gt;, then create a data frame with the state name and its rank. Call the data frame &lt;code&gt;my_df&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Repeat the previous exercise, but this time order &lt;code&gt;my_df&lt;/code&gt; so that the states are ordered from least populous to most populous. Hint: create an object &lt;code&gt;ind&lt;/code&gt; that stores the indexes needed to order the population values. Then use the bracket operator &lt;code&gt;[&lt;/code&gt; to re-order each column in the data frame.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;na_example&lt;/code&gt; vector represents a series of counts. You can quickly examine the object using:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;na_example&amp;quot;)
str(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, when we compute the average with the function &lt;code&gt;mean&lt;/code&gt;, we obtain an &lt;code&gt;NA&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;is.na&lt;/code&gt; function returns a logical vector that tells us which entries are &lt;code&gt;NA&lt;/code&gt;. Assign this logical vector to an object called &lt;code&gt;ind&lt;/code&gt; and determine how many &lt;code&gt;NA&lt;/code&gt;s does &lt;code&gt;na_example&lt;/code&gt; have.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Now compute the average again, but only for the entries that are not &lt;code&gt;NA&lt;/code&gt;. Hint: remember the &lt;code&gt;!&lt;/code&gt; operator.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vector-arithmetics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vector arithmetics&lt;/h2&gt;
&lt;p&gt;California had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)
murders$state[which.max(murders$population)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;California&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;with over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of &lt;code&gt;R&lt;/code&gt; come in handy.&lt;/p&gt;
&lt;div id=&#34;rescaling-a-vector&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rescaling a vector&lt;/h3&gt;
&lt;p&gt;In R, arithmetic operations on vectors occur &lt;em&gt;element-wise&lt;/em&gt;. For a quick example, suppose we have height in inches:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inches &amp;lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and want to convert to centimeters. Notice what happens when we multiply &lt;code&gt;inches&lt;/code&gt; by 2.54:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inches * 2.54&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inches - 69&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0 -7 -3  1  1  4 -2  4 -2  1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;two-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two vectors&lt;/h3&gt;
&lt;p&gt;If we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \begin{pmatrix}
a\\
b\\
c\\
d
\end{pmatrix}
+
  \begin{pmatrix}
e\\
f\\
g\\
h
\end{pmatrix}
=
  \begin{pmatrix}
a +e\\
b + f\\
c + g\\
d + h
\end{pmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The same holds for other mathematical operations, such as &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This implies that to compute the murder rates we can simply type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murder_rate &amp;lt;- murders$total / murders$population * 100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$abb[order(murder_rate)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;VT&amp;quot; &amp;quot;NH&amp;quot; &amp;quot;HI&amp;quot; &amp;quot;ND&amp;quot; &amp;quot;IA&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;UT&amp;quot; &amp;quot;ME&amp;quot; &amp;quot;WY&amp;quot; &amp;quot;OR&amp;quot; &amp;quot;SD&amp;quot; &amp;quot;MN&amp;quot; &amp;quot;MT&amp;quot; &amp;quot;CO&amp;quot; &amp;quot;WA&amp;quot;
## [16] &amp;quot;WV&amp;quot; &amp;quot;RI&amp;quot; &amp;quot;WI&amp;quot; &amp;quot;NE&amp;quot; &amp;quot;MA&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;KS&amp;quot; &amp;quot;NY&amp;quot; &amp;quot;KY&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;OH&amp;quot; &amp;quot;CT&amp;quot; &amp;quot;NJ&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;IL&amp;quot;
## [31] &amp;quot;OK&amp;quot; &amp;quot;NC&amp;quot; &amp;quot;NV&amp;quot; &amp;quot;VA&amp;quot; &amp;quot;AR&amp;quot; &amp;quot;TX&amp;quot; &amp;quot;NM&amp;quot; &amp;quot;CA&amp;quot; &amp;quot;FL&amp;quot; &amp;quot;TN&amp;quot; &amp;quot;PA&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;GA&amp;quot; &amp;quot;MS&amp;quot; &amp;quot;MI&amp;quot;
## [46] &amp;quot;DE&amp;quot; &amp;quot;SC&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;MO&amp;quot; &amp;quot;LA&amp;quot; &amp;quot;DC&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Previously we created this data frame:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp &amp;lt;- c(35, 88, 42, 84, 81, 30)
city &amp;lt;- c(&amp;quot;Beijing&amp;quot;, &amp;quot;Lagos&amp;quot;, &amp;quot;Paris&amp;quot;, &amp;quot;Rio de Janeiro&amp;quot;,
          &amp;quot;San Juan&amp;quot;, &amp;quot;Toronto&amp;quot;)
city_temps &amp;lt;- data.frame(name = city, temperature = temp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is &lt;span class=&#34;math inline&#34;&gt;\(C = \frac{5}{9} \times (F - 32)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write code to compute the following sum &lt;span class=&#34;math inline&#34;&gt;\(1+1/2^2 + 1/3^2 + \dots 1/100^2\)&lt;/span&gt;? &lt;em&gt;Hint:&lt;/em&gt; thanks to Euler, we know it should be close to &lt;span class=&#34;math inline&#34;&gt;\(\pi^2/6\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the per 100,000 murder rate for each state and store it in the object &lt;code&gt;murder_rate&lt;/code&gt;. Then compute the average murder rate for the US using the function &lt;code&gt;mean&lt;/code&gt;. What is the average?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;indexing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Indexing&lt;/h2&gt;
&lt;p&gt;Indexing is a boring name for an important tool. &lt;code&gt;R&lt;/code&gt; provides a powerful and convenient way of referencing specific elements of vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;subsetting-with-logicals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting with logicals&lt;/h3&gt;
&lt;p&gt;We have now calculated the murder rate using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murder_rate &amp;lt;- murders$total / murders$population * 100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Imagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of &lt;code&gt;R&lt;/code&gt; is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- murder_rate &amp;lt; 0.71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we instead want to know if a value is less or equal, we can use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- murder_rate &amp;lt;= 0.71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we get back a logical vector with &lt;code&gt;TRUE&lt;/code&gt; for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$state[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hawaii&amp;quot;        &amp;quot;Iowa&amp;quot;          &amp;quot;New Hampshire&amp;quot; &amp;quot;North Dakota&amp;quot; 
## [5] &amp;quot;Vermont&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to count how many are TRUE, the function &lt;code&gt;sum&lt;/code&gt; returns the sum of the entries of a vector and logical vectors get &lt;em&gt;coerced&lt;/em&gt; to numeric with &lt;code&gt;TRUE&lt;/code&gt; coded as 1 and &lt;code&gt;FALSE&lt;/code&gt; as 0. Thus we can count the states using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(ind)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logical-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logical operators&lt;/h3&gt;
&lt;p&gt;Suppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator &lt;em&gt;and&lt;/em&gt;, which in &lt;code&gt;R&lt;/code&gt; is represented with &lt;code&gt;&amp;amp;&lt;/code&gt;. This operation results in &lt;code&gt;TRUE&lt;/code&gt; only when both logicals are &lt;code&gt;TRUE&lt;/code&gt;. To see this, consider this example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TRUE &amp;amp; TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TRUE &amp;amp; FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FALSE &amp;amp; FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our example, we can form two logicals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;west &amp;lt;- murders$region == &amp;quot;West&amp;quot;
safe &amp;lt;- murder_rate &amp;lt;= 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we can use the &lt;code&gt;&amp;amp;&lt;/code&gt; to get a vector of logicals that tells us which states satisfy both conditions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- safe &amp;amp; west
murders$state[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hawaii&amp;quot;  &amp;quot;Idaho&amp;quot;   &amp;quot;Oregon&amp;quot;  &amp;quot;Utah&amp;quot;    &amp;quot;Wyoming&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;which&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;which&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Suppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function &lt;code&gt;which&lt;/code&gt; tells us which entries of a logical vector are TRUE. So we can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- which(murders$state == &amp;quot;California&amp;quot;)
murder_rate[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.374138&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;in&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;%in%&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function &lt;code&gt;%in%&lt;/code&gt;. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(&amp;quot;Boston&amp;quot;, &amp;quot;Dakota&amp;quot;, &amp;quot;Washington&amp;quot;) %in% murders$state&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we will be using &lt;code&gt;%in%&lt;/code&gt; often throughout the course&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start by loading the library and data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Compute the per 100,000 murder rate for each state and store it in an object called &lt;code&gt;murder_rate&lt;/code&gt;. Then use logical operators to create a logical vector named &lt;code&gt;low&lt;/code&gt; that tells us which entries of &lt;code&gt;murder_rate&lt;/code&gt; are lower than 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the results from the previous exercise and the function &lt;code&gt;which&lt;/code&gt; to determine the indices of &lt;code&gt;murder_rate&lt;/code&gt; associated with values lower than 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the results from the previous exercise to report the names of the states with murder rates lower than 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector &lt;code&gt;low&lt;/code&gt; and the logical operator &lt;code&gt;&amp;amp;&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of &lt;code&gt;murders$abb&lt;/code&gt; that match the three abbreviations, then use the &lt;code&gt;[&lt;/code&gt; operator to extract the states.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;%in%&lt;/code&gt; operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Extend the code you used in exercise 7 to report the one entry that is &lt;strong&gt;not&lt;/strong&gt; an actual abbreviation. Hint: use the &lt;code&gt;!&lt;/code&gt; operator, which turns &lt;code&gt;FALSE&lt;/code&gt; into &lt;code&gt;TRUE&lt;/code&gt; and vice versa, then &lt;code&gt;which&lt;/code&gt; to obtain an index.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;further-help-with-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Further help with &lt;code&gt;R&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;If you are not comfortable with &lt;code&gt;R&lt;/code&gt;, the earlier you seek out help, the better. Quietly letting the course pass by you because you don’t know how to fix an error will do nobody any good. Attend TA office hours or attend TA or Prof. K’s office hours &lt;a href=&#34;https://ssc442.netlify.app/syllabus/&#34;&gt;see Syllabus&lt;/a&gt; for times and Zoom links. Also, join the course Slack (see the front page of our course website for a link) and post questions there.&lt;/p&gt;
&lt;p&gt;Finally, there are also primers on &lt;a href=&#34;https://rstudio.cloud/learn/primers&#34;&gt;Rstudio.cloud&lt;/a&gt; that can be useful. There are many ways we can help you get used to &lt;code&gt;R&lt;/code&gt;, but only if you reach out.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Comments from previous classes indicate that I am not, in fact, funny.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://posit.cloud&#34; class=&#34;uri&#34;&gt;https://posit.cloud&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://rafalab.github.io/dsbook/installing-r-rstudio.html&#34; class=&#34;uri&#34;&gt;https://rafalab.github.io/dsbook/installing-r-rstudio.html&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;This is, without a doubt, my least favorite aspect of &lt;code&gt;R&lt;/code&gt;. I’d even venture to call it stupid. The logic behind this pesky &lt;code&gt;&amp;lt;-&lt;/code&gt; is a total mystery to me, but there &lt;em&gt;is&lt;/em&gt; logic to avoiding &lt;code&gt;=&lt;/code&gt;. But, you do you.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;This equals sign is the reasons we assign values with &lt;code&gt;&amp;lt;-&lt;/code&gt;; then when arguments of a function are assigned values, we don’t end up with multiple equals signs. But… who cares.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;Whether you view this as a feature or a bug is a good indicator whether you’ll enjoy working with &lt;code&gt;R&lt;/code&gt;.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Visualization</title>
      <link>https://ssc442.netlify.app/example/01-example/</link>
      <pubDate>Tue, 09 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/01-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-data-visualization&#34; id=&#34;toc-introduction-to-data-visualization&#34;&gt;Introduction to data visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ggplot2&#34; id=&#34;toc-ggplot2&#34;&gt;ggplot2&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-components-of-a-graph&#34; id=&#34;toc-the-components-of-a-graph&#34;&gt;The components of a graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ggplot-objects&#34; id=&#34;toc-ggplot-objects&#34;&gt;&lt;code&gt;ggplot&lt;/code&gt; objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geometries-briefly&#34; id=&#34;toc-geometries-briefly&#34;&gt;Geometries (briefly)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aesthetic-mappings&#34; id=&#34;toc-aesthetic-mappings&#34;&gt;Aesthetic mappings&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#aesthetics-in-general&#34; id=&#34;toc-aesthetics-in-general&#34;&gt;Aesthetics in general&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#now-back-to-aesthetic-mappings&#34; id=&#34;toc-now-back-to-aesthetic-mappings&#34;&gt;Now, back to aesthetic mappings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#legends-for-aesthetics&#34; id=&#34;toc-legends-for-aesthetics&#34;&gt;Legends for aesthetics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#annotation-layers&#34; id=&#34;toc-annotation-layers&#34;&gt;Annotation Layers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#global-versus-local-aesthetic-mappings&#34; id=&#34;toc-global-versus-local-aesthetic-mappings&#34;&gt;Global versus local aesthetic mappings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#try-it&#34; id=&#34;toc-try-it&#34;&gt;Try it!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction-to-data-visualization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to data visualization&lt;/h1&gt;
&lt;p&gt;Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total
## 1    Alabama  AL  South    4779736   135
## 2     Alaska  AK   West     710231    19
## 3    Arizona  AZ   West    6392017   232
## 4   Arkansas  AR  South    2915918    93
## 5 California  CA   West   37253956  1257
## 6   Colorado  CO   West    5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t &lt;strong&gt;learn&lt;/strong&gt; anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggthemes)
library(ggrepel)

r &amp;lt;- murders %&amp;gt;%
  summarize(pop=sum(population), tot=sum(total)) %&amp;gt;%
  mutate(rate = tot/pop*10^6) %&amp;gt;% pull(rate)

murders %&amp;gt;% ggplot(aes(x = population/10^6, y = total, label = abb)) +
  geom_abline(intercept = log10(r), lty=2, col=&amp;quot;darkgrey&amp;quot;) +
  geom_point(aes(color=region), size = 3) +
  geom_text_repel() +
  scale_x_log10() +
  scale_y_log10() +
  xlab(&amp;quot;Populations in millions (log scale)&amp;quot;) +
  ylab(&amp;quot;Total number of murders (log scale)&amp;quot;) +
  ggtitle(&amp;quot;US Gun Murders in 2010&amp;quot;) +
  scale_color_discrete(name=&amp;quot;Region&amp;quot;) +
  theme_economist_white()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-plot-0-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.&lt;/p&gt;
&lt;p&gt;The growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing &lt;em&gt;data journalism&lt;/em&gt; and including effective &lt;em&gt;infographics&lt;/em&gt; as part of their reporting.&lt;/p&gt;
&lt;p&gt;A particularly salient example—given the current state of the world—is a Wall Street Journal article&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/wsj-vaccines-example-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: &lt;a href=&#34;http://graphics.wsj.com/infectious-diseases-and-vaccines/&#34;&gt;Wall Street Journal&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Another striking example comes from a New York Times chart&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, which summarizes scores from the NYC Regents Exams. As described in
the article&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/regents-exams-example-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: &lt;a href=&#34;http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif&#34;&gt;New York Times&lt;/a&gt; via Amanda Cox)&lt;/p&gt;
&lt;p&gt;The most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.&lt;/p&gt;
&lt;p&gt;This is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call &lt;em&gt;exploratory data analysis&lt;/em&gt; (EDA). John W. Tukey&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, considered the father of EDA, once said,&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“The greatest value of a picture is when it forces us to notice what we never expected to see.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Many widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.&lt;/p&gt;
&lt;p&gt;Data visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; and The Best Stats You’ve Ever Seen&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/gampnider-example-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.&lt;/p&gt;
&lt;p&gt;Today, we will discuss the basics of data visualization and exploratory data analysis. We will use the &lt;strong&gt;ggplot2&lt;/strong&gt; package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.&lt;/p&gt;
&lt;p&gt;Of course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ER Tufte (1983) The visual display of quantitative information.
Graphics Press.&lt;/li&gt;
&lt;li&gt;ER Tufte (1990) Envisioning information. Graphics Press.&lt;/li&gt;
&lt;li&gt;ER Tufte (1997) Visual explanations. Graphics Press.&lt;/li&gt;
&lt;li&gt;WS Cleveland (1993) Visualizing data. Hobart Press.&lt;/li&gt;
&lt;li&gt;WS Cleveland (1994) The elements of graphing data. CRC Press.&lt;/li&gt;
&lt;li&gt;A Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach:
Turning tables into graphs. The American Statistician 56:121-130.&lt;/li&gt;
&lt;li&gt;NB Robbins (2004) Creating more effective graphs. Wiley.&lt;/li&gt;
&lt;li&gt;A Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders.&lt;/li&gt;
&lt;li&gt;N Yau (2013) Data points: Visualization that means something. Wiley.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;https://shiny.rstudio.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://d3js.org/&#34;&gt;https://d3js.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;ggplot2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ggplot2&lt;/h1&gt;
&lt;p&gt;Exploratory data visualization is perhaps the greatest strength of &lt;code&gt;R&lt;/code&gt;. One can quickly go from idea to data to plot with a unique balance of flexibility and ease. For example, Excel may be easier than &lt;code&gt;R&lt;/code&gt; for some plots, but it is nowhere near as flexible. &lt;code&gt;D3.js&lt;/code&gt; may be more flexible and powerful than &lt;code&gt;R&lt;/code&gt;, but it takes much longer to generate a plot. One of the reasons we use &lt;code&gt;R&lt;/code&gt; is its incredible flexibility &lt;strong&gt;and&lt;/strong&gt; ease.&lt;/p&gt;
&lt;p&gt;Throughout this course, we will be creating plots using the &lt;strong&gt;ggplot2&lt;/strong&gt;&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many other approaches are available for creating plots in &lt;code&gt;R&lt;/code&gt;. In fact, the plotting capabilities that come with a basic installation of &lt;code&gt;R&lt;/code&gt; are already quite powerful. There are also other packages for creating graphics such as &lt;strong&gt;grid&lt;/strong&gt; and &lt;strong&gt;lattice&lt;/strong&gt;. We chose to use &lt;strong&gt;ggplot2&lt;/strong&gt; in this course because it breaks plots into components in a way that permits beginners to create relatively complex and aesthetically pleasing plots using syntax that is intuitive and comparatively easy to remember.&lt;/p&gt;
&lt;p&gt;One reason &lt;strong&gt;ggplot2&lt;/strong&gt; is generally more intuitive for beginners is that it uses a so-called “grammar of graphics”&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;, the letters &lt;em&gt;gg&lt;/em&gt; in &lt;strong&gt;ggplot2&lt;/strong&gt;. This is analogous to the way learning grammar can help a beginner construct hundreds of different sentences by learning just a handful of verbs, nouns and adjectives without having to memorize each specific sentence. Similarly, by learning a handful of &lt;strong&gt;ggplot2&lt;/strong&gt; building blocks and its grammar, you will be able to create hundreds of different plots.&lt;/p&gt;
&lt;p&gt;Another reason &lt;strong&gt;ggplot2&lt;/strong&gt; is easy for beginners is that its default behavior is carefully chosen to satisfy the great majority of cases and is visually pleasing. As a result, it is possible to create informative and elegant graphs with relatively simple and readable code.&lt;/p&gt;
&lt;p&gt;One limitation is that &lt;strong&gt;ggplot2&lt;/strong&gt; is designed to work exclusively with data tables in tidy format (where rows are observations and columns are variables). However, a substantial percentage of datasets that beginners work with are in, or can be converted into, this format. An advantage of this approach is that, assuming that our data is tidy, &lt;strong&gt;ggplot2&lt;/strong&gt; simplifies plotting code and the learning of grammar for a variety of plots. You should review the previous content about tidy data if you are feeling lost.&lt;/p&gt;
&lt;p&gt;To use &lt;strong&gt;ggplot2&lt;/strong&gt; you will have to learn several functions and arguments. These are hard to memorize, so we highly recommend you have the ggplot2 cheat sheet handy. Just search for “ggplot2 cheat sheet”.&lt;/p&gt;
&lt;div id=&#34;the-components-of-a-graph&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The components of a graph&lt;/h2&gt;
&lt;p&gt;We will eventually construct a graph that summarizes the US murders dataset that looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.&lt;/p&gt;
&lt;p&gt;This data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.&lt;/p&gt;
&lt;p&gt;The first step in learning &lt;strong&gt;ggplot2&lt;/strong&gt; is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the &lt;strong&gt;ggplot2&lt;/strong&gt; terminology. The main five components to note are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;: The US murders data table is being summarized. We refer to this as the &lt;strong&gt;data&lt;/strong&gt; component.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Geometry&lt;/strong&gt;: The plot above is a scatterplot. This is referred to as the
&lt;strong&gt;geometry&lt;/strong&gt; component. Other possible geometries are barplot, histogram, smooth densities, qqplot, boxplot, pie (ew!), and many, many more. We will learn about these later.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Aesthetic mapping&lt;/strong&gt;: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we &lt;em&gt;map&lt;/em&gt; data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the &lt;strong&gt;aesthetic mapping&lt;/strong&gt; component. How we define the mapping depends on what &lt;strong&gt;geometry&lt;/strong&gt; we are using.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Annotations&lt;/strong&gt;: These are things like axis labels, axis ticks (the lines along the axis at regular intervals or specific points of interest), axis scales (e.g. log-scale), titles, legends, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Style&lt;/strong&gt;: An overall appearance of the graph determined by fonts, color palattes, layout, blank spaces, and more.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The points are labeled with the state abbreviations.&lt;/li&gt;
&lt;li&gt;The range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.&lt;/li&gt;
&lt;li&gt;There are labels, a title, a legend, and we use the style of The Economist magazine.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the flexibility and visualization power of &lt;code&gt;ggplot&lt;/code&gt; is contained in these four elements (plus your data)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ggplot-objects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;ggplot&lt;/code&gt; objects&lt;/h2&gt;
&lt;p&gt;We will now construct the plot piece by piece.&lt;/p&gt;
&lt;p&gt;We start by loading the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first step in creating a &lt;strong&gt;ggplot2&lt;/strong&gt; graph is to define a &lt;code&gt;ggplot&lt;/code&gt; object. We do this with the function &lt;code&gt;ggplot&lt;/code&gt;, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also pipe the data in as the first argument. So this line of code is equivalent to the one above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It renders a plot, in this case a blank slate since no &lt;strong&gt;geometry&lt;/strong&gt; has been defined. The only style choice we see is a grey background.&lt;/p&gt;
&lt;p&gt;What has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = murders)
class(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;gg&amp;quot;     &amp;quot;ggplot&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To render the plot associated with this object, we simply print the object &lt;code&gt;p&lt;/code&gt;. The following two lines of code each produce the same plot we see above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;geometries-briefly&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geometries (briefly)&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;ggplot2&lt;/code&gt; we create graphs by adding geometry &lt;em&gt;layers&lt;/em&gt;. Layers can define geometries, compute summary statistics, define what scales to use, create annotations, or even change styles. To add layers, we use the symbol &lt;code&gt;+&lt;/code&gt;. In general, a line of code will look like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;DATA %&amp;gt;% ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Usually, the first &lt;strong&gt;added&lt;/strong&gt; layer after &lt;code&gt;ggplot() +&lt;/code&gt; defines the geometry. After that, we may add additional geometries, we may rescale an axis, we may add annotations and labels, or we may change the style. For now, we want to make a scatterplot like the one you all created in Lab 0. What geometry do we use?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ex-scatter-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Taking a quick look at the cheat sheet, we see that the &lt;code&gt;ggplot2&lt;/code&gt; function used to create plots with this geometry is &lt;code&gt;geom_point&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;embed src=&#34;https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Image courtesy of RStudio&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. CC-BY-4.0 license&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;!--(Source: [RStudio](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf))--&gt;
&lt;p&gt;Geometry function names follow the pattern: &lt;code&gt;geom_X&lt;/code&gt; where X is the name of some specific geometry. Some examples include &lt;code&gt;geom_point&lt;/code&gt;, &lt;code&gt;geom_bar&lt;/code&gt;, and &lt;code&gt;geom_histogram&lt;/code&gt;. You’ve already seen a few of these. We will start with a scatterplot created using &lt;code&gt;geom_point()&lt;/code&gt; for now, then circle back to more geometries after we cover aesthetic mappings, layers, and annotations.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;geom_point&lt;/code&gt; to run properly we need to provide data and an &lt;strong&gt;aesthetic mapping&lt;/strong&gt;. The simplest mapping for a scatter plot is to say we want one variable on the X-axis, and a different one on the Y-axis, so each point is an {X,Y} pair. That is an &lt;strong&gt;aesthetic mapping&lt;/strong&gt; because X and Y are &lt;strong&gt;aesthetics&lt;/strong&gt; in a &lt;code&gt;geom_point&lt;/code&gt; scatterplot.&lt;/p&gt;
&lt;p&gt;We have already connected the object &lt;code&gt;p&lt;/code&gt; with the &lt;code&gt;murders&lt;/code&gt; data table, and if we add the layer &lt;code&gt;geom_point&lt;/code&gt; it defaults to using this data. To find out what mappings are expected, we read the &lt;strong&gt;Aesthetics&lt;/strong&gt; section of the help file &lt;code&gt;?geom_point&lt;/code&gt; help file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; Aesthetics
&amp;gt;
&amp;gt; geom_point understands the following aesthetics (required aesthetics are in bold):
&amp;gt;
&amp;gt; **x**
&amp;gt;
&amp;gt; **y**
&amp;gt;
&amp;gt; alpha
&amp;gt;
&amp;gt; colour
&amp;gt;
&amp;gt; fill
&amp;gt;
&amp;gt; group
&amp;gt;
&amp;gt; shape
&amp;gt;
&amp;gt; size
&amp;gt;
&amp;gt; stroke&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and—although it does not show in bold above—we see that at least two arguments are required: &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. You can’t have a &lt;code&gt;geom_point&lt;/code&gt; scatterplot unless you state what you want on the X and Y axes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aesthetic-mappings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Aesthetic mappings&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Aesthetic mappings&lt;/strong&gt; describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The &lt;code&gt;aes&lt;/code&gt; function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the &lt;code&gt;aes&lt;/code&gt; function is often used as the argument of a geometry function. This example produces a scatterplot of population in millions (x-axis) versus total murders (y-axis):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot() +
  geom_point(aes(x = population/10^6, y = total))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of defining our plot from scratch, we can also add a layer to the &lt;code&gt;p&lt;/code&gt; object that was defined above as &lt;code&gt;p &amp;lt;- ggplot(data = murders)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The scales and annotations like axis labels are defined by default when adding this layer (note the x-axis label is exactly what we wrote in the function call). Like &lt;strong&gt;dplyr&lt;/strong&gt; functions, &lt;code&gt;aes&lt;/code&gt; also uses the variable names from the object component: we can use &lt;code&gt;population&lt;/code&gt; and &lt;code&gt;total&lt;/code&gt; without having to call them as &lt;code&gt;murders$population&lt;/code&gt; and &lt;code&gt;murders$total&lt;/code&gt;. The behavior of recognizing the variables from the data component is quite specific to &lt;code&gt;aes&lt;/code&gt;. With most functions, if you try to access the values of &lt;code&gt;population&lt;/code&gt; or &lt;code&gt;total&lt;/code&gt; outside of &lt;code&gt;aes&lt;/code&gt; you receive an error.&lt;/p&gt;
&lt;p&gt;Note that we did some rescaling within the &lt;code&gt;aes()&lt;/code&gt; call - we can do simple things like multiplication or division on the variable names in the &lt;code&gt;ggplot&lt;/code&gt; call. The axis labels reflect this. We will change the axis labels later.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;aesthetic mappings&lt;/strong&gt; are very powerful - changing the variable in &lt;code&gt;x=&lt;/code&gt; or &lt;code&gt;y=&lt;/code&gt; changes the meaning of the plot entirely. We’ll come back to additional &lt;strong&gt;aesthetic mappings&lt;/strong&gt; once we talk about aesthetics in general.&lt;/p&gt;
&lt;div id=&#34;aesthetics-in-general&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Aesthetics in general&lt;/h3&gt;
&lt;p&gt;Even without mappings, a plots aesthetics can be useful. Things like color, fill, alpha, and size are aesthetics that can be changed.&lt;/p&gt;
&lt;p&gt;Let’s say we want larger points in our scatterplot. The &lt;code&gt;size&lt;/code&gt; aesthetic can be used to set the size. The scale of &lt;code&gt;size&lt;/code&gt; is “multiples of the defaults” (so &lt;code&gt;size = 1&lt;/code&gt; is the default)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total), size = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;size&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; a mapping so it is &lt;strong&gt;not&lt;/strong&gt; in the &lt;code&gt;aes()&lt;/code&gt; part: whereas mappings use data from specific observations and need to be inside &lt;code&gt;aes()&lt;/code&gt;, operations we want to affect all the points the same way do not need to be included inside &lt;code&gt;aes&lt;/code&gt;. We’ll see what happens if &lt;code&gt;size&lt;/code&gt; is inside &lt;code&gt;aes(size = xxx)&lt;/code&gt; in a second.&lt;/p&gt;
&lt;p&gt;We can change the &lt;code&gt;shape&lt;/code&gt; to one of the many different base-R options found &lt;a href=&#34;http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total), size = 3, shape = 17)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-5shape-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also change the &lt;code&gt;fill&lt;/code&gt; and the &lt;code&gt;color&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total), size = 4, shape = 23, fill = &amp;#39;#18453B&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-5b-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;fill&lt;/code&gt; can take a common name like &lt;code&gt;&#39;green&#39;&lt;/code&gt;, or can take a hex color like &lt;code&gt;&#39;#18453B&#39;&lt;/code&gt;, which is &lt;a href=&#34;https://brand.msu.edu/design-visual/index.html#color&#34;&gt;MSU Green according to MSU’s branding site&lt;/a&gt;. You can also find &lt;a href=&#34;https://youtu.be/0BxNHwJi1y4&#34;&gt;UM Maize&lt;/a&gt; and &lt;a href=&#34;https://youtu.be/dQw4w9WgXcQ&#34;&gt;OSU Scarlet&lt;/a&gt; on respective branding pages, or google “XXX color hex.” We’ll learn how to build a color palatte later on.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;color&lt;/code&gt; (or &lt;code&gt;colour&lt;/code&gt;, same thing because &lt;code&gt;ggplot&lt;/code&gt; creators allow both spellings) is a little tricky with points - it changes the outline of the geometry rather than the fill color, but in &lt;code&gt;geom_point()&lt;/code&gt; most shapes are only the outline, including the default. This is more useful with, say, a barplot where the outline and the fill might be different colors. Still, shapes 21-25 have both &lt;code&gt;fill&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total), size = 5, shape = 23, fill = &amp;#39;#18453B&amp;#39;, color = &amp;#39;white&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-5c-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;color = &#39;white&#39;&lt;/code&gt; makes the outline of the shape white, which you can see if you look closely in the areas where the shapes overlap. This only works with shapes 21-25, or any other geometry that has both an outline and a fill.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;now-back-to-aesthetic-mappings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Now, back to aesthetic mappings&lt;/h3&gt;
&lt;p&gt;Now that we’ve seen a few aesthetics (and know we can find more by looking at which aesthetics work with our geometry in the help file), let’s return to the power of aesthetic mappings.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;aesthetic mapping&lt;/strong&gt; means we can vary an aesthetic (like fill or shape or size) according to some &lt;strong&gt;variable in our data&lt;/strong&gt;. This opens up a world of possibilities! Let’s try adding to our &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; aesthetics with a &lt;code&gt;color&lt;/code&gt; aesthetic (since points respond to &lt;code&gt;color&lt;/code&gt; better than &lt;code&gt;fill&lt;/code&gt;) that varies by &lt;code&gt;region&lt;/code&gt;, which is a column in our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total, color = region), size = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-color-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We include &lt;code&gt;color=region&lt;/code&gt; &lt;strong&gt;inside&lt;/strong&gt; the &lt;code&gt;aes&lt;/code&gt; call, which tells R to find a variable called &lt;code&gt;region&lt;/code&gt; and change color based on that. R will choose a somewhat ghastly color palatte, and &lt;strong&gt;every&lt;/strong&gt; unique value in the data for &lt;code&gt;region&lt;/code&gt; will get a different color if the variable is discrete. If the variable is a continuous value, then &lt;code&gt;ggplot&lt;/code&gt; will automatically make a color ramp. Thus, &lt;strong&gt;discrete&lt;/strong&gt; and &lt;strong&gt;continuous&lt;/strong&gt; values for aesthetic mappings work differently.&lt;/p&gt;
&lt;p&gt;Let’s see a useful example of a continuous aesthetic mapping to &lt;code&gt;color&lt;/code&gt;. In our data, we are making a scatterplot of population and total murders, which really just shows that states with higher populations have higher murders. What we really want is murders per capita (I think COVID taught us a lot about rates vs. levels like “cases” and “cases per 100,000 people”). We can create a variable of “murders per capita” on the fly. Since “murders per capita” is a very small number and hard to read, we’ll multiply by 100 so that we get “percent of population murdered per year”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^5, y = total, color = 100*total/population), size = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-colfill-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While the clear pattern of “more population means more murders” is still there, look at the outlier in light blue in the bottom left. With the color ramp, see how easy it is to see here that there is one location where murders per capita is quite high?&lt;/p&gt;
&lt;p&gt;Note that &lt;code&gt;size&lt;/code&gt; is outside of &lt;code&gt;aes&lt;/code&gt; and is set to an explicit value, not to a variable. What if we set size to a variable in the data?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-color2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;legends-for-aesthetics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Legends for aesthetics&lt;/h3&gt;
&lt;p&gt;Here we see yet another useful default behavior: &lt;strong&gt;ggplot2&lt;/strong&gt; automatically adds a legend that maps color to region, and size to population (which we scaled by 1,000,000). To avoid adding this legend we set the &lt;code&gt;geom_point&lt;/code&gt; argument &lt;code&gt;show.legend = FALSE&lt;/code&gt;. This removes both the &lt;code&gt;size&lt;/code&gt; and the &lt;code&gt;color&lt;/code&gt; legend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6), show.legend = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-color3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Later on, when we get to &lt;strong&gt;annotation layers&lt;/strong&gt;, we’ll talk about controlling the legend text and layout. For now, we just need to know how to turn them off.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;annotation-layers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Annotation Layers&lt;/h2&gt;
&lt;p&gt;A second layer in the plot we wish to make involves adding a label to each point to identify the state. The &lt;code&gt;geom_label&lt;/code&gt; and &lt;code&gt;geom_text&lt;/code&gt; functions permit us to add text to the plot with and without a rectangle behind the text, respectively.&lt;/p&gt;
&lt;p&gt;Because each point (each state in this case) has a label, we need an &lt;strong&gt;aesthetic mapping&lt;/strong&gt; to make the connection between points and labels. By reading the help file &lt;code&gt;?geom_text&lt;/code&gt;, we learn that we supply the mapping between point and label through the &lt;code&gt;label&lt;/code&gt; argument of &lt;code&gt;aes&lt;/code&gt;. That is, &lt;code&gt;label&lt;/code&gt; is an &lt;strong&gt;aesthetic&lt;/strong&gt; that we can map. So the code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) +
  geom_text(aes(x = population/10^6, y = total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have successfully added a second layer to the plot.&lt;/p&gt;
&lt;p&gt;As an example of the unique behavior of &lt;code&gt;aes&lt;/code&gt; mentioned above, note that this call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) + 
  geom_text(aes(population/10^6, total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is fine, whereas this call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) + 
  geom_text(aes(population/10^6, total), label = abb)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will give you an error since &lt;code&gt;abb&lt;/code&gt; is not found because it is outside of the &lt;code&gt;aes&lt;/code&gt; function. The layer &lt;code&gt;geom_text&lt;/code&gt; does not know where to find &lt;code&gt;abb&lt;/code&gt; since it is a column name and not a global variable, and &lt;code&gt;ggplot&lt;/code&gt; does not look for column names for non-mapped aesthetics. For a trivial example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) +
  geom_text(aes(population/10^6, total), label = &amp;#39;abb&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;global-versus-local-aesthetic-mappings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Global versus local aesthetic mappings&lt;/h3&gt;
&lt;p&gt;In the previous line of code, we define the mapping &lt;code&gt;aes(population/10^6, total)&lt;/code&gt; twice, once in each geometry. We can avoid this by using a &lt;em&gt;global&lt;/em&gt; aesthetic mapping. We can do this when we define the blank slate &lt;code&gt;ggplot&lt;/code&gt; object. Remember that the function &lt;code&gt;ggplot&lt;/code&gt; contains an argument that permits us to define aesthetic mappings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;args(ggplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (data = NULL, mapping = aes(), ..., environment = parent.frame()) 
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we define a mapping in &lt;code&gt;ggplot&lt;/code&gt;, all the geometries that are added as layers will default to this mapping. We redefine &lt;code&gt;p&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- murders %&amp;gt;% ggplot(aes(x = population/10^6, y = total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then we can simply write the following code to produce the previous plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(nudge_x = 1.5) # offsets the label&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We keep the &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;nudge_x&lt;/code&gt; arguments in &lt;code&gt;geom_point&lt;/code&gt; and &lt;code&gt;geom_text&lt;/code&gt;, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in &lt;code&gt;aes&lt;/code&gt; then they would apply to both plots. Also note that the &lt;code&gt;geom_point&lt;/code&gt; function does not need a &lt;code&gt;label&lt;/code&gt; argument and therefore ignores that aesthetic.&lt;/p&gt;
&lt;p&gt;If necessary, we can override the global mapping by defining a new mapping within each layer. These &lt;em&gt;local&lt;/em&gt; definitions override the &lt;em&gt;global&lt;/em&gt;. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(aes(x = 10, y = 800, label = &amp;quot;Hello there!&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/01-example_files/figure-html/ggplot-example-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, the second call to &lt;code&gt;geom_text&lt;/code&gt; does not use &lt;code&gt;x = population&lt;/code&gt; and &lt;code&gt;y = total&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;try-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Try it!&lt;/h2&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Let’s break in to smaller groups and try playing with some of the aesthetics and aesthetic mappings. If we’re in person (woohoo!), we’ll form the same number of groups in class.&lt;/p&gt;
&lt;p&gt;In each group, one person should be the main coder - someone who has the packages like &lt;code&gt;dslabs&lt;/code&gt; installed and has successfully run the plots above. Each set of tasks ask you to learn about an aesthetic and put it into action with the &lt;code&gt;murder&lt;/code&gt; data. We’ll leave about 5 minutes to do the task, then have you come back and share your results with the class.&lt;/p&gt;
&lt;p&gt;For each group, we’ll start with the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) +
  geom_text(aes(x = population/10^6, y = total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;code&gt;alpha&lt;/code&gt; aesthetic mapping.
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The &lt;code&gt;alpha&lt;/code&gt; aesthetic can only take a number between 0 and 1. So first, in &lt;code&gt;murders&lt;/code&gt;, create a &lt;code&gt;murders_per_capita&lt;/code&gt; column by dividing &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;population&lt;/code&gt;. Second, find the &lt;code&gt;max(murders$murders_per_capita)&lt;/code&gt; and then create another new column called &lt;code&gt;murders_per_capita_rescaled&lt;/code&gt; which divides &lt;code&gt;murders_per_capita&lt;/code&gt; by the max value. &lt;code&gt;murders_per_capita_rescaled&lt;/code&gt; will be between 0 and 1, with the value of 1 for the state with the max murder rate. This is a little hard to do on the fly in &lt;code&gt;ggplot&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;alpha&lt;/code&gt; aesthetic mapping to &lt;code&gt;murders_per_capita_rescaled&lt;/code&gt; for &lt;code&gt;geom_point&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Turn off the legend using &lt;code&gt;show.legend=FALSE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Include the &lt;code&gt;geom_text&lt;/code&gt; labels, but make sure the aesthetic mapping does &lt;strong&gt;not&lt;/strong&gt; apply to the labels.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;nudge_x = 1.5&lt;/code&gt; as before to offset the labels.&lt;/li&gt;
&lt;li&gt;Be able to explain the plot.
&lt;ul&gt;
&lt;li&gt;Does the &lt;code&gt;alpha&lt;/code&gt; aesthetic help present the data here? It’s OK if it doesn’t!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;stroke&lt;/code&gt; aesthetic mapping.
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The &lt;code&gt;stroke&lt;/code&gt; aesthetic works a bit like the &lt;code&gt;size&lt;/code&gt; aesthetic. It must be used with a plot that has both a border and a fill, like shapes 21-25, so use one of those.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;stroke&lt;/code&gt; aesthetic mapping (meaning the stroke will change according to a value in the data) to set a different stroke size based on murders &lt;em&gt;per capita&lt;/em&gt;. You can create a murders per capita variable on the fly, or add it to your &lt;code&gt;murders&lt;/code&gt; data.
&lt;ul&gt;
&lt;li&gt;Include the text labels as before and use &lt;code&gt;nudge_x = 1.5&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Make sure you’re only setting the aesthetic for the points on the scatterplot!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;angle&lt;/code&gt; aesthetic
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Using the &lt;code&gt;?geom_text&lt;/code&gt; help, note that &lt;code&gt;geom_text&lt;/code&gt; takes an aesthetic of &lt;code&gt;angle&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;angle&lt;/code&gt; aesthetic (not aesthetic mapping) in the appropriate place (e.g. on &lt;code&gt;geom_text&lt;/code&gt; and not on other geometries) to adjust the labels on our plot.&lt;/li&gt;
&lt;li&gt;Now, try using the &lt;code&gt;angle&lt;/code&gt; aesthetic mapping by using the &lt;code&gt;total&lt;/code&gt; field as both the &lt;code&gt;y&lt;/code&gt; value &lt;strong&gt;and&lt;/strong&gt; the &lt;code&gt;angle&lt;/code&gt; value in the &lt;code&gt;geom_text&lt;/code&gt; layer.&lt;/li&gt;
&lt;li&gt;Does using &lt;code&gt;angle&lt;/code&gt; as an aesthetic help? What about as an aesthetic mapping?&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;color&lt;/code&gt; aesthetic mapping
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Set the &lt;code&gt;color&lt;/code&gt; aesthetic mapping in &lt;code&gt;geom_text&lt;/code&gt; to &lt;code&gt;total/population&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;nudge_x = 1.5&lt;/code&gt; aesthetic in &lt;code&gt;geom_text&lt;/code&gt; still&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Try it with and without the legend using &lt;code&gt;show.legend&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Be able to explain the plot.
&lt;ul&gt;
&lt;li&gt;Does the &lt;code&gt;color&lt;/code&gt; aesthetic mapping help present the data here?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geom_label&lt;/code&gt; and the &lt;code&gt;fill&lt;/code&gt; aesthetic
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Looking at &lt;code&gt;?geom_label&lt;/code&gt; (which is the same help as &lt;code&gt;geom_text&lt;/code&gt;), we note that “The &lt;code&gt;fill&lt;/code&gt; aesthetic controls the backgreound colour of the label”.&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;fill&lt;/code&gt; aesthetic mapping to &lt;code&gt;total/population&lt;/code&gt; in &lt;code&gt;geom_label&lt;/code&gt; (replacing &lt;code&gt;geom_text&lt;/code&gt; but still using &lt;code&gt;nudge_x=1.5&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;fill&lt;/code&gt; aesthetic (not mapping) to the color of your choice.&lt;/li&gt;
&lt;li&gt;Be able to explain the plots.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Does the &lt;code&gt;fill&lt;/code&gt; aesthetic mapping help present the data here?&lt;/li&gt;
&lt;li&gt;What color did you choose for the non-mapped fill aesthetic?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e&#34; class=&#34;uri&#34;&gt;http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif&#34; class=&#34;uri&#34;&gt;http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.nytimes.com/2011/02/19/nyregion/19schools.html&#34; class=&#34;uri&#34;&gt;https://www.nytimes.com/2011/02/19/nyregion/19schools.html&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/John_Tukey&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/John_Tukey&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://ggplot2.tidyverse.org/&#34; class=&#34;uri&#34;&gt;https://ggplot2.tidyverse.org/&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.springer.com/us/book/9780387245447&#34; class=&#34;uri&#34;&gt;http://www.springer.com/us/book/9780387245447&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&lt;/a&gt;&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizations</title>
      <link>https://ssc442.netlify.app/example/03-example/</link>
      <pubDate>Tue, 09 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/03-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#adding-graphics-to-an-rmarkdown-file&#34; id=&#34;toc-adding-graphics-to-an-rmarkdown-file&#34;&gt;Adding graphics to an Rmarkdown file&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#from-the-web&#34; id=&#34;toc-from-the-web&#34;&gt;From the web&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#images-inserted-in-code-chunks&#34; id=&#34;toc-images-inserted-in-code-chunks&#34;&gt;Images inserted in code chunks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#images-inserted-via-markdown&#34; id=&#34;toc-images-inserted-via-markdown&#34;&gt;Images inserted via Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#images-from-a-local-file&#34; id=&#34;toc-images-from-a-local-file&#34;&gt;Images from a local file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gapminder&#34; id=&#34;toc-gapminder&#34;&gt;Data visualization in practice&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-new-insights-on-poverty&#34; id=&#34;toc-case-study-new-insights-on-poverty&#34;&gt;Case study: new insights on poverty&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-the-data&#34; id=&#34;toc-exploring-the-data&#34;&gt;Exploring the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slope-charts&#34; id=&#34;toc-slope-charts&#34;&gt;Slope charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bland-altman-plot&#34; id=&#34;toc-bland-altman-plot&#34;&gt;Bland-Altman plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bump-charts&#34; id=&#34;toc-bump-charts&#34;&gt;Bump charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#themes&#34; id=&#34;toc-themes&#34;&gt;Themes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#small-multiples&#34; id=&#34;toc-small-multiples&#34;&gt;Small multiples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sparklines&#34; id=&#34;toc-sparklines&#34;&gt;Sparklines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-ecological-fallacy-and-importance-of-showing-the-data&#34; id=&#34;toc-the-ecological-fallacy-and-importance-of-showing-the-data&#34;&gt;The ecological fallacy and importance of showing the data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#logit&#34; id=&#34;toc-logit&#34;&gt;Logit transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#show-the-data&#34; id=&#34;toc-show-the-data&#34;&gt;Show the data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vaccines&#34; id=&#34;toc-vaccines&#34;&gt;Case study: vaccines and infectious diseases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#try-it&#34; id=&#34;toc-try-it&#34;&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(emo)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;adding-graphics-to-an-rmarkdown-file&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Adding graphics to an Rmarkdown file&lt;/h1&gt;
&lt;div id=&#34;from-the-web&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;From the web&lt;/h3&gt;
&lt;p&gt;If you are incorporating an image (.png or .jpg) from another site on the web, you can refer to the image directly *provided the web address ends in &lt;code&gt;.png&lt;/code&gt; or &lt;code&gt;.jpg&lt;/code&gt; or &lt;code&gt;.gif&lt;/code&gt;. Google image search makes it a little hard to get directly to the image source, so click through a search until you get to the original image. Once you’re there, right-click on the image and select “Copy Image Address” (may vary by system). If you can paste the URL into a new window and get the image itself, you’re good to go with the instructions here. Some sites and formats do not host the images as a separate file – they may be generated by an app. For instance, if we go to &lt;a href=&#34;https://msu.edu/students&#34;&gt;https://msu.edu/students&lt;/a&gt;, the background image address is not available by right-clicking. But scrolling down, the image for the Student Information System is &lt;a href=&#34;https://student.msu.edu/&#34;&gt;https://student.msu.edu/&lt;/a&gt;. In cases where the image address is not readily available, you’ll have to take a screenshot and use the instructions in the next section (or dig into the site code if you know how to do that sort of thing). Let’s work on getting this image into our output: &lt;a href=&#34;https://msu.edu/-/media/assets/msu/images/audience-student/students-sis-home.jpg&#34;&gt;https://msu.edu/-/media/assets/msu/images/audience-student/students-sis-home.jpg&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that using a web address for an image means if the image owner changes the address or removes the image, you won’t be able to re-knit your document. See the next section for downloading the image and inserting into your document.&lt;/p&gt;
&lt;p&gt;There are two ways of inserting an image: in markdown text, or in a code chunk. Both work. I prefer using the code chunk method, which uses &lt;code&gt;knitr::include_graphics&lt;/code&gt;. This is an R function, so you use it inside a code chunk. When you are including an image inside a code chunk, the code chunk options can be used to control the &lt;code&gt;fig.width&lt;/code&gt; or &lt;code&gt;fig.height&lt;/code&gt;, and &lt;code&gt;fig.align&lt;/code&gt;. For instance, &lt;code&gt;fig.width = &#39;75%&#39;&lt;/code&gt; will use 75% of the available page width, whatever it may be. The down side is that you have to pull a copy of the image from the web and save it locally (earlier versions of Rmarkdown would do this for you automatically, but this feature was removed in rmarkdown v1.6 for security reasons). Here’s the code to do so:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;images-inserted-in-code-chunks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Images inserted in code chunks&lt;/h3&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;```
{r insert-image, fig.width = &amp;#39;75%&amp;#39;, fig.align=&amp;#39;center&amp;#39;, echo = TRUE}
download.file(&amp;quot;https://msu.edu/-/media/assets/msu/images/audience-student/students-sis-home.jpg&amp;quot;, destfile = &amp;#39;temporary.jpg&amp;#39;)
knitr::include_graphics(path = &amp;#39;temporary.jpg&amp;#39;)
```&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://msu.edu/-/media/assets/msu/images/audience-student/students-sis-home.jpg&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The file you specify with &lt;code&gt;destfile&lt;/code&gt; doesn’t matter - R will create that file, but you do need to use the corresponding suffix (don’t use &lt;code&gt;temporary.jpg&lt;/code&gt; if you’re downloading a &lt;code&gt;.png&lt;/code&gt;). By default, the file will be saved in the same folder as your .rmd file. When &lt;code&gt;include_graphics&lt;/code&gt; goes to read the file, it will start looking relative to the folder that contains your .rmd file. That is, &lt;code&gt;download.file&lt;/code&gt; will copy the image to the same place that &lt;code&gt;include_graphics&lt;/code&gt; looks for it. See below for more on relative filepaths.&lt;/p&gt;
&lt;p&gt;Note the code chunk option set above as well - &lt;code&gt;fig.width=&#39;75%&#39;&lt;/code&gt;, which is stated in the curly-brackets that head the chunk. This is where Knitr finds details about how you want to handle the output. Before, we saw that &lt;code&gt;echo=T&lt;/code&gt; would add a copy of the code itself to the output (vs. &lt;code&gt;echo=F&lt;/code&gt; which output only the result). Similarly, &lt;code&gt;fig.width=&#39;75%&#39;&lt;/code&gt; should size the output to take up about 75% of the text width. You can also use &lt;code&gt;fig.width=8&lt;/code&gt; to set the output to 8 inches wide (closet to the width of a sheet of paper), which will maximize the space used for your plot. You may need this if/when you start plotting larger things. Hint.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;images-inserted-via-markdown&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Images inserted via Markdown&lt;/h3&gt;
&lt;p&gt;The markdown language is what controls the text outside of the R code chunks. It has its own way of inserting images. Here the image is inserted in the text, &lt;strong&gt;not&lt;/strong&gt; in a code chunk.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;![](https://msu.edu/-/media/assets/msu/images/audience-student/students-sis-home.jpg&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://msu.edu/-/media/assets/msu/images/audience-student/students-sis-home.jpg%22&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;images-from-a-local-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Images from a local file&lt;/h3&gt;
&lt;p&gt;Whether you have the file on your hard drive already, right-click and download from the web to keep a copy for posterity, or save the image from a screenshot, you will often need to insert an image from a local file. In Rmarkdown, the path will always be &lt;em&gt;relative to the folder containing your .Rmd file&lt;/em&gt;. So if you keep your .Rmd in &lt;code&gt;/Users/jkirk/SSC442/Example3&lt;/code&gt; and you have a folder &lt;code&gt;/Users/jkirk/SSC442/Example3/images&lt;/code&gt; that contains a file &lt;code&gt;picture.png&lt;/code&gt;, then you would tell Rmarkdown to find the file at &lt;code&gt;./images/picture.png&lt;/code&gt; (which implies its filepath is &lt;code&gt;/Users/jkirk/SSC442/Example3/images/picture.png&lt;/code&gt;). The &lt;code&gt;./&lt;/code&gt; tells R to start looking in the local directory holding the .Rmd file you’re working on.&lt;/p&gt;
&lt;p&gt;Once you know your local relative path, you can use either of the above methods &lt;code&gt;knitr::include_graphics(&#39;./images/picture.png&#39;)&lt;/code&gt; or &lt;code&gt;![](./images/picture.png)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It is possible for R to find your image with an incorrect filepath when you click the “run chunk” button, but then not be able to find it when you knit. This can be very frustrating. It is almost always because you do not have the right &lt;em&gt;relative&lt;/em&gt; filepath.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;gapminder&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data visualization in practice&lt;/h1&gt;
&lt;p&gt;In this chapter, we will demonstrate how relatively simple &lt;strong&gt;ggplot2&lt;/strong&gt; code can create insightful and aesthetically pleasing plots. As motivation we will create plots that help us better understand trends in world health and economics. We will implement what we learned in previous sections of the class and learn how to augment the code to perfect the plots. As we go through our case study, we will describe relevant general data visualization principles.&lt;/p&gt;
&lt;div id=&#34;case-study-new-insights-on-poverty&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: new insights on poverty&lt;/h2&gt;
&lt;p&gt;Hans Rosling&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; was the co-founder of the Gapminder Foundation&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world. The organization uses data to show how actual trends in health and economics contradict the narratives that emanate from sensationalist media coverage of catastrophes, tragedies, and other unfortunate events. As stated in the Gapminder Foundation’s website:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Journalists and lobbyists tell dramatic stories. That’s their job. They tell stories about extraordinary events and unusual people. The piles of dramatic stories pile up in peoples’ minds into an over-dramatic worldview and strong negative stress feelings: “The world is getting worse!”, “It’s we vs. them!”, “Other people are strange!”, “The population just keeps growing!” and “Nobody cares!”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hans Rosling conveyed actual data-based trends in a dramatic way of his own, using effective data visualization. This section is based on two talks that exemplify this approach to education: [New Insights on Poverty]&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; and The Best Stats You’ve Ever Seen&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Specifically, in this section, we use data to attempt to answer the following two questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Is it a fair characterization of today’s world to say it is divided into western rich nations and the developing world in Africa, Asia, and Latin America?&lt;/li&gt;
&lt;li&gt;Has income inequality across countries worsened during the last 40 years?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To answer these questions, we will be using the &lt;code&gt;gapminder&lt;/code&gt; dataset provided in &lt;strong&gt;dslabs&lt;/strong&gt;. This dataset was created using a number of spreadsheets available from the Gapminder Foundation. You can access the table like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
library(ggrepel)
library(ggthemes)
gapminder = dslabs::gapminder %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exploring-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploring the Data&lt;/h3&gt;
&lt;p&gt;Taking an exercise from the &lt;em&gt;New Insights on Poverty&lt;/em&gt; video, we start by testing our knowledge regarding differences in child mortality across different countries. For each of the six pairs of countries below, which country do you think had the highest child mortality rates in 2015? Which pairs do you think are most similar?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sri Lanka or Turkey&lt;/li&gt;
&lt;li&gt;Poland or South Korea&lt;/li&gt;
&lt;li&gt;Malaysia or Russia&lt;/li&gt;
&lt;li&gt;Pakistan or Vietnam&lt;/li&gt;
&lt;li&gt;Thailand or South Africa&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When answering these questions without data, the non-European countries are typically picked as having higher child mortality rates: Sri Lanka over Turkey, South Korea over Poland, and Malaysia over Russia. It is also common to assume that countries considered to be part of the developing world: Pakistan, Vietnam, Thailand, and South Africa, have similarly high mortality rates.&lt;/p&gt;
&lt;p&gt;To answer these questions &lt;strong&gt;with data&lt;/strong&gt;, we can use &lt;strong&gt;dplyr&lt;/strong&gt;. For example, for the first comparison we see that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dslabs::gapminder %&amp;gt;%
  filter(year == 2015 &amp;amp; country %in% c(&amp;quot;Sri Lanka&amp;quot;,&amp;quot;Turkey&amp;quot;)) %&amp;gt;%
  select(country, infant_mortality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     country infant_mortality
## 1 Sri Lanka              8.4
## 2    Turkey             11.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turkey has the higher infant mortality rate.&lt;/p&gt;
&lt;p&gt;We can use this code on all comparisons and find the following:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
country
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
infant mortality
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
country
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
infant mortality
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sri Lanka
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Turkey
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Poland
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Korea
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Malaysia
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Russia
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pakistan
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65.8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Vietnam
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thailand
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10.5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Africa
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that the European countries on this list have higher child mortality rates: Poland has a higher rate than South Korea, and Russia has a higher rate than Malaysia. We also see that Pakistan has a much higher rate than Vietnam, and South Africa has a much higher rate than Thailand. It turns out that when Hans Rosling gave this quiz to educated groups of people, the average score was less than 2.5 out of 5, worse than what they would have obtained had they guessed randomly. This implies that more than ignorant, we are misinformed. In this chapter we see how data visualization helps inform us.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;slope-charts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slope charts&lt;/h3&gt;
&lt;p&gt;The slopechart is informative when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a &lt;em&gt;slope chart&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;There is no geometry for slope charts in &lt;strong&gt;ggplot2&lt;/strong&gt;, but we can construct one using &lt;code&gt;geom_line&lt;/code&gt;. We need to do some tinkering to add labels. We’ll paste together a character stright with the country name and the starting life expectancy, then do the same with just the later life expectancy for the right side. Below is an example comparing 2010 to 2015 for large western countries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;west &amp;lt;- c(&amp;quot;Western Europe&amp;quot;,&amp;quot;Northern Europe&amp;quot;,&amp;quot;Southern Europe&amp;quot;,
          &amp;quot;Northern America&amp;quot;,&amp;quot;Australia and New Zealand&amp;quot;)

dat &amp;lt;- gapminder %&amp;gt;%
  filter(year%in% c(2010, 2015) &amp;amp; region %in% west &amp;amp;
           !is.na(life_expectancy) &amp;amp; population &amp;gt; 10^7) %&amp;gt;%
    mutate(label_first = ifelse(year == 2010, paste0(country, &amp;quot;: &amp;quot;, round(life_expectancy, 1), &amp;#39; years&amp;#39;), NA),
           label_last = ifelse(year == 2015,  paste0(round(life_expectancy, 1),&amp;#39; years&amp;#39;), NA))

dat %&amp;gt;%
  mutate(location = ifelse(year == 2010, 1, 2),
         location = ifelse(year == 2015 &amp;amp;
                             country %in% c(&amp;quot;United Kingdom&amp;quot;, &amp;quot;Portugal&amp;quot;),
                           location+0.22, location),
         hjust = ifelse(year == 2010, 1, 0)) %&amp;gt;%
  mutate(year = as.factor(year)) %&amp;gt;%
  ggplot(aes(year, life_expectancy, group = country)) +
  geom_line(aes(color = country), show.legend = FALSE) +
  geom_text_repel(aes(label = label_first, color = country), direction = &amp;#39;y&amp;#39;, nudge_x = -1, seed = 1234, show.legend = FALSE) +
  geom_text_repel(aes(label = label_last, color = country), direction = &amp;#39;y&amp;#39;, nudge_x =  1, seed = 1234, show.legend = FALSE) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;Life Expectancy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/slope-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/scatter-plot-instead-of-slope-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the scatterplot, we have followed the principle &lt;em&gt;use common axes&lt;/em&gt; since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bland-altman-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bland-Altman plot&lt;/h3&gt;
&lt;p&gt;Since we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also known as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(country) %&amp;gt;%
  filter(year %in% c(2010, 2015)) %&amp;gt;%
  dplyr::summarize(average = mean(life_expectancy),
                   difference = life_expectancy[year==2015]-life_expectancy[year==2010]) %&amp;gt;%
  ggplot(aes(average, difference, label = country)) +
  geom_point() +
  geom_text_repel() +
  geom_abline(lty = 2) +
  xlab(&amp;quot;Average of 2010 and 2015&amp;quot;) +
  ylab(&amp;quot;Difference between 2015 and 2010&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/bland-altman-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis. You already made a similar Altman plot in an earlier problem set, so we’ll move on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bump-charts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bump charts&lt;/h3&gt;
&lt;p&gt;Finally, we can make a bump chart that shows changes in rankings over time. We’ll look at fertility in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the &lt;code&gt;rank()&lt;/code&gt; function to rank countries by the &lt;code&gt;fertility&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sa_fe &amp;lt;- gapminder %&amp;gt;%
  filter(region == &amp;quot;Southern Asia&amp;quot;) %&amp;gt;%
  filter(year &amp;gt;= 2004, year &amp;lt; 2015) %&amp;gt;%
  group_by(year) %&amp;gt;%
  mutate(rank = rank(fertility))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then plot this with points and lines, reversing the y-axis so 1 is at the top:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sa_fe, aes(x = year, y = rank, color = country)) +
  geom_line() +
  geom_point() +
  scale_y_reverse(breaks = 1:8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/make-bump-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Iran holds the number 1 spot, while Sri Lanka dropped from 2 to 6, and Bangladesh increased from 4 to 2.&lt;/p&gt;
&lt;p&gt;As with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use &lt;code&gt;geom_text()&lt;/code&gt; again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the &lt;code&gt;data&lt;/code&gt; argument in &lt;code&gt;geom_text()&lt;/code&gt; though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot &amp;lt;- ggplot(sa_fe, aes(x = year, y = rank, color = country)) +
  geom_line(size = 2) +
  geom_point(size = 4) +
  geom_text(data = sa_fe %&amp;gt;% dplyr::filter(year==2004) %&amp;gt;% arrange(rank),
            aes(label = country, x = 2003), fontface = &amp;quot;bold&amp;quot;, angle = 45) +
 geom_text(data = sa_fe %&amp;gt;% dplyr::filter(year==2014) %&amp;gt;% arrange(rank),
            aes(label = country, x = 2015), fontface = &amp;quot;bold&amp;quot;, angle = 45) + 
  guides(color = FALSE) +
  scale_y_reverse(breaks = 1:8) +
  scale_x_continuous(breaks = 2004:2014) +
  scale_color_viridis_d(option = &amp;quot;C&amp;quot;, begin = 0.2, end = 0.9) +
  labs(x = NULL, y = &amp;quot;Rank&amp;quot;)

bumpplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/bump-plot-fancier-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you want to be &lt;em&gt;super&lt;/em&gt; fancy, you can use flags instead of country codes, but that’s a little more complicated (you need to install the &lt;a href=&#34;https://github.com/rensa/ggflags&#34;&gt;&lt;strong&gt;ggflags&lt;/strong&gt; package&lt;/a&gt;. &lt;a href=&#34;https://dominikkoch.github.io/Bump-Chart/&#34;&gt;See here for an example&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;themes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Themes&lt;/h3&gt;
&lt;p&gt;We can go a little further towards a clean, easy-to-read data visualization by using &lt;strong&gt;themes&lt;/strong&gt; in our plots. Themes allow us to set a particular range of plot settings in one command, and let us further tweak things like fonts, background colors, and much more. We’ve used them in passing a few times without highlighting them, but we’ll discuss them here.&lt;/p&gt;
&lt;p&gt;A pre-constructed set of instructions for making a visual theme can be had by using a theme’s &lt;code&gt;ggplot&lt;/code&gt; function. Let’s look at two of my favorites.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;theme_bw()&lt;/code&gt; uses the black-and-white theme, which is helpful in making a nice, clean plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The background shading is gone, which gives the plot a nice, crisp feel. It adds the black outline around the plot, but doesn’t mess with the colors in the plot.&lt;/p&gt;
&lt;p&gt;Here’s &lt;code&gt;theme_minimal()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Themes can alter things in the plot as well. If we really want to strip it down and remove the Y-axis (which is rarely a good idea, but in a bump chart, it makes sense):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot + theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now &lt;strong&gt;that’s&lt;/strong&gt; clean!&lt;/p&gt;
&lt;p&gt;In our opening unit, we had a plot that was styled after the plots in the magazine, &lt;em&gt;The Economist&lt;/em&gt;. That’s a theme (in the &lt;code&gt;ggthemes&lt;/code&gt; package that we loaded at the top)!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot + theme_economist()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Themes affect some of the plot elements that we haven’t gotten much into (like length of axis ticks and the color of the panel grid behind the plot). We can use a theme, then make further changes to the theme. We won’t go into a lot of detail, but here’s an example. Use the &lt;code&gt;?theme&lt;/code&gt; to learn more about what you can change. Half the challenge is finding the right term for the thing you want to tweak! Theme changes occur in code order, so you can update a pre-set theme with your own details:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot +   theme_bw() + theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;),
                   plot.title = element_text(face = &amp;quot;bold&amp;quot;),
                   axis.text.x = element_text(angle = 45, hjust = 1),
                   panel.grid.major.y = element_blank(), # turn off all of the Y grid
                   panel.grid.minor.y = element_blank(),
                   panel.grid.minor.x = element_blank()) # turn off small x grid&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/bump-plot-fancierest-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;small-multiples&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Small multiples&lt;/h3&gt;
&lt;p&gt;First we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;life_expectancy_small &amp;lt;- gapminder %&amp;gt;%
  filter(country %in% c(&amp;quot;Argentina&amp;quot;, &amp;quot;Bolivia&amp;quot;, &amp;quot;Brazil&amp;quot;,
                        &amp;quot;Belize&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Chile&amp;quot;))
ggplot(data = life_expectancy_small,
       mapping = aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_wrap(vars(country))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Small multiples! That’s all we need to do.&lt;/p&gt;
&lt;p&gt;We can do some fancier things, though. We can make this plot hyper minimalist with a &lt;strong&gt;theme&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = life_expectancy_small,
       mapping = aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_wrap(vars(country), scales = &amp;quot;free_y&amp;quot;) +
  theme_void() +
  theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/life-expectancy-small-minimalist-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can do a whole part of a continent (poor Syria 😿)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;life_expectancy_mena &amp;lt;- gapminder %&amp;gt;%
  filter(region == &amp;quot;Northern Africa&amp;quot; | region == &amp;quot;Western Asia&amp;quot;)

ggplot(data = life_expectancy_mena,
       mapping = aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_wrap(vars(country), scales = &amp;quot;free_y&amp;quot;, nrow = 3) +
  theme_void() +
  theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/life-expectancy-mena-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use the &lt;a href=&#34;https://hafen.github.io/geofacet/&#34;&gt;&lt;strong&gt;geofacet&lt;/strong&gt; package&lt;/a&gt; to arrange these facets by geography:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(geofacet)

life_expectancy_eu &amp;lt;- gapminder %&amp;gt;%
  filter(region== &amp;#39;Western Europe&amp;#39; | region==&amp;#39;Northern Europe&amp;#39; | region==&amp;#39;Southern Europe&amp;#39;)

ggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_geo(vars(country), grid = &amp;quot;europe_countries_grid1&amp;quot;, scales = &amp;quot;free_y&amp;quot;) +
  labs(x = NULL, y = NULL, title = &amp;quot;Life expectancy from 1960–2015&amp;quot;,
       caption = &amp;quot;Source: Gapminder&amp;quot;) +
  theme_minimal() +
  theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;),
        plot.title = element_text(face = &amp;quot;bold&amp;quot;),
        axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/life-expectancy-eu-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Neat!&lt;/p&gt;
&lt;p&gt;Anybody see any problems here?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sparklines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sparklines&lt;/h3&gt;
&lt;p&gt;Sparklines are just line charts (or bar charts) that are really really small.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;india_fert &amp;lt;- gapminder %&amp;gt;%
  filter(country == &amp;quot;India&amp;quot;)
plot_india &amp;lt;- ggplot(india_fert, aes(x = year, y = fertility)) +
  geom_line() +
  theme_void()
plot_india&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/india-spark-1.png&#34; width=&#34;96&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;india_fert.pdf&amp;quot;, plot_india, width = 1, height = 0.15, units = &amp;quot;in&amp;quot;)
ggsave(&amp;quot;india_fert.png&amp;quot;, plot_india, width = 1, height = 0.15, units = &amp;quot;in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;china_fert &amp;lt;- gapminder %&amp;gt;%
  filter(country == &amp;quot;China&amp;quot;)
plot_china &amp;lt;- ggplot(china_fert, aes(x = year, y = fertility)) +
  geom_line() +
  theme_void()
plot_china&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/china-spark-1.png&#34; width=&#34;96&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;china_fert.pdf&amp;quot;, plot_china, width = 1, heighlt = 0.15, units = &amp;quot;in&amp;quot;)
ggsave(&amp;quot;china_fert.png&amp;quot;, plot_china, width = 1, height = 0.15, units = &amp;quot;in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then use those saved tiny plots in your text (with a little html extra in there).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Both India &amp;lt;img class=&amp;quot;img-inline&amp;quot; src=&amp;quot;/your/path/to/india_fert.png&amp;quot; width = &amp;quot;100&amp;quot;/&amp;gt; and 
China &amp;lt;img class=&amp;quot;img-inline&amp;quot; src=&amp;quot;/your/path/to/china-fert.png&amp;quot; width = &amp;quot;100&amp;quot;/&amp;gt; have 
seen decreased fertility over the past 20 years.&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Both India &lt;img class=&#34;img-inline&#34; src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/india-spark-1.png&#34; width = &#34;100&#34;/&gt; and China &lt;img class=&#34;img-inline&#34; src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/china-spark-1.png&#34; width = &#34;100&#34;/&gt; have seen decreased fertility over the past 20 years.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-ecological-fallacy-and-importance-of-showing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The ecological fallacy and importance of showing the data&lt;/h2&gt;
&lt;p&gt;Throughout this section, we have been comparing regions of the world. We have seen that, on average, some regions do better than others. In this section, we focus on describing the importance of variability within the groups when examining the relationship between a country’s infant mortality rates and average income.&lt;/p&gt;
&lt;p&gt;We define a few more regions and compare the averages across regions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/ecological-fallacy-averages-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The relationship between these two variables is almost perfectly linear and the graph shows a dramatic difference. While in the West less than 0.5% of infants die, in Sub-Saharan Africa the rate is higher than 6%!&lt;/p&gt;
&lt;p&gt;Note that the plot uses a new transformation, the logit transformation.&lt;/p&gt;
&lt;div id=&#34;logit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logit transformation&lt;/h3&gt;
&lt;p&gt;The logit transformation for a proportion or rate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(p) = \log \left( \frac{p}{1-p} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is a proportion or probability, the quantity that is being logged, &lt;span class=&#34;math inline&#34;&gt;\(p/(1-p)\)&lt;/span&gt;, is called the &lt;em&gt;odds&lt;/em&gt;. In this case &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the proportion of infants that survived. The odds tell us how many more infants are expected to survive than to die. The log transformation makes this symmetric. If the rates are the same, then the log odds is 0. Fold increases or decreases turn into positive and negative increments, respectively.&lt;/p&gt;
&lt;p&gt;This scale is useful when we want to highlight differences near 0 or 1. For survival rates this is important because a survival rate of 90% is unacceptable, while a survival of 99% is relatively good. We would much prefer a survival rate closer to 99.9%. We want our scale to highlight these difference and the logit does this. Note that 99.9/0.1 is about 10 times bigger than 99/1 which is about 10 times larger than 90/10. By using the log, these fold changes turn into constant increases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;show-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Show the data&lt;/h3&gt;
&lt;p&gt;Now, back to our plot. Based on the plot above, do we conclude that a country with a low income is destined to have low survival rate? Do we conclude that survival rates in Sub-Saharan Africa are all lower than in Southern Asia, which in turn are lower than in the Pacific Islands, and so on?&lt;/p&gt;
&lt;p&gt;Jumping to this conclusion based on a plot showing averages is referred to as the &lt;em&gt;ecological fallacy&lt;/em&gt;. The almost perfect relationship between survival rates and income is only observed for the averages at the region level. Once we show all the data, we see a somewhat more complicated story:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/ecological-fallacy-all-data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Specifically, we see that there is a large amount of variability. We see that countries from the same regions can be quite different and that countries with the same income can have different survival rates. For example, while on average Sub-Saharan Africa had the worse health and economic outcomes, there is wide variability within that group. Mauritius and Botswana are doing better than Angola and Sierra Leone, with Mauritius comparable to Western countries.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vaccines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: vaccines and infectious diseases&lt;/h2&gt;
&lt;p&gt;Vaccines have helped save millions of lives. In the 19th century, before herd immunization was achieved through vaccination programs, deaths from infectious diseases, such as smallpox and polio, were common. However, today vaccination programs have become somewhat controversial despite all the scientific evidence for their importance.&lt;/p&gt;
&lt;p&gt;The controversy started with a paper&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; published in 1988 and led by Andrew Wakefield claiming
there was a link between the administration of the measles, mumps, and rubella (MMR) vaccine and the appearance of autism and bowel disease.
Despite much scientific evidence contradicting this finding, sensationalist media reports and fear-mongering from conspiracy theorists led parts of the public into believing that vaccines were harmful. As a result, many parents ceased to vaccinate their children. This dangerous practice can be potentially disastrous given that the Centers for Disease Control (CDC) estimates that vaccinations will prevent more than 21 million hospitalizations and 732,000 deaths among children born in the last 20 years (see Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;).
The 1988 paper has since been retracted and Andrew Wakefield was eventually “struck off the UK medical register, with a statement identifying deliberate falsification in the research published in The Lancet, and was thereby barred from practicing medicine in the UK.” (source: Wikipedia&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;). Yet misconceptions persist, in part due to self-proclaimed activists who continue to disseminate misinformation about vaccines.&lt;/p&gt;
&lt;p&gt;Effective communication of data is a strong antidote to misinformation and fear-mongering. Earlier we used an example provided by a Wall Street Journal article&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; showing data related to the impact of vaccines on battling infectious diseases. Here we reconstruct that example.&lt;/p&gt;
&lt;p&gt;The data used for these plots were collected, organized, and distributed by the Tycho Project&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. They include weekly reported counts for seven diseases from 1928 to 2011, from all fifty states. The yearly totals are helpfully included in the &lt;strong&gt;dslabs&lt;/strong&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RColorBrewer)
data(us_contagious_diseases)
names(us_contagious_diseases)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;disease&amp;quot;         &amp;quot;state&amp;quot;           &amp;quot;year&amp;quot;            &amp;quot;weeks_reporting&amp;quot;
## [5] &amp;quot;count&amp;quot;           &amp;quot;population&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a temporary object &lt;code&gt;dat&lt;/code&gt; that stores only the measles data, includes a per 100,000 rate, orders states by average value of disease and removes Alaska and Hawaii since they only became states in the late 1950s. Note that there is a &lt;code&gt;weeks_reporting&lt;/code&gt; column that tells us for how many weeks of the year data was reported. We have to adjust for that value when computing the rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_disease &amp;lt;- &amp;quot;Measles&amp;quot;
dat &amp;lt;- us_contagious_diseases %&amp;gt;%
  filter(!state%in%c(&amp;quot;Hawaii&amp;quot;,&amp;quot;Alaska&amp;quot;) &amp;amp; disease == the_disease) %&amp;gt;%
  mutate(rate = count / population * 10000 * 52 / weeks_reporting) %&amp;gt;%
  mutate(state = reorder(state, rate))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now easily plot disease rates per year. Here are the measles data from California:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% filter(state == &amp;quot;California&amp;quot; &amp;amp; !is.na(rate)) %&amp;gt;%
  ggplot(aes(year, rate)) +
  geom_line() +
  ylab(&amp;quot;Cases per 10,000&amp;quot;)  +
  geom_vline(xintercept=1963, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/california-measles-time-series-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We add a vertical line at 1963 since this is when the vaccine was introduced [Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 (the yellow book). p. 250. ISBN 9780199948505].&lt;/p&gt;
&lt;p&gt;Now can we show data for all states in one plot? We have three variables to show: year, state, and rate. In the WSJ figure, they use the x-axis for year, the y-axis for state, and color hue to represent rates. However, the color scale they use, which goes from yellow to blue to green to orange to red, can be improved.&lt;/p&gt;
&lt;p&gt;In our example, we want to use a sequential palette since there is no meaningful center, just low and high rates.&lt;/p&gt;
&lt;p&gt;We use the geometry &lt;code&gt;geom_tile&lt;/code&gt; to tile the region with colors representing disease rates. We use a square root transformation to avoid having the really high counts dominate the plot. Notice that missing values are shown in grey. Note that once a disease was pretty much eradicated, some states stopped reporting cases all together. This is why we see so much grey after 1980.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% ggplot(aes(year, state, fill = rate)) +
  geom_tile(color = &amp;quot;grey50&amp;quot;) +
  scale_x_continuous(expand=c(0,0)) +
  scale_fill_gradientn(colors = brewer.pal(9, &amp;quot;Reds&amp;quot;), trans = &amp;quot;sqrt&amp;quot;) +
  geom_vline(xintercept=1963, col = &amp;quot;blue&amp;quot;) +
  theme_minimal() +
  theme(panel.grid = element_blank(),
        legend.position=&amp;quot;bottom&amp;quot;,
        text = element_text(size = 8)) +
  ggtitle(the_disease) +
  ylab(&amp;quot;&amp;quot;) + xlab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/vaccines-plot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot makes a very striking argument for the contribution of vaccines. However, one limitation of this plot is that it uses color to represent quantity, which we earlier explained makes it harder to know exactly how high values are going. Position and lengths are better cues. If we are willing to lose state information, we can make a version of the plot that shows the values with position. We can also show the average for the US, which we compute like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg &amp;lt;- us_contagious_diseases %&amp;gt;%
  filter(disease==the_disease) %&amp;gt;% group_by(year) %&amp;gt;%
  summarize(us_rate = sum(count, na.rm = TRUE) /
              sum(population, na.rm = TRUE) * 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to make the plot we simply use the &lt;code&gt;geom_line&lt;/code&gt; geometry:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  filter(!is.na(rate)) %&amp;gt;%
    ggplot() +
  geom_line(aes(year, rate, group = state),  color = &amp;quot;grey50&amp;quot;,
            show.legend = FALSE, alpha = 0.2, size = 1) +
  geom_line(mapping = aes(year, us_rate),  data = avg, size = 1) +
  scale_y_continuous(trans = &amp;quot;sqrt&amp;quot;, breaks = c(5, 25, 125, 300)) +
  ggtitle(&amp;quot;Cases per 10,000 by state&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  geom_text(data = data.frame(x = 1955, y = 50),
            mapping = aes(x, y, label=&amp;quot;US average&amp;quot;),
            color=&amp;quot;black&amp;quot;) +
  geom_vline(xintercept=1963, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/time-series-vaccines-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In theory, we could use color to represent the categorical value state, but it is hard to pick 50 distinct colors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;try-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/h2&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Reproduce the heatmap plot we previously made but for smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now reproduce the time series plot we previously made, but this time following the instructions of the previous question for smallpox.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the state of California, make a time series plot showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now do the same for the rates for the US. Hint: compute the US rate by using summarize: the total divided by total population.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Hans_Rosling&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Hans_Rosling&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.gapminder.org/&#34; class=&#34;uri&#34;&gt;http://www.gapminder.org/&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract&#34; class=&#34;uri&#34;&gt;http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm&#34; class=&#34;uri&#34;&gt;https://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Andrew_Wakefield&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Andrew_Wakefield&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://graphics.wsj.com/infectious-diseases-and-vaccines/&#34; class=&#34;uri&#34;&gt;http://graphics.wsj.com/infectious-diseases-and-vaccines/&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.tycho.pitt.edu/&#34; class=&#34;uri&#34;&gt;http://www.tycho.pitt.edu/&lt;/a&gt;&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonparametric Regression</title>
      <link>https://ssc442.netlify.app/example/09-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/09-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#setup&#34; id=&#34;toc-setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#breakout-1---predicting-wage&#34; id=&#34;toc-breakout-1---predicting-wage&#34;&gt;Breakout #1 - Predicting Wage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#automating-models&#34; id=&#34;toc-automating-models&#34;&gt;Automating models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#loops&#34; id=&#34;toc-loops&#34;&gt;Loops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-lapply-to-repeat-something&#34; id=&#34;toc-using-lapply-to-repeat-something&#34;&gt;Using &lt;code&gt;lapply&lt;/code&gt; to repeat something&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#connection-to-previous-labs&#34; id=&#34;toc-connection-to-previous-labs&#34;&gt;Connection to Previous Lab(s)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-2-looping-through-the-trees&#34; id=&#34;toc-part-2-looping-through-the-trees&#34;&gt;Part 2: Looping through the Trees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-3-if-time-evaluating-your-trees&#34; id=&#34;toc-part-3-if-time-evaluating-your-trees&#34;&gt;Part 3 (if time): Evaluating your Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;We are going to use the &lt;code&gt;wooldridge::wage2&lt;/code&gt; data on wages to generate and tune a non-parametric model of wages using a regression tree (or decision tree, same thing). Use &lt;code&gt;?wage2&lt;/code&gt; (after you’ve loaded the &lt;code&gt;wooldridge&lt;/code&gt; package) to see the data dictionary.&lt;/p&gt;
&lt;p&gt;We’ve learned that our RMSE calculations have a hard time with &lt;code&gt;NA&lt;/code&gt;s in the data. So let’s use the &lt;code&gt;skim&lt;/code&gt; output to tell us which variables have too many &lt;code&gt;NA&lt;/code&gt; (see &lt;code&gt;n_missing&lt;/code&gt;) values and should be dropped. Let’s set a high bar here, and drop anything that isn’t 100% complete. Of course, there are other things we can do (impute the &lt;code&gt;NA&lt;/code&gt;s, or make dummies for them), but for now, it’s easiest to drop them.&lt;/p&gt;
&lt;p&gt;Remember, &lt;code&gt;skim()&lt;/code&gt; is for your own exploration, it’s not something that you should include in your output.&lt;/p&gt;
&lt;p&gt;Once cleaned, we should be able to use &lt;code&gt;rpart(wage ~ ., data = wage_clean)&lt;/code&gt; and not have any &lt;code&gt;NA&lt;/code&gt;s in our prediction. That’s what we’ll do in our first Breakout.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;breakout-1---predicting-wage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Breakout #1 - Predicting Wage&lt;/h2&gt;
&lt;p&gt;I’m going to have you form groups of 2-3 to work on live-coding. One person in the group will need to have Rstudio up, be able to share screen, and have the correct packages loaded (&lt;code&gt;caret&lt;/code&gt;, &lt;code&gt;rpart&lt;/code&gt;, and &lt;code&gt;rpart.plot&lt;/code&gt;, plus &lt;code&gt;skimr&lt;/code&gt;). Copy the &lt;code&gt;rmse&lt;/code&gt; and &lt;code&gt;get_rmse&lt;/code&gt; code into a blank R script (you don’t have to use RMarkdown, just use a blank R script and run from there). You’ll also want to have these functions from last week loaded:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the first fifteen minutes, all I want you to do is the following. We are trying to predict &lt;code&gt;wage&lt;/code&gt; for this exercise:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Estimate a decision tree on &lt;code&gt;wage_clean&lt;/code&gt; using the default parameters (use the whole dataset for now, we’ll train-test on the problem set).&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;rpart.plot&lt;/code&gt; to vizualize your regression tree, and &lt;em&gt;talk through the interpretation&lt;/em&gt; with each other.&lt;/li&gt;
&lt;li&gt;Calculate the RMSE for your regression tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again, we’ll spend about 15 minutes on this. Remember, you can use &lt;code&gt;?wage2&lt;/code&gt; to see the variable names. Make sure you know what variables are showing up in the plot and explaining &lt;code&gt;wage&lt;/code&gt; in your model. You may find something odd at first and may need to drop more variables…&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automating-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automating models&lt;/h2&gt;
&lt;p&gt;Let’s talk about a little code shortcut that helps iterate through your model selection.&lt;/p&gt;
&lt;p&gt;First, before, we used &lt;code&gt;list()&lt;/code&gt; to store all of our models. This is because &lt;code&gt;list()&lt;/code&gt; can “hold” anything at all, unlike a &lt;code&gt;matrix&lt;/code&gt;, which is only numeric, or a &lt;code&gt;data.frame&lt;/code&gt; which needs all rows in a column to be the same data type. &lt;code&gt;list()&lt;/code&gt; is also recursive, so each element in a list can be a list. Of lists. Of lists!&lt;/p&gt;
&lt;p&gt;Lists are also really easy to add to iteratively. We can initiate an empty list using &lt;code&gt;myList &amp;lt;- list()&lt;/code&gt;, then we can add things to it. Note that we use the double &lt;code&gt;[&lt;/code&gt; to index:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList &amp;lt;- list()
myList[[&amp;#39;first&amp;#39;]] = &amp;#39;This is the first thing on my list&amp;#39;
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] &amp;quot;This is the first thing on my list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lists let you name the “containers” (much like you can name colums in a &lt;code&gt;data.frame&lt;/code&gt;). Our first one is called “first”. We can add more later, but they always have to be unique:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList[[&amp;#39;second&amp;#39;]] = c(1,2,3)
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] &amp;quot;This is the first thing on my list&amp;quot;
## 
## $second
## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And still more:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList[[&amp;#39;third&amp;#39;]] = data.frame(a = c(1,2,3), b = c(&amp;#39;Albert&amp;#39;,&amp;#39;Alex&amp;#39;,&amp;#39;Alice&amp;#39;))
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] &amp;quot;This is the first thing on my list&amp;quot;
## 
## $second
## [1] 1 2 3
## 
## $third
##   a      b
## 1 1 Albert
## 2 2   Alex
## 3 3  Alice&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a &lt;code&gt;data.frame&lt;/code&gt; in there! We can use &lt;code&gt;lapply&lt;/code&gt; to do something to every element in the list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(myList, length) # the length() function with the first entry being the list element&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] 1
## 
## $second
## [1] 3
## 
## $third
## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get back a list of equal length but each (still-named) container is now the length of the original list’s contents.&lt;/p&gt;
&lt;div id=&#34;loops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loops&lt;/h3&gt;
&lt;p&gt;R has a very useful looping function that takes the form:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in c(&amp;#39;first&amp;#39;,&amp;#39;second&amp;#39;,&amp;#39;third&amp;#39;)){
print(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;first&amp;quot;
## [1] &amp;quot;second&amp;quot;
## [1] &amp;quot;third&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, &lt;code&gt;R&lt;/code&gt; is repeating the thing in the loop (&lt;code&gt;print(i)&lt;/code&gt;) with a different value for &lt;code&gt;i&lt;/code&gt; each time. &lt;code&gt;R&lt;/code&gt; repeats only what is &lt;em&gt;inside the curly-brackets&lt;/em&gt;, then when it reaches the close-curly-bracket, it goes back to the top, changes &lt;code&gt;i&lt;/code&gt; to the next element, and repeats.&lt;/p&gt;
&lt;p&gt;We can use this to train our models. First, we clear our list object &lt;code&gt;myList&lt;/code&gt; by setting it equal to an empty list. Then, we loop over some regression tree tuning parameters. First, we have to figure out how to use the loop to set a &lt;em&gt;unique&lt;/em&gt; name for each list container. To do this, we’ll use &lt;code&gt;paste0(&#39;Tuning&#39;,i)&lt;/code&gt; which will result in a character string of &lt;code&gt;Tuning0&lt;/code&gt; when &lt;code&gt;i=0&lt;/code&gt;, &lt;code&gt;Tuning0.01&lt;/code&gt; when &lt;code&gt;i=0.01&lt;/code&gt;, etc.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList &amp;lt;- list()   # resets the list. Otherwise, you&amp;#39;ll just be adding to your old list!

for(i in c(0, 0.01, 0.02)){
  myList[[paste0(&amp;#39;Tuning&amp;#39;,i)]] = rpart(wage ~ ., data = wage_clean, cp = i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you use &lt;code&gt;names(myList)&lt;/code&gt; you’ll see the result of our &lt;code&gt;paste0&lt;/code&gt; naming. If you want to see the plotted results, you can use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(myList[[&amp;#39;Tuning0.01&amp;#39;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/09-example_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-lapply-to-repeat-something&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using &lt;code&gt;lapply&lt;/code&gt; to repeat something&lt;/h3&gt;
&lt;p&gt;Once we have a list built, &lt;code&gt;lapply&lt;/code&gt; lets us do something to each element of the list, and then returns a list of results (of the same length as the input).&lt;/p&gt;
&lt;p&gt;Note that in the above &lt;code&gt;myList&lt;/code&gt;, we don’t actually have the &lt;code&gt;cp&lt;/code&gt; parameter recorded anywhere except in the name of the list element. If we want to plot with &lt;code&gt;cp&lt;/code&gt; on the x-axis, we’d need to have the number. Later on, we’ll cover text as data and learn how to extract it, but an easier way is to &lt;code&gt;lapply&lt;/code&gt; a function that extracts the &lt;code&gt;cp&lt;/code&gt; used over the list.&lt;/p&gt;
&lt;p&gt;You can retrieve the &lt;code&gt;cp&lt;/code&gt; and &lt;code&gt;minsplit&lt;/code&gt; arguments from the tree by noting that the rpart object is itself technically a list, and it has an element called &lt;code&gt;control&lt;/code&gt; that is itself a list, and that list has &lt;code&gt;cp&lt;/code&gt; and &lt;code&gt;minsplit&lt;/code&gt; as used in the estimation. You can use the &lt;code&gt;$&lt;/code&gt; accessor on lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myTree = myList[[2]]

myTree$control$cp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only problem here is that we don’t have a function written that does &lt;code&gt;$control$cp&lt;/code&gt;. We can use an &lt;em&gt;anonymous function&lt;/em&gt; in &lt;code&gt;lapply&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cp_list = lapply(myList, function(x){
  x$control$cp
})

unlist(cp_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Tuning0 Tuning0.01 Tuning0.02 
##       0.00       0.01       0.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, it gets us the right answer, but whaaaaaat is going on? Curly brackets? &lt;code&gt;x&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;This is an “anonymous function”, or a function created on the fly. Here’s how it works in &lt;code&gt;lapply&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first argument is the list you want to do something to&lt;/li&gt;
&lt;li&gt;The second argument would usually be the function you want to apply, like &lt;code&gt;get_rmse&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Here, we’re going to ask R to &lt;em&gt;temporarily&lt;/em&gt; create a function that takes one argument, &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is going to be each list element in &lt;code&gt;myList&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Think of it as a loop:
&lt;ul&gt;
&lt;li&gt;Take the first element of &lt;code&gt;myList&lt;/code&gt; and refer to it as &lt;code&gt;x&lt;/code&gt;. Run the function’s code.&lt;/li&gt;
&lt;li&gt;Then it’ll take the second element of &lt;code&gt;myList&lt;/code&gt; and refer to it as &lt;code&gt;x&lt;/code&gt; and run the function’s code.&lt;/li&gt;
&lt;li&gt;Repeat until all elements of &lt;code&gt;x&lt;/code&gt; have been used.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Once the anonymous function has been applied to &lt;code&gt;x&lt;/code&gt;, the result is passed back and saved as the new element of the list output, always in the same position from where the &lt;code&gt;x&lt;/code&gt; was taken.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So you can think of it like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x = myList[[1]]
myList[[1]] = x$control$cp

x = myList[[2]]
myList[[2]] = x$control$cp

x = myList[[3]]
myList[[3]] = x$control$cp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As long as every entry in &lt;code&gt;myList&lt;/code&gt; is an rpart object with &lt;code&gt;control$cp&lt;/code&gt;, then you’ll get those values in a list. &lt;code&gt;unlist&lt;/code&gt; just coerces the list to a vector. This is also the same way we get the RMSE, but instead of the anonymous function, we write one out and use that.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;connection-to-previous-labs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Connection to Previous Lab(s)&lt;/h3&gt;
&lt;p&gt;In a previous lab (on linear models) you were asked to make increasingly complex models by adding variables. In regression trees and kNN, complexity comes from a tuning parameter. But many people have asked about ways of doing Lab 7, so here are two possible solutions.&lt;/p&gt;
&lt;div id=&#34;solution-1-make-a-list-of-models&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Solution 1: Make a list of models&lt;/h4&gt;
&lt;p&gt;First, we make a vector of all of our possible explanatory variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wagenames = names(wage2) 
wagenames = wagenames[!wagenames %in% c(&amp;#39;wage&amp;#39;,&amp;#39;lwage&amp;#39;)]
wagenames&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;hours&amp;quot;   &amp;quot;IQ&amp;quot;      &amp;quot;KWW&amp;quot;     &amp;quot;educ&amp;quot;    &amp;quot;exper&amp;quot;   &amp;quot;tenure&amp;quot;  &amp;quot;age&amp;quot;    
##  [8] &amp;quot;married&amp;quot; &amp;quot;black&amp;quot;   &amp;quot;south&amp;quot;   &amp;quot;urban&amp;quot;   &amp;quot;sibs&amp;quot;    &amp;quot;brthord&amp;quot; &amp;quot;meduc&amp;quot;  
## [15] &amp;quot;feduc&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we’ll do all the interactions, quadratic terms, and put them all in one long vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wagenamesintx = expand.grid(wagenames, wagenames)
wagenamesintx = paste0(wagenamesintx[,1], &amp;#39;:&amp;#39;, wagenamesintx[,2])

wagenamessquared = paste0(wagenames, &amp;#39;^2&amp;#39;)

wageparameters = c(wagenames, wagenamesintx, wagenamessquared)
wageparameters[c(1,10,100,200)] # just to see a few&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;hours&amp;quot;         &amp;quot;south&amp;quot;         &amp;quot;south:tenure&amp;quot;  &amp;quot;exper:brthord&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we need to make a list of formulas that start with &lt;code&gt;wage ~&lt;/code&gt; and include an ever-increasing number of entries in &lt;code&gt;wageparameters&lt;/code&gt; (and, to be nested, always includes the previous iteration). We’ll do this by taking an index of &lt;code&gt;[1:N]&lt;/code&gt; where &lt;code&gt;N&lt;/code&gt; gets bigger.&lt;/p&gt;
&lt;p&gt;There are 255 possible entries, so we want to sequence along that and always need round numbers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;complexity = round(seq(from = 1, to = length(wageparameters),  length.out = 15)) 

modelList = lapply(complexity, function(x){
  list_formula_start = &amp;#39;wage ~ &amp;#39;
  list_formula_rhs = paste0(wageparameters[1:x], collapse = &amp;#39; + &amp;#39;)
  return(paste0(list_formula_start, list_formula_rhs))
})

print(modelList[1:3])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;wage ~ hours&amp;quot;
## 
## [[2]]
## [1] &amp;quot;wage ~ hours + IQ + KWW + educ + exper + tenure + age + married + black + south + urban + sibs + brthord + meduc + feduc + hours:hours + IQ:hours + KWW:hours + educ:hours&amp;quot;
## 
## [[3]]
## [1] &amp;quot;wage ~ hours + IQ + KWW + educ + exper + tenure + age + married + black + south + urban + sibs + brthord + meduc + feduc + hours:hours + IQ:hours + KWW:hours + educ:hours + exper:hours + tenure:hours + age:hours + married:hours + black:hours + south:hours + urban:hours + sibs:hours + brthord:hours + meduc:hours + feduc:hours + hours:IQ + IQ:IQ + KWW:IQ + educ:IQ + exper:IQ + tenure:IQ + age:IQ&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we start using &lt;code&gt;lapply&lt;/code&gt; to estimate the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myOLS = lapply(modelList, function(y){
  lm(y, data = wage2)
})

lapply(myOLS[1:3], summary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## 
## Call:
## lm(formula = y, data = wage2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -839.72 -287.21  -52.38  200.46 2131.26 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  981.315     81.575   12.03   &amp;lt;2e-16 ***
## hours         -0.532      1.832   -0.29    0.772    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 404.6 on 933 degrees of freedom
## Multiple R-squared:  9.033e-05,	Adjusted R-squared:  -0.0009814 
## F-statistic: 0.08429 on 1 and 933 DF,  p-value: 0.7716
## 
## 
## [[2]]
## 
## Call:
## lm(formula = y, data = wage2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -823.41 -209.85  -36.73  169.56 1945.67 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -333.4545   673.8072  -0.495 0.620853    
## hours        -12.1500    14.0223  -0.866 0.386553    
## IQ             8.4166     6.9960   1.203 0.229395    
## KWW          -16.7569    12.7468  -1.315 0.189112    
## educ          29.6737    48.0285   0.618 0.536903    
## exper          9.9812     4.4862   2.225 0.026435 *  
## tenure         2.9739     2.9258   1.016 0.309808    
## age            8.9932     6.0635   1.483 0.138520    
## married      178.8599    46.4966   3.847 0.000132 ***
## black       -100.5218    56.4193  -1.782 0.075271 .  
## south        -23.3841    31.0616  -0.753 0.451828    
## urban        179.0588    31.5547   5.675  2.1e-08 ***
## sibs           8.6548     7.9696   1.086 0.277895    
## brthord      -18.0249    11.7300  -1.537 0.124871    
## meduc          7.1703     6.2323   1.151 0.250362    
## feduc          8.8008     5.4714   1.609 0.108214    
## hours:IQ      -0.1226     0.1566  -0.783 0.434026    
## hours:KWW      0.4812     0.2801   1.718 0.086282 .  
## hours:educ     0.1991     1.0457   0.190 0.849036    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 354.1 on 644 degrees of freedom
##   (272 observations deleted due to missingness)
## Multiple R-squared:  0.262,	Adjusted R-squared:  0.2414 
## F-statistic:  12.7 on 18 and 644 DF,  p-value: &amp;lt; 2.2e-16
## 
## 
## [[3]]
## 
## Call:
## lm(formula = y, data = wage2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -876.85 -208.34  -41.58  184.31 1964.55 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)   2526.42104 1778.98063   1.420  0.15606   
## hours          -28.84096   27.34871  -1.055  0.29203   
## IQ             -11.05195   14.41481  -0.767  0.44354   
## KWW            -32.51353   21.95935  -1.481  0.13921   
## educ           -65.27306   87.11894  -0.749  0.45399   
## exper          -56.47871   43.73030  -1.292  0.19700   
## tenure          61.99723   27.55945   2.250  0.02482 * 
## age            -20.60242   53.42036  -0.386  0.69987   
## married        174.59592  291.22705   0.600  0.54904   
## black          129.07837  389.95580   0.331  0.74075   
## south         -219.74759  209.99378  -1.046  0.29576   
## urban          294.24907  215.20016   1.367  0.17201   
## sibs            33.26592   57.47098   0.579  0.56291   
## brthord        -12.35032   81.74380  -0.151  0.87996   
## meduc          -14.65740   42.07236  -0.348  0.72767   
## feduc           45.44872   37.63676   1.208  0.22767   
## hours:IQ        -0.13457    0.17547  -0.767  0.44345   
## hours:KWW        0.50497    0.31608   1.598  0.11064   
## hours:educ       1.70417    1.35782   1.255  0.20992   
## hours:exper      1.28802    0.68683   1.875  0.06121 . 
## hours:tenure    -1.23958    0.44602  -2.779  0.00561 **
## hours:age       -0.12173    0.82131  -0.148  0.88222   
## hours:married    0.10483    6.63109   0.016  0.98739   
## hours:black     -5.90930    8.93377  -0.661  0.50856   
## hours:south      4.48976    4.76877   0.941  0.34681   
## hours:urban     -2.64292    4.85060  -0.545  0.58604   
## hours:sibs      -0.59065    1.30658  -0.452  0.65138   
## hours:brthord   -0.14475    1.88695  -0.077  0.93888   
## hours:meduc      0.54974    0.95729   0.574  0.56600   
## hours:feduc     -0.86832    0.83946  -1.034  0.30136   
## IQ:KWW           0.14886    0.15533   0.958  0.33823   
## IQ:educ          0.24425    0.57544   0.424  0.67138   
## IQ:exper         0.09552    0.31666   0.302  0.76302   
## IQ:tenure       -0.04357    0.20709  -0.210  0.83342   
## IQ:age           0.32399    0.41337   0.784  0.43347   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 353.2 on 628 degrees of freedom
##   (272 observations deleted due to missingness)
## Multiple R-squared:  0.2838,	Adjusted R-squared:  0.2451 
## F-statistic:  7.32 on 34 and 628 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-looping-through-the-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 2: Looping through the Trees&lt;/h2&gt;
&lt;p&gt;Using the loop method or the &lt;code&gt;lapply&lt;/code&gt; method, generate 10 regression trees to explain &lt;code&gt;wage&lt;/code&gt; in &lt;code&gt;wage_clean&lt;/code&gt;. You can iterate through values of &lt;code&gt;cp&lt;/code&gt;, the complexity parameter, &lt;strong&gt;or&lt;/strong&gt; &lt;code&gt;minsplit&lt;/code&gt;, the minimum # of points that have to be in each split.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-3-if-time-evaluating-your-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 3 (if time): Evaluating your Trees&lt;/h2&gt;
&lt;p&gt;Use &lt;code&gt;lapply&lt;/code&gt; to get a list of your RMSE’s (one for each of your models). The anonymous function may come in handy here (though it’s not necessary). Note that we are not yet splitting into &lt;code&gt;test&lt;/code&gt; and &lt;code&gt;train&lt;/code&gt; (which you &lt;strong&gt;will&lt;/strong&gt; need to do on your lab assignment).&lt;/p&gt;
&lt;p&gt;Once you have your list, &lt;strong&gt;create the plot of RMSEs&lt;/strong&gt; similar to the one we looked at in Content this week. Note: you can use &lt;code&gt;unlist(myRMSE)&lt;/code&gt; to get a numeric vector of the RMSE’s (as long as all of the elements in &lt;code&gt;myRMSE&lt;/code&gt; are numeric). Then, it’s a matter of plotting either with base &lt;code&gt;plot&lt;/code&gt; or with &lt;code&gt;ggplot&lt;/code&gt; (if you use &lt;code&gt;ggplot&lt;/code&gt; you’ll have to &lt;code&gt;tidy&lt;/code&gt; the RMSE by adding the index column or naming the x-axis).&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Illustrating Bias vs. Variance</title>
      <link>https://ssc442.netlify.app/example/10-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/10-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#under-revision&#34; id=&#34;toc-under-revision&#34;&gt;UNDER REVISION&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#review-and-clarify&#34; id=&#34;toc-review-and-clarify&#34;&gt;Review and Clarify&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#illustration-of-bias-vs.-variance&#34; id=&#34;toc-illustration-of-bias-vs.-variance&#34;&gt;Illustration of Bias vs. Variance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-quick-bit-of-r-code-to-help-with-todays-example&#34; id=&#34;toc-a-quick-bit-of-r-code-to-help-with-todays-example&#34;&gt;A quick bit of R code to help with todays example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#todays-example&#34; id=&#34;toc-todays-example&#34;&gt;Today’s Example&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simulation&#34; id=&#34;toc-simulation&#34;&gt;Simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#heres-the-code&#34; id=&#34;toc-heres-the-code&#34;&gt;Here’s the code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;under-revision&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;UNDER REVISION&lt;/h1&gt;
&lt;div id=&#34;review-and-clarify&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Review and Clarify&lt;/h2&gt;
&lt;p&gt;Bias and Variance are tricky subjects. Hopefully the illustrations from yesterday are helpful. Let’s talk through a few things based on questions some of you have asked since last week.&lt;/p&gt;
&lt;div id=&#34;illustration-of-bias-vs.-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Illustration of Bias vs. Variance&lt;/h3&gt;
&lt;p&gt;Bias is about how close you are on average to the correct answer. Variance is about how scattered your estimates would be if you repeated your experiment with new data.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://www.machinelearningplus.com/wp-content/uploads/2020/10/output_31_0.png&#34; alt=&#34;Image from MachineLearningPlus.com&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Image from MachineLearningPlus.com
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We care about these things because we usually only have our one dataset (when we’re not creating simulated data, that is), but need to know something about how bias and variance tend to look when we change our model complexity.&lt;/p&gt;
&lt;div id=&#34;deriving-bias-and-variance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Deriving Bias and Variance&lt;/h4&gt;
&lt;p&gt;For this section, recall our model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y = f(x) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This tells us that some of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; can be predicted by the &lt;strong&gt;true&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, and some is just noise &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. In our simulation from last week, &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x^2\)&lt;/span&gt;, so &lt;span class=&#34;math inline&#34;&gt;\(y = x^2 + \epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We want to predict &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We call our prediction &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Our best guess for &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; is our model. It might be from a linear regression with 1, 2, 9, 15, etc. predictors or interactions of predictors. It might be from a k-nearest-neighbors estimation with &lt;code&gt;k = 4&lt;/code&gt;. It might be from a regression tree with &lt;code&gt;cp = .1&lt;/code&gt; and &lt;code&gt;minsplit=2&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even when we really nail &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; (which means &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x) = f(x)\)&lt;/span&gt;), there is &lt;em&gt;still&lt;/em&gt; error in our prediction because of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y \neq \hat{y}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we think of two different measures of error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
EPE = E[(y - \hat{y})^2] =
\underbrace{\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]}_\textrm{reducible error - MSE from imperfect model} +
\underbrace{\mathbb{V}_{Y \mid X} \left[ Y \mid X = x \right]}_{\textrm{irreducible error from }\epsilon}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MSE(f(x), \hat{f}(x)) = \mathbb{E}_{\mathcal{D}}\left[\left(f(x) - \hat{f}(x)\right)^2\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Some of you asked about this equation from last time that decomposed our MSE:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE}\left(f(x), \hat{f}(x)\right) =
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] =
\underbrace{\left(f(x) - \mathbb{E} \left[ \hat{f}(x) \right]  \right)^2}_{\text{bias}^2 \left(\hat{f}(x) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}(x) - \mathbb{E} \left[ \hat{f}(x) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}(x) \right)}
\]&lt;/span&gt;
This can be derived by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] &amp;amp;=&amp;amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)] + E[\hat{f}(x)] - \hat{f}(x)\right)^2 \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)]\right)^2 + \left(E[\hat{f}(x)] - \hat{f}(x)\right)^2 + 2\left(f(x) - E[\hat{f}(x)]\right) \left(E[\hat{f}(x)] - \hat{f}(x)\right) \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)]\right)^2\right] + \mathbb{E}_{\mathcal{D}} \left[\left(E[\hat{f}(x)] - \hat{f}(x)\right)^2\right] +  \mathbb{E}_{\mathcal{D}} \left[2\left(f(x) - E[\hat{f}(x)]\right) \left(E[\hat{f}(x)] - \hat{f}(x)\right) \right] \\
&amp;amp;=&amp;amp; \left(f(x) - E[\hat{f}(x)]\right)^2 + Var\left(\hat{f}(x)\right) + 0
\end{eqnarray*}
\]&lt;/span&gt;
Let’s talk about what’s in this equation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MSE is the Mean Squared Error between &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;It does not have the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; in it&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;It is an expectation over all the possible &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; draws of the data we could have
&lt;ul&gt;
&lt;li&gt;Because of this &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(E\left[\hat{f}(x)\right]\)&lt;/span&gt; can move out of the expectation. This lets us cancel that last term with the “2” in it.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main takeaway is that, even given the error, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;, we &lt;em&gt;still&lt;/em&gt; have additional error coming from our inability to perfectly get &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x) = f(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-quick-bit-of-r-code-to-help-with-todays-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A quick bit of R code to help with todays example&lt;/h2&gt;
&lt;p&gt;We saw before the usefulness of having a list&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList = list()
myList[[&amp;#39;thisThing&amp;#39;]] = c(1,2,3)
myList[[&amp;#39;thisOtherThing&amp;#39;]] = c(&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;,&amp;#39;C&amp;#39;,&amp;#39;D&amp;#39;)
myList&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $thisThing
## [1] 1 2 3
## 
## $thisOtherThing
## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot; &amp;quot;D&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked really well for holding results from models since we could name the things in the list. But if we put it into a loop:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}

print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $I
## [1] &amp;quot;This loop is on I&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened? Every time we used the loop, it re-initiated the list, so we only get the last result!&lt;/p&gt;
&lt;p&gt;So what we want is to create the list &lt;strong&gt;if&lt;/strong&gt; it doesn’t exist, and add to it afterwards. We can do that with &lt;code&gt;exists(&#39;myList&#39;)&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  if(!exists(&amp;#39;myList&amp;#39;)) myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $I
## [1] &amp;quot;This loop is on I&amp;quot;
## 
## $G
## [1] &amp;quot;This loop is on G&amp;quot;
## 
## $H
## [1] &amp;quot;This loop is on H&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re almost there. It turns out, we have our original &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; in there left over from the previous creation of the list. That’s why its out of order. What we want to do is start with a fresh, clean list. If we run &lt;code&gt;rm(myList)&lt;/code&gt;, the old list will no longer exist, and &lt;em&gt;our code will create a fresh one when we run it again!&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(myList)
for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  if(!exists(&amp;#39;myList&amp;#39;)) myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $G
## [1] &amp;quot;This loop is on G&amp;quot;
## 
## $H
## [1] &amp;quot;This loop is on H&amp;quot;
## 
## $I
## [1] &amp;quot;This loop is on I&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’re going to use this in your groups / breakout rooms today. You’re going to be asked to run some code that stores a plot in a list. To reset the list that stores things, just use &lt;code&gt;rm(listName)&lt;/code&gt; (where &lt;code&gt;listName&lt;/code&gt; is the name of the list).&lt;/p&gt;
&lt;p&gt;Alternatively, you can make sure you run code (outside of the loop) that makes a new list. This will re-set the list to be empty going into the loop:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList = list()
for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  if(!exists(&amp;#39;myList&amp;#39;)) myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $G
## [1] &amp;quot;This loop is on G&amp;quot;
## 
## $H
## [1] &amp;quot;This loop is on H&amp;quot;
## 
## $I
## [1] &amp;quot;This loop is on I&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;todays-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Today’s Example&lt;/h2&gt;
&lt;p&gt;Our goal today is to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;See the code that produced this week’s Content&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why? Because it helps to illustrate the &lt;em&gt;true&lt;/em&gt; sources of noise in the data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See what larger sample sizes and higher/lower irreducible error does to our Bias vs. Variance tradeoff.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simulation&lt;/h3&gt;
&lt;p&gt;We will use the exact code from Content 10, which I have reproduced here. I have removed the in-between parts with notation so we can focus on the example. I have &lt;strong&gt;copied all of the relevant code into one chunk down at the bottom as well&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We’ll need the following libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(patchwork)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here, I’ve made a little change to Content 10’s code so we can play with sample size &lt;code&gt;NN&lt;/code&gt; and the SD of the irreducible Bayes error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NN = 100   #----&amp;gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = .75   #-----&amp;gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;begin-content-10-code-here&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Begin Content 10 code here:&lt;/h4&gt;
&lt;p&gt;We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f = function(x) {
  x ^ 2
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  tibble(x, y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To completely specify the data generating process, we have made more model assumptions than simply &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[Y \mid X = x] = x^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{V}[Y \mid X = x] = \sigma ^ 2\)&lt;/span&gt;. In particular,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from a uniform distribution over &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; are independent.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from the conditional normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using this setup, we will generate datasets, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, with a sample size &lt;span class=&#34;math inline&#34;&gt;\(NN\)&lt;/span&gt; and fit four models.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\texttt{predict(fit0, x)} &amp;amp;= \hat{f}_0(x) = \hat{\beta}_0\\
\texttt{predict(fit1, x)} &amp;amp;= \hat{f}_1(x) = \hat{\beta}_0 + \hat{\beta}_1 x \\
\texttt{predict(fit2, x)} &amp;amp;= \hat{f}_2(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \\
\texttt{predict(fit9, x)} &amp;amp;= \hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
sim_data = get_sim_data(f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/10-example_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;p&gt;We will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates for &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; given by these four models at the point &lt;span class=&#34;math inline&#34;&gt;\(x = 0.90\)&lt;/span&gt;. We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile all of the results:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/10-example_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;In your group / breakout room and using the code below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First Breakout&lt;/strong&gt;: set &lt;code&gt;NN&lt;/code&gt; = 100, the value we used in our Content 10 lecture. The value is set in one of the first code chunks. Step through the code to get your finalPlot and make sure it looks like the plot in Content 10. I changed the code to use ggplot (easier to save output), so the formatting and colors will be different - that’s OK, we want to get the same results, not copy the layout of the plot. Note that at the end of the code, a list is created that will hold all of your results. In case you need to clear this list, &lt;code&gt;rm(FinalResults)&lt;/code&gt; will do so and the code will initate a new blank list to hold subsequent results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Second Breakout&lt;/strong&gt;: set NN to a larger number. Usually, more data means more precise predictions. Run your code again stepping through it, until you get to this plot. Note that at the end of the code provided, there is a list that aggregates your results. &lt;strong&gt;Repeat this&lt;/strong&gt; with a 3rd, even larger value for NN. Don’t go much beyond 50,000 or it’ll take too long. Your &lt;code&gt;FinalResults&lt;/code&gt; list should have 3 elements in it. Use &lt;code&gt;patchwork::wrap_plots(FinalResults, nrow = 1)&lt;/code&gt; to see all 3 side-by-side.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Third Breakout&lt;/strong&gt;: Finally, change the &lt;code&gt;SD.of.Bayes.Error&lt;/code&gt; value to make it higher or lower. Remember, this is the &lt;em&gt;irreducible&lt;/em&gt; error. Run your code again with your first, second, and third different value for sample size &lt;code&gt;NN&lt;/code&gt;. You should have 6 plots in your &lt;code&gt;FinalResults&lt;/code&gt; list - 3 from before, and 3 more with the new SD of Bayes Error. Use &lt;code&gt;wrap_plots&lt;/code&gt; with the right number of rows to see a 2x3 grid of the results.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usually we think larger sample sizes and lower error lead to better overall prediction. Do we see any change in the bias vs. tradeoff relationship with lower/higher sample size &lt;code&gt;NN&lt;/code&gt; and lower/higher SD of Bayes Error?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;heres-the-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Here’s the code&lt;/h3&gt;
&lt;p&gt;I’ve merged all of the code together for you here. Copy this into a new .R script - you don’t need to use a full Markdown.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(patchwork)
library(tidyverse)



NN = 100   #----&amp;gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = 0.75   #-----&amp;gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.


f = function(x) {
  x ^ 2
}



get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  tibble(x, y)
}






## See the fit of the four models (viz only)
set.seed(1)
sim_data = get_sim_data(f)



fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)


plot(y ~ x, data = sim_data, col = &amp;quot;grey&amp;quot;, pch = 20,
     main = &amp;quot;Four Polynomial Models fit to a Simulated Dataset&amp;quot;)

grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = &amp;quot;black&amp;quot;, lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = &amp;quot;dodgerblue&amp;quot;,  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &amp;quot;firebrick&amp;quot;,   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &amp;quot;springgreen&amp;quot;, lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = &amp;quot;darkorange&amp;quot;,  lwd = 2, lty = 5)

legend(&amp;quot;topleft&amp;quot;, 
       c(&amp;quot;y ~ 1&amp;quot;, &amp;quot;y ~ poly(x, 1)&amp;quot;, &amp;quot;y ~ poly(x, 2)&amp;quot;,  &amp;quot;y ~ poly(x, 9)&amp;quot;, &amp;quot;truth&amp;quot;), 
       col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;, &amp;quot;black&amp;quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)













set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)



for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}



predictions.proc = (predictions)
colnames(predictions.proc) = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;9&amp;quot;)
predictions.proc = as.data.frame(predictions.proc)

tall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)

## Here, you can save your ggplot output 
FinalPlot &amp;lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + 
                      geom_boxplot() +
                      geom_jitter(alpha = .5) +
                      geom_hline(yintercept = f(x = .90)) +
                      labs(col = &amp;#39;Model&amp;#39;, x = &amp;#39;Model&amp;#39;, y = &amp;#39;Prediction&amp;#39;, title = paste0(&amp;#39;Bias v Var - Sample Size: &amp;#39;,NN), subtitle = paste0(&amp;#39;SD of Bayes Err: &amp;#39;,SD.of.Bayes.Error)) +
                      theme_bw()



FinalPlot


## This is going to aggregate your results for you:
if(!exists(&amp;#39;FinalResults&amp;#39;)) FinalResults = list()

FinalResults[[paste0(&amp;#39;finalPlot.NN.&amp;#39;,NN,&amp;#39;.SDBayes.&amp;#39;,SD.of.Bayes.Error)]] = FinalPlot

# Old plot code:
# boxplot(value ~ key, data = tall_predictions, border = &amp;quot;darkgrey&amp;quot;, xlab = &amp;quot;Polynomial Degree&amp;quot;, ylab = &amp;quot;Predictions&amp;quot;,
#         main = &amp;quot;Simulated Predictions for Polynomial Models&amp;quot;)
# grid()
# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = &amp;quot;jitter&amp;quot;, jitter = 0.15, pch = 1, col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;))
# abline(h = f(x = 0.90), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to output &lt;em&gt;whatever is in your list of ggplot objects&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This not run automatically.
## To plot the whole list of ggplot objects in FinalResults:
patchwork::wrap_plots(FinalResults, nrow = 2, guides = &amp;#39;collect&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Illustrating Classification</title>
      <link>https://ssc442.netlify.app/example/11-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/11-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#this-piece-of-wood-aint-big-enough-for-the-two-of-us&#34; id=&#34;toc-this-piece-of-wood-aint-big-enough-for-the-two-of-us&#34;&gt;This Piece of Wood Ain’t Big Enough for the Two of Us&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will build on material in the “Content” tab.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-piece-of-wood-aint-big-enough-for-the-two-of-us&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;This Piece of Wood Ain’t Big Enough for the Two of Us&lt;/h3&gt;
&lt;p&gt;In this exercise, you will perform binary classification to predict the survival of passengers on the Titanic. You will use logistic regression, a fundamental technique in data analytics for binary prediction. The goal is to understand how to prepare data for modeling, implement logistic regression in R, and evaluate the model’s performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://queensransom.files.wordpress.com/2013/03/ill-never-let-go.gif&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;dataset&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Dataset&lt;/h4&gt;
&lt;p&gt;The Titanic dataset contains information about the passengers, including their age, sex, class of travel, and whether they survived the sinking of the Titanic. The ‘Survived’ column is the target variable, where 1 means the passenger survived and 0 means they did not. Of course, you’ll need the ‘tidyverse’ and ‘caret’ packages installed in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-1-data-preparation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Part 1: Data Preparation&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Load the Dataset&lt;/strong&gt;: The Titanic dataset can be loaded from the ‘titanic’ package. If you haven’t already, install the package and load the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;titanic&amp;quot;)
library(titanic)
data &amp;lt;- titanic::titanic_train&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data Inspection&lt;/strong&gt;: Inspect the dataset to understand its structure, missing values, and the types of variables it contains.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(data)
summary(data)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Data Cleaning&lt;/strong&gt;: Handle missing values in the dataset. For simplicity, you can remove rows with missing values or impute them.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- na.omit(data)
# Or use imputation methods&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Feature Engineering&lt;/strong&gt;: Convert categorical variables into dummy variables as necessary. Focus on the ‘Sex’ and ‘Pclass’ variables.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data$Pclass &amp;lt;- as.factor(data$Pclass)
data &amp;lt;- tidyr::pivot_wider(data, names_from = Pclass, values_from = Pclass, values_fill = list(Pclass = 0), names_prefix = &amp;quot;Class_&amp;quot;)
data$Sex &amp;lt;- ifelse(data$Sex == &amp;#39;male&amp;#39;, 1, 0) # Convert Sex to binary (male: 1, female: 0)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-logistic-regression-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Part 2: Logistic Regression Model&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Fit the Logistic Regression Model&lt;/strong&gt;: Use the ‘glm’ function to fit a logistic regression model using the training set.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- glm(Survived ~ WHATEVER STUFF YOU WANT, family = binomial(link = &amp;quot;logit&amp;quot;), data = data)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Model Summary&lt;/strong&gt;: Describe your model output. What do you get and does it make any sense?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;part-3-model-evaluation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Part 3: Model Evaluation&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Predictions&lt;/strong&gt;: Make predictions on the test set and convert probabilities to binary outcomes (0 or 1). Try using a Bayes Classifier cutoff of .50 to generate your classifier output. Do you need to alter the cutoff?&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testData &amp;lt;- titanic::titanic_test
probabilities &amp;lt;- predict(model, testData, type = &amp;quot;response&amp;quot;)
predictions &amp;lt;- ifelse(probabilities &amp;gt; 0.5, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Evaluate Performance&lt;/strong&gt;: Use confusion matrix to evaluate the model’s performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confusionMatrix &amp;lt;- table(testData$Survived, predictions)
print(confusionMatrix)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Discuss Results&lt;/strong&gt;: Analyze the confusion matrix to calculate accuracy, precision, recall, and F1 score. Discuss the implications of these metrics in the context of the problem.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;guidance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Guidance:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ensure you understand each step of the process, especially how the logistic regression model is applied and interpreted.&lt;/li&gt;
&lt;li&gt;Pay attention to the model’s assumptions and how the choice of features may influence its performance.&lt;/li&gt;
&lt;li&gt;Reflect on the ethical implications of predictive modeling, especially in historical contexts like the Titanic dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;In our culminating exercise, we will create a ROC curve from our final output. To do this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Take your best model from above and, using a loop (or &lt;code&gt;sapply&lt;/code&gt;), step through a large number of possible cutoffs for classification ranging from 0 to 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each cutoff, generate a confusion matrix with &lt;em&gt;accuracy&lt;/em&gt;, &lt;em&gt;sensitivity&lt;/em&gt; and &lt;em&gt;specificity&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Combine the cutoff with the &lt;em&gt;sensitivity&lt;/em&gt; and &lt;em&gt;specificity&lt;/em&gt; results and make a ROC plot. Use &lt;code&gt;ggplot&lt;/code&gt; for your plot and map the color aesthetic to the cutoff value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Calculate the AUC (the &lt;em&gt;area under the curve&lt;/em&gt;). This is a little tricky! But it &lt;em&gt;can&lt;/em&gt; be done with your data.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applications of Text as Data</title>
      <link>https://ssc442.netlify.app/example/12-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/12-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#reintroduction-to-text-as-data&#34; id=&#34;toc-reintroduction-to-text-as-data&#34;&gt;(Re)Introduction to Text as Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-preprocessing&#34; id=&#34;toc-text-preprocessing&#34;&gt;Text Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basic-text-analysis&#34; id=&#34;toc-basic-text-analysis&#34;&gt;Basic Text Analysis&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-preparation&#34; id=&#34;toc-data-preparation&#34;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#text-preprocessing-1&#34; id=&#34;toc-text-preprocessing-1&#34;&gt;Text Preprocessing&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tokenization&#34; id=&#34;toc-tokenization&#34;&gt;Tokenization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#word-frequency-analysis&#34; id=&#34;toc-word-frequency-analysis&#34;&gt;Word Frequency Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sentiment-analysis&#34; id=&#34;toc-sentiment-analysis&#34;&gt;Sentiment Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-generative-ai&#34; id=&#34;toc-introduction-to-generative-ai&#34;&gt;Introduction to Generative AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some-reading-and-resources&#34; id=&#34;toc-some-reading-and-resources&#34;&gt;Some Reading and Resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;reintroduction-to-text-as-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;(Re)Introduction to Text as Data&lt;/h1&gt;
&lt;p&gt;Text data is ubiquitous and offers rich insights in various domains, from social media analytics to literature and beyond. Analyzing text data helps uncover patterns, sentiments, and trends. Moreover, the advent of generative AI has opened new frontiers in how we generate and interpret text data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;text-preprocessing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Text Preprocessing&lt;/h1&gt;
&lt;p&gt;Text data often requires preprocessing to convert raw text into a structured form that can be analyzed. Common preprocessing steps include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tokenization: Splitting text into words or tokens.&lt;/li&gt;
&lt;li&gt;Stop words removal: Eliminating common words that do not contribute to the meaning of the text.&lt;/li&gt;
&lt;li&gt;Stemming and Lemmatization: Reducing words to their base or root form.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)
library(janeaustenr)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.4     ✔ readr     2.1.5
## ✔ forcats   1.0.0     ✔ stringr   1.5.1
## ✔ ggplot2   3.5.0     ✔ tibble    3.2.1
## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
## ✔ purrr     1.0.2     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text &amp;lt;- c(&amp;quot;Text data preprocessing is crucial for any text analysis task.&amp;quot;)
text_data &amp;lt;- tibble(line = 1, text = text)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-text-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Basic Text Analysis&lt;/h1&gt;
&lt;p&gt;In this example, we will explore text data using the novels of Jane Austen. We’ll perform text preprocessing, explore the most common words, and conduct a simple sentiment analysis. These steps are fundamental in understanding the data preprocessing phase for text in data analytics and the basics of sentiment analysis, laying the groundwork for more advanced topics like generative AI.&lt;/p&gt;
&lt;div id=&#34;data-preparation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Preparation&lt;/h2&gt;
&lt;p&gt;We will use the &lt;code&gt;janeaustenr&lt;/code&gt; package to access the text data. This package contains the full texts of Jane Austen’s 6 completed, published novels.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;austen_books&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in data(&amp;quot;austen_books&amp;quot;): data set &amp;#39;austen_books&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;text-preprocessing-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Text Preprocessing&lt;/h2&gt;
&lt;p&gt;Text preprocessing involves cleaning and preparing text data for analysis. We’ll tokenize the text, remove stop words, and then stem the remaining words. Note that code provided below WILL NOT WORK.&lt;/p&gt;
&lt;div id=&#34;tokenization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tokenization&lt;/h3&gt;
&lt;p&gt;Tokenization is the process of breaking text down into individual terms or tokens.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;austen_tokens &amp;lt;- austen_books %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;word-frequency-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Word Frequency Analysis&lt;/h3&gt;
&lt;p&gt;Let’s analyze the most common words found across Jane Austen’s novels after removing stop words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;word_freq &amp;lt;- austen_tokens %&amp;gt;%
  count(word, sort = TRUE) %&amp;gt;%
  top_n(20, n)

ggplot(word_freq, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  xlab(NULL) +
  ylab(&amp;quot;Frequency&amp;quot;) +
  coord_flip() +
  labs(title = &amp;quot;Top 20 Words in Jane Austen&amp;#39;s Novels&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sentiment-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sentiment Analysis&lt;/h3&gt;
&lt;p&gt;Sentiment analysis helps in determining the attitude or emotion of the text. We will use the &lt;code&gt;bing&lt;/code&gt; sentiment lexicon available in the &lt;code&gt;tidytext&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;austen_sentiment &amp;lt;- austen_tokens %&amp;gt;%
  inner_join(get_sentiments(&amp;quot;bing&amp;quot;)) %&amp;gt;%
  count(index = linenumber %/% 80, sentiment, sort = TRUE) %&amp;gt;%
  spread(key = sentiment, value = n, fill = 0) %&amp;gt;%
  mutate(sentiment = positive - negative)

ggplot(austen_sentiment, aes(x = index, y = sentiment, fill = sentiment &amp;gt; 0)) +
  geom_col(show.legend = FALSE) +
  labs(title = &amp;quot;Sentiment Analysis Over the Course of Austen&amp;#39;s Novels&amp;quot;, y = &amp;quot;Sentiment&amp;quot;, x = &amp;quot;Section of Text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These techniques are essential for understanding text data, which is a critical component in the field of data analytics. Understanding these foundational concepts is crucial for moving towards more advanced topics such as machine learning and generative AI in text analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction-to-generative-ai&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction to Generative AI&lt;/h2&gt;
&lt;p&gt;Generative AI refers to algorithms and models that can generate new content. In the context of text, generative models like GPT (Generative Pre-trained Transformer) learn from vast amounts of text data to generate coherent and contextually relevant text based on input prompts. Below are some slides I produced last year on the topic.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/slides/NN.pdf&#34;&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;code&gt;Slides&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-reading-and-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some Reading and Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tidy Text Mining with R: &lt;a href=&#34;https://www.tidytextmining.com/&#34; class=&#34;uri&#34;&gt;https://www.tidytextmining.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Introduction to Deep Learning for Natural Language Processing: &lt;a href=&#34;https://www.deeplearningbook.org/contents/nlp.html&#34; class=&#34;uri&#34;&gt;https://www.deeplearningbook.org/contents/nlp.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;OpenAI API documentation for practical examples of generative AI in action: &lt;a href=&#34;https://beta.openai.com/docs/&#34; class=&#34;uri&#34;&gt;https://beta.openai.com/docs/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;https://www.youtube.com/watch?v=wjZofJX0v4M&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>https://ssc442.netlify.app/example/13-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/13-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-to-be-wrangled&#34; id=&#34;toc-data-to-be-wrangled&#34;&gt;Data to be wrangled&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-to-be-wrangled&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data to be wrangled&lt;/h1&gt;
&lt;p&gt;You work for a travel booking website as a data analyst. A hotel has asked your company for data on corporate bookings at the hotel via your site. Specifically, they have five corporations that are frequent customers of the hotel, and they want to know who spends the most with them. They’ve asked you to help out. Most of the corporate spending is in the form of room reservations, but there are also parking fees that the hotel wants included in the analysis. Your goal (and indeed this week’s writing): total up spending by corporation and report the biggest and smallest spenders inclusive of rooms and parking.&lt;/p&gt;
&lt;p&gt;Unfortunately, you only have the following data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Example11_booking.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;booking.csv&lt;/code&gt;&lt;/a&gt; - Contains the corporation name, the room type, and the dates someone from the corporation stayed at the hoted.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Example11_roomrates.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;roomrates.csv&lt;/code&gt;&lt;/a&gt; - Contains the price of each room on each day&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Example11_parking.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;parking.csv&lt;/code&gt;&lt;/a&gt; - Contains the corporations who negotiated free parking for employees&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Parking at the hotel is $60 per night if you don’t have free parking. This hotel is in California, so everyone drives and parks when they stay.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Some tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Right-click on each of the links, copy the address, and read the URL in using &lt;code&gt;read.csv&lt;/code&gt; to read .csv’s&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll find you need to use most of the tools we covered on Tuesday including &lt;code&gt;gather&lt;/code&gt;, &lt;code&gt;separate&lt;/code&gt; and more.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll need &lt;code&gt;lubridate&lt;/code&gt; and &lt;code&gt;tidyverse&lt;/code&gt; loaded up.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Your lab will be based on similar data (with more wrinkles to fix) so it might be helpful to share your code with your group when you’re done.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Geospatial with R</title>
      <link>https://ssc442.netlify.app/example/14-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/14-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-little-more-on-geospatial-in-r&#34; id=&#34;toc-a-little-more-on-geospatial-in-r&#34;&gt;A little more on Geospatial in R&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tigris-for-polygons&#34; id=&#34;toc-tigris-for-polygons&#34;&gt;Tigris for polygons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidycensus&#34; id=&#34;toc-tidycensus&#34;&gt;Tidycensus&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#finding-variables&#34; id=&#34;toc-finding-variables&#34;&gt;Finding variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidycensus-and-geographies&#34; id=&#34;toc-tidycensus-and-geographies&#34;&gt;Tidycensus and geographies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wrapping-up&#34; id=&#34;toc-wrapping-up&#34;&gt;Wrapping up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Our last class!&lt;/p&gt;
&lt;div id=&#34;a-little-more-on-geospatial-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A little more on Geospatial in R&lt;/h1&gt;
&lt;p&gt;We went over the basics and got some practice using geospatial data in R. Let’s look closely at the &lt;code&gt;tidycensus&lt;/code&gt; packages. Thus far, we have used &lt;code&gt;tigris&lt;/code&gt; to get TIGER/line census boundaries for things like counties. &lt;code&gt;tidycensus&lt;/code&gt; does more than this – we can pull actual census data with its geospatial references (the polygons referring to the counties, tracts, block groups, and blocks) &lt;em&gt;along with&lt;/em&gt; census data because &lt;code&gt;tidycensus&lt;/code&gt; interfaces with &lt;code&gt;tigris&lt;/code&gt; to combine the census data and the census geographies.&lt;/p&gt;
&lt;div id=&#34;tigris-for-polygons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tigris for polygons&lt;/h2&gt;
&lt;p&gt;Make sure you load &lt;code&gt;tigris&lt;/code&gt; (for spatial data) and &lt;code&gt;tidycensus&lt;/code&gt; for census data to attach to the spatial data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tigris)
library(tidycensus)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We already saw that we could extract counties using the &lt;code&gt;tigris::counties&lt;/code&gt; function. Some “census geographies” change from year to year. Counties don’t (usually), but census tracts, block groups, and blocks do. These last three are all nested units of observation - tracts hold multiple block groups, each block group holds multiple blocks. Blocks are pretty small, and a lot of census data isn’t reported at that level for confidentiality. Block groups don’t report all data, either. Tracts are usually the most reliable balance between “census data availability” and “small geographic area”. We’ll use the 2010 census boundaries by specifying &lt;code&gt;year = 2010&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s look at MI counties, then look at Ingham County census tracts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MI.counties = tigris::counties(state=&amp;#39;MI&amp;#39;, year = 2010, progress_bar = FALSE)
ggplot(MI.counties, aes(fill = NAME10) ) + 
  geom_sf() + 
  theme_minimal() + theme(legend.position = &amp;#39;none&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/14-example_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, we get some counties that extend into the Great Lakes. That’s OK - we can use &lt;code&gt;st_intersect&lt;/code&gt; with a map of the Great Lakes to clean that up, but I won’t do that here.&lt;/p&gt;
&lt;p&gt;Now, the census tracts for Ingham County. We can use &lt;code&gt;str_detect&lt;/code&gt; to find the County FIPS for Ingham:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Ingham_FIPS = MI.counties %&amp;gt;%
  dplyr::filter(str_detect(NAME10, &amp;#39;Ingham&amp;#39;)) %&amp;gt;%
  dplyr::select(COUNTYFP, STATEFP) %&amp;gt;%
  pull(COUNTYFP)

Ingham.tracts = tigris::tracts(state=&amp;#39;MI&amp;#39;, county = Ingham_FIPS, year = 2010, progress_bar = FALSE)

print(Ingham.tracts %&amp;gt;% dplyr::select(TRACTCE10, NAME10, GEOID10))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Simple feature collection with 81 features and 3 fields
## Geometry type: MULTIPOLYGON
## Dimension:     XY
## Bounding box:  xmin: -84.60314 ymin: 42.42194 xmax: -84.1406 ymax: 42.77664
## Geodetic CRS:  NAD83
## First 10 features:
##      TRACTCE10 NAME10     GEOID10                       geometry
## 2011    006600     66 26065006600 MULTIPOLYGON (((-84.54269 4...
## 2012    003103  31.03 26065003103 MULTIPOLYGON (((-84.52985 4...
## 2013    006800     68 26065006800 MULTIPOLYGON (((-84.56177 4...
## 2014    006700     67 26065006700 MULTIPOLYGON (((-84.56159 4...
## 2015    006001  60.01 26065006001 MULTIPOLYGON (((-84.2875 42...
## 2016    006002  60.02 26065006002 MULTIPOLYGON (((-84.21456 4...
## 2017    002101  21.01 26065002101 MULTIPOLYGON (((-84.53305 4...
## 2018    007000     70 26065007000 MULTIPOLYGON (((-84.58218 4...
## 2023    001703  17.03 26065001703 MULTIPOLYGON (((-84.58662 4...
## 2024    004494  44.94 26065004494 MULTIPOLYGON (((-84.46658 4...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice the &lt;code&gt;GEOID10&lt;/code&gt; starts with the MI FIPS (26), the Ingham FIPS (065), then the 6 digits that make up the &lt;code&gt;TRACTCE10&lt;/code&gt; value. This is because tracts are nested in states and counties.&lt;/p&gt;
&lt;p&gt;Finally, let’s show all the Ingham County tracts. I’ve changed the color mapping for the &lt;code&gt;col&lt;/code&gt; aesthetic so that the tract that holds MSU will be outlined in green:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(Ingham.tracts, aes(fill = TRACTCE10, col = (NAME10==&amp;#39;9800&amp;#39;))) + 
  geom_sf() + theme_minimal() + theme(legend.position = &amp;#39;none&amp;#39;) + scale_fill_viridis_d() +
  scale_color_manual(values = c(&amp;#39;TRUE&amp;#39; = &amp;#39;green&amp;#39;,&amp;#39;FALSE&amp;#39; = &amp;#39;gray50&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/14-example_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidycensus&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidycensus&lt;/h2&gt;
&lt;p&gt;Tidycensus takes some getting used to because census data is very complicated, has many geographies, and many subsets (e.g. you could be asking about the age of a specific combination of race and income). Here, we’re going to learn two things: how to find some basic census data by tract, and how to extract it with &lt;code&gt;tidycensus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First, we can use the &lt;code&gt;tidyverse&lt;/code&gt; author’s “basic usage” page as a guide: &lt;a href=&#34;https://walker-data.com/tidycensus/articles/basic-usage.html&#34;&gt;https://walker-data.com/tidycensus/articles/basic-usage.html&lt;/a&gt;. &lt;strong&gt;There are important directions for getting a Census API key, which must be added to your system&lt;/strong&gt;. &lt;code&gt;tidycensus&lt;/code&gt; makes it easy - once you get your key (free from the US census, see the link for instructions), you just add it to your system using the &lt;code&gt;census_api_key&lt;/code&gt; function. You only have to do it once, it records the API key in your system files, and will find it automatically on subsequent projects.&lt;/p&gt;
&lt;div id=&#34;finding-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Finding variables&lt;/h3&gt;
&lt;p&gt;This is actually pretty tricky. The census asks many questions about demographics and other topics (house characteristics, earnings, health, etc.) and it can be hard to find exactly what you’re looking for. We’re going to seek some simple data: the average income, and the average “tenure” (how long someone has lived in their current residence) in each tract in Ingham County.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tidycensus&lt;/code&gt; has some useful functions for finding variables, especially when combined with &lt;code&gt;str_detect&lt;/code&gt;, which helps us find certain words or phrases (our RegExp skills come in handy!) in the census variable names. Variables are identified by an ID, not by name. “P013001”, for instance, is the median age in a given geography. All variables are not available at all geographic units: “P013001” is available for all state, county, tract, and block group, but not block. The link above shows all of the geographies available.&lt;/p&gt;
&lt;p&gt;To make things even more complicated, there is more than the decennial census. The American Community Survey (ACS) samples 1-5% of the population each year, and reports yearly but on a limited number of variables. We’ll use the ACS five-year (acs5) as it has more easily available data, even if it asks fewer questions. We’ll use the 2019 data, the most recently available.&lt;/p&gt;
&lt;p&gt;Let’s start by finding all variables that contain “income”. Variable descriptions have the variable name as well as the “concept”. We want to use “concept” to search for our term. Each concept has multiple variables with it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;allvars = load_variables(2019, &amp;#39;acs5&amp;#39;)
incvars = allvars %&amp;gt;%
  dplyr::filter(str_detect(concept, &amp;#39;MEDIAN INCOME&amp;#39;))

incvars[1:5,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 4
##   name         label                                           concept geography
##   &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;                                           &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    
## 1 B06011PR_001 Estimate!!Median income in the past 12 months … MEDIAN… &amp;lt;NA&amp;gt;     
## 2 B06011PR_002 Estimate!!Median income in the past 12 months … MEDIAN… &amp;lt;NA&amp;gt;     
## 3 B06011PR_003 Estimate!!Median income in the past 12 months … MEDIAN… &amp;lt;NA&amp;gt;     
## 4 B06011PR_004 Estimate!!Median income in the past 12 months … MEDIAN… &amp;lt;NA&amp;gt;     
## 5 B06011PR_005 Estimate!!Median income in the past 12 months … MEDIAN… &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got lucky - the first &lt;code&gt;concept&lt;/code&gt; is “MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2019 INFLATION-ADJUSTED DOLLARS) BY PLACE OF BIRTH”. Now, we didn’t want the “place of birth” part (though…sounds interesting), but look at the first &lt;code&gt;label&lt;/code&gt;. It is the &lt;code&gt;!!Total:&lt;/code&gt;, meaning it is the median income when you combine all places of birth. The second label is &lt;code&gt;!!Total:!!Born in state of residence&lt;/code&gt;, which is a &lt;em&gt;subset&lt;/em&gt; of the total. Same with the next. &lt;strong&gt;We do not need to use the subsets in order to use the &lt;code&gt;!!Total&lt;/code&gt; value&lt;/strong&gt;. Note the &lt;code&gt;name&lt;/code&gt; is &lt;code&gt;B06011_001&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Lots and lots of variables will have the same total - the next variable &lt;code&gt;B07011_001&lt;/code&gt; (after the Puerto Rico version) is the median income broken down by whether or not the household lives in the same house, instead of by place of birth. It’s long, but you can see that it is “MEDIAN INCOME IN THE PAST 12 MONTHS (IN 2019 INFLATION-ADJUSTED DOLLARS) BY GEOGRAPHICAL MOBILITY IN THE PAST YEAR FOR CURRENT RESIDENCE IN THE UNITED STATES”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;incvars[6:15,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 4
##    name         label                                          concept geography
##    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;                                          &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    
##  1 B06011_001   Estimate!!Median income in the past 12 months… MEDIAN… tract    
##  2 B06011_002   Estimate!!Median income in the past 12 months… MEDIAN… tract    
##  3 B06011_003   Estimate!!Median income in the past 12 months… MEDIAN… tract    
##  4 B06011_004   Estimate!!Median income in the past 12 months… MEDIAN… tract    
##  5 B06011_005   Estimate!!Median income in the past 12 months… MEDIAN… tract    
##  6 B07011PR_001 Estimate!!Median income in the past 12 months… MEDIAN… &amp;lt;NA&amp;gt;     
##  7 B07011PR_002 Estimate!!Median income in the past 12 months… MEDIAN… &amp;lt;NA&amp;gt;     
##  8 B07011PR_003 Estimate!!Median income in the past 12 months… MEDIAN… &amp;lt;NA&amp;gt;     
##  9 B07011PR_004 Estimate!!Median income in the past 12 months… MEDIAN… &amp;lt;NA&amp;gt;     
## 10 B07011PR_005 Estimate!!Median income in the past 12 months… MEDIAN… &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the values of the subsets will be different, the &lt;code&gt;!!Total:&lt;/code&gt; variable will be the same. So, we can really choose any &lt;code&gt;!!Total:&lt;/code&gt; variable for any set of variables in a &lt;code&gt;concept&lt;/code&gt; that fits our search.&lt;/p&gt;
&lt;p&gt;Let’s call up the median income using &lt;code&gt;B06011_001&lt;/code&gt;. Note that we are naming our variables in the third line, and specifying that we only want &lt;code&gt;county = &#39;065&#39;&lt;/code&gt; for Ingham:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ing.medincome = get_acs(geography = &amp;#39;tract&amp;#39;,
                        county = &amp;#39;065&amp;#39;, state = &amp;#39;MI&amp;#39;,
                        variables = c(medincome = &amp;#39;B06011_001&amp;#39;),
                        year = 2019)
head(ing.medincome)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 5
##   GEOID       NAME                                     variable  estimate   moe
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                                    &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 26065000100 Census Tract 1, Ingham County, Michigan  medincome    22994  6149
## 2 26065000400 Census Tract 4, Ingham County, Michigan  medincome    29059  4043
## 3 26065000600 Census Tract 6, Ingham County, Michigan  medincome    18549  4830
## 4 26065000700 Census Tract 7, Ingham County, Michigan  medincome    17196  8455
## 5 26065000800 Census Tract 8, Ingham County, Michigan  medincome    23142  5845
## 6 26065001000 Census Tract 10, Ingham County, Michigan medincome    30704  1982&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the ACS is a sample, the variable is returned as the &lt;code&gt;estimate&lt;/code&gt; along with a margin of error, which we’ll ignore for now. Note that the data is “tidy” for now - each row is one observation of one variable. If we have multiple variables, we’ll have to keep it tidy. We’ll compare the &lt;code&gt;B06011_001&lt;/code&gt; with &lt;code&gt;B007011_001&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ing.medincome = get_acs(geography = &amp;#39;tract&amp;#39;,
                        county = &amp;#39;065&amp;#39;, state = &amp;#39;MI&amp;#39;,
                        variables = c(medincome = &amp;#39;B06011_001&amp;#39;,
                                      medincome2 = &amp;#39;B07011_001&amp;#39;), 
                        year = 2019) %&amp;gt;% arrange(GEOID)
head(ing.medincome)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 5
##   GEOID       NAME                                    variable   estimate   moe
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                                   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 26065000100 Census Tract 1, Ingham County, Michigan medincome     22994  6149
## 2 26065000100 Census Tract 1, Ingham County, Michigan medincome2    22994  6149
## 3 26065000400 Census Tract 4, Ingham County, Michigan medincome     29059  4043
## 4 26065000400 Census Tract 4, Ingham County, Michigan medincome2    29059  4043
## 5 26065000600 Census Tract 6, Ingham County, Michigan medincome     18549  4830
## 6 26065000600 Census Tract 6, Ingham County, Michigan medincome2    18549  4830&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got to name the variables (useful!) as you see in the &lt;code&gt;variable&lt;/code&gt; column. However, there is more than one row per observation now: the results are &lt;strong&gt;not&lt;/strong&gt; “tidy”. But clearly the two variable’s totals are the same, so we have convinced ourselves that &lt;code&gt;!!Total:&lt;/code&gt; works for any of the different concepts that cover median income.&lt;/p&gt;
&lt;p&gt;Often, the data we wish to pull is a total, but it is broken down (by race, income groups, etc.). This means we want one observation per row (census tract), but multiple columns representing the total and the breakdown of a variable. Imagine “race” by tract with counts of households by race. We’ll have to use &lt;code&gt;output = &#39;wide&#39;&lt;/code&gt; in our &lt;code&gt;get_acs&lt;/code&gt; call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ing.medincome.wide = get_acs(geography = &amp;#39;tract&amp;#39;,
                        county = &amp;#39;065&amp;#39;, state = &amp;#39;MI&amp;#39;,
                        variables = c(medincome = &amp;#39;B06011_001&amp;#39;,
                                      medincome2 = &amp;#39;B07011_001&amp;#39;), 
                        year = 2019,
                        output = &amp;#39;wide&amp;#39;) %&amp;gt;% arrange(GEOID)
head(ing.medincome.wide)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 6
##   GEOID       NAME                 medincomeE medincomeM medincome2E medincome2M
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 26065000100 Census Tract 1, Ing…      22994       6149       22994        6149
## 2 26065000400 Census Tract 4, Ing…      29059       4043       29059        4043
## 3 26065000600 Census Tract 6, Ing…      18549       4830       18549        4830
## 4 26065000700 Census Tract 7, Ing…      17196       8455       17196        8455
## 5 26065000800 Census Tract 8, Ing…      23142       5845       23142        5845
## 6 26065001000 Census Tract 10, In…      30704       1982       30704        1982&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still get to name the variables (so name wisely!). We get an &lt;strong&gt;E&lt;/strong&gt;stimate and w &lt;strong&gt;M&lt;/strong&gt;argin of error for each variable. We’ll mostly stick with the estimate and ignore the MoE.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidycensus-and-geographies&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tidycensus and geographies&lt;/h3&gt;
&lt;p&gt;Now, let’s find the other variable we’re interested in - housing tenure. We’ll keep it simple and calculate the percentage of people in a tract who lived in the same house last year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tenvars = allvars %&amp;gt;%
  dplyr::filter(str_detect(concept, &amp;#39;TENURE&amp;#39;))

tenvars[1:5,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 4
##   name         label                                           concept geography
##   &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;                                           &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    
## 1 B07013PR_001 Estimate!!Total:                                GEOGRA… &amp;lt;NA&amp;gt;     
## 2 B07013PR_002 Estimate!!Total:!!Householder lived in owner-o… GEOGRA… &amp;lt;NA&amp;gt;     
## 3 B07013PR_003 Estimate!!Total:!!Householder lived in renter-… GEOGRA… &amp;lt;NA&amp;gt;     
## 4 B07013PR_004 Estimate!!Total:!!Same house 1 year ago:        GEOGRA… &amp;lt;NA&amp;gt;     
## 5 B07013PR_005 Estimate!!Total:!!Same house 1 year ago:!!Hous… GEOGRA… &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we get &lt;code&gt;B07013_001&lt;/code&gt;, which is the total number of households, and &lt;code&gt;B07014_004&lt;/code&gt;, which is the total number of households that lived in the same house a year ago. The ratio of these two tells us what percent didn’t move in the last year, which is what we want. We’re going to take the output in &lt;code&gt;wide&lt;/code&gt; format so that we can calculate this ratio.&lt;/p&gt;
&lt;p&gt;Now, with variable numbers in hand, we can pretty easily get the attached geography of interest. We just add &lt;code&gt;geometry = TRUE&lt;/code&gt; to the &lt;code&gt;get_acs&lt;/code&gt; call. We’re also going to ask for &lt;code&gt;wide&lt;/code&gt; format, which gives us on row per tract and a column for each variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ing.tenure = get_acs(geography = &amp;#39;tract&amp;#39;,
                     county = &amp;#39;065&amp;#39;, state = &amp;#39;MI&amp;#39;,
                     variables = c(samehouse = &amp;#39;B07013_004&amp;#39;,
                                   totalhouse = &amp;#39;B07013_001&amp;#39;),
                     geometry = TRUE, output = &amp;#39;wide&amp;#39;, progress_bar = FALSE,
                     year = 2019)
ing.tenure&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Simple feature collection with 81 features and 6 fields
## Geometry type: MULTIPOLYGON
## Dimension:     XY
## Bounding box:  xmin: -84.60314 ymin: 42.42195 xmax: -84.14062 ymax: 42.77664
## Geodetic CRS:  NAD83
## First 10 features:
##          GEOID                                        NAME samehouseE
## 1  26065004302 Census Tract 43.02, Ingham County, Michigan        498
## 2  26065002800    Census Tract 28, Ingham County, Michigan       2396
## 3  26065005201 Census Tract 52.01, Ingham County, Michigan       5216
## 4  26065001200    Census Tract 12, Ingham County, Michigan       1201
## 5  26065006302 Census Tract 63.02, Ingham County, Michigan       3660
## 6  26065002200    Census Tract 22, Ingham County, Michigan       1349
## 7  26065000800     Census Tract 8, Ingham County, Michigan       2601
## 8  26065004494 Census Tract 44.94, Ingham County, Michigan          0
## 9  26065004700    Census Tract 47, Ingham County, Michigan       2346
## 10 26065006301 Census Tract 63.01, Ingham County, Michigan       3580
##    samehouseM totalhouseE totalhouseM                       geometry
## 1         165        2383         326 MULTIPOLYGON (((-84.47205 4...
## 2         310        2656         296 MULTIPOLYGON (((-84.54808 4...
## 3         539        5961         481 MULTIPOLYGON (((-84.5826 42...
## 4         233        2137         239 MULTIPOLYGON (((-84.54113 4...
## 5         242        4139         201 MULTIPOLYGON (((-84.48344 4...
## 6         143        1577         110 MULTIPOLYGON (((-84.53812 4...
## 7         400        2987         403 MULTIPOLYGON (((-84.54287 4...
## 8          10           0          10 MULTIPOLYGON (((-84.46662 4...
## 9         181        2908         186 MULTIPOLYGON (((-84.43269 4...
## 10        257        4221         213 MULTIPOLYGON (((-84.48351 4...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get an “E” at the end of the variable which stands for “Estimate” (the “M” is “Margin of Error” since the ACS is a sample). We can now calculate the percent living in the same house they were in last year:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ing.tenure = ing.tenure %&amp;gt;%
  dplyr::mutate(percentSameHouse = samehouseE/totalhouseE) %&amp;gt;%
  dplyr::select(GEOID, NAME, percentSameHouse)

ing.tenure&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Simple feature collection with 81 features and 3 fields
## Geometry type: MULTIPOLYGON
## Dimension:     XY
## Bounding box:  xmin: -84.60314 ymin: 42.42195 xmax: -84.14062 ymax: 42.77664
## Geodetic CRS:  NAD83
## First 10 features:
##          GEOID                                        NAME percentSameHouse
## 1  26065004302 Census Tract 43.02, Ingham County, Michigan        0.2089803
## 2  26065002800    Census Tract 28, Ingham County, Michigan        0.9021084
## 3  26065005201 Census Tract 52.01, Ingham County, Michigan        0.8750210
## 4  26065001200    Census Tract 12, Ingham County, Michigan        0.5620028
## 5  26065006302 Census Tract 63.02, Ingham County, Michigan        0.8842716
## 6  26065002200    Census Tract 22, Ingham County, Michigan        0.8554217
## 7  26065000800     Census Tract 8, Ingham County, Michigan        0.8707734
## 8  26065004494 Census Tract 44.94, Ingham County, Michigan              NaN
## 9  26065004700    Census Tract 47, Ingham County, Michigan        0.8067400
## 10 26065006301 Census Tract 63.01, Ingham County, Michigan        0.8481403
##                          geometry
## 1  MULTIPOLYGON (((-84.47205 4...
## 2  MULTIPOLYGON (((-84.54808 4...
## 3  MULTIPOLYGON (((-84.5826 42...
## 4  MULTIPOLYGON (((-84.54113 4...
## 5  MULTIPOLYGON (((-84.48344 4...
## 6  MULTIPOLYGON (((-84.53812 4...
## 7  MULTIPOLYGON (((-84.54287 4...
## 8  MULTIPOLYGON (((-84.46662 4...
## 9  MULTIPOLYGON (((-84.43269 4...
## 10 MULTIPOLYGON (((-84.48351 4...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NA’s come from tracts with zero households in them (the &lt;a href=&#34;https://www.wzzm13.com/article/news/local/michigan-life/seven-gables-road-haunted-michigan/69-e2f92018-59da-473c-ae32-1d7de68dd65e&#34;&gt;haunted&lt;/a&gt; Seven Gables Nature Area, GM’s Lansing plant, industrial areas, etc.). Let’s plot this!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(ing.tenure, aes(fill = percentSameHouse)) + 
  geom_sf() + theme_minimal() + scale_fill_viridis_c() +
  labs(fill = &amp;#39;Percent Same House\n1 yr ago&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/14-example_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;an-alternative-source-for-finding-census-variables&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;An alternative source for finding Census variables&lt;/h4&gt;
&lt;p&gt;We can also find census variables by searching the &lt;a href=&#34;data.census.gov&#34;&gt;data.census.gov&lt;/a&gt; website. We are looking for &lt;strong&gt;tables&lt;/strong&gt;, so click on “explore tables”.&lt;/p&gt;
&lt;p&gt;First thing we want to do is set our level (e.g. block, block-group, tract, county, etc.) so that we are searching for data that exists at the level we want. No sense in searching for something you want at the county level to find out it’s only available at the state level! On the left side under “geography” click on the level you want (say, “census tract”) then choose a random state and tract – you’re only using this to find the variable’s code, not to actually retrieve data. I always use the first county in Alabama just because it’s small-ish so it doesn’t take long to load up.&lt;/p&gt;
&lt;p&gt;With the tract(s) set on geography, go up to the search box and type in the topic you’re interested in. It’ll draw from decennial census, the ACS, and more. You can view the data (for the set of selected geographies) by clicking on the results to make sure the numbers look like you’d expect. That will also let you see the crosstabs – some data will have further breakdowns (by race, by tenure, etc.) and the crosstabs will show you what those breakdowns are, and if there is a “topline” number that serves your purpose.&lt;/p&gt;
&lt;p&gt;Once you find the series you want, you’re getting close to being done. The variable code will be on the results (you’ll see “B….” or “H…” or “S….”). Note that &lt;strong&gt;and&lt;/strong&gt; the source (ACS, decennial) and the year(s) you want. What’s missing, though? The &lt;code&gt;_001&lt;/code&gt; suffix! Frustrating, I know.&lt;/p&gt;
&lt;p&gt;To find the exact variable you need, you’ll need what’s called a “table shell”. You can &lt;a href=&#34;https://www.census.gov/programs-surveys/acs/technical-documentation/table-shells.html&#34;&gt;find them all here&lt;/a&gt;. Download the “table shells for all detailed data” for the data series and year you need (ACS 2021, decennial 2020, etc.) and search using your variable name. That’ll take you to the complete crosstabs available for that variable, including the &lt;code&gt;_00X&lt;/code&gt; suffixes. Those are the variables (&lt;a href=&#34;https://www.census.gov/programs-surveys/acs/data/data-tables/table-ids-explained.html&#34;&gt;“table ID’s” in census terms&lt;/a&gt;) you’ll need to feed to &lt;code&gt;tidycensus&lt;/code&gt;. You can name each of them as you see fit, take whichever ones you need (just the topline, or all of the crosstabs – it’s up to you).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrapping up&lt;/h3&gt;
&lt;p&gt;Finding census variables via &lt;code&gt;tidycensus&lt;/code&gt; can be frustrating, and other sources exist to help lookup census tables. One useful tip is that you can use the “filter” button in your Rstudio &lt;code&gt;View()&lt;/code&gt; window to interactively filter the &lt;code&gt;concept&lt;/code&gt; column from &lt;code&gt;load_variables&lt;/code&gt;. Once you find what you want there, then use the &lt;code&gt;get_acs&lt;/code&gt; function in your code. Fundamentally, it’s a lot of information with the ability to subset in many way, which makes it hard to wade through and find what you’re looking for.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Try it! (This will be your Lab 14, due on Monday)&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Load up all the necessary packages&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find the FIPS for your home county (or a county you’re interested in)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose a variable you’re interested in. Finding census variables can be tricky! If you want a challenge, find more than one variable that can construct something you’re interested in (like our “percent of housholds residing at the same address a year ago” measure).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using &lt;code&gt;load_variables&lt;/code&gt;, find the best representation of that variable&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make a map of that variable at the census tract level&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Regression</title>
      <link>https://ssc442.netlify.app/example/05-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/05-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preliminaries&#34; id=&#34;toc-preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34; id=&#34;toc-code&#34;&gt;Code&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-the-data&#34; id=&#34;toc-load-the-data&#34;&gt;Load the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scatterplot-matrices&#34; id=&#34;toc-scatterplot-matrices&#34;&gt;Scatterplot matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlograms&#34; id=&#34;toc-correlograms&#34;&gt;Correlograms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simple-regression&#34; id=&#34;toc-simple-regression&#34;&gt;Simple regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;For this example, we’re going to use historical weather data from &lt;a href=&#34;https://darksky.net/forecast/33.7546,-84.39/us12/en&#34;&gt;Dark Sky&lt;/a&gt; about wind speed and temperature trends for downtown Atlanta (&lt;a href=&#34;https://www.google.com/maps/place/33°45&amp;#39;16.4%22N+84°23&amp;#39;24.0%22W/@33.754557,-84.3921977,17z/&#34;&gt;specifically &lt;code&gt;33.754557, -84.390009&lt;/code&gt;&lt;/a&gt;) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the &lt;a href=&#34;https://github.com/hrbrmstr/darksky&#34;&gt;&lt;strong&gt;darksky&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…” and place it in a &lt;code&gt;data&lt;/code&gt; folder next to your working R script):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/atl-weather-2019.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;atl-weather-2019.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;div id=&#34;load-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load the data&lt;/h3&gt;
&lt;p&gt;First, we load the libraries we’ll be using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)  # For ggplot, dplyr, and friends
library(patchwork)  # For combining ggplot plots
library(GGally)     # New one, for scatterplot matrices
library(broom)      # For converting model objects to data frames&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we load the data with &lt;code&gt;read_csv()&lt;/code&gt;. This is &lt;code&gt;tidyverse&lt;/code&gt;’s CSV reader (base &lt;code&gt;R&lt;/code&gt; is &lt;code&gt;read.csv&lt;/code&gt;). Here we assume that the CSV file was downloaded and lives in a subfolder named &lt;code&gt;data&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl &amp;lt;- read_csv(&amp;quot;data/atl-weather-2019.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplot-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scatterplot matrices&lt;/h3&gt;
&lt;p&gt;The foundation of linear regression is corrleation. We can visualize the correlations between pairs of variables with the &lt;code&gt;ggpairs()&lt;/code&gt; function in the &lt;strong&gt;GGally&lt;/strong&gt; package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into &lt;code&gt;ggpairs()&lt;/code&gt; to see all the correlation information:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GGally)

weather_correlations &amp;lt;- weather_atl %&amp;gt;%
  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability)

ggpairs(weather_correlations)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/ggpairs-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind speed and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).&lt;/p&gt;
&lt;p&gt;Even though &lt;code&gt;ggpairs()&lt;/code&gt; doesn’t use the standard &lt;code&gt;ggplot(...) + geom_whatever()&lt;/code&gt; syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpairs(weather_correlations) + 
  labs(title = &amp;quot;Correlations!&amp;quot;) +
  theme_dark()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/ggpairs-layers-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here’s some code to load data from the Residential Energy Consumption Survey (RECS), a &lt;a href=&#34;https://www.eia.gov/consumption/residential/data/2020/&#34;&gt;once-a-decade survey of household energy consumption done by the Energy Information Administration.&lt;/a&gt;. Use the code here to load the data. I’ve selected a few variables and renamed them to something intuitive. Each observation is a household surveyed by the EIA. The variable named &lt;code&gt;EnergyUsed&lt;/code&gt; is the total BTU’s of energy consumed by that household.&lt;/p&gt;
&lt;p&gt;Most of the variables are self-explanatory except the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;AgeOfHome&lt;/code&gt; is on a 1-9 scale where 1 is the oldest and 9 is the newest.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;HeaterAge&lt;/code&gt; is the age of the home’s heater from 1 to 6 where 6 is the oldest and 1 is the newest.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;TVSize&lt;/code&gt; is on a scale of 1 (smallest) to 4 (largest)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;TempHome&lt;/code&gt; tells us the home’s thermostat setting when home (in degrees farenheit)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your task is to make a ggpairs plot for &lt;code&gt;EnergyUsed&lt;/code&gt; and five of the variables in the RECS data. What variables do you think are correlated with total energy use &lt;code&gt;EnergyUsed&lt;/code&gt;? And &lt;strong&gt;what can we learn about energy consumption from these correlations?&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RECS = read.csv(&amp;#39;https://www.eia.gov/consumption/residential/data/2009/csv/recs2009_public.csv&amp;#39;, stringsAsFactors = F) %&amp;gt;%
  as_tibble() %&amp;gt;%  
  dplyr::select(EnergyUsed = TOTALBTU,
                ColdDays = HDD65, HotDays = CDD65, SquareFeet = TOTHSQFT, CarsGarage = SIZEOFGARAGE, 
                AgeOfHome = YEARMADERANGE, TreeShade = TREESHAD, TVSize = TVSIZE1, HeaterAge = EQUIPAGE, HasAC = AIRCOND,
                TempHome = TEMPHOME) %&amp;gt;%
  dplyr::filter(HeaterAge != -2 &amp;amp; TempHome !=-2)  # these are NAs&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;correlograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlograms&lt;/h3&gt;
&lt;p&gt;Scatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In the readings for today, Claus Wilke showed a type of plot called a &lt;a href=&#34;https://serialmentor.com/dataviz/visualizing-associations.html#associations-correlograms&#34;&gt;&lt;em&gt;correlogram&lt;/em&gt;&lt;/a&gt; which &lt;em&gt;is&lt;/em&gt; more appropriate for publication.&lt;/p&gt;
&lt;p&gt;These are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.&lt;/p&gt;
&lt;p&gt;First we need to build a correlation matrix of the main variables we care about. Ordinarily the &lt;code&gt;cor()&lt;/code&gt; function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into &lt;code&gt;cor()&lt;/code&gt; though, it’ll calculate the correlation between each pair of columns. &lt;span style=&#34;     color: red !important;&#34;&gt;But be careful - don’t feed in hundreds of variables by accident!&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a correlation matrix
things_to_correlate &amp;lt;- weather_atl %&amp;gt;%
  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %&amp;gt;%
  cor()

things_to_correlate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   temperatureHigh temperatureLow humidity windSpeed precipProbability
## temperatureHigh              1.00          0.920   -0.030    -0.377            -0.124
## temperatureLow               0.92          1.000    0.112    -0.450            -0.026
## humidity                    -0.03          0.112    1.000     0.011             0.722
## windSpeed                   -0.38         -0.450    0.011     1.000             0.196
## precipProbability           -0.12         -0.026    0.722     0.196             1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to &lt;code&gt;NA&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get rid of the lower triangle
things_to_correlate[lower.tri(things_to_correlate)] &amp;lt;- NA
things_to_correlate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   temperatureHigh temperatureLow humidity windSpeed precipProbability
## temperatureHigh                 1           0.92    -0.03    -0.377            -0.124
## temperatureLow                 NA           1.00     0.11    -0.450            -0.026
## humidity                       NA             NA     1.00     0.011             0.722
## windSpeed                      NA             NA       NA     1.000             0.196
## precipProbability              NA             NA       NA        NA             1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the &lt;code&gt;things_to_correlate&lt;/code&gt; matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named &lt;code&gt;measure1&lt;/code&gt;, and take all the correlation numbers and put them in a column named &lt;code&gt;cor&lt;/code&gt; In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;things_to_correlate_long &amp;lt;- things_to_correlate %&amp;gt;%
  # Convert from a matrix to a data frame
  as.data.frame() %&amp;gt;%
  # Matrixes have column names that don&amp;#39;t get converted to columns when using
  # as.data.frame(), so this adds those names as a column
  rownames_to_column(&amp;quot;measure2&amp;quot;) %&amp;gt;%
  # Make this long. Take all the columns except measure2 and put their names in
  # a column named measure1 and their values in a column named cor
  pivot_longer(cols = -measure2,
               names_to = &amp;quot;measure1&amp;quot;,
               values_to = &amp;quot;cor&amp;quot;) %&amp;gt;%
  # Make a new column with the rounded version of the correlation value
  mutate(nice_cor = round(cor, 2)) %&amp;gt;%
  # Remove rows where the two measures are the same (like the correlation
  # between humidity and humidity)
  filter(measure2 != measure1) %&amp;gt;%
  # Get rid of the empty triangle
  filter(!is.na(cor)) %&amp;gt;%
  # Put these categories in order
  mutate(measure1 = fct_inorder(measure1),
         measure2 = fct_inorder(measure2))

things_to_correlate_long&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 4
##    measure2        measure1              cor nice_cor
##    &amp;lt;fct&amp;gt;           &amp;lt;fct&amp;gt;               &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 temperatureHigh temperatureLow     0.920      0.92
##  2 temperatureHigh humidity          -0.0301    -0.03
##  3 temperatureHigh windSpeed         -0.377     -0.38
##  4 temperatureHigh precipProbability -0.124     -0.12
##  5 temperatureLow  humidity           0.112      0.11
##  6 temperatureLow  windSpeed         -0.450     -0.45
##  7 temperatureLow  precipProbability -0.0255    -0.03
##  8 humidity        windSpeed          0.0108     0.01
##  9 humidity        precipProbability  0.722      0.72
## 10 windSpeed       precipProbability  0.196      0.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Phew. With the data all tidied like that, we can make a correlogram with a heatmap. We have manipulated the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(things_to_correlate_long,
       aes(x = measure2, y = measure1, fill = cor)) +
  geom_tile() +
  geom_text(aes(label = nice_cor)) +
  scale_fill_gradient2(low = &amp;quot;#E16462&amp;quot;, mid = &amp;quot;white&amp;quot;, high = &amp;quot;#0D0887&amp;quot;,
                       limits = c(-1, 1)) +
  labs(x = NULL, y = NULL, fill = &amp;#39;Corr.&amp;#39;) +
  coord_equal() +
  theme_minimal() +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/cor-heatmap-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instead of using a heatmap, we can also use points, which encode the correlation information both as color &lt;em&gt;and&lt;/em&gt; as size. To do that, we just need to switch &lt;code&gt;geom_tile()&lt;/code&gt; to &lt;code&gt;geom_point()&lt;/code&gt; and set the &lt;code&gt;size = cor&lt;/code&gt; mapping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(things_to_correlate_long,
       aes(x = measure2, y = measure1, color = cor)) +
  # Size by the absolute value so that -0.7 and 0.7 are the same size
  geom_point(aes(size = abs(cor))) +
  scale_color_gradient2(low = &amp;quot;#E16462&amp;quot;, mid = &amp;quot;white&amp;quot;, high = &amp;quot;#0D0887&amp;quot;,
                        limits = c(-1, 1)) +
  scale_size_area(max_size = 15, limits = c(-1, 1), guide = &amp;#39;none&amp;#39;) +
  labs(x = NULL, y = NULL, color = &amp;#39;Corr.&amp;#39;) +
  coord_equal() +
  theme_minimal() +
  theme(panel.grid.minor = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/cor-points-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple regression&lt;/h2&gt;
&lt;p&gt;We finally get to this week’s material. Although correlation is nice, we eventually may want to visualize regression. The lecture has shown us some very intuitive, straightforward ways to think about regression (aka, a line). Simple regression is easy to visualize, since you’re only working with an &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and a &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. For instance, what’s the relationship between humidity and high temperatures during the summer?&lt;/p&gt;
&lt;p&gt;First, let’s filter the Atlanta weather data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_summer &amp;lt;- weather_atl %&amp;gt;%
  filter(time &amp;gt;= &amp;quot;2019-05-01&amp;quot;, time &amp;lt;= &amp;quot;2019-09-30&amp;quot;) %&amp;gt;%
  mutate(humidity_scaled = humidity * 100,
         moonPhase_scaled = moonPhase * 100,
         precipProbability_scaled = precipProbability * 100,
         cloudCover_scaled = cloudCover * 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can build a simple regression model for the high temperature “regressed on” humidity. First, we’ll use the formula from &lt;a href=&#34;content/05-content/#the-regression-line-1&#34;&gt;Content 05&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_results = weather_atl_summer %&amp;gt;%
  dplyr::summarize(mu_x = mean(humidity_scaled),
                   mu_y = mean(temperatureHigh),
                   sd_x = sd(humidity_scaled),
                   sd_y = sd(temperatureHigh),
                   rho = sum(((temperatureHigh-mu_y)/sd_y)*((humidity_scaled-mu_x)/sd_x))/(n()-1) ) %&amp;gt;%
                  # Note in rho we had to use that same &amp;quot;N-1&amp;quot; correction. cor() does this for us automatically
  dplyr::mutate(beta_1 = rho*(sd_y/sd_x),
                beta_0 = mu_y - beta_1*mu_x)

print(weather_atl_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 7
##    mu_x  mu_y  sd_x  sd_y    rho beta_1 beta_0
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1  64.8  88.5  10.6  5.33 -0.481 -0.241   104.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the formulas from &lt;a href=&#34;content/05-content/#the-regression-line-1&#34;&gt;Content 06&lt;/a&gt; have a direct translation into the code. Also note that we used &lt;code&gt;summarize&lt;/code&gt; on the data so that we get only &lt;strong&gt;one&lt;/strong&gt; row of data (no more 1,000+ rows, we’ve summarized them using &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sd&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We can interpret these coefficients like so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;beta_0&lt;/code&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;) is the intercept. By definition of an intercept, it shows that the average temperature when there’s &lt;strong&gt;0% humidity&lt;/strong&gt; is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;beta_1&lt;/code&gt; (or &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;) is the coefficient for &lt;code&gt;humidity_scaled&lt;/code&gt;. It shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This (and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;) are random variables – if we re-draw the sample, we’ll see a slightly different relationship. We’ll talk about the standard error of this in a little bit.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Visualizing this model is simple, since there are only two variables. We visualized the regression line in the Galton Heights data during Content 05, so let’s just use that code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl_summer, aes(x = humidity_scaled, y = temperatureHigh)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = weather_atl_results$beta_0, slope = weather_atl_results$beta_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/plot-simple-formula-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Every point along the regression line is the &lt;span class=&#34;math inline&#34;&gt;\(E[temperatureHigh|humidity scaled]\)&lt;/span&gt;. Put in whatever value of humidity you want, and the line tells you what the expectation of the high temperature will be.&lt;/p&gt;
&lt;p&gt;Another way we can add a regression line in a 2-D plot is using &lt;code&gt;geom_smooth&lt;/code&gt;. &lt;code&gt;geom_smooth&lt;/code&gt; is a &lt;code&gt;ggplot&lt;/code&gt; geometry that adds a line of best fit. This can be flexible, or this can be based on a linear relationship – the regression line. We’ll see how to use built-in functions in R to do regression later, but for now, we’ll just ask ggplot to make a &lt;strong&gt;l&lt;/strong&gt;inear &lt;strong&gt;m&lt;/strong&gt;odel (“lm”) for a line of best fit. The slope of that line is going to be exactly our estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl_summer,
       aes(x = humidity_scaled, y = temperatureHigh)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/plot-simple-model-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And indeed, we see the same line as we made “manually” before.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let’s use that &lt;code&gt;RECS&lt;/code&gt; data to regress &lt;code&gt;EnergyUsed&lt;/code&gt; on one of the variables from your scatterplot matrix. Note that we say the left-hand-side variable (the “Y”) is &lt;em&gt;regressed on&lt;/em&gt; the right-hand-side (the “X”) variable.&lt;/p&gt;
&lt;p&gt;Using the formulas and codes for regression, choose the variable you want to use in the regression, run it, and visualize the result.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression: Interpreting Coefficients</title>
      <link>https://ssc442.netlify.app/example/06-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/06-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preliminaries&#34; id=&#34;toc-preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dummy-variables&#34; id=&#34;toc-dummy-variables&#34;&gt;Dummy Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interactions&#34; id=&#34;toc-interactions&#34;&gt;Interactions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#factor-variables&#34; id=&#34;toc-factor-variables&#34;&gt;Factor Variables&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#factors-with-more-than-two-levels&#34; id=&#34;toc-factors-with-more-than-two-levels&#34;&gt;Factors with More Than Two Levels&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameterization&#34; id=&#34;toc-parameterization&#34;&gt;Parameterization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-larger-models&#34; id=&#34;toc-building-larger-models&#34;&gt;Building Larger Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will pivot between the content from this week and the example below.&lt;/p&gt;
&lt;p&gt;We will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;So far in each of our analyses, we have only used numeric variables as predictors. We have also only used &lt;em&gt;additive models&lt;/em&gt;, meaning the effect any predictor had on the response was not dependent on the other predictors. In this chapter, we will remove both of these restrictions. We will fit models with categorical predictors, and use models that allow predictors to &lt;em&gt;interact&lt;/em&gt;. The mathematics of multiple regression will remain largely unchanging, however, we will pay close attention to interpretation, as well as some difference in &lt;code&gt;R&lt;/code&gt; usage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dummy-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dummy Variables&lt;/h2&gt;
&lt;p&gt;For this example and discussion, we will briefly use the built in dataset &lt;code&gt;mtcars&lt;/code&gt; before introducing the &lt;code&gt;autompg&lt;/code&gt; dataset. The reason to use these easy, straightforward datasets is that they make visualization of the &lt;strong&gt;entire dataset&lt;/strong&gt; trivially easy. Accordingly, the &lt;code&gt;mtcars&lt;/code&gt; dataset is small, so we’ll quickly take a look at the entire dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.4     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.1
## ✔ ggplot2   3.4.4     ✔ tibble    3.2.1
## ✔ lubridate 1.9.3     ✔ tidyr     1.3.0
## ✔ purrr     1.0.2     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&amp;lt;http://conflicted.r-lib.org/&amp;gt;) to force all conflicts to become errors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be interested in three of the variables: &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;hp&lt;/code&gt;, and &lt;code&gt;am&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mpg&lt;/code&gt;: fuel efficiency, in miles per gallon.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hp&lt;/code&gt;: horsepower, in foot-pounds per second.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;am&lt;/code&gt;: transmission. Automatic or manual.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we often do, we will start by plotting the data. We are interested in &lt;code&gt;mpg&lt;/code&gt; as the response variable, and &lt;code&gt;hp&lt;/code&gt; as a predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, cex = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since we are also interested in the transmission type, we could also label the points accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Automatic&amp;quot;, &amp;quot;Manual&amp;quot;), col = c(1, 2), pch = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now fit the SLR model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;hp&lt;/code&gt;. For notational brevity, we drop the index &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; for observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp_slr = lm(mpg ~ hp, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then re-plot the data and add the fitted line to the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
abline(mpg_hp_slr, lwd = 3, col = &amp;quot;grey&amp;quot;)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Automatic&amp;quot;, &amp;quot;Manual&amp;quot;), col = c(1, 2), pch = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We should notice a pattern here. The red, manual observations largely fall above the line, while the black, automatic observations are mostly below the line. This means our model underestimates the fuel efficiency of manual transmissions, and overestimates the fuel efficiency of automatic transmissions. To correct for this, we will add a predictor to our model, namely, &lt;code&gt;am&lt;/code&gt; as &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our new model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; remain the same, but now&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_2 =
  \begin{cases}
   1 &amp;amp; \text{manual transmission} \\
   0       &amp;amp; \text{automatic transmission}
  \end{cases}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, we call &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; a &lt;strong&gt;dummy variable&lt;/strong&gt;. A dummy variable (also called an “indicator” variable) is somewhat unfortunately named, as it is in no way “dumb”. In fact, it is actually somewhat clever. A dummy variable is a numerical variable that is used in a regression analysis to “code” for a binary categorical variable. Let’s see how this works.&lt;/p&gt;
&lt;p&gt;First, note that &lt;code&gt;am&lt;/code&gt; is already a dummy variable, since it uses the values &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; to represent automatic and manual transmissions. Often, a variable like &lt;code&gt;am&lt;/code&gt; would store the character values &lt;code&gt;auto&lt;/code&gt; and &lt;code&gt;man&lt;/code&gt; and we would either have to convert these to &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, or, as we will see later, &lt;code&gt;R&lt;/code&gt; will take care of creating dummy variables for us.&lt;/p&gt;
&lt;p&gt;So, to fit the above model, we do so like any other multiple regression model we have seen before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp_add = lm(mpg ~ hp + am, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Briefly checking the output, we see that &lt;code&gt;R&lt;/code&gt; has estimated the three &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp_add&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ hp + am, data = mtcars)
## 
## Coefficients:
## (Intercept)           hp           am  
##    26.58491     -0.05889      5.27709&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; can only take values &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, we can effectively write two different models, one for manual and one for automatic transmissions.&lt;/p&gt;
&lt;p&gt;For automatic transmissions, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 0\)&lt;/span&gt;, we have,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then for manual transmissions, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 1\)&lt;/span&gt;, we have,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = (\beta_0 + \beta_2) + \beta_1 x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice that these models share the same slope, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, but have different intercepts, differing by &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt;. So the change in &lt;code&gt;mpg&lt;/code&gt; is the same for both models, but on average &lt;code&gt;mpg&lt;/code&gt; differs by &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; between the two transmission types.&lt;/p&gt;
&lt;p&gt;We’ll now calculate the estimated slope and intercept of these two models so that we can add them to a plot. Note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0\)&lt;/span&gt; = &lt;code&gt;coef(mpg_hp_add)[1]&lt;/code&gt; = 26.5849137&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt; = &lt;code&gt;coef(mpg_hp_add)[2]&lt;/code&gt; = -0.0588878&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_2\)&lt;/span&gt; = &lt;code&gt;coef(mpg_hp_add)[3]&lt;/code&gt; = 5.2770853&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can then combine these to calculate the estimated slope and intercepts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_auto = coef(mpg_hp_add)[1]
int_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3]

slope_auto = coef(mpg_hp_add)[2]
slope_manu = coef(mpg_hp_add)[2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Re-plotting the data, we use these slopes and intercepts to add the “two” fitted models to the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
abline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # add line for auto
abline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # add line for manual
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Automatic&amp;quot;, &amp;quot;Manual&amp;quot;), col = c(1, 2), pch = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We notice right away that the points are no longer systematically incorrect. The red, manual observations vary about the red line in no particular pattern without underestimating the observations as before. The black, automatic points vary about the black line, also without an obvious pattern.&lt;/p&gt;
&lt;p&gt;They say a picture is worth a thousand words, but as a statistician, sometimes a picture is worth an entire analysis. The above picture makes it plainly obvious that &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; is significant, but let’s verify mathematically. Essentially we would like to test:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_2 = 0 \quad \text{vs} \quad H_1: \beta_2 \neq 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is nothing new. Again, the math is the same as the multiple regression analyses we have seen before. We could perform either a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test here. The only difference is a slight change in interpretation. We could think of this as testing a model with a single line (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) against a model that allows two lines (&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;To obtain the test statistic and p-value for the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test, we would use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mpg_hp_add)$coefficients[&amp;quot;am&amp;quot;,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate   Std. Error      t value     Pr(&amp;gt;|t|) 
## 5.277085e+00 1.079541e+00 4.888270e+00 3.460318e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To do the same for the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test, we would use a call to &lt;code&gt;anova&lt;/code&gt;. One way of comparing the fit of models (which accounts for the fact that one model is more “flexible” than another) is to use &lt;code&gt;anova&lt;/code&gt;. The null hypothesis of the &lt;code&gt;anova&lt;/code&gt; is that the two models explain the same amount of variation in the dependent variable (&lt;code&gt;mpg&lt;/code&gt;). If we reject the F-test in the &lt;code&gt;anova&lt;/code&gt; (if the p-value is small), then the more flexible model is actually explaining more.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_hp_slr, mpg_hp_add)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ hp
## Model 2: mpg ~ hp + am
##   Res.Df    RSS Df Sum of Sq      F   Pr(&amp;gt;F)    
## 1     30 447.67                                 
## 2     29 245.44  1    202.24 23.895 3.46e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that these are indeed testing the same thing, as the p-values are exactly equal. (And the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test statistic is the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; test statistic squared.) This is only the case for different models that differ only by one additional variable.&lt;/p&gt;
&lt;p&gt;Recapping some interpretations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 = 26.5849137\)&lt;/span&gt; is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a car with an automatic transmission and &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;hp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 + \hat{\beta}_2 = 31.8619991\)&lt;/span&gt; is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a car with a manual transmission and &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;hp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_2 = 5.2770853\)&lt;/span&gt; is the estimated &lt;strong&gt;difference&lt;/strong&gt; in average &lt;code&gt;mpg&lt;/code&gt; for cars with manual transmissions as compared to those with automatic transmission, for &lt;strong&gt;any&lt;/strong&gt; &lt;code&gt;hp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 = -0.0588878\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in one &lt;code&gt;hp&lt;/code&gt;, for &lt;strong&gt;either&lt;/strong&gt; transmission types.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We should take special notice of those last two. In the model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we see &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the average change in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for an increase in &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;, &lt;em&gt;no matter&lt;/em&gt; the value of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. Also, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; is always the difference in the average of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for &lt;em&gt;any&lt;/em&gt; value of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;. These are two restrictions we won’t always want, so we need a way to specify a more flexible model.&lt;/p&gt;
&lt;p&gt;Here we restricted ourselves to a single numerical predictor &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and one dummy variable &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. However, the concept of a dummy variable can be used with larger multiple regression models. We only use a single numerical predictor here for ease of visualization since we can think of the “two lines” interpretation. But in general, we can think of a dummy variable as creating “two models,” one for each category of a binary categorical variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interactions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactions&lt;/h2&gt;
&lt;p&gt;To remove the “same slope” restriction, we will now discuss &lt;strong&gt;interactions&lt;/strong&gt;. To illustrate this concept, we will use the &lt;code&gt;autompg&lt;/code&gt; dataset with a few modifications.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read data frame from the web
autompg = read.table(
  &amp;quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&amp;quot;,
  quote = &amp;quot;\&amp;quot;&amp;quot;,
  comment.char = &amp;quot;&amp;quot;,
  stringsAsFactors = FALSE)
# give the dataframe headers
colnames(autompg) = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;acc&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;origin&amp;quot;, &amp;quot;name&amp;quot;)


autompg = autompg %&amp;gt;%
  filter(hp !=&amp;#39;?&amp;#39; &amp;amp; name !=&amp;#39;plymouth reliant&amp;#39;) %&amp;gt;% # the reliant causes some issues
  mutate(hp = as.numeric(hp),
         domestic = as.numeric(origin==1)) %&amp;gt;%
  filter(!cyl %in% c(5,3)) %&amp;gt;%
  mutate(cyl = as.factor(cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:	383 obs. of  10 variables:
##  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...
##  $ cyl     : Factor w/ 3 levels &amp;quot;4&amp;quot;,&amp;quot;6&amp;quot;,&amp;quot;8&amp;quot;: 3 3 3 3 3 3 3 3 3 3 ...
##  $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...
##  $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...
##  $ wt      : num  3504 3693 3436 3433 3449 ...
##  $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
##  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...
##  $ origin  : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ name    : chr  &amp;quot;chevrolet chevelle malibu&amp;quot; &amp;quot;buick skylark 320&amp;quot; &amp;quot;plymouth satellite&amp;quot; &amp;quot;amc rebel sst&amp;quot; ...
##  $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve removed cars with &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; cylinders , as well as created a new variable &lt;code&gt;domestic&lt;/code&gt; which indicates whether or not a car was built in the United States. Removing the &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; cylinders is simply for ease of demonstration later in the chapter and would not be done in practice. The new variable &lt;code&gt;domestic&lt;/code&gt; takes the value &lt;code&gt;1&lt;/code&gt; if the car was built in the United States, and &lt;code&gt;0&lt;/code&gt; otherwise, which we will refer to as “foreign.” (We are arbitrarily using the United States as the reference point here.) We have also made &lt;code&gt;cyl&lt;/code&gt; and &lt;code&gt;origin&lt;/code&gt; into factor variables, which we will discuss later.&lt;/p&gt;
&lt;p&gt;We’ll now be concerned with three variables: &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;disp&lt;/code&gt;, and &lt;code&gt;domestic&lt;/code&gt;. We will use &lt;code&gt;mpg&lt;/code&gt; as the response. We can fit a model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;domestic&lt;/code&gt; as described above, which is a dummy variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_2 =
  \begin{cases}
   1 &amp;amp; \text{Domestic} \\
   0 &amp;amp; \text{Foreign}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will fit this model, extract the slope and intercept for the “two lines,” plot the data and add the lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_add = lm(mpg ~ disp + as.factor(domestic), data = autompg)

int_for = coef(mpg_disp_add)[1]
int_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3]

slope_for = coef(mpg_disp_add)[2]
slope_dom = coef(mpg_disp_add)[2] #--&amp;gt; same slope!

ggplot(autompg, aes(x = disp, y = mpg, col = as.factor(domestic))) + 
  geom_point() +
  geom_abline(intercept = int_dom, 
              slope = slope_dom, col = &amp;#39;blue&amp;#39;) +
  geom_abline(intercept = int_for,
              slope = slope_for, col = &amp;#39;red&amp;#39;) +
  labs(color = &amp;#39;Origin&amp;#39;) +
  scale_color_manual(labels = c(&amp;#39;Foreign&amp;#39;,&amp;#39;Domestic&amp;#39;), values = c(&amp;#39;red&amp;#39;,&amp;#39;blue&amp;#39;)) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a model that allows for two &lt;em&gt;parallel&lt;/em&gt; lines, meaning the &lt;code&gt;mpg&lt;/code&gt; can be different on average between foreign and domestic cars of the same engine displacement, but the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in displacement is the same for both. We can see this model isn’t doing very well here. The red line fits the red points fairly well, but the black line isn’t doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes.&lt;/p&gt;
&lt;p&gt;Consider the following model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are the same as before, but we have added a new &lt;strong&gt;interaction&lt;/strong&gt; term &lt;span class=&#34;math inline&#34;&gt;\(x_1 x_2\)&lt;/span&gt; which multiplies &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, so we also have an additional &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This model essentially creates two slopes and two intercepts, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; being the difference in intercepts and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; being the difference in slopes. To see this, we will break down the model into the two “sub-models” for foreign and domestic cars.&lt;/p&gt;
&lt;p&gt;For foreign cars, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 0\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For domestic cars, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 1\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These two models have both different slopes and intercepts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a foreign car with &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt;, for &lt;strong&gt;foreign&lt;/strong&gt; cars.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_2\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a domestic car with &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1 + \beta_3\)&lt;/span&gt; is the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt;, for &lt;strong&gt;domestic&lt;/strong&gt; cars.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do we fit this model in &lt;code&gt;R&lt;/code&gt;? There are a number of ways.&lt;/p&gt;
&lt;p&gt;One method would be to simply create a new variable, then fit a model like any other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autompg$x3 = autompg$disp * autompg$domestic # THIS CODE NOT RUN!
do_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # THIS CODE NOT RUN!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should only do this as a last resort. We greatly prefer not to have to modify our data simply to fit a model. Instead, we can tell &lt;code&gt;R&lt;/code&gt; we would like to use the existing data with an interaction term, which it will create automatically when we use the &lt;code&gt;:&lt;/code&gt; operator.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative method, which will fit the exact same model as above would be to use the &lt;code&gt;*&lt;/code&gt; operator. This method automatically creates the interaction term, as well as any “lower order terms,” which in this case are the first order terms for &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;domestic&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can quickly verify that these are doing the same thing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(mpg_disp_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)          disp      domestic disp:domestic 
##    46.0548423    -0.1569239   -12.5754714     0.1025184&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(mpg_disp_int2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)          disp      domestic disp:domestic 
##    46.0548423    -0.1569239   -12.5754714     0.1025184&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that both the variables, and their coefficient estimates are indeed the same for both models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mpg_disp_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp + domestic + disp:domestic, data = autompg)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.8332  -2.8956  -0.8332   2.2828  18.7749 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)    46.05484    1.80582  25.504  &amp;lt; 2e-16 ***
## disp           -0.15692    0.01668  -9.407  &amp;lt; 2e-16 ***
## domestic      -12.57547    1.95644  -6.428 3.90e-10 ***
## disp:domestic   0.10252    0.01692   6.060 3.29e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 4.308 on 379 degrees of freedom
## Multiple R-squared:  0.7011,	Adjusted R-squared:  0.6987 
## F-statistic: 296.3 on 3 and 379 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that using &lt;code&gt;summary()&lt;/code&gt; gives the usual output for a multiple regression model. We pay close attention to the row for &lt;code&gt;disp:domestic&lt;/code&gt; which tests,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_3 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, testing for &lt;span class=&#34;math inline&#34;&gt;\(\beta_3 = 0\)&lt;/span&gt; is testing for two lines with parallel slopes versus two lines with possibly different slopes. The &lt;code&gt;disp:domestic&lt;/code&gt; line in the &lt;code&gt;summary()&lt;/code&gt; output uses a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test to perform the test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_for = coef(mpg_disp_int)[1]
int_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3]

slope_for = coef(mpg_disp_int)[2]
slope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we again calculate the slope and intercepts for the two lines for use in plotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(autompg, aes(x = disp, y = mpg, col = as.factor(domestic))) + 
  geom_point() +
  geom_abline(intercept = int_dom, 
              slope = slope_dom, col = &amp;#39;blue&amp;#39;) +
  geom_abline(intercept = int_for,
              slope = slope_for, col = &amp;#39;red&amp;#39;) +
  labs(color = &amp;#39;Origin&amp;#39;) +
  scale_color_manual(labels = c(&amp;#39;Foreign&amp;#39;,&amp;#39;Domestic&amp;#39;), values = c(&amp;#39;red&amp;#39;,&amp;#39;blue&amp;#39;)) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that these lines fit the data much better, which matches the result of our tests.&lt;/p&gt;
&lt;p&gt;So far we have only seen interaction between a categorical variable (&lt;code&gt;domestic&lt;/code&gt;) and a numerical variable (&lt;code&gt;disp&lt;/code&gt;). While this is easy to visualize, since it allows for different slopes for two lines, it is not the only type of interaction we can use in a model. We can also consider interactions between two numerical variables.&lt;/p&gt;
&lt;p&gt;Consider the model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;hp&lt;/code&gt;, the horsepower, in foot-pounds per second.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How does &lt;code&gt;mpg&lt;/code&gt; change based on &lt;code&gt;disp&lt;/code&gt; in this model? We can rearrange some terms to see how.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + (\beta_1 + \beta_3 x_2) x_1 + \beta_2 x_2 + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, for a one unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;), the mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;) increases &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 + \beta_3 x_2\)&lt;/span&gt;, which is a different value depending on the value of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; (&lt;code&gt;hp&lt;/code&gt;)!&lt;/p&gt;
&lt;p&gt;Since we’re now working in three dimensions, this model can’t be easily justified via visualizations like the previous example. Instead, we will have to rely on a test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg)
mpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg)
summary(mpg_disp_int_hp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * hp, data = autompg)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.7849  -2.3104  -0.5699   2.1453  17.9211 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  5.241e+01  1.523e+00   34.42   &amp;lt;2e-16 ***
## disp        -1.002e-01  6.638e-03  -15.09   &amp;lt;2e-16 ***
## hp          -2.198e-01  1.987e-02  -11.06   &amp;lt;2e-16 ***
## disp:hp      5.658e-04  5.165e-05   10.96   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.896 on 379 degrees of freedom
## Multiple R-squared:  0.7554,	Adjusted R-squared:  0.7535 
## F-statistic: 390.2 on 3 and 379 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;summary()&lt;/code&gt; we focus on the row for &lt;code&gt;disp:hp&lt;/code&gt; which tests,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_3 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, we see a very low p-value so we reject the null (additive model) in favor of the interaction model.
We can take a closer look at the coefficients of our fitted interaction model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(mpg_disp_int_hp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)          disp            hp       disp:hp 
## 52.4081997848 -0.1001737655 -0.2198199720  0.0005658269&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 = 52.4081998\)&lt;/span&gt; is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a car with 0 &lt;code&gt;disp&lt;/code&gt; and 0 &lt;code&gt;hp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 = -0.1001738\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in 1 &lt;code&gt;disp&lt;/code&gt;, &lt;strong&gt;for a car with 0 &lt;code&gt;hp&lt;/code&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_2 = -0.21982\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in 1 &lt;code&gt;hp&lt;/code&gt;, &lt;strong&gt;for a car with 0 &lt;code&gt;disp&lt;/code&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_3 = 5.658269\times 10^{-4}\)&lt;/span&gt; is an estimate of the modification to the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in &lt;code&gt;disp&lt;/code&gt;, for a car of a certain &lt;code&gt;hp&lt;/code&gt; (or vice versa).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That last coefficient needs further explanation. Recall the rearrangement we made earlier&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + (\beta_1 + \beta_3 x_2) x_1 + \beta_2 x_2 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, our estimate for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 + \beta_3 x_2\)&lt;/span&gt;, is &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 + \hat{\beta}_3 x_2\)&lt;/span&gt;, which in this case is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-0.1001738 + 5.658269\times 10^{-4} x_2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This says that, for an increase of one &lt;code&gt;disp&lt;/code&gt; we see an estimated change in average &lt;code&gt;mpg&lt;/code&gt; of &lt;span class=&#34;math inline&#34;&gt;\(-0.1001738 + 5.658269\times 10^{-4} x_2\)&lt;/span&gt;. So how &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;mpg&lt;/code&gt; are related, depends on the &lt;code&gt;hp&lt;/code&gt; of the car.&lt;/p&gt;
&lt;p&gt;So for a car with 50 &lt;code&gt;hp&lt;/code&gt;, the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-0.1001738 + 5.658269\times 10^{-4} \cdot 50 = -0.0718824
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And for a car with 350 &lt;code&gt;hp&lt;/code&gt;, the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-0.1001738 + 5.658269\times 10^{-4} \cdot 350 = 0.0978657
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice the sign changed!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factor-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Factor Variables&lt;/h2&gt;
&lt;p&gt;So far in this chapter, we have limited our use of categorical variables to binary categorical variables. Specifically, we have limited ourselves to dummy variables which take a value of &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt; and represent a categorical variable numerically.&lt;/p&gt;
&lt;p&gt;We will now discuss &lt;strong&gt;factor&lt;/strong&gt; variables, which is a special way that &lt;code&gt;R&lt;/code&gt; deals with categorical variables. With factor variables, a human user can simply think about the categories of a variable, and &lt;code&gt;R&lt;/code&gt; will take care of the necessary dummy variables without any 0/1 assignment being done by the user.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(autompg$domestic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Earlier when we used the &lt;code&gt;domestic&lt;/code&gt; variable, it was &lt;strong&gt;not&lt;/strong&gt; a factor variable. It was simply a numerical variable that only took two possible values, &lt;code&gt;1&lt;/code&gt; for domestic, and &lt;code&gt;0&lt;/code&gt; for foreign. Let’s create a new variable &lt;code&gt;origin&lt;/code&gt; that stores the same information, but in a different way. First, we create an empty (all-&lt;code&gt;NA&lt;/code&gt;) variable of type &lt;code&gt;character&lt;/code&gt;. Then, we update it. Yes, we could also do this with &lt;code&gt;ifelse&lt;/code&gt; or &lt;code&gt;case_when&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autompg$origin = as.character(NA)
autompg$origin[autompg$domestic == 1] = &amp;quot;domestic&amp;quot;
autompg$origin[autompg$domestic == 0] = &amp;quot;foreign&amp;quot;
head(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the &lt;code&gt;origin&lt;/code&gt; variable stores &lt;code&gt;&#34;domestic&#34;&lt;/code&gt; for domestic cars and &lt;code&gt;&#34;foreign&#34;&lt;/code&gt; for foreign cars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this is simply a vector of character values. A vector of car models is a character variable in &lt;code&gt;R&lt;/code&gt;. A vector of Vehicle Identification Numbers (VINs) is a character variable as well. But those don’t represent a short list of levels that might influence a response variable. We will want to &lt;strong&gt;coerce&lt;/strong&gt; this origin variable to be something more: a factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autompg$origin = as.factor(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now when we check the structure of the &lt;code&gt;autompg&lt;/code&gt; dataset, we see that &lt;code&gt;origin&lt;/code&gt; is a factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:	383 obs. of  10 variables:
##  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...
##  $ cyl     : Factor w/ 3 levels &amp;quot;4&amp;quot;,&amp;quot;6&amp;quot;,&amp;quot;8&amp;quot;: 3 3 3 3 3 3 3 3 3 3 ...
##  $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...
##  $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...
##  $ wt      : num  3504 3693 3436 3433 3449 ...
##  $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
##  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...
##  $ origin  : Factor w/ 2 levels &amp;quot;domestic&amp;quot;,&amp;quot;foreign&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ name    : chr  &amp;quot;chevrolet chevelle malibu&amp;quot; &amp;quot;buick skylark 320&amp;quot; &amp;quot;plymouth satellite&amp;quot; &amp;quot;amc rebel sst&amp;quot; ...
##  $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Factor variables have &lt;strong&gt;levels&lt;/strong&gt; which are the possible values (categories) that the variable may take, in this case foreign or domestic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;domestic&amp;quot; &amp;quot;foreign&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recall that previously we have fit the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;domestic&lt;/code&gt; a dummy variable where &lt;code&gt;1&lt;/code&gt; indicates a domestic car.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mod_dummy = lm(mpg ~ disp * domestic, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * domestic, data = autompg)
## 
## Coefficients:
##   (Intercept)           disp       domestic  disp:domestic  
##       46.0548        -0.1569       -12.5755         0.1025&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here we see that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 + \hat{\beta}_2 = 46.0548423 + -12.5754714 = 33.4793709
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a &lt;strong&gt;domestic&lt;/strong&gt; car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now let’s try to do the same, but using our new factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mod_factor = lm(mpg ~ disp * origin, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * origin, data = autompg)
## 
## Coefficients:
##        (Intercept)                disp       originforeign  disp:originforeign  
##           33.47937            -0.05441            12.57547            -0.10252&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that it doesn’t produce the same results. Right away we notice that the intercept is different, as is the the coefficient in front of &lt;code&gt;disp&lt;/code&gt;. We also notice that the remaining two coefficients are of the same magnitude as their respective counterparts using the domestic variable, but with a different sign. Why is this happening?&lt;/p&gt;
&lt;p&gt;It turns out, that by using a factor variable, &lt;code&gt;R&lt;/code&gt; is automatically creating a dummy variable for us. However, it is not the dummy variable that we had originally used ourselves.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is fitting the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; &lt;strong&gt;is a dummy variable created by &lt;code&gt;R&lt;/code&gt;.&lt;/strong&gt; It uses &lt;code&gt;1&lt;/code&gt; to represent a &lt;strong&gt;foreign car&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You may recall that we saw this in action in Assignment 05 when we used &lt;code&gt;model.matrix&lt;/code&gt; on &lt;code&gt;GarageType&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So now,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 = 33.4793709
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a &lt;strong&gt;domestic&lt;/strong&gt; car with 0 &lt;code&gt;disp&lt;/code&gt;, which is indeed the same as before.&lt;/p&gt;
&lt;p&gt;When &lt;code&gt;R&lt;/code&gt; created &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, the dummy variable, it used domestic cars as the &lt;strong&gt;reference&lt;/strong&gt; level, that is the default value of the factor variable. So when the dummy variable is &lt;code&gt;0&lt;/code&gt;, the model represents this reference level, which is domestic. (&lt;code&gt;R&lt;/code&gt; makes this choice because domestic comes before foreign alphabetically.)&lt;/p&gt;
&lt;p&gt;So the two models have different estimated coefficients, but due to the different model representations, they are actually the same model.&lt;/p&gt;
&lt;div id=&#34;factors-with-more-than-two-levels&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factors with More Than Two Levels&lt;/h3&gt;
&lt;p&gt;Let’s now consider a factor variable with more than two levels. In this dataset, &lt;code&gt;cyl&lt;/code&gt; is an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(autompg$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(autompg$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;4&amp;quot; &amp;quot;6&amp;quot; &amp;quot;8&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the &lt;code&gt;cyl&lt;/code&gt; variable has three possible levels: &lt;code&gt;4&lt;/code&gt;, &lt;code&gt;6&lt;/code&gt;, and &lt;code&gt;8&lt;/code&gt;. You may wonder, why not simply use &lt;code&gt;cyl&lt;/code&gt; as a numerical variable? You certainly could.&lt;/p&gt;
&lt;p&gt;However, that would force the difference in average &lt;code&gt;mpg&lt;/code&gt; between &lt;code&gt;4&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; cylinders to be the same as the difference in average mpg between &lt;code&gt;6&lt;/code&gt; and &lt;code&gt;8&lt;/code&gt; cylinders. That usually make senses for a continuous variable, but not for a discrete variable with so few possible values. In the case of this variable, there is no such thing as a 7-cylinder engine or a 6.23-cylinder engine in personal vehicles. For these reasons, we will simply consider &lt;code&gt;cyl&lt;/code&gt; to be categorical. This is a decision that will commonly need to be made with ordinal variables. Often, with a large number of categories, the decision to treat them as numerical variables is appropriate because, otherwise, a large number of dummy variables are then needed to represent these variables.&lt;/p&gt;
&lt;p&gt;Let’s define three dummy variables related to the &lt;code&gt;cyl&lt;/code&gt; factor variable.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v_1 =
  \begin{cases}
   1 &amp;amp; \text{4 cylinder} \\
   0       &amp;amp; \text{not 4 cylinder}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v_2 =
  \begin{cases}
   1 &amp;amp; \text{6 cylinder} \\
   0       &amp;amp; \text{not 6 cylinder}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v_3 =
  \begin{cases}
   1 &amp;amp; \text{8 cylinder} \\
   0       &amp;amp; \text{not 8 cylinder}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s fit an additive model in &lt;code&gt;R&lt;/code&gt;, using &lt;code&gt;mpg&lt;/code&gt; as the response, and &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;cyl&lt;/code&gt; as predictors. This should be a model that uses “three regression lines” to model &lt;code&gt;mpg&lt;/code&gt;, one for each of the possible &lt;code&gt;cyl&lt;/code&gt; levels. They will all have the same slope (since it is an additive model), but each will have its own intercept.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp + cyl, data = autompg)
## 
## Coefficients:
## (Intercept)         disp         cyl6         cyl8  
##    34.99929     -0.05217     -3.63325     -2.03603&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The question is, what is the model that &lt;code&gt;R&lt;/code&gt; has fit here? It has chosen to use the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_3\)&lt;/span&gt; are the dummy variables define above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Why doesn’t &lt;code&gt;R&lt;/code&gt; use &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;? Essentially because it doesn’t need to. To create three lines, it only needs two dummy variables since it is using a reference level, which in this case is a 4 cylinder car. The three “sub models” are then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_2) + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_3) + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that they all have the same slope. However, using the two dummy variables, we achieve the three intercepts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a 4 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_2\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a 6 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_3\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a 8 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So because 4 cylinder is the reference level, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is specific to 4 cylinders, but &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; are used to represent quantities relative to 4 cylinders.&lt;/p&gt;
&lt;p&gt;As we have done before, we can extract these intercepts and slopes for the three lines, and plot them accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_4cyl = coef(mpg_disp_add_cyl)[1]
int_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3]
int_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4]

slope_all_cyl = coef(mpg_disp_add_cyl)[2]

plot_colors = c(&amp;quot;Darkorange&amp;quot;, &amp;quot;Darkgrey&amp;quot;, &amp;quot;Dodgerblue&amp;quot;)
plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))
abline(int_4cyl, slope_all_cyl, col = plot_colors[1], lty = 1, lwd = 2)
abline(int_6cyl, slope_all_cyl, col = plot_colors[2], lty = 2, lwd = 2)
abline(int_8cyl, slope_all_cyl, col = plot_colors[3], lty = 3, lwd = 2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;4 Cylinder&amp;quot;, &amp;quot;6 Cylinder&amp;quot;, &amp;quot;8 Cylinder&amp;quot;),
       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On this plot, we have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: orange dots, solid orange line.&lt;/li&gt;
&lt;li&gt;6 Cylinder: grey dots, dashed grey line.&lt;/li&gt;
&lt;li&gt;8 Cylinder: blue dots, dotted blue line.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The odd result here is that we’re estimating that 8 cylinder cars have better fuel efficiency than 6 cylinder cars at &lt;strong&gt;any&lt;/strong&gt; displacement! The dotted blue line is always above the dashed grey line. That doesn’t seem right. Maybe for very large displacement engines that could be true, but that seems wrong for medium to low displacement.&lt;/p&gt;
&lt;p&gt;To attempt to fix this, we will try using an interaction model, that is, instead of simply three intercepts and one slope, we will allow for three slopes. Again, we’ll let &lt;code&gt;R&lt;/code&gt; take the wheel, (no pun intended) then figure out what model it has applied.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * cyl, data = autompg)
## 
## Coefficients:
## (Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  
##    43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# could also use mpg ~ disp + cyl + disp:cyl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; has again chosen to use 4 cylinder cars as the reference level, but this also now has an effect on the interaction terms. &lt;code&gt;R&lt;/code&gt; has fit the model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \gamma_2 x v_2 + \gamma_3 x v_3 + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’re using &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; like a &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter for simplicity, so that, for example &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_2\)&lt;/span&gt; are both associated with &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, the three “sub models” are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \epsilon\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_2) + (\beta_1 + \gamma_2) x + \epsilon\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_3) + (\beta_1 + \gamma_3) x + \epsilon\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interpreting some parameters and coefficients then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\beta_0 + \beta_2)\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; of a 6 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\hat{\beta}_1 + \hat{\gamma}_3) = -0.1306935 + 0.1081714 = -0.0225221\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt;, for an 8 cylinder car.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, as we have seen before &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; change the intercepts for 6 and 8 cylinder cars relative to the reference level of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; for 4 cylinder cars.&lt;/p&gt;
&lt;p&gt;Now, similarly &lt;span class=&#34;math inline&#34;&gt;\(\gamma_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_3\)&lt;/span&gt; change the slopes for 6 and 8 cylinder cars relative to the reference level of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; for 4 cylinder cars.&lt;/p&gt;
&lt;p&gt;Once again, we extract the coefficients and plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_4cyl = coef(mpg_disp_int_cyl)[1]
int_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3]
int_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4]

slope_4cyl = coef(mpg_disp_int_cyl)[2]
slope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5]
slope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6]

plot_colors = c(&amp;quot;Darkorange&amp;quot;, &amp;quot;Darkgrey&amp;quot;, &amp;quot;Dodgerblue&amp;quot;)
plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))
abline(int_4cyl, slope_4cyl, col = plot_colors[1], lty = 1, lwd = 2)
abline(int_6cyl, slope_6cyl, col = plot_colors[2], lty = 2, lwd = 2)
abline(int_8cyl, slope_8cyl, col = plot_colors[3], lty = 3, lwd = 2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;4 Cylinder&amp;quot;, &amp;quot;6 Cylinder&amp;quot;, &amp;quot;8 Cylinder&amp;quot;),
       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks much better! We can see that for medium displacement cars, 6 cylinder cars now perform better than 8 cylinder cars, which seems much more reasonable than before.&lt;/p&gt;
&lt;p&gt;To completely justify the interaction model (i.e., a unique slope for each &lt;code&gt;cyl&lt;/code&gt; level) compared to the additive model (single slope), we can perform an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test. Notice first, that there is no &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test that will be able to do this since the difference between the two models is not a single parameter.&lt;/p&gt;
&lt;p&gt;We will test,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \gamma_2 = \gamma_3 = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which represents the parallel regression lines we saw before,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, this is a difference of two parameters, thus no &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test will be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_disp_add_cyl, mpg_disp_int_cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + cyl
## Model 2: mpg ~ disp * cyl
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    379 7299.5                                  
## 2    377 6551.7  2    747.79 21.515 1.419e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, we see a very low p-value, and thus reject the null. We prefer the interaction model over the additive model.&lt;/p&gt;
&lt;p&gt;Recapping a bit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Null Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Number of parameters: &lt;span class=&#34;math inline&#34;&gt;\(q = 4\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Full Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \gamma_2 x v_2 + \gamma_3 x v_3 + \epsilon\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Number of parameters: &lt;span class=&#34;math inline&#34;&gt;\(p = 6\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(coef(mpg_disp_int_cyl)) - length(coef(mpg_disp_add_cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see there is a difference of two parameters, which is also displayed in the resulting ANOVA table from &lt;code&gt;R&lt;/code&gt;. Notice that the following two values also appear on the ANOVA table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(autompg) - length(coef(mpg_disp_int_cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 377&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(autompg) - length(coef(mpg_disp_add_cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 379&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;parameterization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameterization&lt;/h2&gt;
&lt;p&gt;So far we have been simply letting &lt;code&gt;R&lt;/code&gt; decide how to create the dummy variables, and thus &lt;code&gt;R&lt;/code&gt; has been deciding the parameterization of the models. To illustrate the ability to use alternative parameterizations, we will recreate the data, but directly creating the dummy variables ourselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_param_data = data.frame(
  y = autompg$mpg,
  x = autompg$disp,
  v1 = 1 * as.numeric(autompg$cyl == 4),
  v2 = 1 * as.numeric(autompg$cyl == 6),
  v3 = 1 * as.numeric(autompg$cyl == 8))

head(new_param_data, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     y   x v1 v2 v3
## 1  18 307  0  0  1
## 2  15 350  0  0  1
## 3  18 318  0  0  1
## 4  16 304  0  0  1
## 5  17 302  0  0  1
## 6  15 429  0  0  1
## 7  14 454  0  0  1
## 8  14 440  0  0  1
## 9  14 455  0  0  1
## 10 15 390  0  0  1
## 11 15 383  0  0  1
## 12 14 340  0  0  1
## 13 15 400  0  0  1
## 14 14 455  0  0  1
## 15 24 113  1  0  0
## 16 22 198  0  1  0
## 17 18 199  0  1  0
## 18 21 200  0  1  0
## 19 27  97  1  0  0
## 20 26  97  1  0  0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; is &lt;code&gt;mpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;v1&lt;/code&gt;, &lt;code&gt;v2&lt;/code&gt;, and &lt;code&gt;v3&lt;/code&gt; are dummy variables as defined above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First let’s try to fit an additive model using &lt;code&gt;x&lt;/code&gt; as well as the three dummy variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ x + v1 + v2 + v3, data = new_param_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + v1 + v2 + v3, data = new_param_data)
## 
## Coefficients:
## (Intercept)            x           v1           v2           v3  
##    32.96326     -0.05217      2.03603     -1.59722           NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is happening here? Notice that &lt;code&gt;R&lt;/code&gt; is essentially ignoring &lt;code&gt;v3&lt;/code&gt;, but why? Well, because &lt;code&gt;R&lt;/code&gt; uses an intercept, it cannot also use &lt;code&gt;v3&lt;/code&gt;. This is because&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{1} = v_1 + v_2 + v_3
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which means that &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{1}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(v_3\)&lt;/span&gt; are linearly dependent. This would make the &lt;span class=&#34;math inline&#34;&gt;\(X^\top X\)&lt;/span&gt; matrix singular, but we need to be able to invert it to solve the normal equations and obtain &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}.\)&lt;/span&gt; With the intercept, &lt;code&gt;v1&lt;/code&gt;, and &lt;code&gt;v2&lt;/code&gt;, &lt;code&gt;R&lt;/code&gt; can make the necessary “three intercepts”. So, in this case &lt;code&gt;v3&lt;/code&gt; is the reference level.&lt;/p&gt;
&lt;p&gt;If we remove the intercept, then we can directly obtain all “three intercepts” without a reference level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ 0 + x + v1 + v2 + v3, data = new_param_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ 0 + x + v1 + v2 + v3, data = new_param_data)
## 
## Coefficients:
##        x        v1        v2        v3  
## -0.05217  34.99929  31.36604  32.96326&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we are fitting the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \mu_1 v_1 + \mu_2 v_2 + \mu_3 v_3 + \beta x +\epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_1 + \beta x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_2 + \beta x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_3 + \beta x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We could also do something similar with the interaction model, and give each line an intercept and slope, without the need for a reference level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)
## 
## Coefficients:
##       v1        v2        v3      v1:x      v2:x      v3:x  
## 43.59052  30.39026  22.73346  -0.13069  -0.04770  -0.02252&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \mu_1 v_1 + \mu_2 v_2 + \mu_3 v_3 + \beta_1 x v_1 + \beta_2 x v_2 + \beta_3 x v_3 +\epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_1 + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_2 + \beta_2 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_3 + \beta_3 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the original data, we have (at least) three equivalent ways to specify the interaction model with &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(mpg ~ disp * cyl, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * cyl, data = autompg)
## 
## Coefficients:
## (Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  
##    43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ 0 + cyl + disp:cyl, data = autompg)
## 
## Coefficients:
##      cyl4       cyl6       cyl8  cyl4:disp  cyl6:disp  cyl8:disp  
##  43.59052   30.39026   22.73346   -0.13069   -0.04770   -0.02252&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ 0 + disp + cyl + disp:cyl, data = autompg)
## 
## Coefficients:
##      disp       cyl4       cyl6       cyl8  disp:cyl6  disp:cyl8  
##  -0.13069   43.59052   30.39026   22.73346    0.08299    0.10817&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They all fit the same model, importantly each using six parameters, but the coefficients mean slightly different things in each. However, once they are interpreted as slopes and intercepts for the “three lines” they will have the same result.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;?all.equal&lt;/code&gt; to learn about the &lt;code&gt;all.equal()&lt;/code&gt; function, and think about how the following code verifies that the residuals of the two models are the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)),
          fitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let’s try some interaction terms using our &lt;code&gt;Ames&lt;/code&gt; data from before (linked at the top of this page). For now, we’ll stick to interacting square footage (&lt;code&gt;GrLivArea&lt;/code&gt;) with dummy or factor variables. As usual, we’re trying to predict &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;What factor variables should we use? Here are some options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;CentralAir&lt;/code&gt; (binary, but stored as “Y”/“N”)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Neighborhood&lt;/code&gt; (25 different neighborhoods, stored as &lt;code&gt;chr&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;YearBuilt&lt;/code&gt; (not a factor, but &lt;code&gt;case_when&lt;/code&gt; can be used to make a few bins based on year)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OverallCond&lt;/code&gt; (subjective 1-9 scale, but stored as an integer – use &lt;code&gt;as.factor&lt;/code&gt;!)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;???&lt;/code&gt; Anything else interesting you see?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I’d like you to choose one of the above and run two models: one with &lt;code&gt;GrLivArea&lt;/code&gt; and your chosen factor variable, and one with &lt;code&gt;GrLivArea&lt;/code&gt;, your chosen factor variable, &lt;em&gt;and the interaction of the two&lt;/em&gt;. Once you have run these:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Be prepared to interpret the coefficients on the “baseline” coefficient on &lt;code&gt;GrLivArea&lt;/code&gt;, as well as one of the coefficients on the interaction.&lt;/li&gt;
&lt;li&gt;Test the two models using &lt;code&gt;anova(mod1, mod2)&lt;/code&gt; and tell us if your more flexible model is statistically better than the less flexible model.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-larger-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building Larger Models&lt;/h2&gt;
&lt;p&gt;Now that we have seen how to incorporate categorical predictors as well as interaction terms, we can start to build much larger, much more flexible models which can potentially fit data better.&lt;/p&gt;
&lt;p&gt;Let’s define a “big” model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;hp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt; is &lt;code&gt;domestic&lt;/code&gt;, which is a dummy variable we defined, where &lt;code&gt;1&lt;/code&gt; is a domestic vehicle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First thing to note here, we have included a new term &lt;span class=&#34;math inline&#34;&gt;\(x_1 x_2 x_3\)&lt;/span&gt; which is a three-way interaction. Interaction terms can be larger and larger, up to the number of predictors in the model.&lt;/p&gt;
&lt;p&gt;Since we are using the three-way interaction term, we also use all possible two-way interactions, as well as each of the first order (&lt;strong&gt;main effect&lt;/strong&gt;) terms. This is the concept of a &lt;strong&gt;hierarchy&lt;/strong&gt;. Any time a “higher-order” term is in a model, the related “lower-order” terms should also be included. Mathematically their inclusion or exclusion is sometimes irrelevant, but from an interpretation standpoint, it is best to follow the hierarchy rules.&lt;/p&gt;
&lt;p&gt;Let’s do some rearrangement to obtain a “coefficient” in front of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_2 x_2 + \beta_3 x_3 + \beta_6 x_2 x_3 + (\beta_1 + \beta_4 x_2 + \beta_5 x_3 + \beta_7 x_2 x_3)x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Specifically, the “coefficient” in front of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + \beta_5 x_3 + \beta_7 x_2 x_3).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s discuss this “coefficient” to help us understand the idea of the &lt;em&gt;flexibility&lt;/em&gt; of a model. Recall that,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the coefficient for a first order term,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_4\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_5\)&lt;/span&gt; are coefficients for two-way interactions,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_7\)&lt;/span&gt; is the coefficient for the three-way interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the two and three way interactions were not in the model, the whole “coefficient” would simply be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\beta_1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, no matter the values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; would determine the relationship between &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;With the addition of the two-way interactions, now the “coefficient” would be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + \beta_5 x_3).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, changing &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;) has a different effect on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;), depending on the values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Lastly, adding the three-way interaction gives the whole “coefficient”&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + \beta_5 x_3 + \beta_7 x_2 x_3)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is even more flexible. Now changing &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;) has a different effect on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;), depending on the values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;, but in a more flexible way which we can see with some more rearrangement. Now the “coefficient” in front of &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt; in this “coefficient” is dependent on &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + (\beta_5 + \beta_7 x_2) x_3)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is so flexible, it is becoming hard to interpret!&lt;/p&gt;
&lt;p&gt;Let’s fit this three-way interaction model in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;big_model = lm(mpg ~ disp * hp * domestic, data = autompg)
summary(big_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * hp * domestic, data = autompg)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.9410  -2.2147  -0.4008   1.9430  18.4094 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)       6.065e+01  6.600e+00   9.189  &amp;lt; 2e-16 ***
## disp             -1.416e-01  6.344e-02  -2.232   0.0262 *  
## hp               -3.545e-01  8.123e-02  -4.364 1.65e-05 ***
## domestic         -1.257e+01  7.064e+00  -1.780   0.0759 .  
## disp:hp           1.369e-03  6.727e-04   2.035   0.0426 *  
## disp:domestic     4.933e-02  6.400e-02   0.771   0.4414    
## hp:domestic       1.852e-01  8.709e-02   2.126   0.0342 *  
## disp:hp:domestic -9.163e-04  6.768e-04  -1.354   0.1766    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.88 on 375 degrees of freedom
## Multiple R-squared:   0.76,	Adjusted R-squared:  0.7556 
## F-statistic: 169.7 on 7 and 375 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do we actually need this large of a model? Let’s first test for the necessity of the three-way interaction term. That is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_7 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Null Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We fit the null model in &lt;code&gt;R&lt;/code&gt; as &lt;code&gt;two_way_int_mod&lt;/code&gt;. Again, we check to see if the big model is any more explanatory:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;two_way_int_mod = lm(mpg ~ disp * hp + disp * domestic + hp * domestic, data = autompg)
#two_way_int_mod = lm(mpg ~ (disp + hp + domestic) ^ 2, data = autompg)
anova(two_way_int_mod, big_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp * hp + disp * domestic + hp * domestic
## Model 2: mpg ~ disp * hp * domestic
##   Res.Df    RSS Df Sum of Sq      F Pr(&amp;gt;F)
## 1    376 5673.2                           
## 2    375 5645.6  1    27.599 1.8332 0.1766&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see the p-value is somewhat large, so we would fail to reject. We prefer the smaller, less flexible, null model, without the three-way interaction.&lt;/p&gt;
&lt;p&gt;A quick note here: the full model does still “fit better.” Notice that it has a smaller RMSE than the null model, which means the full model makes smaller (squared) errors on average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(resid(big_model) ^ 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14.74053&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(resid(two_way_int_mod) ^ 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14.81259&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, it is not much smaller. We could even say that, the difference is insignificant. This is an idea we will return to later in greater detail.&lt;/p&gt;
&lt;p&gt;Now that we have chosen the model without the three-way interaction, can we go further? Do we need the two-way interactions? Let’s test&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_4 = \beta_5 = \beta_6 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Remember we already chose &lt;span class=&#34;math inline&#34;&gt;\(\beta_7 = 0\)&lt;/span&gt;, so,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Null Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We fit the null model in &lt;code&gt;R&lt;/code&gt; as &lt;code&gt;additive_mod&lt;/code&gt;, then use &lt;code&gt;anova()&lt;/code&gt; to perform an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test as usual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;additive_mod = lm(mpg ~ disp + hp + domestic, data = autompg)
anova(additive_mod, two_way_int_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + hp + domestic
## Model 2: mpg ~ disp * hp + disp * domestic + hp * domestic
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    379 7369.7                                  
## 2    376 5673.2  3    1696.5 37.478 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the p-value is small, so we reject the null, and we prefer the full (alternative) model. Of the models we have considered, our final preference is for&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression: Model Selection</title>
      <link>https://ssc442.netlify.app/example/07-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/07-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-selection&#34; id=&#34;toc-model-selection&#34;&gt;Model Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assesing-model-accuracy&#34; id=&#34;toc-assesing-model-accuracy&#34;&gt;Assesing Model Accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-complexity&#34; id=&#34;toc-model-complexity&#34;&gt;Model Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#test-train-split&#34; id=&#34;toc-test-train-split&#34;&gt;Test-Train Split&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overfitting-in-action&#34; id=&#34;toc-overfitting-in-action&#34;&gt;Overfitting in action&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#avoiding-overfitting&#34; id=&#34;toc-avoiding-overfitting&#34;&gt;Avoiding overfitting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-flexibility-to-linear-models&#34; id=&#34;toc-adding-flexibility-to-linear-models&#34;&gt;Adding Flexibility to Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-a-model&#34; id=&#34;toc-choosing-a-model&#34;&gt;Choosing a Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;model-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Selection&lt;/h2&gt;
&lt;p&gt;Often when we are developing a linear regression model, part of our goal is to &lt;strong&gt;explain&lt;/strong&gt; a relationship. Now, we will ignore much of what we have learned and instead simply use regression as a tool to &lt;strong&gt;predict&lt;/strong&gt;. Instead of a model which explains relationships, we seek a model which minimizes errors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/regression.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First, note that a linear model is one of many methods used in regression.&lt;/p&gt;
&lt;p&gt;To discuss linear models in the context of prediction, we introduce the (very boring) &lt;code&gt;Advertising&lt;/code&gt; data that is discussed in the ISL text (see supplemental readings). It can be found at &lt;a href=&#34;https://www.statlearning.com/s/Advertising.csv&#34;&gt;https://www.statlearning.com/s/Advertising.csv&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Advertising &amp;lt;- read_csv(&amp;#39;https://www.statlearning.com/s/Advertising.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New names:
## Rows: 200 Columns: 5
## ── Column specification
## ──────────────────────────────────────────────────────── Delimiter: &amp;quot;,&amp;quot; dbl
## (5): ...1, TV, radio, newspaper, sales
## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ
## Specify the column types or set `show_col_types = FALSE` to quiet this message.
## • `` -&amp;gt; `...1`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Advertising = Advertising %&amp;gt;% 
  dplyr::select(TV, radio, newspaper, sales)
Advertising&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 200 × 4
##       TV radio newspaper sales
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 230.   37.8      69.2  22.1
##  2  44.5  39.3      45.1  10.4
##  3  17.2  45.9      69.3   9.3
##  4 152.   41.3      58.5  18.5
##  5 181.   10.8      58.4  12.9
##  6   8.7  48.9      75     7.2
##  7  57.5  32.8      23.5  11.8
##  8 120.   19.6      11.6  13.2
##  9   8.6   2.1       1     4.8
## 10 200.    2.6      21.2  10.6
## # ℹ 190 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a look at the relationship between &lt;code&gt;sales&lt;/code&gt; and the three modes of advertising using the &lt;code&gt;patchwork&lt;/code&gt; package (or any number of other ways)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)
tv = ggplot(Advertising, aes(x = TV, y = sales)) + geom_point() + theme_bw()
rad = ggplot(Advertising, aes(x = radio, y = sales)) + geom_point() + theme_bw()
news = ggplot(Advertising, aes(x = newspaper, y = sales)) + geom_point() + theme_bw()
tv + rad + news&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see a relationship between each of the advertising modes and sales, but we might want to know more. Specifically, we probably want to know something about the sales-maximizing combination of the three modes. For that, we’d need a good model of sales. But what does “good” mean?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assesing-model-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assesing Model Accuracy&lt;/h2&gt;
&lt;p&gt;There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}(\hat{f}, \text{Data}) = \sqrt{\frac{1}{n}\displaystyle\sum_{i = 1}^{n}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA (&lt;code&gt;summary(lm(...))&lt;/code&gt;) context, the denominator for MSE may not be &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For a linear model , the estimate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;, is given by the fitted regression line.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{y}({\bf{x}_i}) = \hat{f}({\bf{x}_i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can write an &lt;code&gt;R&lt;/code&gt; function that will be useful for performing this calculation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-complexity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Complexity&lt;/h2&gt;
&lt;p&gt;Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. What is a “nested model”? When you have “nested models”, then one model contains all of the &lt;em&gt;same&lt;/em&gt; terms that the other model has. If we have &lt;code&gt;TV&lt;/code&gt;, &lt;code&gt;Radio&lt;/code&gt;, &lt;code&gt;Newspaper&lt;/code&gt;, then we would have a “nested model” in this case:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model1 = lm(sales ~ TV + radio, data = Advertising)
model2 = lm(sales ~ TV + radio + newspaper, data = Advertising)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, &lt;code&gt;model1&lt;/code&gt; is nested in &lt;code&gt;model2&lt;/code&gt;. Here are &lt;strong&gt;non-nested models&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model1 = lm(sales ~ TV + radio, data = Advertising)
model2 = lm(sales ~ TV + newspaper, data = Advertising)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we add polynomial terms, we always add the lower-order terms as well. This will always make a nested model:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model1 = lm(sales ~ TV + radio, data = Advertising)
model2 = lm(sales ~ TV + TV^2 + radio + radio^2, data = Advertising)
model3 = lm(sales ~ TV + TV^2 + radio + radio^2 + TV:radio, data = Advertising)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Those are nested models – 1 and 2 use a subset of terms from 3, and 1 uses a subset of 2.&lt;/p&gt;
&lt;p&gt;When models are nested, then the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We write a simple &lt;code&gt;R&lt;/code&gt; function to extract this information from a model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_complexity = function(model) {
  length(coef(model)) - 1
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;test-train-split&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Test-Train Split&lt;/h2&gt;
&lt;p&gt;There is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts. It is essentially cheating! As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down, or in very specific cases, stay the same.&lt;/p&gt;
&lt;p&gt;This would suggest that to predict well, we should use the largest possible model! However, in reality we have hard fit to a specific dataset, but as soon as we see new data, a large model may in fact predict poorly. This is called &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;overfitting-in-action&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overfitting in action&lt;/h3&gt;
&lt;p&gt;Let’s take a quick look at why overfitting may harm us, despite the notion that we want to minimize RMSE. I’m going to take a 20-row subset of &lt;code&gt;Advertising&lt;/code&gt; and fit a 16th-degree polynomial. If you remember your mathematics training, an Nth degree polynomial has N-1 “inflection points”, which translates to fitting a curve with 15 inflections. That’s pretty flexible! R is smart and won’t let us fit a 20-degree polynomial, though. Here’s as close as we can get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;smallset = Advertising %&amp;gt;% slice(1:20)
flexible.lm = lm(sales ~ poly(TV, 16), smallset)
summary(flexible.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = sales ~ poly(TV, 16), data = smallset)
## 
## Residuals:
##          1          2          3          4          5          6          7 
##  6.804e-03 -1.362e-01 -8.586e-02  1.154e+00 -7.661e-01  9.626e-01  5.371e-01 
##          8          9         10         11         12         13         14 
##  1.509e-01 -9.328e-01 -7.052e+00 -1.591e+00 -2.264e-01  8.249e-02 -1.121e-01 
##         15         16         17         18         19         20 
##  3.317e+00  4.554e+00  7.361e-02 -1.741e-06  1.143e+00 -1.078e+00 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)     13.4950     1.2288  10.982  0.00162 **
## poly(TV, 16)1   20.4353     5.4954   3.719  0.03384 * 
## poly(TV, 16)2    1.6908     5.4954   0.308  0.77845   
## poly(TV, 16)3    2.4704     5.4954   0.450  0.68354   
## poly(TV, 16)4   -1.2439     5.4954  -0.226  0.83547   
## poly(TV, 16)5    0.9185     5.4954   0.167  0.87789   
## poly(TV, 16)6   -3.9055     5.4954  -0.711  0.52854   
## poly(TV, 16)7   -2.2700     5.4954  -0.413  0.70730   
## poly(TV, 16)8    0.2120     5.4954   0.039  0.97165   
## poly(TV, 16)9    1.4021     5.4954   0.255  0.81510   
## poly(TV, 16)10  -2.3341     5.4954  -0.425  0.69965   
## poly(TV, 16)11  -1.1539     5.4954  -0.210  0.84713   
## poly(TV, 16)12  -0.6833     5.4954  -0.124  0.90891   
## poly(TV, 16)13   1.2426     5.4954   0.226  0.83564   
## poly(TV, 16)14   1.5138     5.4954   0.275  0.80084   
## poly(TV, 16)15   2.0444     5.4954   0.372  0.73461   
## poly(TV, 16)16  -1.8923     5.4954  -0.344  0.75331   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 5.495 on 3 degrees of freedom
## Multiple R-squared:  0.8385,	Adjusted R-squared:  -0.02313 
## F-statistic: 0.9732 on 16 and 3 DF,  p-value: 0.5933&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks great - we are explaining a &lt;strong&gt;lot&lt;/strong&gt; of the variation in sales! Let’s plot this 16-degree polynomial:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot a curve using predict over a very fine set of values
plotseq = seq(from = min(smallset$TV), to = max(smallset$TV), length = 300)
predseq = predict(flexible.lm, newdata = data.frame(TV = plotseq))

plot(sales ~ TV, smallset, ylim = c(0, 100))
lines(y = predseq, x = plotseq)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and this model has complexity of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_complexity(flexible.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we don’t really believe that TV advertising of around 20 would result in almost 40 in sales. We certainly don’t trust that huge spike at 250, either! But how do we stop ourselves from overfitting?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;avoiding-overfitting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Avoiding overfitting&lt;/h3&gt;
&lt;p&gt;Frequently we will take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the &lt;strong&gt;training&lt;/strong&gt; data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the &lt;strong&gt;test&lt;/strong&gt; data. Test data should &lt;em&gt;never&lt;/em&gt; be used to train a model.&lt;/p&gt;
&lt;p&gt;Note that sometimes the terms &lt;em&gt;evaluation set&lt;/em&gt; and &lt;em&gt;test set&lt;/em&gt; are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.&lt;/p&gt;
&lt;p&gt;Here we use the &lt;code&gt;sample()&lt;/code&gt; function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the &lt;code&gt;set.seed()&lt;/code&gt; function to allow use to reproduce the same random split each time we perform this analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(90)
num_obs = nrow(Advertising)

train_index = sample(num_obs, size = trunc(0.50 * num_obs))
train_data = Advertising[train_index, ]
test_data = Advertising[-train_index, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will look at two measures that assess how well a model is predicting, the &lt;strong&gt;train RMSE&lt;/strong&gt; and the &lt;strong&gt;test RMSE&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_{\text{Train}} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Tr}}}\displaystyle\sum_{i \in \text{Train}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Tr}\)&lt;/span&gt; is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_{\text{Test}} = \text{RMSE}(\hat{f}, \text{Test Data}) = \sqrt{\frac{1}{n_{\text{Te}}}\displaystyle\sum_{i \in \text{Test}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Te}\)&lt;/span&gt; is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict &lt;strong&gt;in general&lt;/strong&gt;, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.&lt;/p&gt;
&lt;p&gt;We will start with the simplest possible linear model, that is, a model with no predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(sales ~ 1, data = train_data)
get_complexity(fit_0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
sqrt(mean((train_data$sales - predict(fit_0, train_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.224106&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
sqrt(mean((test_data$sales - predict(fit_0, test_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.211881&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
rmse(actual = train_data$sales, predicted = predict(fit_0, train_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.224106&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
rmse(actual = test_data$sales, predicted = predict(fit_0, test_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.211881&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = train_data, response = &amp;quot;sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.224106&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = test_data, response = &amp;quot;sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.211881&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-flexibility-to-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding Flexibility to Linear Models&lt;/h2&gt;
&lt;p&gt;Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_1 = lm(sales ~ ., data = train_data)
get_complexity(fit_1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_1, data = train_data, response = &amp;quot;sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.649346&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_1, data = test_data, response = &amp;quot;sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.741642&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_2 = lm(sales ~ radio * newspaper * TV, data = train_data)
get_complexity(fit_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_2, data = train_data, response = &amp;quot;sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9875248&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_2, data = test_data, response = &amp;quot;sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9014459&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_3 = lm(sales ~ radio * newspaper * TV + I(TV ^ 2), data = train_data)
get_complexity(fit_3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_3, data = train_data, response = &amp;quot;sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6685005&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_3, data = test_data, response = &amp;quot;sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6242491&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_4 = lm(sales ~ radio * newspaper * TV +
           I(TV ^ 2) + I(radio ^ 2) + I(newspaper ^ 2), data = train_data)
get_complexity(fit_4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_4, data = train_data, response = &amp;quot;sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6676294&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_4, data = test_data, response = &amp;quot;sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6232819&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_5 = lm(sales ~ radio * newspaper * TV +
           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2), data = train_data)
get_complexity(fit_5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_5, data = train_data, response = &amp;quot;sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6359041&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_5, data = test_data, response = &amp;quot;sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7809674&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing a Model&lt;/h2&gt;
&lt;p&gt;To better understand the relationship between train RMSE, test RMSE, and model complexity, we summarize our results, as the above is somewhat cluttered.&lt;/p&gt;
&lt;p&gt;First, we recap the models that we have fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_1 = lm(sales ~ ., data = train_data)
fit_2 = lm(sales ~ radio * newspaper * TV, data = train_data)
fit_3 = lm(sales ~ radio * newspaper * TV + I(TV ^ 2), data = train_data)
fit_4 = lm(sales ~ radio * newspaper * TV +
           I(TV ^ 2) + I(radio ^ 2) + I(newspaper ^ 2), data = train_data)
fit_5 = lm(sales ~ radio * newspaper * TV +
           I(TV ^ 2) * I(radio ^ 2) * I(newspaper ^ 2), data = train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we create a list of the models fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then obtain train RMSE, test RMSE, and model complexity for each.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_rmse = sapply(model_list, get_rmse, data = train_data, response = &amp;quot;sales&amp;quot;)
test_rmse = sapply(model_list, get_rmse, data = test_data, response = &amp;quot;sales&amp;quot;)
model_complexity = sapply(model_list, get_complexity)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model_complexity, train_rmse, type = &amp;quot;b&amp;quot;,
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02,
              max(c(train_rmse, test_rmse)) + 0.02),
     col = &amp;quot;dodgerblue&amp;quot;,
     xlab = &amp;quot;Model Size&amp;quot;,
     ylab = &amp;quot;RMSE&amp;quot;)
lines(model_complexity, test_rmse, type = &amp;quot;b&amp;quot;, col = &amp;quot;darkorange&amp;quot;)
legend(&amp;#39;topright&amp;#39;, legend = c(&amp;#39;train&amp;#39;,&amp;#39;test&amp;#39;), col = c(&amp;#39;dodgerblue&amp;#39;,&amp;#39;darkorange&amp;#39;), lty=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We also summarize the results as a table. &lt;code&gt;fit_1&lt;/code&gt; is the least flexible, and &lt;code&gt;fit_5&lt;/code&gt; is the most flexible. We see the Train RMSE decrease as flexibility increases. We see that the Test RMSE is smallest for &lt;code&gt;fit_3&lt;/code&gt;, thus is the model we believe will perform the best on future data not used to train the model. Note this may not be the best model, but it is the best model of the models we have seen in this example.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;26%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Train RMSE&lt;/th&gt;
&lt;th&gt;Test RMSE&lt;/th&gt;
&lt;th&gt;Predictors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;1.6493463&lt;/td&gt;
&lt;td&gt;1.7416416&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.9875248&lt;/td&gt;
&lt;td&gt;0.9014459&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.6685005&lt;/td&gt;
&lt;td&gt;0.6242491&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.6676294&lt;/td&gt;
&lt;td&gt;0.6232819&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.6359041&lt;/td&gt;
&lt;td&gt;0.7809674&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To summarize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Underfitting models:&lt;/strong&gt; In general &lt;em&gt;High&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE. Seen in &lt;code&gt;fit_1&lt;/code&gt; and &lt;code&gt;fit_2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overfitting models:&lt;/strong&gt; In general &lt;em&gt;Low&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE. Seen in &lt;code&gt;fit_4&lt;/code&gt; and &lt;code&gt;fit_5&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE. Then a model is underfitting if there exists a more complex model with lower Test RMSE.&lt;/p&gt;
&lt;p&gt;A number of notes on these results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The labels of under and overfitting are &lt;em&gt;relative&lt;/em&gt; to the best model we see, &lt;code&gt;fit_3&lt;/code&gt;. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.&lt;/li&gt;
&lt;li&gt;The train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. Here we see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later.&lt;/li&gt;
&lt;li&gt;Often we expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get lucky and this will not be true.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that &lt;strong&gt;predicted&lt;/strong&gt; well, and paid no attention to a model for &lt;strong&gt;explaination&lt;/strong&gt;. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonparametric Regression</title>
      <link>https://ssc442.netlify.app/example/08-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/08-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#not-used&#34; id=&#34;toc-not-used&#34;&gt;Not used&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#weekly-writing&#34; id=&#34;toc-weekly-writing&#34;&gt;Weekly Writing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;not-used&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Not used&lt;/h1&gt;
&lt;p&gt;We used the “content” tab for this; see Topic 9b.&lt;/p&gt;
&lt;p&gt;…but look, I’ve hidden some tasty little treats in here.&lt;/p&gt;
&lt;div id=&#34;weekly-writing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Weekly Writing&lt;/h3&gt;
&lt;p&gt;This week, your task is to write a short description (approximately one paragraph) that &lt;strong&gt;describes the workflow&lt;/strong&gt; of building training data, building a simple model, then refining that model using either forward selection or a shrinkage method. In short: imagine you were explaining this to your cousin who is a freshman at MSU. Give a basic primer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Illustrating Bias vs. Variance</title>
      <link>https://ssc442.netlify.app/example/code_aggregated-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/code_aggregated-2/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NN = 100   #----&amp;gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = 0.75   #-----&amp;gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.



library(ggplot2)
library(patchwork)



f = function(x) {
  x ^ 2
}



get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  data.frame(x, y)
}



set.seed(1)
sim_data = get_sim_data(f)



fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)





set.seed(42)
plot(y ~ x, data = sim_data, col = &amp;quot;grey&amp;quot;, pch = 20,
     main = &amp;quot;Four Polynomial Models fit to a Simulated Dataset&amp;quot;)

grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = &amp;quot;black&amp;quot;, lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = &amp;quot;dodgerblue&amp;quot;,  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &amp;quot;firebrick&amp;quot;,   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &amp;quot;springgreen&amp;quot;, lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = &amp;quot;darkorange&amp;quot;,  lwd = 2, lty = 5)

legend(&amp;quot;topleft&amp;quot;, 
       c(&amp;quot;y ~ 1&amp;quot;, &amp;quot;y ~ poly(x, 1)&amp;quot;, &amp;quot;y ~ poly(x, 2)&amp;quot;,  &amp;quot;y ~ poly(x, 9)&amp;quot;, &amp;quot;truth&amp;quot;), 
       col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;, &amp;quot;black&amp;quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)



for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}



predictions.proc = (predictions)
colnames(predictions.proc) = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;9&amp;quot;)
predictions.proc = as.data.frame(predictions.proc)

tall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)

## Here, you can save your ggplot output 
FinalPlot &amp;lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + 
                      geom_boxplot() +
                      geom_jitter(alpha = .5) +
                      geom_hline(yintercept = f(x = .90)) +
                      labs(col = &amp;#39;Model&amp;#39;, x = &amp;#39;Model&amp;#39;, y = &amp;#39;Prediction&amp;#39;, title = paste0(&amp;#39;Bias v Var - Sample Size: &amp;#39;,NN), subtitle = paste0(&amp;#39;SD of Bayes Err: &amp;#39;,SD.of.Bayes.Error)) +
                      theme_bw()



FinalPlot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This is going to aggregate your results for you:
if(!exists(&amp;#39;FinalResults&amp;#39;)) FinalResults = list()

FinalResults[[paste0(&amp;#39;finalPlot.NN.&amp;#39;,NN,&amp;#39;.SDBayes.&amp;#39;,SD.of.Bayes.Error)]] = FinalPlot

# 
# boxplot(value ~ key, data = tall_predictions, border = &amp;quot;darkgrey&amp;quot;, xlab = &amp;quot;Polynomial Degree&amp;quot;, ylab = &amp;quot;Predictions&amp;quot;,
#         main = &amp;quot;Simulated Predictions for Polynomial Models&amp;quot;)
# grid()
# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = &amp;quot;jitter&amp;quot;, jitter = 0.15, pch = 1, col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;))
# abline(h = f(x = 0.90), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This not run automatically.
## To plot the whole list of ggplot objects in FinalResults:
wrap_plots(FinalResults, nrow = 2, guides = &amp;#39;collect&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Illustrating Bias vs. Variance</title>
      <link>https://ssc442.netlify.app/example/code_aggregated-3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/code_aggregated-3/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NN = 100   #----&amp;gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = 0.75   #-----&amp;gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.



library(ggplot2)
library(patchwork)



f = function(x) {
  x ^ 2
}



get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  data.frame(x, y)
}



set.seed(1)
sim_data = get_sim_data(f)



fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)





set.seed(42)
plot(y ~ x, data = sim_data, col = &amp;quot;grey&amp;quot;, pch = 20,
     main = &amp;quot;Four Polynomial Models fit to a Simulated Dataset&amp;quot;)

grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = &amp;quot;black&amp;quot;, lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = &amp;quot;dodgerblue&amp;quot;,  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &amp;quot;firebrick&amp;quot;,   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &amp;quot;springgreen&amp;quot;, lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = &amp;quot;darkorange&amp;quot;,  lwd = 2, lty = 5)

legend(&amp;quot;topleft&amp;quot;, 
       c(&amp;quot;y ~ 1&amp;quot;, &amp;quot;y ~ poly(x, 1)&amp;quot;, &amp;quot;y ~ poly(x, 2)&amp;quot;,  &amp;quot;y ~ poly(x, 9)&amp;quot;, &amp;quot;truth&amp;quot;), 
       col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;, &amp;quot;black&amp;quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)



for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}



predictions.proc = (predictions)
colnames(predictions.proc) = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;9&amp;quot;)
predictions.proc = as.data.frame(predictions.proc)

tall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)

## Here, you can save your ggplot output 
FinalPlot &amp;lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + 
                      geom_boxplot() +
                      geom_jitter(alpha = .5) +
                      geom_hline(yintercept = f(x = .90)) +
                      labs(col = &amp;#39;Model&amp;#39;, x = &amp;#39;Model&amp;#39;, y = &amp;#39;Prediction&amp;#39;, title = paste0(&amp;#39;Bias v Var - Sample Size: &amp;#39;,NN), subtitle = paste0(&amp;#39;SD of Bayes Err: &amp;#39;,SD.of.Bayes.Error)) +
                      theme_bw()



FinalPlot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This is going to aggregate your results for you:
if(!exists(&amp;#39;FinalResults&amp;#39;)) FinalResults = list()

FinalResults[[paste0(&amp;#39;finalPlot.NN.&amp;#39;,NN,&amp;#39;.SDBayes.&amp;#39;,SD.of.Bayes.Error)]] = FinalPlot

# 
# boxplot(value ~ key, data = tall_predictions, border = &amp;quot;darkgrey&amp;quot;, xlab = &amp;quot;Polynomial Degree&amp;quot;, ylab = &amp;quot;Predictions&amp;quot;,
#         main = &amp;quot;Simulated Predictions for Polynomial Models&amp;quot;)
# grid()
# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = &amp;quot;jitter&amp;quot;, jitter = 0.15, pch = 1, col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;))
# abline(h = f(x = 0.90), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This not run automatically.
## To plot the whole list of ggplot objects in FinalResults:
wrap_plots(FinalResults, nrow = 2, guides = &amp;#39;collect&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Illustrating Bias vs. Variance</title>
      <link>https://ssc442.netlify.app/example/code_aggregated-4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/code_aggregated-4/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NN = 100   #----&amp;gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = 0.75   #-----&amp;gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.



library(ggplot2)
library(patchwork)



f = function(x) {
  x ^ 2
}



get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  data.frame(x, y)
}



set.seed(1)
sim_data = get_sim_data(f)



fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)





set.seed(42)
plot(y ~ x, data = sim_data, col = &amp;quot;grey&amp;quot;, pch = 20,
     main = &amp;quot;Four Polynomial Models fit to a Simulated Dataset&amp;quot;)

grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = &amp;quot;black&amp;quot;, lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = &amp;quot;dodgerblue&amp;quot;,  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &amp;quot;firebrick&amp;quot;,   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &amp;quot;springgreen&amp;quot;, lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = &amp;quot;darkorange&amp;quot;,  lwd = 2, lty = 5)

legend(&amp;quot;topleft&amp;quot;, 
       c(&amp;quot;y ~ 1&amp;quot;, &amp;quot;y ~ poly(x, 1)&amp;quot;, &amp;quot;y ~ poly(x, 2)&amp;quot;,  &amp;quot;y ~ poly(x, 9)&amp;quot;, &amp;quot;truth&amp;quot;), 
       col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;, &amp;quot;black&amp;quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)



for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}



predictions.proc = (predictions)
colnames(predictions.proc) = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;9&amp;quot;)
predictions.proc = as.data.frame(predictions.proc)

tall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)

## Here, you can save your ggplot output 
FinalPlot &amp;lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + 
                      geom_boxplot() +
                      geom_jitter(alpha = .5) +
                      geom_hline(yintercept = f(x = .90)) +
                      labs(col = &amp;#39;Model&amp;#39;, x = &amp;#39;Model&amp;#39;, y = &amp;#39;Prediction&amp;#39;, title = paste0(&amp;#39;Bias v Var - Sample Size: &amp;#39;,NN), subtitle = paste0(&amp;#39;SD of Bayes Err: &amp;#39;,SD.of.Bayes.Error)) +
                      theme_bw()



FinalPlot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This is going to aggregate your results for you:
if(!exists(&amp;#39;FinalResults&amp;#39;)) FinalResults = list()

FinalResults[[paste0(&amp;#39;finalPlot.NN.&amp;#39;,NN,&amp;#39;.SDBayes.&amp;#39;,SD.of.Bayes.Error)]] = FinalPlot

# 
# boxplot(value ~ key, data = tall_predictions, border = &amp;quot;darkgrey&amp;quot;, xlab = &amp;quot;Polynomial Degree&amp;quot;, ylab = &amp;quot;Predictions&amp;quot;,
#         main = &amp;quot;Simulated Predictions for Polynomial Models&amp;quot;)
# grid()
# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = &amp;quot;jitter&amp;quot;, jitter = 0.15, pch = 1, col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;))
# abline(h = f(x = 0.90), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This not run automatically.
## To plot the whole list of ggplot objects in FinalResults:
wrap_plots(FinalResults, nrow = 2, guides = &amp;#39;collect&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Illustrating Bias vs. Variance</title>
      <link>https://ssc442.netlify.app/example/code_aggregated/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/code_aggregated/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NN = 100   #----&amp;gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = 0.75   #-----&amp;gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.



library(ggplot2)
library(patchwork)



f = function(x) {
  x ^ 2
}



get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  data.frame(x, y)
}



set.seed(1)
sim_data = get_sim_data(f)



fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)





set.seed(42)
plot(y ~ x, data = sim_data, col = &amp;quot;grey&amp;quot;, pch = 20,
     main = &amp;quot;Four Polynomial Models fit to a Simulated Dataset&amp;quot;)

grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = &amp;quot;black&amp;quot;, lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = &amp;quot;dodgerblue&amp;quot;,  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &amp;quot;firebrick&amp;quot;,   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &amp;quot;springgreen&amp;quot;, lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = &amp;quot;darkorange&amp;quot;,  lwd = 2, lty = 5)

legend(&amp;quot;topleft&amp;quot;, 
       c(&amp;quot;y ~ 1&amp;quot;, &amp;quot;y ~ poly(x, 1)&amp;quot;, &amp;quot;y ~ poly(x, 2)&amp;quot;,  &amp;quot;y ~ poly(x, 9)&amp;quot;, &amp;quot;truth&amp;quot;), 
       col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;, &amp;quot;black&amp;quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)



for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}



predictions.proc = (predictions)
colnames(predictions.proc) = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;9&amp;quot;)
predictions.proc = as.data.frame(predictions.proc)

tall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)

## Here, you can save your ggplot output 
FinalPlot &amp;lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + 
                      geom_boxplot() +
                      geom_jitter(alpha = .5) +
                      geom_hline(yintercept = f(x = .90)) +
                      labs(col = &amp;#39;Model&amp;#39;, x = &amp;#39;Model&amp;#39;, y = &amp;#39;Prediction&amp;#39;, title = paste0(&amp;#39;Bias v Var - Sample Size: &amp;#39;,NN), subtitle = paste0(&amp;#39;SD of Bayes Err: &amp;#39;,SD.of.Bayes.Error)) +
                      theme_bw()



FinalPlot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This is going to aggregate your results for you:
if(!exists(&amp;#39;FinalResults&amp;#39;)) FinalResults = list()

FinalResults[[paste0(&amp;#39;finalPlot.NN.&amp;#39;,NN,&amp;#39;.SDBayes.&amp;#39;,SD.of.Bayes.Error)]] = FinalPlot

# 
# boxplot(value ~ key, data = tall_predictions, border = &amp;quot;darkgrey&amp;quot;, xlab = &amp;quot;Polynomial Degree&amp;quot;, ylab = &amp;quot;Predictions&amp;quot;,
#         main = &amp;quot;Simulated Predictions for Polynomial Models&amp;quot;)
# grid()
# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = &amp;quot;jitter&amp;quot;, jitter = 0.15, pch = 1, col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;))
# abline(h = f(x = 0.90), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This not run automatically.
## To plot the whole list of ggplot objects in FinalResults:
wrap_plots(FinalResults, nrow = 2, guides = &amp;#39;collect&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
